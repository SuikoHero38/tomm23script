Database,Item Type,Publication Year,Title,Venue,Venue Rank,Abstract,Keywords,Found In Group1,Found In Group2,Duplicate
IEEE,conferencePaper,2024,Understanding Parentsâ€™ Perceptions and Practices Toward Childrenâ€™s Security and Privacy in Virtual Reality,SP - IEEE Symposium on Security and Privacy,A*,"Recent years have seen a sharp increase in the number of underage users in virtual reality (VR), where security and privacy (S&P) risks such as data surveillance and self-disclosure in social interaction have been increasingly prominent. Prior work shows children largely rely on parents to mitigate S&P risks in their technology use. Therefore, understanding parentsâ€™ S&P knowledge, perceptions, and practices is critical for identifying the gaps for parents, technology designers, and policymakers to enhance childrenâ€™s S&P. While such empirical knowledge is substantial in other consumer technologies, it remains largely unknown in the context of VR. To address the gap, we conducted in-depth semi-structured interviews with 20 parents of children under the age of 18 who use VR at home. Our findings highlight parents generally lack S&P awareness due to the perception that VR is still in its infancy. To protect their childrenâ€™s interactions with VR, parents currently primarily rely on active strategies such as verbal education about S&P. Passive strategies such as using parental controls in VR are not commonly used among our interviewees, mainly due to their perceived technical constraints. Parents also highlight that a multi-stakeholder ecosystem must be established towards more S&P support for children in VR. Based on the findings, we propose actionable S&P recommendations for critical stakeholders, including parents, educators, VR companies, and governments.",Virtual Reality;Security and Privacy;Qualitative studies;Human-centered computing;Technology Use of Parents and Children,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2024,Eavesdropping on Controller Acoustic Emanation for Keystroke Inference Attack in Virtual Reality,NDSS - Usenix Network and Distributed System Security Symposium,A*,"Understanding the vulnerability of virtual reality (VR) is crucial for protecting sensitive data and building user trust in VR ecosystems. Previous attacks have demonstrated the feasibility of inferring VR keystrokes inside head-mounted displays (HMDs) by recording side-channel signals generated during user-HMD interactions. However, these attacks are heavily constrained by the physical layout or victim pose in the attack scenario since the recording device must be strictly positioned and oriented in a particular way with respect to the victim. In this paper, we unveil a placement-flexible keystroke inference attack in VR by eavesdropping the clicking sounds of the moving hand controller during keystrokes. The malicious recording smartphone can be placed anywhere surrounding the victim, making the attack more flexible and practical to deploy in VR environments. As the first acoustic attack in VR, our system, Heimdall, overcomes unique challenges unaddressed by previous acoustic attacks on physical keyboards and touchscreens. These challenges include differentiating sounds in a 3D space, adaptive mapping between keystroke sound and key in varying recording placement, and handling occasional hand rotations. Experiments with 30 participants show that Heimdall achieves key inference accuracy of 96.51% and top-5 accuracy of 85.14%–91.22% for inferring passwords with 4–8 characters. Heimdall is also robust under various practical impacts such as smartphone-user placement, attack environments, hardware models, and victim conditions.",,Title_Abstract,True,
Scopus,conferencePaper,2024,That Doesn’t Go There: Attacks on Shared State in Multi-User Augmented Reality Applications,USS - Usenix Security Symposium,A*,"Augmented Reality (AR) can enable shared virtual experiences between multiple users. In order to do so, it is crucial for multi-user AR applications to establish a consensus on the “shared state” of the virtual world and its augmentations through which users interact. Current methods to create and access shared state collect sensor data from devices (e.g., camera images), process them, and integrate them into the shared state. However, this process introduces new vulnerabilities and opportunities for attacks. Maliciously writing false data to “poison” the shared state is a major concern for the security of the downstream victims that depend on it. Another type of vulnerability arises when reading the shared state: by providing false inputs, an attacker can view hologram augmentations at locations they are not allowed to access. In this work, we demonstrate a series of novel attacks on multiple AR frameworks with shared states, focusing on three publicly accessible frameworks. We show that these frameworks, while using different underlying implementations, scopes, and mechanisms to read from and write to the shared state, have shared vulnerability to a unified threat model. Our evaluations of these state-of-the-art AR frameworks demonstrate reliable attacks both on updating and accessing the shared state across different systems. To defend against such threats, we discuss a number of potential mitigation strategies that can help enhance the security of multi-user AR applications and implement an initial prototype.",,Title_Abstract,True,
Scopus,conferencePaper,2024,Penetration Vision through Virtual Reality Headsets: Identifying 360-degree Videos from Head Movements,USS - Usenix Security Symposium,A*,"In this paper, we present the first contactless side-channel attack for identifying 360◦ videos being viewed in a Virtual Reality (VR) Head Mounted Display (HMD). Although the video content is displayed inside the HMD without any external exposure, we observe that user head movements are driven by the video content, which creates a unique side channel that does not exist in traditional 2D videos. By recording the user whose vision is blocked by the HMD via a malicious camera, an attacker can analyze the correlation between the user’s head movements and the victim video to infer the video title. To exploit this new vulnerability, we present INTRUDE, a system for identifying 360◦ videos from recordings of user head movements. INTRUDE is empowered by an HMD-based head movement estimation scheme to extract a head movement trace from the recording and a video saliency-based trace-fingerprint matching framework to infer the video title. Evaluation results show that INTRUDE achieves over 96% of accuracy for video identification and is robust under different recording environments. Moreover, INTRUDE maintains its effectiveness in the open-world identification scenario.",,Title_Abstract,True,
Scopus,conferencePaper,2024,"Enabling Developers, Protecting Users: Investigating Harassment and Safety in VR",USS - Usenix Security Symposium,A*,"Virtual Reality (VR) has witnessed a rising issue of harassment, prompting the integration of safety controls like muting and blocking in VR applications. However, the lack of standardized safety measures across VR applications hinders their universal effectiveness, especially across contexts like socializing, gaming, and streaming. While prior research has studied safety controls in social VR applications, our user study (n = 27) takes a multi-perspective approach, examining both users’ perceptions of safety control usability and effectiveness as well as the challenges that developers face in designing and deploying VR safety controls. We identify challenges VR users face while employing safety controls, such as finding users in crowded virtual spaces to block them. VR users also find controls ineffective in addressing harassment; for instance, they fail to eliminate the harassers’ presence from the environment. Further, VR users find the current methods of submitting evidence for reports time-consuming and cumbersome. Improvements desired by users include live moderation and behavior tracking across VR apps; however, developers cite technological, financial, and legal obstacles to implementing such solutions, often due to a lack of awareness and high development costs. We emphasize the importance of establishing technical and legal guidelines to enhance user safety in virtual environments.",,Abstract,True,
Scopus,conferencePaper,2024,Can Virtual Reality Protect Users from Keystroke Inference Attacks?,USS - Usenix Security Symposium,A*,"Virtual Reality (VR) has gained popularity by providing immersive and interactive experiences without geographical limitations. It also provides a sense of personal privacy through physical separation. In this paper, we show that despite assumptions of enhanced privacy, VR is unable to shield its users from side-channel attacks that steal private information. Ironically, this vulnerability arises from VR’s greatest strength, its immersive and interactive nature. We demonstrate this by designing and implementing a new set of keystroke inference attacks in shared virtual environments, where an attacker (VR user) can recover the content typed by another VR user by observing their avatar. While the avatar displays noisy telemetry of the user’s hand motion, an intelligent attacker can use that data to recognize typed keys and reconstruct typed content, without knowing the keyboard layout or gathering labeled data. We evaluate the proposed attacks using IRB-approved user studies across multiple VR scenarios. For 13 out of 15 tested users, our attacks accurately recognize 86%-98% of typed keys, and the recovered content retains up to 98% of the meaning of the original typed content. We also discuss potential defenses.",,Title_Abstract,True,
Scopus,conferencePaper,2024,When the User Is Inside the User Interface: An Empirical Study of UI Security Properties in Augmented Reality,USS - Usenix Security Symposium,A*,"Augmented reality (AR) experiences place users inside the user interface (UI), where they can see and interact with three-dimensional virtual content. This paper explores UI security for AR platforms, for which we identify three UI security-related properties: Same Space (how does the platform handle virtual content placed at the same coordinates?), Invisibility (how does the platform handle invisible virtual content?), and Synthetic Input (how does the platform handle simulated user input?). We demonstrate the security implications of different instantiations of these properties through five proof-of-concept attacks between distrusting AR application components (i.e., a main app and an included library) — including a clickjacking attack and an object erasure attack. We then empirically investigate these UI security properties on five current AR platforms: ARCore (Google), ARKit (Apple), Hololens (Microsoft), Oculus (Meta), and WebXR (browser). We find that all platforms enable at least three of our proofof-concept attacks to succeed. We discuss potential future defenses, including applying lessons from 2D UI security and identifying new directions for AR UI security.",,Title_Abstract,True,
ACM DL,conferencePaper,2024,OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs,CHI - Human Factors in Computing Systems,A*,"The progression to “Pervasive Augmented Reality” envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users’ context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.",dataset; diary study; digital follow-up actions; large language models; pervasive augmented reality; predictive interface,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults to Explore and Learn Smartphone Applications,CHI - Human Factors in Computing Systems,A*,"The global aging trend compels older adults to navigate the evolving digital landscape, presenting a substantial challenge in mastering smartphone applications. While Augmented Reality (AR) holds promise for enhancing learning and user experience, its role in aiding older adults’ smartphone app exploration remains insufficiently explored. Therefore, we conducted a two-phase study: (1) a workshop with 18 older adults to identify app exploration challenges and potential AR interventions, and (2) tech-probe participatory design sessions with 15 participants to co-create AR support tools. Our research highlights AR’s effectiveness in reducing physical and cognitive strain among older adults during app exploration, especially during multi-app usage and the trial-and-error learning process. We also examined their interactional experiences with AR, yielding design considerations on tailoring AR tools for smartphone app exploration. Ultimately, our study unveils the prospective landscape of AR in supporting the older demographic, both presently and in future scenarios.",augmented reality; independent learning; older adults; smartphone exploration,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effect on Target Selection and Manipulation Tasks in VR,CHI - Human Factors in Computing Systems,A*,"Recent studies show the promise of VR in improving physical, cognitive, and emotional health of older adults. However, prior work on optimizing object selection and manipulation performance in VR was mostly conducted among younger adults. It remains unclear how older adults would perform such tasks compared to younger adults and the challenges they might face. To fill in this gap, we conducted two studies with both older and younger adults to understand their performances and user experiences of object selection and manipulation in VR respectively. Based on the results, we delineated interaction difficulties that older adults exhibited in VR and identified multiple factors, such as headset-related neck fatigue, extra head movements from out-of-view interactions, and slow spatial perceptions, that significantly decreased the motor performance of older adults. We further proposed design recommendations for improving the accessibility of direct interaction experiences in VR for older adults.",Empirical study that tells us about people; Lab Study; Older Adults; Virtual/Augmented Reality,Title_Keywords,True,
ACM DL,conferencePaper,2024,"Communication, Collaboration, and Coordination in a Co-located Shared Augmented Reality Game: Perspectives From Deaf and Hard of Hearing People",CHI - Human Factors in Computing Systems,A*,"Co-located collaborative shared augmented reality (CS-AR) environments have gained considerable research attention, mainly focusing on design, implementation, accuracy, and usability. Yet, a gap persists in our understanding regarding the accessibility and inclusivity of such environments for diverse user groups, such as deaf and Hard of Hearing (DHH) people. To investigate this domain, we used Urban Legends, a multiplayer game in a co-located CS-AR setting. We conducted a user study followed by one-on-one interviews with 17 DHH participants. Our findings revealed the usage of multimodal communication (verbal and non-verbal) before and during the game, impacting the amount of collaboration among participants and how their coordination with AR components, their surroundings, and other participants improved throughout the rounds. We utilize our data to propose design enhancements, including onscreen visuals and speech-to-text transcription, centered on participant perspectives and our analysis.",Co-located AR; Collaborative AR; Deaf and Hard of Hearing; Shared AR,Title_Abstract,True,
ACM DL,conferencePaper,2024,"FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search Support",CHI - Human Factors in Computing Systems,A*,"Parcel lockers have become an increasingly prevalent last-mile delivery method. Yet, a recent study revealed its accessibility challenges to blind and low-vision people (BLV). Informed by the study, we designed FetchAid, a standalone intelligent mobile app assisting BLV in using a parcel locker in real-time by integrating computer vision and augmented reality (AR) technologies. FetchAid first uses a deep network to detect the user’s fingertip and relevant buttons on the touch screen of the parcel locker to guide the user to reveal and scan the QR code to open the target compartment door and then guide the user to reach the door safely with AR-based context-aware audio feedback. Moreover, FetchAid provides an error-recovery mechanism and real-time feedback to keep the user on track. We show that FetchAid substantially improved task accomplishment and efficiency, and reduced frustration and overall effort in a study with 12 BLV participants, regardless of their vision conditions and previous experience.",Accessibility; Assistive technology; Augmented reality; Blind and low vision; Computer vision; KuaiDiGui; Mobile devices; Object detection; Package delivery; People with vision impairments,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Barriers to Photosensitive Accessibility in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) systems have grown in popularity as an immersive modality for daily activities such as gaming, socializing, and working. However, this technology is not always accessible for people with photosensitive epilepsy (PSE) who may experience seizures or other adverse symptoms when exposed to certain light stimuli (e.g., flashes or strobes). How can VR be made more inclusive and safer for people with PSE? In this paper, we report on a series of semi-structured interviews about current perceptions of accessibility in VR among people with PSE. We identify 12 barriers to accessibility that fall into four categories: physical VR equipment, VR interfaces and content, specific VR applications, and individual differences in sensitivity. Our findings allow researchers and practitioners to better understand the meaning of photosensitive accessibility in the context of VR, and provide a step towards enabling people with PSE to enjoy the benefits offered by immersive technology.",accessibility; photosensitive epilepsy; virtual reality,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR,CHI - Human Factors in Computing Systems,A*,"Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR. Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space. However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform. We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures. Participants predominantly favored creating gestures with their hands. The type of tasks and participants’ abilities influence their choice of body parts for gesture design. Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing. Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR.",people with spinal muscular atrophy; upper-body gestures; user-defined gestures; virtual reality,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,People with Disabilities Redefining Identity through Robotic and Virtual Avatars: A Case Study in Avatar Robot Cafe,CHI - Human Factors in Computing Systems,A*,"Robotic avatars and telepresence technology enable people with disabilities to engage in physical work. Despite the recent popularity of the metaverse, few studies have explored the use of virtual avatars and environments by people with disabilities. In this study, seven disabled participants working in a cafe where remote customer service is provided via robotic avatars, were engaged in the development and use of personalized virtual avatars displayed on a large screen in-situ in combination with existing physical robots, creating a hybrid cyber-physical space. We conducted longitudinal semi-structured interviews to investigate the psychological changes experienced by the participants. The results revealed that mass-produced robotic avatars allowed participants to not disclose their disability if they did not want to, but also backgrounded their identities; by contrast, customized virtual avatars shaped without physical constraints, highlighted their personalities. The combined use of robotic and virtual avatars complemented each other and can support pilots in redefining their identity.",Avatar; people with disabilities; remote collaboration; remote customer survice,Abstract,True,
ACM DL,conferencePaper,2024,Examining the Use of VR as a Study Aid for University Students with ADHD,CHI - Human Factors in Computing Systems,A*,"Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental condition characterized by patterns of inattention and impulsivity, which lead to difficulties maintaining concentration and motivation while completing academic tasks. University settings, characterized by a high student-to-staff ratio, make treatments relying on human monitoring challenging. One potential replacement is Virtual Reality (VR) technology, which has shown potential to enhance learning outcomes and promote flow experience. In this study, we investigate the usage of VR with 27 university students with ADHD in an effort to improve their performance in completing homework, including an exploration of automated feedback via a technology probe. Quantitative results show significant increases in concentration, motivation, and effort levels during these VR sessions and qualitative data offers insight into considerations like comfort and deployment. Together, the results suggest that VR can be a valuable tool in leveling the playing field for university students with ADHD.",,Abstract,True,
ACM DL,conferencePaper,2024,From Letterboards to Holograms: Advancing Assistive Technology for Nonspeaking Autistic Individuals with the HoloBoard,CHI - Human Factors in Computing Systems,A*,"About one-third of autistic individuals are nonspeaking, i.e., they cannot use speech to convey their thoughts reliably. Many in this population communicate via spelling, a process in which they point to letters on a letterboard held upright in their field of view by a trained Communication and Regulation Partner (CRP). This paper focuses on transitioning such individuals to more independent, digital spelling that requires less support from the CRP, a goal most nonspeakers we consulted with desire. To enable this transition, we followed an approach that mimics an environment familiar to the nonspeaker and that harnesses the skills they already possess from physical letterboard training. Using this approach, we developed HoloBoard, a system that allows a nonspeaker, their CRP, and others, e.g., researchers, to share a common Augmented Reality (AR) environment containing a virtual letterboard. We configured the system to offer a brief (less than 10 minutes, on average) training module with graduated spelling tasks on the virtual letterboard. In a study involving 23 participants, 16 completed the entire module. These participants were able to spell words on the virtual letterboard without the CRP holding that board, an outcome we had not expected. When offered the opportunity to continue interacting with the virtual letterboard after the training module, 14 performed more complicated tasks than we had anticipated, spelling full sentences, or even offering feedback on the HoloBoard using solely the virtual board. Furthermore, five of these participants used the system solo, i.e., with the CRP and researchers absent from the virtual environment. These results suggest that training with the HoloBoard can lay the foundation for more independent communication, providing new social and educational opportunities for this marginalized population.",accessibility; assistive technology; Cross-reality; extended reality; nonspeaking autistic people,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Augmented Reality Cues Facilitate Task Resumption after Interruptions in Computer-Based and Physical Tasks,CHI - Human Factors in Computing Systems,A*,"Many work domains include numerous interruptions, which can contribute to errors. We investigated the potential of augmented reality (AR) cues to facilitate primary task resumption after interruptions of varying lengths. Experiment 1 (N&nbsp;=&nbsp;83) involved a computer-based primary task with a red AR arrow at the to-be-resumed task step which was placed via a gesture by the participants or automatically. Compared to no cue, both cues significantly reduced the resumption lag (i.e., the time between the end of the interruption and the resumption of the primary task) following long but not short interruptions. Experiment 2 (N&nbsp;=&nbsp;38) involved a tangible sorting task, utilizing only the automatic cue. The AR cue facilitated task resumption compared to not cue after both short and long interruptions. We demonstrated the potential of AR cues in mitigating the negative effects of interruptions and make suggestions for integrating AR technologies for task resumption.",Augmented Reality; Human Error; Interruption; Resumption Lag; Task Resumption,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Augmented Reality at Zoo Exhibits: A Design Framework for Enhancing the Zoo Experience,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) offers unique opportunities for contributing to zoos’ objectives of public engagement and education about animal and conservation issues. However, the diversity of animal exhibits pose challenges in designing AR applications that are not encountered in more controlled environments, such as museums. To support the design of AR applications that meaningfully engage the public with zoo objectives, we first conducted two scoping reviews to interrogate previous work on AR and broader technology use at zoos. We then conducted a workshop with zoo representatives to understand the challenges and opportunities in using AR to achieve zoo objectives. Additionally, we conducted a field trip to a public zoo to identify exhibit characteristics that impacts AR application design. We synthesise the findings from these studies into a framework that enables the design of diverse AR experiences. We illustrate the utility of the framework by presenting two concepts for feasible AR applications.",augmented reality; design framework; ethnography; field study,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Comfortable Mobility vs. Attractive Scenery: The Key to Augmenting Narrative Worlds in Outdoor Locative Augmented Reality Storytelling,CHI - Human Factors in Computing Systems,A*,"We investigate how path context, encompassing both comfort and attractiveness, shapes user experiences in outdoor locative storytelling using Augmented Reality (AR). Addressing a research gap that predominantly concentrates on indoor settings or narrative backdrops, our user-focused research delves into the interplay between perceived path context and locative AR storytelling on routes with diverse walkability levels. We examine the correlation and causation between narrative engagement, spatial presence, perceived workload, and perceived path context. Our findings show that on paths with reasonable path walkability, attractive elements positively influence the narrative experience. However, even in environments with assured narrative walkability, inappropriate safety elements can divert user attention to mobility, hindering the integration of real-world features into the narrative. These results carry significant implications for path creation in outdoor locative AR storytelling, underscoring the importance of ensuring comfort and maintaining a balance between comfort and attractiveness to enrich the outdoor AR storytelling experience.",Augmented Reality; Locative Storytelling; Mixed Reality; Walkability,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Investigating the Design of Augmented Narrative Spaces Through Virtual-Real Connections: A Systematic Literature Review,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) is regarded as an innovative storytelling medium that presents novel experiences by layering a virtual narrative space over a real 3D space. However, understanding of how the virtual narrative space and the real space are connected with one another in the design of augmented narrative spaces has been limited. For this, we conducted a systematic literature review of 64 articles featuring AR storytelling applications and systems in HCI, AR, and MR research. We investigated how virtual narrative spaces have been paired, functionalized, placed, and registered in relation to the real spaces they target. Based on these connections, we identified eight dominant types of augmented narrative spaces that are primarily categorized by whether they virtually narrativize reality or realize the virtual narrative. We discuss our findings to propose design recommendations on how virtual-real connections can be incorporated into a more structured approach to AR storytelling.",augmented narrative space; Augmented Reality; Mixed Reality; storytelling,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Jigsaw: Authoring Immersive Storytelling Experiences with Augmented Reality and Internet of Things,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) presents new opportunities for immersive storytelling. However, this immersiveness faces two main hurdles. First, AR’s immersive quality is often confined to visual elements, such as pixels on a screen. Second, crafting immersive narratives is complex and generally beyond the reach of amateurs due to the need for advanced technical skills. We introduce Jigsaw, a system that empowers beginners to both experience and craft immersive stories, blending virtual and physical elements. Jigsaw uniquely combines mobile AR with readily available Internet-of-things (IoT) devices. We conducted a qualitative study with 20 participants to assess Jigsaw’s effectiveness in both consuming and creating immersive narratives. The results were promising: participants not only successfully created their own immersive stories but also found the playback of three such stories deeply engaging. However, sensory overload emerged as a significant challenge in these experiences. We discuss design trade-offs and considerations for future endeavors in immersive storytelling involving AR and IoT.",augmented reality; authoring tool; internet-of-things; storytelling,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring the Impact of Interconnected External Interfaces in Autonomous Vehicles on Pedestrian Safety and Experience,CHI - Human Factors in Computing Systems,A*,"Policymakers advocate for the use of external Human-Machine Interfaces (eHMIs) to allow autonomous vehicles (AVs) to communicate their intentions or status. Nonetheless, scalability concerns in complex traffic scenarios arise, such as potentially increasing pedestrian cognitive load or conveying contradictory signals. Building upon precursory works, our study explores ‘interconnected eHMIs,’ where multiple AV interfaces are interconnected to provide pedestrians with clear and unified information. In a virtual reality study (N=32), we assessed the effectiveness of this concept in improving pedestrian safety and their crossing experience. We compared these results against two conditions: no eHMIs and unconnected eHMIs. Results indicated interconnected eHMIs enhanced safety feelings and encouraged cautious crossings. However, certain design elements, such as the use of the colour red, led to confusion and discomfort. Prior knowledge slightly influenced perceptions of interconnected eHMIs, underscoring the need for refined user education. We conclude with practical implications and future eHMI design research directions.",autonomous vehicles; eHMIs; external communication; scalability; vehicle-pedestrian interaction; vulnerable road users,Abstract,True,
ACM DL,conferencePaper,2024,"Process, Roles, Tools, and Team: Understanding the Emerging Medium of Virtual Reality Theatre",CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) theatre artists are combining theatre production and game development practices to create live performances in VR. To date, little is known about VR theatre creators’ experiences of this process or how staging a play in VR might affect the audience’s experience. To capture the experience of developing a VR theatre production we interviewed the production team behind the VR play You Should Have Stayed Home. Members of this team felt the process was a learning experience and shared the lessons they plan to incorporate into their future work. We report on the team’s efforts to understand the VR theatre medium, how this team was constructed, and challenges that they encountered. In this paper we present the opportunities that the production team members identified for creating novel experiences for VR audiences, and their own needs as creators.",Design Process; Drama; Intermedial Theatre; Virtual Reality Theatre,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment,CHI - Human Factors in Computing Systems,A*,"Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants’ Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players’ hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.",avatar co-embodiment; body ownership; co-presence; haptics; perceptual crossing; sense of agency; Virtual reality,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,TimeTunnel: Integrating Spatial and Temporal Motion Editing for Character Animation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Editing character motion in Virtual Reality is challenging as it requires working with both spatial and temporal data using controls with multiple degrees-of-freedom. The spatial and temporal controls are separated, making it difficult to adjust poses over time and predict the effects across adjacent frames. To address this challenge, we propose TimeTunnel, an immersive motion editing interface that integrates spatial and temporal control for 3D character animation in VR. TimeTunnel provides an approachable editing experience via KeyPoses and Trajectories. KeyPoses are a set of representative poses automatically computed to concisely depict motion. Trajectories are 3D animation curves that pass through the joints of KeyPoses to represent in-betweens. TimeTunnel integrates spatial and temporal control by superimposing Trajectories and KeyPoses onto a 3D character. We conducted two studies to evaluate TimeTunnel. In our quantitative study, TimeTunnel reduced the amount of time required for editing motion, and saved effort in locating target poses. Our qualitative study with domain experts demonstrated how TimeTunnel is an approachable interface that can simplify motion editing, while still preserving a direct representation of motion.",3D interface; immersive animation authoring; keypose; motion editing; motion path,Title_Abstract,True,
ACM DL,conferencePaper,2024,"""I Shot the Interviewer!"": The Effects of In-VR Interviews on Participant Feedback and Rapport",CHI - Human Factors in Computing Systems,A*,"The integration of questionnaires into virtual reality experiences has recently been proposed as a way to reduce the potential biases introduced through the negative effects of leaving VR, however there has been little attention paid to how qualitative interviews could similarly be integrated into the virtual world for the purposes of user evaluation. In this paper we explore how conducting interviews within the virtual environment may affect the outcome of the evaluation and the relationship between participant and interviewer, and how this may differ with and without visual representation of the interviewer through use of an avatar. We conclude that in-VR interviews are a valid and promising method of data collection for user evaluation with similar data quality to in-person interviews, but that the interviewer should have a visual presence in the environment to maintain their relationship with the participant and the perceived realism of the environment.",Interview; Qualitative Methods; Virtual/Augmented Reality,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Stretch your reach: Studying Self-Avatar and Controller Misalignment in Virtual Reality Interaction,CHI - Human Factors in Computing Systems,A*,"Immersive Virtual Reality typically requires a head-mounted display (HMD) to visualize the environment and hand-held controllers to interact with the virtual objects. Recently, many applications display full-body avatars to represent the user and animate the arms to follow the controllers. Embodiment is higher when the self-avatar movements align correctly with the user. However, having a full-body self-avatar following the user’s movements can be challenging due to the disparities between the virtual body and the user’s body. This can lead to misalignments in the hand position that can be noticeable when interacting with virtual objects. In this work, we propose five different interaction modes to allow the user to interact with virtual objects despite the self-avatar and controller misalignment and study their influence on embodiment, proprioception, preference, and task performance. We modify aspects such as whether the virtual controllers are rendered, whether controllers are rendered in their real physical location or attached to the user’s hand, and whether stretching the avatar arms to always reach the real controllers. We evaluate the interaction modes both quantitatively (performance metrics) and qualitatively (embodiment, proprioception, and user preference questionnaires). Our results show that the stretching arms solution, which provides body continuity and guarantees that the virtual hands or controllers are in the correct location, offers the best results in embodiment, user preference, proprioception, and performance. Also, rendering the controller does not have an effect on either embodiment or user preference.",3D interaction; avatars; embodiment; perception; virtual reality,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Virtual Body Swapping: A VR-Based Approach to Embodied Third-Person Self-Processing in Mind-Body Therapy,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers various opportunities for innovative therapeutic approaches, especially regarding self-related mind-body interventions. We introduce a VR body swap system enabling multiple users to swap their perspectives and appearances and evaluate its effects on virtual sense of embodiment (SoE) and perception- and cognition-based self-related processes. In a self-compassion-framed scenario, twenty participants embodied their personalized, photorealistic avatar, swapped bodies with an unfamiliar peer, and reported their SoE, interoceptive awareness (perception), and self-compassion (cognition). Participants’ experiences differed between bottom-up and top-down processes. Regarding SoE, their agency and self-location shifted to the swap avatar, while their top-down self-identification remained with their personalized avatar. Further, the experience positively affected interoceptive awareness but not self-compassion. Our outcomes offer novel insights into the SoE in a multiple-embodiment scenario and highlight the need to differentiate between the different processes in intervention design. They raise concerns and requirements for future research on avatar-based mind-body interventions.",body awareness; body swap; embodiment; perspective taking.; self-compassion; Virtual reality,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"""Pikachu would electrocute people who are misbehaving"": Expert, Guardian and Child Perspectives on Automated Embodied Moderators for Safeguarding Children in Social Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Automated embodied moderation has the potential to create safer spaces for children in social VR, providing a protective figure that takes action to mitigate harmful interactions. However, little is known about how such moderation should be employed in practice. Through interviews with 16 experts in online child safety and psychology, and workshops with 8 guardians and 13 children, we contribute a comprehensive overview of how Automated Embodied Moderators (AEMs) can safeguard children in social VR. We explore perceived concerns, benefits and preferences across the stakeholder groups and gather first-of-their-kind recommendations and reflections around AEM design. The results stress the need to adapt AEMs to children, whether victims or harassers, based on age and development, emphasising empowerment, psychological impact and humans/guardians-in-the-loop. Our work provokes new participatory design-led directions to consider in the development of AEMs for children in social VR taking child, guardian, and expert insights into account.",child online safety; children; design workshops; experts; grandparent; guardian; interviews; metaverse; parent; social virtual reality,Title_Keywords,True,
ACM DL,conferencePaper,2024,LegacySphere: Facilitating Intergenerational Communication Through Perspective-Taking and Storytelling in Embodied VR,CHI - Human Factors in Computing Systems,A*,"Intergenerational communication can enhance well-being and family cohesion, but stereotypes and low empathy can be barriers to achieving effective communication. VR perspective-taking is a potential approach that is known to enhance understanding and empathy toward others by allowing a user to take another’s viewpoint. In this study, we introduce LegacySphere, a novel VR perspective-taking experience leveraging the combination of embodiment, role-play, and storytelling. To explore LegacySphere’s design and impact, we conducted an observational study involving five dyads with a one-generation gap. We found that LegacySphere promotes empathetic and reflexive intergenerational dialogue. Specifically, avatar embodiment encourages what we term “relationship cushioning,” fostering a trustful, open environment for genuine communications. The blending of real and embodied identities prompts insightful questions, merging both perspectives. The experience also nurtures a sense of unity and stimulates reflections on aging. Our work highlights the potential of immersive technologies for enhancing empathetic intergenerational relationships.",Empathy; Intergenerational communication; Perspective-taking; Proteus effect; Role-play; Storytelling; Virtual Reality,Keywords,True,
ACM DL,conferencePaper,2024,An Artists' Perspectives on Natural Interactions for Virtual Reality 3D Sketching,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) applications like OpenBrush offer artists access to 3D sketching tools within the digital 3D virtual space. These 3D sketching tools allow users to “paint” using virtual digital strokes that emulate real-world mark-making. Yet, users paint these strokes through (unimodal) VR controllers. Given that sketching in VR is a relatively nascent field, this paper investigates ways to expand our understanding of sketching in virtual space, taking full advantage of what an immersive digital canvas offers. Through a study conducted with the participation of artists, we identify potential methods for natural multimodal and unimodal interaction techniques in 3D sketching. These methods demonstrate ways to incrementally improve existing interaction techniques and incorporate artistic feedback into the design.",3D Sketching; Gestures; Multimodal Interaction; Speech; Virtual Reality,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,EyeGuide &amp; EyeConGuide: Gaze-based Visual Guides to Improve 3D Sketching Systems,CHI - Human Factors in Computing Systems,A*,"Visual guides help to align strokes and raise accuracy in Virtual Reality (VR) sketching tools. Automatic guides that appear at relevant sketching areas are convenient to have for a seamless sketching with a guide. We explore guides that exploit eye-tracking to render them adaptive to the user’s visual attention. EyeGuide and EyeConGuide cause visual grid fragments to appear spatially close to the user’s intended sketches, based on the information of the user’s eye-gaze direction and the 3D position of the hand. Here we evaluated the techniques in two user studies across simple and complex sketching objectives in VR. The results show that gaze-based guides have a positive effect on sketching accuracy, perceived usability and preference over manual activation in the tested tasks. Our research contributes to integrating gaze-contingent techniques for assistive guides and presents important insights into multimodal design applications in VR.",3D Sketching; 3D User Interface; Eye-Gaze; VR,Abstract,True,
ACM DL,conferencePaper,2024,Choosing the Right Reality: A Comparative Analysis of Tangibility in Immersive Trauma Simulations,CHI - Human Factors in Computing Systems,A*,"In the field of medical first responder training, the choice of training modality is crucial for skill retention and real-world application. This study introduces the Green Manikin, an advanced Mixed Reality (MR) tool, conceptually combining the immersiveness of Virtual Reality (VR) with the tangibility of real-world training, and compares it against traditional real-world simulations and VR training. Our findings indicate that MR and real-world settings excel in Self and Social Presence, and in intention to use, offering heightened psychological presence suitable for complex training scenarios. Effort expectancy was highest in real-world environments, suggesting their ease of use for basic skill acquisition. This nuanced understanding allows for better tailoring of training modalities to specific educational objectives. Our research validates the utility of MR and offers a framework for selecting the most effective training environment for different learning outcomes in medical first responder training.",chroma-key; comparative study; medical first responder; mixed reality; modality; training,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation. Effectively visually communicating such threats in the environment around the user is not straightforward. This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display. We leverage the AR device’s GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization. We perform a user study (25 participants) of different visualizations and obtain user feedback. Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely. We also discuss the evaluation approaches and provide recommendations.",Augmented Reality; CBRN Response Training; Spatial Awareness; Visualization,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field,CHI - Human Factors in Computing Systems,A*,"First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment. Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs.",Augmented Reality; Design; First Responders; In-the-field Tasks,Keywords,True,
ACM DL,conferencePaper,2024,VisTorch: Interacting with Situated Visualizations using Handheld Projectors,CHI - Human Factors in Computing Systems,A*,"Spatial data is best analyzed in situ, but existing mixed reality technologies can be bulky, expensive, or unsuitable for collaboration. We present VisTorch: a handheld device for projected situated analytics consisting of a pico-projector, a multi-spectrum camera, and a touch surface. VisTorch enables viewing charts situated in physical space by simply pointing the device at a surface to reveal visualizations in that location. We evaluated the approach using both a user study and an expert review. In the former, we asked 20 participants to first organize charts in space and then refer to these charts to answer questions. We observed three spatial and one temporal pattern in participant analyses. In the latter, four experts—a museum designer, a statistical software developer, a theater stage designer, and an environmental educator—utilized VisTorch to derive practical usage scenarios. Results from our study showcase the utility of situated visualizations for memory and recall.",augmented reality; immersive analytics; situated visualization.; Ubiquitous analytics,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D Spatio-Temporal Data in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Tangible interfaces in mixed reality (MR) environments allow for intuitive data interactions. Tangible cubes, with their rich interaction affordances, high maneuverability, and stable structure, are particularly well-suited for exploring multi-dimensional data types. However, the design potential of these cubes is underexplored. This study introduces a design space for tangible cubes in MR, focusing on interaction space, visualization space, sizes, and multiplicity. Using spatio-temporal data, we explored the interaction affordances of these cubes in a workshop (N=24). We identified unique interactions like rotating, tapping, and stacking, which are linked to augmented reality (AR) visualization commands. Integrating user-identified interactions, we created a design space for tangible-cube interactions and visualization. A prototype visualizing global health spending with small cubes was developed and evaluated, supporting both individual and combined cube manipulation. This research enhances our grasp of tangible interaction in MR, offering insights for future design and application in diverse data contexts.",mixed reality; spatio-temporal data; tangible interaction,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Embodied Tentacle: Mapping Design to Control of Non-Analogous Body Parts with the Human Body,CHI - Human Factors in Computing Systems,A*,"Manipulating a non-humanoid body using a mapping approach that translates human body activity into different structural movements enables users to perform tasks that are difficult with their innate bodies. However, a key challenge is how to design an effective mapping to control non-analogous body parts with the human body. To address this challenge, we designed an articulated virtual arm and investigated the effect of mapping methods on a user’s manipulation experience. Specifically, we developed an unbranched 12-joint virtual arm with an octopus-like appearance. Using this arm, we conducted a user study to compare the effects of several mapping methods with different arrangements on task performance and subjective evaluations of embodiment and user preference. As a result, we identified three important factors in mapping: “Visual and Configurational Similarity”, “Kinematics Suitability for the User”, and “Correspondence with Everyday Actions.” Based on these findings, we discuss a mapping design for non-humanoid body manipulation.",Virtual Reality; embodiment; human augmentation; gestural interaction; body schema; non-anthropomorphic avatars; non-humanoid avatars,Keywords,True,
ACM DL,conferencePaper,2024,Grand challenges in WaterHCI,CHI - Human Factors in Computing Systems,A*,"Recent combinations of interactive technology, humans, and water have resulted in “WaterHCI”. WaterHCI design seeks to complement the many benefits of engagement with the aquatic domain, by offering, for example, augmented reality systems for snorkelers, virtual reality in floatation tanks, underwater musical instruments for artists, robotic systems for divers, and wearables for swimmers. We conducted a workshop in which WaterHCI experts articulated the field's grand challenges, aiming to contribute towards a systematic WaterHCI research agenda and ultimately advance the field.",grand challenges; fluid user interfaces; human-water interaction,Abstract,True,
ACM DL,conferencePaper,2024,AdapTics: A Toolkit for Creative Design and Integration of Real-Time Adaptive Mid-Air Ultrasound Tactons,CHI - Human Factors in Computing Systems,A*,"Mid-air ultrasound haptic technology can enhance user interaction and immersion in extended reality (XR) applications through contactless touch feedback. Yet, existing design tools for mid-air haptics primarily support creating tactile sensations (i.e., tactons) which cannot change at runtime. These tactons lack expressiveness in interactive scenarios where a continuous closed-loop response to user movement or environmental states is desirable. This paper introduces AdapTics, a toolkit featuring a graphical interface for rapid prototyping of adaptive tactons—dynamic sensations that can adjust at runtime based on user interactions, environmental changes, or other inputs. A software library and a Unity package accompany the graphical interface to enable integration of adaptive tactons in existing applications. We present the design space offered by AdapTics for creating adaptive mid-air ultrasound tactons and show the design tool can improve Creativity Support Index ratings for Exploration and Expressiveness in a user study with 12 XR and haptic designers.",design tool; haptic design; mid-air ultrasound haptics; real-time adaptation; tacton,Abstract,True,
ACM DL,conferencePaper,2024,To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation,CHI - Human Factors in Computing Systems,A*,"Rehabilitation therapies are widely employed to assist people with motor impairments in regaining control over their affected body parts. Nevertheless, factors such as fatigue and low self-efficacy can hinder patient compliance during extensive rehabilitation processes. Utilizing hand redirection in virtual reality (VR) enables patients to accomplish seemingly more challenging tasks, thereby bolstering their motivation and confidence. While previous research has investigated user experience and hand redirection among able-bodied people, its effects on motor-impaired people remain unexplored. In this paper, we present a VR rehabilitation application that harnesses hand redirection. Through a user study and semi-structured interviews, we examine the impact of hand redirection on the rehabilitation experiences of people with motor impairments and its potential to enhance their motivation for upper limb rehabilitation. Our findings suggest that patients are not sensitive to hand movement inconsistency, and the majority express interest in incorporating hand redirection into future long-term VR rehabilitation programs.",Motor impairments; Upper limb rehabilitation; Virtual hand redirection,Abstract,True,
ACM DL,conferencePaper,2024,Can You Hazard a Guess?: Evaluating the Effect of Augmented Reality Cues on Driver Hazard Prediction,CHI - Human Factors in Computing Systems,A*,"Semi-autonomous vehicles allow drivers to engage with non-driving related tasks (NDRTs). However, these tasks interfere with the driver’s situational awareness, key when they need to safely retake control of the vehicle. This paper investigates if Augmented Reality (AR) could be used to present NDRTs to reduce their impact on situational awareness. Two experiments compared driver performance on a hazard prediction task whilst interacting with an NDRT, presented either as an AR Heads-Up Display or a traditional Heads-Down Display. The results demonstrate that an AR display including a novel dynamic attentional cue improves situational awareness, depending on the workload of the NDRT and design of the cue. The results provide novel insights for designers of in-car systems about how to design NDRTs to aid driver situational awareness in future vehicles.",Augmented Reality; Autonomous vehicles; In-car; Attention; Cueing; Situational Awareness; Takeover Request,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Portobello: Extending Driving Simulation from the Lab to the Road,CHI - Human Factors in Computing Systems,A*,"In automotive user interface design, testing often starts with lab-based driving simulators and migrates toward on-road studies to mitigate risks. Mixed reality (XR) helps translate virtual study designs to the real road to increase ecological validity. However, researchers rarely run the same study in both in-lab and on-road simulators due to the challenges of replicating studies in both physical and virtual worlds. To provide a common infrastructure to port in-lab study designs on-road, we built a platform-portable infrastructure, Portobello, to enable us to run twinned physical-virtual studies. As a proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32) both in-lab and on-road to investigate study design portability and platform-driven influences on study outcomes. To our knowledge, this is the first system that enables the twinning of studies originally designed for in-lab simulators to be carried out in an on-road platform.",Driving Simulations; Human-Autonomous Vehicle Interaction,Abstract,True,
ACM DL,conferencePaper,2024,SYNC-VR: Synchronizing Your Senses to Conquer Motion Sickness for Enriching In-Vehicle Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Passengers can engage more in nondriving-related tasks owing to recent advancements in autonomous vehicles (AVs), making immersive tools such as virtual reality (VR) appealing; however, motion sickness (MS) remains a significant challenge. We present SYNC-VR, a system that aligns with visual, haptic, and auditory cues and provides proprioceptive feedback to illustrate its effect on MS and presence within the in-vehicle VR. We conducted an experiment with 24 participants using a real vehicle along a route with known MS-triggering events. Using subjective and physiological measures, we assessed participants’ presence and MS under four conditions by gradually varying the level of synchronized input sensations. Results reveal that SYNC-VR reduces MS and increases the sense of presence. Additionally, it emphasizes the impact of our interactive VR content and its role in achieving proprioceptive feedback with haptic feedback through electrical muscle stimulation, introducing an innovative approach to MS mitigation in in-vehicle VR.",presence; virtual reality; haptic feedback; autonomous vehicles; motion sickness; visual cues; kinesthetic feedback; sensory alignment,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,From Slow-Mo to Ludicrous Speed: Comfortably Manipulating the Perception of Linear In-Car VR Motion Through Vehicular Translational Gain and Attenuation,CHI - Human Factors in Computing Systems,A*,"To prevent motion sickness, Virtual Reality (VR) experiences for vehicle passengers typically present “matched motion”: real vehicle movements are replicated 1:1 by movements in VR. This significantly limits virtual applications. We provide foundations for in-car VR experiences that break this constraint by manipulating the passenger’s visual perception of linear velocity through amplifying and reducing the virtual speed. In two on-the-road studies, we examined the application of Vehicular Translational Gain (1.5-9.5x) and Attenuation (0.66-0.14x) to real car speeds ( 50km/h) across two VR tasks (reading and gaming), exploring journey perception, impact on motion sickness, travel experience and tasks. We found that vehicular gain/attenuation can be applied without significantly increasing motion sickness. Gain was more noticeable and affected perceived speed, distance, safety, relaxation and excitement, being well-suited to gaming, while attenuation was more suitable for productivity. Our work unlocks new ways that VR applications can enhance and alter the passenger experience through novel perceptual manipulations of vehicle velocity.",Virtual Reality; Motion Sickness; In-Car; Automated Vehicles; Perceptual Manipulation; Translational Gain; Vehicular Attenuation; Vehicular Gain; Velocity,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Investigating the Effects of External Communication and Platoon Behavior on Manual Drivers at Highway Access,CHI - Human Factors in Computing Systems,A*,"Automated vehicles are expected to improve traffic safety and efficiency. One approach to achieve this is via platooning, that is, (automated) vehicles can drive behind each other at very close proximity to reduce air resistance. However, this behavior could lead to difficulties in mixed traffic, for example, when manual drivers try to enter a highway. Therefore, we report the results of a within-subject Virtual Reality study (N=29) evaluating different platoon behaviors (single vs. multiple, i.e., four, gaps) and communication strategies (HUD, AR, attached displays). Results show that AR communication reduced mental workload, improved perceived safety, and a single big gap led to the safest merging behavior. Our work helps to incorporate novel behavior enabled by automation into general traffic better.",Automated vehicles; external communication; self-driving vehicles; Virtual Reality.,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Swarm Body: Embodied Swarm Robots,CHI - Human Factors in Computing Systems,A*,"The human brain’s plasticity allows for the integration of artificial body parts into the human body. Leveraging this, embodied systems realize intuitive interactions with the environment. We introduce a novel concept: embodied swarm robots. Swarm robots constitute a collective of robots working in harmony to achieve a common objective, in our case, serving as functional body parts. Embodied swarm robots can dynamically alter their shape, density, and the correspondences between body parts and individual robots. We contribute an investigation of the influence on embodiment of swarm robot-specific factors derived from these characteristics, focusing on a hand. Our paper is the first to examine these factors through virtual reality (VR) and real-world robot studies to provide essential design considerations and applications of embodied swarm robots. Through quantitative and qualitative analysis, we identified a system configuration to achieve the embodiment of swarm robots.",tangible interaction; embodiment; swarm robotics,Abstract,True,
ACM DL,conferencePaper,2024,"Scientific and Fantastical: Creating Immersive, Culturally Relevant Learning Experiences with Augmented Reality and Large Language Models",CHI - Human Factors in Computing Systems,A*,"Motivating children to learn is a major challenge in education. One way to inspire motivation to learn is through immersion. We combine the immersive potential of augmented reality (AR), narrative, and large language models (LLMs) to bridge fantasy with reality in a mobile application, Moon Story, that teaches elementary schoolers astronomy and environmental science. Our system also builds upon learning theories such as culturally-relevant pedagogy. Using our application, a child embarks on a journey inspired by Chinese mythology, engages in real-world AR activities, and converses with a fictional character powered by an LLM. We conducted a controlled experiment (N = 50) with two conditions: one using an LLM and one that was hard-coded. Both conditions resulted in learning gains, high engagement levels, and increased science learning motivation. Participants in the LLM condition also wrote more relevant answers. Finally, participants of both Chinese and non-Chinese heritage found the culturally-based narrative compelling.",Artifact or System; Children/Parents; Education/Learning,Title_Abstract,True,
ACM DL,conferencePaper,2024,Promoting Eco-Friendly Behaviour through Virtual Reality - Implementation and Evaluation of Immersive Feedback Conditions of a Virtual CO2 Calculator,CHI - Human Factors in Computing Systems,A*,"Climate change is one of the most pressing global challenges in the 21st century. Urgent actions favoring the environment’s well-being are essential to mitigate its potentially irreversible consequences. However, the delayed and often distant nature of the effects of sustainable behavior makes it challenging for individuals to connect with the issue personally. Immersive media are an opportunity to introduce innovative feedback mechanisms to highlight the urgency of behavior effects. We introduce a VR carbon calculator that visualizes users’ annual carbon footprint as CO2-filled balloons over multiple periods. In a 2 × 2 design, participants calculated and visualized their carbon footprint numerically or as balloons over one or three years. We found no effect of our visualization but a significant impact of the visualized period on participants’ environmental self-efficacy. These findings emphasize the importance of target-oriented design in VR behavior interventions.",Virtual reality; intention-behavior gap; pro-environmental behavior.,Title_Keywords,True,
ACM DL,conferencePaper,2024,"Born to Run, Programmed to Play: Mapping the Extended Reality Exergames Landscape",CHI - Human Factors in Computing Systems,A*,"Many people struggle to exercise regularly, raising the risk of serious health-related issues. Extended reality (XR) exergames address these hurdles by combining physical exercises with enjoyable, immersive gameplay. While a growing body of research explores XR exergames, no previous review has structured this rapidly expanding research landscape. We conducted a scoping review of the current state of XR exergame research to (i) provide a structured overview, (ii) highlight trends, and (iii) uncover knowledge gaps. After identifying 1318 papers in human-computer interaction and medical databases, we ultimately included 186 papers in our analysis. We provide a quantitative and qualitative summary of XR exergame research, showing current trends and potential future considerations. Finally, we provide a taxonomy of XR exergames to help future design and methodological investigation and reporting.",virtual reality; mixed reality; review; augmented reality; extended reality; exergames; taxonomy; exercise; games; active games; active video games; motion games; movement games; sports games,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring Opportunities for Augmenting Homes to Support Exercising,CHI - Human Factors in Computing Systems,A*,"Although exercising at home has benefits, it is not always engaging or motivating. Augmented Reality (AR) head-mounted displays (HMDs) offer the potential to make in-home exercising and exergaming more inclusive and immersive, but there is limited research investigating how such systems can be designed. We employed a participatory design approach involving semi-structured interviews to investigate how homes can be augmented to facilitate exercising experiences. We developed 10 recommendations for developing home-based exercising experiences using AR HMDs. Our results further contribute to the existing body of research on the use of AR for exercising, home applications, and everyday objects by presenting the first foundational study investigating the wide range of exercises that can be supported through AR HMDs in home environments and the different ways home elements may support these exercises, and laying the groundwork for future work developing home-based exergaming through AR HMDs to increase people’s physical activity levels.",Exergaming; Augmented reality; Participatory Design; Home environment; Exercising,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Tandem: Reproducible Digital Fabrication Workflows as Multimodal Programs,CHI - Human Factors in Computing Systems,A*,"Experimental digital fabrication workflows are increasingly common in human-computer interaction research, but are difficult to reproduce. We present Tandem, a software library that lets a fabricator implement an end-to-end fabrication workflow as a computational notebook program that others can run to physically reproduce the workflow. Tandem notebook programs read and write to CAD and CAM software, project augmented reality interfaces onto machines for manual interventions, and directly control fabrication machines. Fabricators can also denote potential mismatches between the physical and the digital as explicit assertions in code. Using two-sided CNC milling as an example, we demonstrate how to implement a complex workflow as a single program that can be re-run by others while supporting quality control and improving reproducibility.",open-source software; programming languages; computational notebooks; Digital fabrication,Abstract,True,
ACM DL,conferencePaper,2024,pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication,CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n = 20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.",Mixed Reality; Personal Fabrication; 3D-modeling; Customizer Interfaces; Design Customization; In-Situ Design; In-Situ Modeling; pARam; Parametric Designs; Remixing,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Palette-PrintAR: augmented reality design and simulation for multicolor resin 3D printing,CHI - Human Factors in Computing Systems,A*,"While 3D printing affords designers unprecedented geometrical complexity, fewer interactive design tools for multimaterial platforms exist. Recent work in resin 3D printing specifically promises fast, multicolor printing by growing fluidic channels concurrent with the object itself, infusing different resins spatioselectively into the vat; however, no design tools have been developed enabling users to interact with such novel personal fabrication machines in situ. Here, we introduce an augmented reality-based design tool allowing users to engage with this multicolor fabrication method so as to ""paint"" growing 3D objects. We define the design process and mode of user interaction with our tool, Palette-PrintAR, which integrates situated 3D model manipulation with real-time computational fluid dynamics simulation and computer vision-based tracking and analysis. We detail our 3D printer hardware add-on implementation and AR software architecture, along with characterizing the design flexibilities and limitations of our AR-based multicolor fabrication method.",augmented reality; 3D printing; interactive fabrication,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Füpop: ""Real Food"" Flavor Delivery via Focused Ultrasound",CHI - Human Factors in Computing Systems,A*,"Food and flavors are integral to our existence in the world. Nonetheless, taste remains an under-explored sense in interaction design. We present Füpop, a technical platform for delivering in-mouth flavors that leverages advances in electronics and molecular gastronomy. Füpop comprises a fully edible pouch placed inside the mouth against a cheek that programmatically releases different flavors when wirelessly triggered by a focused ultrasound transducer from outside the cheek. Füpop does not interfere with activities such as chewing and drinking, and its electronics may be integrated into devices already used near the cheek, such as mobile phones, audio headphones, and head-mounted displays. Füpop’s flavors are from “real foods,” not ones imitated with synthetic reagents, providing authentic, nutritive flavors. We envision that with Füpop, flavors may be synced to music, a phone call, or events in virtual reality to enhance a user’s experience of their food and the world.",ultrasound; human-food interaction; edible; gastronomy; taste interactions,Abstract,True,
ACM DL,conferencePaper,2024,FocusFlow: 3D Gaze-Depth Interaction in Virtual Reality Leveraging Active Visual Depth Manipulation,CHI - Human Factors in Computing Systems,A*,"Gaze interaction presents a promising avenue in Virtual Reality (VR) due to its intuitive and efficient user experience. Yet, the depth control inherent in our visual system remains underutilized in current methods. In this study, we introduce FocusFlow, a hands-free interaction method that capitalizes on human visual depth perception within the 3D scenes of Virtual Reality. We first develop a binocular visual depth detection algorithm to understand eye input characteristics. We then propose a layer-based user interface and introduce the concept of “Virtual Window” that offers an intuitive and robust gaze-depth VR interaction, despite the constraints of visual depth accuracy and precision spatially at further distances. Finally, to help novice users actively manipulate their visual depth, we propose two learning strategies that use different visual cues to help users master visual depth control. Our user studies on 24 participants demonstrate the usability of our proposed virtual window concept as a gaze-depth interaction method. In addition, our findings reveal that the user experience can be enhanced through an effective learning process with adaptive visual cues, helping users to develop muscle memory for this brand-new input mechanism. We conclude the paper by discussing potential future research topics of gaze-depth interaction.",Virtual Reality; Eye Tracking; Gaze Interaction; 3D User Interface; Visual Depth,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Gaze on the Go: Effect of Spatial Reference Frame on Visual Target Acquisition During Physical Locomotion in Extended Reality,CHI - Human Factors in Computing Systems,A*,"Spatial interaction relies on fast and accurate visual acquisition. In this work, we analyse how visual acquisition and tracking of targets presented in a head-mounted display is affected by the user moving linearly at walking and jogging paces. We study four reference frames in which targets can be presented: Head and World where targets are affixed relative to the head and environment, respectively; HeadDelay where targets are presented in the head coordinate system but follow head movement with a delay, and novel Path where targets remain at fixed distance in front of the user, in the direction of their movement. Results of our study in virtual reality demonstrate that the more stable the target is relative to the environment, the faster and more precise it can be fixated. The results have practical significance as head-mounted displays enable interaction during mobility, and in particular when eye tracking is considered as input.",extended reality; eye tracking; gaze interaction; physical locomotion; reference frames; spatial UIs; UI placement,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Snap, Pursuit and Gain: Virtual Reality Viewport Control by Gaze",CHI - Human Factors in Computing Systems,A*,"Head-mounted displays let users explore virtual environments through a viewport that is coupled with head movement. In this work, we investigate gaze as an alternative modality for viewport control, enabling exploration of virtual worlds with less head movement. We designed three techniques that leverage gaze based on different eye movements: Dwell Snap for viewport rotation in discrete steps, Gaze Gain for amplified viewport rotation based on gaze angle, and Gaze Pursuit for central viewport alignment of gaze targets. All three techniques enable 360-degree viewport control through naturally coordinated eye and head movement. We evaluated the techniques in comparison with controller snap and head amplification baselines, for both coarse and precise viewport control, and found them to be as fast and accurate. We observed a high variance in performance which may be attributable to the different degrees to which humans tend to support gaze shifts with head movement.",virtual reality; Eye tracking; user study; gaze interaction; gaze-based interaction; eye-head coordination; viewport control,Title_Keywords,True,
ACM DL,conferencePaper,2024,EITPose: Wearable and Practical Electrical Impedance Tomography for Continuous Hand Pose Estimation,CHI - Human Factors in Computing Systems,A*,"Real-time hand pose estimation has a wide range of applications spanning gaming, robotics, and human-computer interaction. In this paper, we introduce EITPose, a wrist-worn, continuous 3D hand pose estimation approach that uses eight electrodes positioned around the forearm to model its interior impedance distribution during pose articulation. Unlike wrist-worn systems relying on cameras, EITPose has a slim profile (12 mm thick sensing strap) and is power-efficient (consuming only 0.3 W of power), making it an excellent candidate for integration into consumer electronic devices. In a user study involving 22 participants, EITPose achieves with a within-session mean per joint positional error of 11.06 mm. Its camera-free design prioritizes user privacy, yet it maintains cross-session and cross-user accuracy levels comparable to camera-based wrist-worn systems, thus making EITPose a promising technology for practical hand pose estimation.",Extended Reality; Input; Electrical Impedance Tomography; Natural User Interfaces; Hand Gesture; Hand Pose; Interaction Technique,Keywords,True,
ACM DL,conferencePaper,2024,ArmDeformation: Inducing the Sensation of Arm Deformation in Virtual Reality Using Skin-Stretching,CHI - Human Factors in Computing Systems,A*,"With the development of virtual reality (VR) technology, research is being actively conducted on how incorporating multisensory feedback can create the illusion that virtual avatars are perceived as an extension of the body in VR. In line with this research direction, we introduce ArmDeformation, a wearable device employing skin-stretching to enhance virtual forearm ownership during arm deformation illusion. We conducted five user studies with 98 participants. Using a developed tabletop device, we confirmed the optimal number of actuators and the ideal skin-stretching design effectively increases the user’s body ownership. Additionally, we explored the maximum visual threshold for forearm bending and the minimum detectable bending direction angle when using skin-stretching in VR. Finally, our study demonstrates that using ArmDeformation in VR applications enhances user realism and enjoyment compared to relying on visual feedback alone.",Virtual Reality; Body Illusion; Body Ownership; Skin-stretching,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users’ spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask “what’s over there?” or “how do I solve this math problem?” simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems, (2) examining GazePointAR’s pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.",augmented reality; voice assistants; gaze tracking; multimodal input; LLM; pointing gesture recognition,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,QuadStretcher: A Forearm-Worn Skin Stretch Display for Bare-Hand Interaction in AR/VR,CHI - Human Factors in Computing Systems,A*,"The paradigm of bare-hand interaction has become increasingly prevalent in Augmented Reality (AR) and Virtual Reality (VR) environments, propelled by advancements in hand tracking technology. However, a significant challenge arises in delivering haptic feedback to users’ hands, due to the necessity for the hands to remain bare. In response to this challenge, recent research has proposed an indirect solution of providing haptic feedback to the forearm. In this work, we present QuadStretcher, a skin stretch display featuring four independently controlled stretching units surrounding the forearm. While achieving rich haptic expression, our device also eliminates the need for a grounding base on the forearm by using a pair of counteracting tactors, thereby reducing bulkiness. To assess the effectiveness of QuadStretcher in facilitating immersive bare-hand experiences, we conducted a comparative user evaluation (n = 20) with a baseline solution, Squeezer. The results confirmed that QuadStretcher outperformed Squeezer in terms of expressing force direction and heightening the sense of realism, particularly in 3-DoF VR interactions such as pulling a rubber band, hooking a fishing rod, and swinging a tennis racket. We further discuss the design insights gained from qualitative user interviews, presenting key takeaways for future forearm-haptic systems aimed at advancing AR/VR bare-hand experiences.",Haptics; Wearable; AR; VR; Bare-Hand Interaction; Skin Stretch,Abstract,True,
ACM DL,conferencePaper,2024,"Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Space agencies are in the process of drawing up carefully thought-out Concepts of Operations (ConOps) for future human missions on the Moon. These are typically assessed and validated through costly and logistically demanding analogue field studies. While interactive simulations in Virtual Reality (VR) offer a comparatively cost-effective alternative, they have faced criticism for lacking the fidelity of real-world deployments. This paper explores the applicability of passive haptic interfaces in bridging the gap between simulated and real-world ConOps assessments. Leveraging passive haptic props (equipment mockup and astronaut gloves), we virtually recreated the Apollo 12 mission procedure and assessed it with experienced astronauts and other space experts. Quantitative and qualitative findings indicate that haptics increased presence and embodiment, thus improving perceived simulation fidelity and validity of user reflections. We conclude by discussing the potential role of passive haptic modalities in facilitating early-stage ConOps assessments for human endeavours on the Moon and beyond.",presence; virtual reality; embodiment; passive haptic feedback; concepts of operations; scenario assessment; space exploration,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,ErgoPulse: Electrifying Your Lower Body With Biomechanical Simulation-based Electrical Muscle Stimulation Haptic System in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"This study presents ErgoPulse, a system that integrates biomechanical simulation with electrical muscle stimulation (EMS) to provide kinesthetic force feedback to the lower-body in virtual reality (VR). ErgoPulse features two main parts: a biomechanical simulation part that calculates the lower-body joint torques to replicate forces from VR environments, and an EMS part that translates torques into muscle stimulations. In the first experiment, we assessed users’ ability to discern haptic force intensity and direction, and observed variations in perceived resolution based on force direction. The second experiment evaluated ErgoPulse’s ability to increase haptic force accuracy and user presence in both continuous and impulse force VR game environments. The experimental results showed that ErgoPulse’s biomechanical simulation increased the accuracy of force delivery compared to traditional EMS, enhancing the overall user presence. Furthermore, the interviews proposed improvements to the haptic experience by integrating additional stimuli such as temperature, skin stretch, and impact.",Virtual Reality; Wearable Device; Simulation; Electrical Muscle Stimulation; Haptic; Biomechanics,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Augmenting Perceived Length of Handheld Controllers: Effects of Object Handle Properties,CHI - Human Factors in Computing Systems,A*,"In the realm of virtual reality (VR), shape-changing controllers have emerged as a means to enhance visuo-haptic congruence during user interactions. The major emphasis has been placed on manipulating the inertia tensor of a shape-changing controller to control the perceived shape. This paper delves deeper by exploring how the material properties of the controller’s handle, distinct from the inertial information, affect the perceived shape, focusing on the perceived length. We conducted three perceptual experiments to examine the effects of the handle’s softness, thermal conductivity, and texture, respectively. Results demonstrated that a softer handle increases the perceived length, whereas a handle with higher thermal conductivity reduces it. Texture, in the form of varying bumps, also alters the length perception. These results provide more comprehensive knowledge of the intricate relationship between perceived length and controller handle properties, expanding the design alternatives for shape-changing controllers for immersive VR experiences.",Virtual Reality; Dynamic Touch; Handheld Controller; Haptic Perceptual Cue; Shape Perception,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Experiencing Dynamic Weight Changes in Virtual Reality Through Pseudo-Haptics and Vibrotactile Feedback,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) objects react dynamically to users’ touch interactions in real-time. However, experiencing changes in weight through the haptic sense remains challenging with consumer VR controllers due to their limited vibrotactile feedback. While prior works successfully applied pseudo-haptics to perceive absolute weight by manipulating the control-display (C/D) ratio, we continuously adjusted the C/D ratio to mimic weight changes. Vibrotactile feedback additionally emphasises the modulation in the virtual object’s physicality. In a study (N=18), we compared our multimodal technique with pseudo-haptics alone and a baseline condition to assess participants’ experiences of weight changes. Our findings demonstrate that participants perceived varying degrees of weight change when the C/D ratio was adjusted, validating its effectiveness for simulating dynamic weight in VR. However, the additional vibrotactile feedback did not improve weight change perception. This work extends the understanding of designing haptic experiences for lightweight VR systems by leveraging perceptual mechanisms.",virtual reality; haptic illusions; pseudo-haptics; weight perception; vibration; multisensory integration; weight change,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring Mobile Devices as Haptic Interfaces for Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Dedicated handheld controllers facilitate haptic experiences of virtual objects in mixed reality (MR). However, as mobile MR becomes more prevalent, we observe the emergence of controller-free MR interactions. To retain immersive haptic experiences, we explore the use of mobile devices as a substitute for specialised MR controller. In an exploratory gesture elicitation study (n = 18), we examined users’ (1) intuitive hand gestures performed with prospective mobile devices and (2) preferences for real-time haptic feedback when exploring haptic object properties. Our results reveal three haptic exploration modes for the mobile device, as an object, hand substitute, or as an additional tool, and emphasise the benefits of incorporating the device’s unique physical features into the object interaction. This work expands the design possibilities using mobile devices for tangible object interaction, guiding the future design of mobile devices for haptic MR experiences.",mixed reality; mobile phones; haptic feedback; gesture elicitation; haptic exploration; haptic interfaces; mobile gestures,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,InflatableBots: Inflatable Shape-Changing Mobile Robots for Large-Scale Encountered-Type Haptics in VR,CHI - Human Factors in Computing Systems,A*,"We introduce InflatableBots, shape-changing inflatable robots for large-scale encountered-type haptics in VR. Unlike traditional inflatable shape displays, which are immobile and limited in interaction areas, our approach combines mobile robots with fan-based inflatable structures. This enables safe, scalable, and deployable haptic interactions on a large scale. We developed three coordinated inflatable mobile robots, each of which consists of an omni-directional mobile base and a reel-based inflatable structure. The robot can simultaneously change its height and position rapidly (horizontal: 58.5 cm/sec, vertical: 10.4 cm/sec, from 40 cm to 200 cm), which allows for quick and dynamic haptic rendering of multiple touch points to simulate various body-scale objects and surfaces in real-time across large spaces (3.5 m x 2.5 m). We evaluated our system with a user study (N = 12), which confirms the unique advantages in safety, deployability, and large-scale interactability to significantly improve realism in VR experiences.",Virtual Reality; Haptics; Inflatables; Mobile Robots; Shape-Changing Interfaces; Encountered-Type Haptics,Keywords,True,
ACM DL,conferencePaper,2024,"VeeR: Exploring the Feasibility of Deliberately Designing VR Motion that Diverges from Mundane, Everyday Physical Motion to Create More Entertaining VR Experiences",CHI - Human Factors in Computing Systems,A*,"This paper explores the feasibility of deliberately designing VR motion that diverges from users’ physical movements to turn mundane, everyday transportation motion (e.g., metros, trains, and cars) into more entertaining VR motion experiences, in contrast to prior car-based VR approaches that synchronize VR motion to physical car movement exactly. To gain insight into users’ preferences for veering rate and veering direction for turning (left/right) and pitching (up/down) during the three phases of acceleration (accelerating, cruising, and decelerating), we conducted a formative, perceptual study (n=24) followed by a VR experience evaluation (n=18), all conducted on metro trains moving in a mundane, straight-line motion. Results showed that participants preferred relatively high veering rates, and preferred pitching upward during acceleration and downward during deceleration. Furthermore, while veering decreased comfort as expected, it significantly enhanced immersion (p&lt;.01) and entertainment (p&lt;.001) and the overall experience, with comfort being considered, was preferred by 89% of participants.",Virtual Reality; User Experience Design; Motion Sensation; Opportunistic Haptic,Keywords,True,
ACM DL,conferencePaper,2024,Paired-EMS: Enhancing Electrical Muscle Stimulation (EMS)-based Force Feedback Experience by Stimulating Both Muscles in Antagonistic Pairs,CHI - Human Factors in Computing Systems,A*,"Electrical Muscle Stimulation (EMS) has emerged as a key wearable haptic feedback technology capable of simulating a wide range of force feedback, such as the impact force of boxing punches, the weight of virtual objects, and the reaction force from pushing on a wall. To simulate these external forces, EMS stimulates the muscles that oppose (i.e. antagonistic to) the actual muscles that users activate, causing involuntary muscle contraction and haptic sensations that differ from real-world experiences. In this work, we propose Paired-EMS which simultaneously stimulates both the muscles that users activate and that prior EMS stimulates (i.e. antagonistic muscle pairs) to enhance the external force feedback experience. We first conducted a small formative study (n=8) to help design the stimulation intensity of muscle pairs, then conducted a user experience study to evaluate Paired-EMS vs. prior EMS approaches for both isometric and isotonic user actions. Study results (n=32) showed that Paired-EMS significantly improved realism, harmony, and entertainment (p&lt;.05) with similar comfort (p&gt;.36), and was overall preferred by 78% of participants (p&lt;.01).",Virtual Reality; User Experience Design; Haptic,Keywords,True,
ACM DL,conferencePaper,2024,ALCool: Utilizing Alcohol's Evaporative Cooling for Ubiquitous Cold Sensation Feedback,CHI - Human Factors in Computing Systems,A*,"Tactile technologies are important for novel user experiences. Among several tactile submodalities, cold sensation is essential for realistically portraying materials and environments. However, current cold presentations such as Peltier devices face challenges like low energy efficiency and the need for complicated equipment. To address these, we suggest leveraging alcohol’s endothermic property during evaporation. Our prototype, a wristwatch wearable with a fan, capitalizes on alcohol’s high volatility by absorbing ambient heat upon evaporation. The device further enhances the cooling effect by circulating air around the skin. This approach simplifies the setup required for cooling technologies and is more energy-efficient than Peltier-based systems. We also integrated perfume, which is a mixture of alcohol and scent substance, and presented a unique cooling and scent experience. The use of alcohol as a cooling method was not considered conventional, but social changes after COVID-19 made it easy to obtain a tiny amount of alcohol.",virtual reality; alcohol; chemical haptics,Keywords,True,
ACM DL,conferencePaper,2024,AirPush: A Pneumatic Wearable Haptic Device Providing Multi-Dimensional Force Feedback on a Fingertip,CHI - Human Factors in Computing Systems,A*,"Finger wearable haptic devices enrich virtual reality experiences by offering haptic feedback corresponding to the virtual environment. However, despite the effectiveness of current finger wearable haptic devices in delivering haptic feedback, many are often constrained in their ability to provide force feedback across a diverse range of directions or to sustain it. Therefore, we present AirPush, a finger wearable haptic device capable of generating continuously adjustable force feedback in multiple directions using compressed air. To evaluate its usability, we conducted a technical evaluation and four user studies: (1) we obtained the user’s perceptual thresholds of angles under different directions on horizontal and vertical planes, (2) in perception studies, we found that users can identify five different magnitudes of force and eight different motion when using AirPush, and (3) using it in VR applications, we confirmed that users felt more realistic and immersed when using AirPush than the HTC VIVE Controller or AirPush with a fixed nozzle.",Virtual Reality; Compressed Air; Finger Wearable Haptic Device,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Shaping Compliance: Inducing Haptic Illusion of Compliance in Different Shapes with Electrotactile Grains,CHI - Human Factors in Computing Systems,A*,"Compliance, the degree of displacement under applied force, is pivotal in determining the material perception when touching an object. Vibrotactile actuators can be used for creating grain-based virtual compliance, but they have poor spatial resolution and a limiting rigid form factor. We propose a novel electrotactile compliance illusion that renders grains of electrical pulses on an electrode array in response to finger force changes. We demonstrate its ability to render compliance in distinct shapes through a thin, lightweight, and flexible finger-worn interface. Detailed technical parameters and the implementation of our device are provided. A controlled experiment confirms the technique can (1) create virtual compliance; (2) adjust the compliance magnitude with grain and electrode parameters; and (3) render compliance with specific shapes. In three example applications, we present how this illusion can enhance physical objects, elements in graphical user interfaces, and virtual reality experiences.",Haptics; virtual reality.; compliance; electrotactile; haptic illusion,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Stick&amp;Slip: Altering Fingerpad Friction via Liquid Coatings,CHI - Human Factors in Computing Systems,A*,"We present Stick&amp;Slip, a novel approach that alters friction between the fingerpad &amp; surfaces by depositing liquid droplets that coat the fingerpad. The liquid coating modifies the finger's coefficient of friction, allowing users to feel surfaces up to ±60% more slippery or sticky. We selected our fluids to rapidly evaporate so that the surface returns to its original friction. Unlike traditional friction-feedback, such as electroadhesion or vibration, our approach: (1) alters friction on a wide range of surfaces and geometries, making it possible to modulate nearly any non-absorbent surface; (2) scales to many objects without requiring instrumenting the target surfaces (e.g., with conductive electrode coatings or vibromotors); and (3) both in/decreases friction via a single device. We identified nine liquids and characterized their practicality by measuring evaporation rates, etc. To illustrate the applicability of our approach, we demonstrate how it enables friction in virtual/mixed-reality or, even, while using everyday objects/tools.",mixed reality; haptics; grasp; friction; adhesion,Keywords,True,
ACM DL,conferencePaper,2024,Facilitating Virtual Reality Integration in Medical Education: A Case Study of Acceptability and Learning Impact in Childbirth Delivery Training,CHI - Human Factors in Computing Systems,A*,"Advancements in Virtual Reality (VR) technology have opened new frontiers in medical education, igniting interest among medical educators to incorporate it into mainstream curriculum, complementing traditional training modalities such as manikin training. Despite numerous VR simulators on the market, their uptake in medical education remains limited. This paper explores the acceptability and educational effectiveness of VR in the context of vaginal childbirth delivery training, with the simulator providing a walkthrough for the second and third stages of labour, contrasting it with established manikin-based methods. We conducted a large-scale empirical study with 117 medical students, revealing a significant 24.9% improvement in knowledge scores when using VR as compared to manikin. However, VR received significantly lower self-reported feasibility scores in Confidence, Usability, Enjoyment, Feedback and Presence, indicating low acceptance. The study provides critical insights into the relationship between technological innovation and educational impact, guiding future integration of VR into medical training curricula.",Virtual Reality; User Experience Design; Gynaecology; Medical Education; Obstetrics &amp,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,MR Microsurgical Suture Training System with Level-Appropriate Support,CHI - Human Factors in Computing Systems,A*,"The integration of advanced technologies in healthcare necessitates the development of systems accommodating the daily routines in medical practices. Neurosurgeons, in particular, require extensive practice in microsurgical suturing in the long term, even in the busy routine of a medical practice. This study collaboratively developed a Mixed Reality system with neurosurgeons to support self-training in microscopic suturing. Based on the neurosurgeons’ opinions, we implemented a level-appropriate microsurgical suture training system. For novices, the system offers shadow-matching training to support the practice of precise movements under the high-sensitivity environment of the microscope. For intermediates, it provides a real-time feedback system, which allows users to practice attention to details. Evaluation involved testing the novice system on students with no medical background and the intermediate system on neurosurgery residents. The effectiveness of the system was demonstrated through the experimental results and subsequent discussion.",MR; CV-based tracking; Neurosurgical training; Real-time feedback; Visual feedback,Abstract,True,
ACM DL,conferencePaper,2024,LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults,CHI - Human Factors in Computing Systems,A*,"The decline of cognitive inhibition significantly impacts older adults’ quality of life and well-being, making it a vital public health problem in today’s aging society. Previous research has demonstrated that Virtual reality (VR) exergames have great potential to enhance cognitive inhibition among older adults. However, existing commercial VR exergames were unsuitable for older adults’ long-term cognitive training due to the inappropriate cognitive activation paradigm, unnecessary complexity, and unbefitting difficulty levels. To bridge these gaps, we developed a customized VR cognitive training exergame (LightSword) based on Dual-task and Stroop paradigms for long-term cognitive inhibition training among healthy older adults. Subsequently, we conducted an eight-month longitudinal user study with 12 older adults aged 60 years and above to demonstrate the effectiveness of LightSword in improving cognitive inhibition. After the training, the cognitive inhibition abilities of older adults were significantly enhanced, with benefits persisting for 6 months. This result indicated that LightSword has both short-term and long-term effects in enhancing cognitive inhibition. Furthermore, qualitative feedback revealed that older adults exhibited a positive attitude toward long-term training with LightSword, which enhanced their motivation and compliance.",Virtual Reality; Health; Older Adults,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,“X-Ray Vision” as a Compensatory Augmentation for Slowing Cognitive Map Decay in Older Adults,CHI - Human Factors in Computing Systems,A*,"Safe and efficient navigation often relies on the development and retention of accurate cognitive maps that include inter-landmark relations. For many older adults, cognitive maps are difficult to form and remember over time, which introduces serious challenges for independence and mobility. To address this problem, we explore an innovative compensatory augmentation solution enabling enhanced inter-landmark learning via an “X-Ray Vision” simulation. Results with (n=45) user study participants suggest superior older adult cognitive map retention over time from a single learning session with the augmentation versus a control condition without the augmentation. Furthermore, results characterize differences in decay of cognitive maps between older adults and a control of younger adults. These findings suggest important implications for future augmented reality devices and the ways in which they can be used to promote memory and independence among older adults.",Augmented Reality; Older Adults; Cognitive Maps; X-Ray Vision,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,A Survey On Measuring Presence in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Presence is a defining element of virtual reality (VR), but it is also increasingly used when assessing mixed reality (MR) experiences. The increased interest in measuring presence in MR and recent works underpinning the specific nature of presence in MR raise the question of the current state and practice of assessing presence in MR. To address this question, we present an analysis of more than 320 studies that report on presence measurements in MR. Our analysis showed that questionnaires are the dominant measurement but also identify problematic trends that stem from the lack of a generally agreed-upon concept or measurement for presence in MR. More specifically, we show that using measurements that are not validated in MR or custom questionnaires limiting the comparability of results is commonplace and could contribute to a looming replication crisis in an increasingly relevant field.",Augmented Reality; Mixed Reality; Virtual Reality; Extended Reality; Sense of Presence; Spatial Presence,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,ARCADIA: A Gamified Mixed Reality System for Emotional Regulation and Self-Compassion,CHI - Human Factors in Computing Systems,A*,"Mental health and wellbeing have become one of the significant challenges in global society, for which emotional regulation strategies hold the potential to offer a transversal approach to addressing them. However, the persistently declining adherence of patients to therapeutic interventions, coupled with the limited applicability of current technological interventions across diverse individuals and diagnoses, underscores the need for innovative solutions. We present ARCADIA, a Mixed-Reality platform strategically co-designed with therapists to enhance emotional regulation and self-compassion. ARCADIA comprises several gamified therapeutic activities, with a strong emphasis on fostering patient motivation. Through a dual study involving therapists and mental health patients, we validate the fully functional prototype of ARCADIA. Encouraging results are observed in terms of system usability, user engagement, and therapeutic potential. These findings lead us to believe that the combination of Mixed Reality and gamified therapeutic activities could be a significant tool in the future of mental health.",mixed reality; mental health; gamification; emotional regulation,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Behind the Scenes: Adapting Cinematography and Editing Concepts to Navigation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Teleportation is a popular method of navigation in virtual reality (VR) because it does not induce symptoms of VR sickness, such as nausea and disorientation. However, teleportation may reduce spatial awareness, causing users to miss important aspects of their surroundings. We present ACTIVE, a novel approach to teleportation that uses techniques from cinematography to enhance the user experience of navigation in VR. ACTIVE adapts heuristics from continuity editing to dynamically reposition and reorient the camera after teleportation. This approach aims to improve the aesthetic quality of entities and environmental features while respecting users’ intended trajectory through the virtual environment. In a user study, we found that even though ACTIVE did not improve users’ recall of which entities were present in the environment, it increased engagement by significantly improving aesthetic appeal. Lastly, despite removing some agency from users, ACTIVE had no impact on presence or VR sickness compared to teleportation.",virtual reality; navigation; user engagement; multi-objective optimization; cinematography,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Implementation of Virtual Reality Motivated Physical Activity via Omnidirectional Treadmill in a Supported Living Facility for Older Adults: A Mixed-Methods Evaluation.: Virtual reality to motivate physical activity for older adults,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) can support healthy ageing, but few devices have been trialed with frail older adults to increase physical activity. We conducted a preliminary mixed-methods implementation evaluation of an omnidirectional VR treadmill and a static VR experience with seven older adults over a six-week period in a supported living facility. Frequency of use and pre-post physical functioning measures were collected, mainly to establish technology suitability based on person characteristics. Diary entries following technology use, resident focus group and staff interview revealed technology acceptance and perceived potential for increasing physical activity, health and wellbeing through accessing virtual environments, which motivated continued activity. Results demonstrated technology suitability for a range of older adults with various mobility and physical impairments. However, residents noted interest in a seated treadmill for physical activity without perceived risks of falls with standing treadmills. Staff raised considerations around care home implementations including usability, cost and space.",virtual reality; physical activity; innovation; technology; Social care,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,MobileGravity: Mobile Simulation of a High Range of Weight in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Simulating accurate weight forces in Virtual Reality (VR) is an unsolved challenge. Therefore, providing real weight sensations by transferring liquid mass has emerged as a promising approach. However, key objectives conceptually interfere with each other. In particular, previous designs that support a high range of weight or high flow rate lack mobility. In this work, we present MobileGravity, a system, that decouples the weight-changing object from the liquid supply and the pump. It enables weight changes of up to 1 kg at a rate of 235 g/s and allows the user to walk around freely. Through a study with 30 participants, we show that the system enables users to perceive the weight of different virtual objects and enhances realism, as well as enjoyment.",virtual reality; haptics; weight perception; weight simulation,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,The Impact of Avatar Completeness on Embodiment and the Detectability of Hand Redirection in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"To enhance interactions in VR, many techniques introduce offsets between the virtual and real-world position of users’ hands. Nevertheless, such hand redirection (HR) techniques are only effective as long as they go unnoticed by users—not disrupting the VR experience. While several studies consider how much unnoticeable redirection can be applied, these focus on mid-air floating hands that are disconnected from users’ bodies. Increasingly, VR avatars are embodied as being directly connected with the user’s body, which provide more visual cue anchoring, and may therefore reduce the unnoticeable redirection threshold. In this work, we studied more complete avatars and their effect on the sense of embodiment and the detectability of HR. We found that higher avatar completeness increases embodiment, and we provide evidence for the absence of practically relevant effects on the detectability of HR.",Virtual reality; illusions; avatar embodiment; detection thresholds; hand redirection,Title_Keywords,True,
ACM DL,conferencePaper,2024,Stochastic Machine Witnesses at Work: Today's Critiques of Taylorism are Inadequate for Workplace Surveillance Epistemologies of the Future,CHI - Human Factors in Computing Systems,A*,"I argue that epistemologies of workplace surveillance are shifting in fundamental ways, and so critiques must shift accordingly. I begin the paper by relating Scientific Management to Human-Centred Computing’s ways of knowing through a study of ‘metaverse’ virtual reality workplaces. From this, I develop two observations. The first is that today’s workplace measurement science does not resemble the science that Taylor developed for Scientific Management. Contemporary workplace science is more passive, more intermediated and less controlled. The second observation is that new forms of workplace measurement challenge the norms of empirical science. Instead of having credentialed human witnesses observe phenomena and agree facts about them, we instead make outsourced, uncredentialed stochastic machine witnesses responsible for producing facts about work. With these observations in mind, I assert that critiques of workplace surveillance still framed by Taylorism will not be fit for interrogating workplace surveillance practices of the future.",Metaverse; Taylorism; Ubiquitous Computing; Neo-Taylorism; Scientific Management; Work Measurement; Workplace Surveillance,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Cohabitant: The Design, Implementation, and Evaluation of a Virtual Reality Application for Interfaith Learning and Empathy Building",CHI - Human Factors in Computing Systems,A*,"Lack of interfaith communication often gives rise to prejudice and group-based conflict in multi-faith societies. Nurturing this communication via interfaith learning may reduce this conflict by fostering interfaith empathy. HCI has a dearth of knowledge on interfaith coexistence and empathy building. To address this gap, we present the design, implementation, and usability of Cohabitant: a virtual reality (VR) application that promotes interfaith learning and empathy. Cohabitant’s design is theoretically underpinned by Allport’s intergroup contact theory and informed by insights from a participatory workshop we ran with members of three religious groups: Christians, Hindus, and Muslims. Our evaluation study, combining quantitative and qualitative data from 30 participants, suggests that Cohabitant may enhance general interpersonal empathy, but falls short for ethnocultural empathy. We discuss the possible design and policy implications of using this kind of VR technology for interfaith learning and empathy building.",Virtual Reality; Learning; Empathy; Interfaith,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Navigating the Virtual Gaze: Social Anxiety's Role in VR Proxemics,CHI - Human Factors in Computing Systems,A*,"For individuals with Social Anxiety (SA), interacting with others can be a challenging experience, a concern that extends into the virtual world. While technology has made significant strides in creating more realistic virtual human agents (VHA), the interplay of gaze and interpersonal distance when interacting with VHAs is often neglected. This paper investigates the effect of dynamic and static Gaze animations in VHAs on interpersonal distance and their relation to SA. A Bayesian analysis shows that static centered and dynamic centering gaze led participants to stand closer to VHAs than static averted and dynamic averting gaze, respectively. In the static gaze conditions, this pattern was found to be reversed in SA: participants with higher SA kept larger distances for static-centered gaze than for averted gaze VHAs. These findings update theory, elucidate how nuanced interactions with VHAs must be designed, and offer renewed guidelines for pleasant VHA interaction design.",Virtual Reality; Proxemics; Virtual Human Agents,Keywords,True,
ACM DL,conferencePaper,2024,LLMR: Real-time Prompting of Interactive Worlds using Large Language Models,CHI - Human Factors in Computing Systems,A*,"We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR’s cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.",mixed reality; artificial intelligence; large language model; spatial reasoning,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Meaning Follows Purpose: Unravelling the Architectural Design Conventions in the Contemporary Metaverse,CHI - Human Factors in Computing Systems,A*,"Thousands of people regularly meet, work and play in the architectural spaces that the metaverse offers today. Yet despite the creative potential to disrupt how the built environment is represented, there exists a prevalent belief that the architectural design of the metaverse is rather conventional and reliant on simulating physical reality. We investigated this claim by conducting a design critique study of the most apparent architectural design conventions within the current most popular metaverse platforms, as determined by a scoping review and Google Trends analysis. Based on the opinions of 21 architectural experts on the design of interiors, buildings, and plazas within these platforms, we elicited three overarching design conventions that capture the representation, engagement, and purpose of metaverse architecture. By discussing the impact of these conventions on architectural quality, we inform the future design of metaverse spaces to more purposefully, and perhaps less frequently, use realism to convey meaning.",Virtual Reality; User Study; Metaverse; Virtual Worlds; Scoping Review; Architectural Design; Architectural Drawing; Cyberspace; Design Science; Design Studies,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Unlocking Understanding: An Investigation of Multimodal Communication in Virtual Reality Collaboration,CHI - Human Factors in Computing Systems,A*,"Communication in collaboration, especially synchronous, remote communication, is crucial to the success of task-specific goals. Insufficient or excessive forms of communication may lead to detrimental effects on task performance while increasing mental fatigue. However, identifying which combinations of communication modalities provide the most efficient transfer of information in collaborative settings will greatly improve collaboration. To investigate this, we developed a remote, synchronous, asymmetric VR collaborative assembly task application, where users play the role of either mentor or mentee, and were exposed to different combinations of three communication modalities: voice, gestures, and gaze. Through task-based experiments with 25 pairs of participants (50 individuals), we evaluated quantitative and qualitative data and found that gaze did not differ significantly from multiple combinations of communication modalities. Our qualitative results indicate that mentees experienced more difficulty and frustration in completing tasks than mentors, with both types of users preferring all three modalities to be present.",virtual reality; communication; collaboration,Title_Keywords,True,
ACM DL,conferencePaper,2024,Using the Visual Language of Comics to Alter Sensations in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) excels at altering what we see but non-visual sensations are difficult to augment. To augment non-visual sensations in AR, we draw on the visual language of comic books. Synthesizing comic studies, we create a design space describing how to use comic elements (e.g., onomatopoeia) to depict non-visual sensations (e.g., hearing). To demonstrate this design space, we built eight demos, such as speed lines to make a user think they are faster and smell lines to make a scent seem stronger. We evaluate these elements in a qualitative user study (N=20) where participants performed everyday tasks with comic elements added as augmentations. All participants stated feeling a change in perception for at least one sensation, with perceived changes detected by between four participants (touch) and 15 participants (hearing). The elements also had positive effects on emotion and user experience, even when participants did not feel changes in perception.",augmented reality; sensory augmentation; comics,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Assessing User Apprehensions About Mixed Reality Artifacts and Applications: The Mixed Reality Concerns (MRC) Questionnaire,CHI - Human Factors in Computing Systems,A*,"Current research in Mixed Reality (MR) presents a wide range of novel use cases for blending virtual elements with the real world. This yet-to-be-ubiquitous technology challenges how users currently work and interact with digital content. While offering many potential advantages, MR technologies introduce new security, safety, and privacy challenges. Thus, it is relevant to understand users’ apprehensions towards MR technologies, ranging from security concerns to social acceptance. To address this challenge, we present the Mixed Reality Concerns (MRC) Questionnaire, designed to assess users’ concerns towards MR artifacts and applications systematically. The development followed a structured process considering previous work, expert interviews, iterative refinements, and confirmatory tests to analytically validate the questionnaire. The MRC Questionnaire offers a new method of assessing users’ critical opinions to compare and assess novel MR artifacts and applications regarding security, privacy, social implications, and trust.",Mixed Reality; Privacy; Security; Trust; Safety; Concerns; Social Acceptance; User Apprehensions,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Development and Validation of the Collision Anxiety Questionnaire for VR Applications,CHI - Human Factors in Computing Systems,A*,"The high degree of sensory immersion is a distinctive feature of head-mounted virtual reality (VR) systems. While the visual detachment from the real world enables unique immersive experiences, users risk collisions due to their inability to perceive physical obstacles in their environment. Even the mere anticipation of a collision can adversely affect the overall experience and erode user confidence in the VR system. However, there are currently no valid tools for assessing collision anxiety. We present the iterative development and validation of the Collision Anxiety Questionnaire (CAQ), involving an exploratory and a confirmatory factor analysis with a total of 159 participants. The results provide evidence for both discriminant and convergent validity and a good model fit for the final CAQ with three subscales: general collision anxiety, orientation, and interpersonal collision anxiety. By utilizing the CAQ, researchers can examine potential confounding effects of collision anxiety and evaluate methods for its mitigation.",virtual reality; discomfort; user experience; fear; assessment; collision anxiety,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts’ workflow. To further improve comparison, we have designed and implemented a Branching&amp;Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&amp;Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.",immersive analytics; data science; interaction; 3D UI &amp; computational notebook system,Title_Abstract,True,
ACM DL,conferencePaper,2024,Virtual Unreality: Augmentation-Oriented Ideation Through Design Cards,CHI - Human Factors in Computing Systems,A*,"While realism is a common design goal for virtual reality (VR), VR also offers opportunities that are impossible in the real world (e.g., telekinesis). So far, there is no design support to exploit the potential of such “impossible” augmentations, especially for serious applications. We developed a card set and a workshop format, which features 15 opportunities to facilitate the ideation of augmentation-oriented VR. We piloted the method in five workshops with people in the early stages of developing a VR application (N=35). Participants found the cards easy to use and to inspire viable new concepts that differed from earlier ideas. Analysis of the concepts with interaction criticism identified two strategies: (1) augmentations that are only loosely related to the purpose of the application, simply to increase “fun”, and (2) augmentations that are closely related to the core purpose and thereby subtly facilitate its fulfillment. The latter has the greater potential.",virtual reality; design tools; augmented human capabilities; superpowers,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,MineXR: Mining Personalized Extended Reality Interfaces,CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences. This knowledge is challenging to obtain without the widespread adoption of XR devices. We introduce &nbsp;MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data. &nbsp;MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset. Using &nbsp;MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications. We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement. Our open-source tools and data support researchers and designers in developing future XR interfaces.",Extended Reality; Datasets; Personalized UI,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"UI Mobility Control in XR: Switching UI Positionings between Static, Dynamic, and Self Entities",CHI - Human Factors in Computing Systems,A*,"Extended reality (XR) has the potential for seamless user interface (UI) transitions across people, objects, and environments. However, the design space, applications, and common practices of 3D UI transitions remain underexplored. To address this gap, we conducted a need-finding study with 11 participants, identifying and distilling a taxonomy based on three types of UI placements — affixed to static, dynamic, or self entities. We further surveyed 113 commercial applications to understand the common practices of 3D UI mobility control, where only 6.2% of these applications allowed users to transition UI between entities. In response, we built interaction prototypes to facilitate UI transitions between entities. We report on results from a qualitative user study (N=14) on 3D UI mobility control using our FingerSwitches technique, which suggests that perceived usefulness is affected by types of entities and environments. We aspire to tackle a vital need in UI mobility within XR.",Virtual Reality; Extended Reality; Hand Gestures; Embodied Interactions; Mode Switching; UI Mobility; User Interface Behaviors,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,On the Role of Materials Experience for Novel Interactions with Digital Representations of Historical Pop-up and Movable Books,CHI - Human Factors in Computing Systems,A*,"Direct interaction with cultural heritage (CH) artefacts is frequently unavailable to visitors, offering an opportunity for HCI designers to explore integrating material aspects into digitally-mediated encounters with CH artefacts. We argue that a thorough understanding of the material experiences of CH artefacts can open a novel design space, enabling engaging and meaningful interactions with digital representations. Capitalising on this potential, we present a user study where we systematically explore the material experiences of historic pop-up and movable books. Our analysis identifies five key material qualities to inspire augmentation: fold-ability, slide-ability, tear-ability, age-ability, and print-ability. Highlighting how these material qualities can inspire novel interactions with their digital representations, we present two extended-reality (XR) prototypes of a CH book. With our work, we present HCI designers with a novel approach on designing CH experiences, firmly rooted in materiality, challenging the prevalent paradigms of ‘technology-driven’ or ‘as-realistic-as-possible’ sensory experiences often found in CH-HCI.",Mixed Reality; Virtual Reality; Materiality; Cultural Heritage; Books; Materials Experience,Keywords,True,
ACM DL,conferencePaper,2024,Effects of Device Environment and Information Layout on Spatial Memory and Performance in VR Selection Tasks,CHI - Human Factors in Computing Systems,A*,"Virtual Reality systems are increasingly proposed as a platform for everyday interactive software. Many applications are dependent on actions such as navigation and selection, but it is not clear how well immersive environments support these basic activities. Previous studies have suggested advantages for spatial learning in VR, so we carried out a study that investigated two aspects of immersion on spatial memory and selection: the degree to which the user is immersed in the data, and whether the system uses immersive input and output. The study showed that more-immersive conditions had substantially worse selection performance, and did not improve spatial learning. However, most participants believed that the immersive conditions were better for learning object locations, and most people preferred the immersive layout and the HMD. Our study suggests that designers should be cautious about assuming that everyday software applications will benefit from being deployed in an immersive VR environment.",virtual reality; spatial memory; immersiveness; selection performance,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Rowing Beyond: Investigating Steering Methods for Rowing-based Locomotion in Virtual Environments,CHI - Human Factors in Computing Systems,A*,"Rowing has great potential in Virtual Reality (VR) exergames as it requires physical effort and uses physical motion to map the locomotion in a virtual space. However, rowing in VR is currently restricted to locomotion along one axis, leaving 2D and 3D locomotion out of the scope. To facilitate rowing-based locomotion, we implemented three steering techniques based on head, hands, and feet movements for 2D and 3D VR environments. To investigate these methods, we conducted a controlled experiment (N = 24) to assess the user performance, experience and VR sickness. We found that head steering leads to fast and precise steering in 2D and 3D, and hand steering is the most realistic. Feet steering had the largest performance difference between 2D and 3D but comparable precision to hands in 2D. Lastly, head steering is the least mentally demanding, and all methods had comparable VR sickness.",virtual reality; locomotion; exergame; steering; rowing,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Sicknificant Steps: A Systematic Review and Meta-analysis of VR Sickness in Walking-based Locomotion for Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Walking-based locomotion techniques in virtual reality (VR) can use redirection to enable walking in a virtual environment larger than the physical one. This results in a mismatch between the perceived virtual and physical movement, which is known to cause VR sickness. However, it is unclear if different types of walking techniques (e.g., resetting, reorientation, or self-overlapping spaces) affect VR sickness differently. To address this, we conducted a systematic review and meta-analysis of 96 papers published in 2016–2022 that measure VR sickness in walking-based locomotion. We find different VR sickness effects between types of redirection and between normal walking and redirection. However, we also identified several problems with the use and reporting of VR sickness measures. We discuss the challenges in understanding VR sickness differences between walking techniques and present guidelines for measuring VR sickness in locomotion studies.",locomotion; walking; virtual; vr; reality; sickness; ssq; vrise,Title_Abstract,True,
ACM DL,conferencePaper,2024,Spatial Gaze Markers: Supporting Effective Task Switching in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Task switching can occur frequently in daily routines with physical activity. In this paper, we introduce Spatial Gaze Markers, an augmented reality tool to support users in immediately returning to the last point of interest after an attention shift. The tool is task-agnostic, using only eye-tracking information to infer distinct points of visual attention and to mark the corresponding area in the physical environment. We present a user study that evaluates the effectiveness of Spatial Gaze Markers in simulated physical repair and inspection tasks against a no-marker baseline. The results give insights into how Spatial Gaze Markers affect user performance, task load, and experience of users with varying levels of task type and distractions. Our work is relevant to assist physical workers with simple AR techniques and render task switching faster with less effort.",augmented reality; gaze interaction; eye-tracking; task switching; attention switching,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,The RayHand Navigation: A Virtual Navigation Method with Relative Position between Hand and Gaze-Ray,CHI - Human Factors in Computing Systems,A*,"In this paper, we introduce a novel Virtual Reality (VR) navigation method using gaze ray and hand, named RayHand navigation. It supports controlling navigation speed and direction by quickly indicating the initial direction using gaze and then using dexterous hand movement for controlling the speed and direction based on the relative position between the gaze ray and user's hand. We conducted a user study comparing our approach to the head-hand and torso-leaning-based navigation methods, and also evaluated their learning effect. The results showed that the RayHand and head-hand navigations were less physically demanding than the torso-leaning navigation, and the RayHand supported rich navigation experience with high hedonic quality and solved the issue of the user unintentionally stepping out from the designated interaction area. In addition, our approach showed a significant improvement over time with a learning effect.",virtual reality; navigation; gaze-ray,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Assessing the Influence of Visual Cues in Virtual Reality on the Spatial Perception of Physical Thermal Stimuli,CHI - Human Factors in Computing Systems,A*,"Advancements in haptics for Virtual Reality (VR) increased the quality of immersive content. Particularly, recent efforts to provide realistic temperature sensations have gained traction, but most often require very specialized or large complex devices to create precise thermal actuations. However, being largely detached from the real world, such a precise correspondence between the physical location of thermal stimuli and the shown visuals in VR might not be necessary for an authentic experience. In this work, we contribute the findings of a controlled experiment with 20 participants, investigating the spatial localization accuracy of thermal stimuli while having matching and non-matching visual cues of a virtual heat source in VR. Although participants were highly confident in their localization decisions, their ability to accurately pinpoint thermal stimuli was notably deficient.",virtual reality; user study; haptic feedback; temperature; thermal stimuli,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring Visualizations for Precisely Guiding Bare Hand Gestures in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Bare hand interaction in augmented or virtual reality (AR/VR) systems, while intuitive, often results in errors and frustration. However, existing methods, such as a static icon or a dynamic tutorial, can only inform simple and coarse hand gestures and lack corrective feedback. This paper explores various visualizations for enhancing precise hand interaction in VR. Through a comprehensive two-part formative study with 11 participants, we identified four types of essential information for visual guidance and designed different visualizations that manifest these information types. We further distilled four visual designs and conducted a controlled lab study with 15 participants to assess their effectiveness for various single- and double-handed gestures. Our results demonstrate that visual guidance significantly improved users’ gesture performance, reducing time and workload while increasing confidence. Moreover, we found that the visualization did not disrupt most users’ immersive VR experience or their perceptions of hand tracking and gesture recognition reliability.",Virtual reality; visual guidance; error visualization; hand gesture recognition.,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Improving Electromyographic Muscle Response Times through Visual and Tactile Prior Stimulation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Electromyography (EMG) enables hands-free interactions by detecting muscle activity at different human body locations. Previous studies have demonstrated that input performance based on isometric contractions is muscle-dependent and can benefit from synchronous biofeedback. However, it remains unknown whether stimulation before interaction can help to localize and tense a muscle faster. In a response-based VR experiment (N=21), we investigated whether prior stimulation using visual or tactile cues at four different target muscles (biceps, triceps, upper leg, calf) can help reduce the time to perform isometric muscle contractions. The results show that prior stimulation decreases EMG reaction times with visual, vibrotactile, and electrotactile cues. Our experiment also revealed important findings regarding learning and fatigue at the different body locations. We provide qualitative insights into the participants’ perceptions and discuss potential reasons for the improved interaction. We contribute with implications and use cases for prior stimulated muscle activation.",Virtual Reality; Electromyography; Physiological Sensing; Electrical Muscle Stimulation; Assistive Systems,Title_Keywords,True,
ACM DL,conferencePaper,2024,PhoneInVR: An Evaluation of Spatial Anchoring and Interaction Techniques for Smartphone Usage in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"When users wear a virtual reality (VR) headset, they lose access to their smartphone and accompanying apps. Past work has proposed smartphones as enhanced VR controllers, but little work has explored using existing smartphone apps and performing traditional smartphone interactions while in VR. In this paper, we consider three potential spatial anchorings for rendering smartphones in VR: On top of a tracked physical smartphone which the user holds (Phone-locked), on top of the user’s empty hand, as if holding a virtual smartphone (Hand-locked), or in a static position in front of the user (World-locked). We conducted a comparative study of target acquisition, swiping, and scrolling tasks across these anchorings using direct Touch or above-the-surface Pinch. Our findings indicate that physically holding a smartphone with Touch improves accuracy and speed for all tasks, and Pinch performed better with virtual smartphones. These findings provide a valuable foundation to enable smartphones in VR.",Virtual Reality; Smartphones; Touch Input,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,ClassMeta: Designing Interactive Virtual Classmate to Promote VR Classroom Participation,CHI - Human Factors in Computing Systems,A*,"Peer influence plays a crucial role in promoting classroom participation, where behaviors from active students can contribute to a collective classroom learning experience. However, the presence of these active students depends on several conditions and is not consistently available across all circumstances. Recently, Large Language Models (LLMs) such as GPT have demonstrated the ability to simulate diverse human behaviors convincingly due to their capacity to generate contextually coherent responses based on their role settings. Inspired by this advancement in technology, we designed ClassMeta, a GPT-4 powered agent to help promote classroom participation by playing the role of an active student. These agents, which are embodied as 3D avatars in virtual reality, interact with actual instructors and students with both spoken language and body gestures. We conducted a comparative study to investigate the potential of ClassMeta for improving the overall learning experience of the class.",collaborative learning; pedagogical agent; large language Model; VR classroom,Abstract,True,
ACM DL,conferencePaper,2024,Simulator-based Mixed Reality eVTOL Pilot Training: The Instructor Operator Station,CHI - Human Factors in Computing Systems,A*,"Advanced Air Mobility aircraft designs following the Simplified Vehicle Operations (SVO) concept require novel environments for practical and intuitive pilot training. Mixed Reality (MR) technologies can support immersive and interactive learning methods for operating several SVO aircraft, including electric Vertical Take-Off and Landing (eVTOL) systems. Despite this potential, regulatory guidelines for simulator-based eVTOL pilot training, especially concerning the Instructor Operator Station (IOS) design, are nascent and require substantive development. This paper investigates the feasibility of an MR eVTOL research simulator as a training tool for instructors. A user study forms the basis for a bottom-up categorization of the instructor’s performance shaping factors, which are pivotal for the design of an MR IOS. This paper contributes to the discourse on MR integration in pilot training by identifying key enhancements necessary for an IOS design.",virtual reality; mixed reality; aviation; cockpit interaction; flight simulator; flight training; human performance shaping factors,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Virtual Reality, Real Pedagogy: A Contextual Inquiry of Instructor Practices with VR Video",CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers promise in education given its immersive and socially engaging nature, but it can pose challenges for educators when creating VR-specific content. VR videos can function as a new educational tool for VR content creation due to their creation affordability and user-friendliness. However, little empirical research exists on how educators utilize VR videos and associated pedagogy in real classes. Our research employed a contextual inquiry, through in-person interviews and online surveys with 11 instructors to gain actionable insights from envisioned teaching scenarios for VR videos that are informed by actual instructional practices. Our study aims to understand the factors that motivate instructors’ adoption of VR videos, identify challenges educators face when incorporating VR videos into instructional units, and examine pedagogical adjustments when integrating VR videos into teaching. Through empirical evidence, we provide design implications for the development of VR-based learning experiences across diverse educational contexts. Our study also serves as a practical case of how VR can be adopted and integrated into education.",Virtual Reality; higher education; educational VR; 360-degree videos; VR videos,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,“Oh My God! It’s Recreating Our Room!” Understanding Children’s Experiences with A Room-Scale Augmented Reality Authoring Toolkit,CHI - Human Factors in Computing Systems,A*,"Human-Computer Interaction (HCI) and education researchers have applied Augmented Reality (AR) to support spatial thinking in K-12 education. However, fewer studies support spatial thinking through spatial exploration. Room-scale AR, a recent technology development, brings new opportunities not yet researched. We developed NetLogo AR, an AR authoring toolkit, that allows children to play with, design, and create room-scale AR experiences that combine AR with computational models. To acquire a deeper and more nuanced understanding of children’s interactions with this new technology, we conducted eight-week participatory design sessions with seven children aged 11-13. We analyzed 48 hours of video data, interview transcripts, and design artifacts. Children were enthusiastic and engaged in spatial thinking activities. We affirmed room-scale AR’s role in spatial exploration by comparing it with other supported modalities. Building on existing studies, we propose a new AR design framework around spatial movement and exploration that could help inform design decisions.",Augmented Reality; Participatory Design; AR and Children; AR Authoring Toolkit; NetLogo AR; Spatial AR,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Designing Instructions using Self-Determination Theory to Improve Motivation and Engagement for Learning Craft,CHI - Human Factors in Computing Systems,A*,"Recent HCI research has shown significant interest in investigating digital working instructions for guiding novices to perform manual tasks. While performance enhancement has been a primary focus, it is increasingly recognized that technology’s impact extends beyond objective metrics. Trainee motivation and engagement plays a pivotal role in enhancing learning outcomes and effectiveness. This paper investigates the utilization of principles from Self Determination Theory–clear attainable goals, meaningful rationale, and perspective taking–in designing multimedia instructions to enhance novice users’ indicators of psychological well-being. We present findings from an experiment involving real-world woodworking, where novice users, in a between-subjects study, followed interactive, in-situ projection-based guidance. Results demonstrate that adhering to SDT postulates can positively influence perceived competence, intrinsic motivation and task execution quality. These findings offer valuable insights for designing digital instructions to guide and train novices, emphasizing the importance of psychological well-being alongside task performance.",augmented reality; intrinsic motivation; instructions; perceived self–competence; self–determination theory; well–being,Keywords,True,
ACM DL,conferencePaper,2024,Doorways Do Not Always Cause Forgetting: Studying the Effect of Locomotion Technique and Doorway Visualization in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The “doorway effect” predicts that crossing an environmental boundary affects memory negatively. In virtual reality (VR), we can design the crossing and the appearance of such boundaries in non-realistic ways. However, it is unclear whether locomotion techniques like teleportation, which avoid crossing the boundary altogether, still induce the effect. Furthermore, it is unclear how different appearances of a doorway act as a boundary and thus induce the effect. To address these questions, we conducted two lab studies. First, we conceptually replicated prior doorway effect studies in VR using natural walking and teleportation. Second, we investigated the effect of five doorway visualizations, ranging from doors to portals. The results show no difference in object recognition performance due to the presence of a doorway, locomotion technique, or doorway visualization. We discuss the implications of these findings on the role of boundaries in event-based memory and the design of boundary interactions in VR.",recognition; forgetting; memory; walking; virtual; vr; environment; event; teleportation; object; reality; boundary; doorway; effect,Title_Abstract,True,
ACM DL,conferencePaper,2024,Investigating Virtual Reality Locomotion Techniques with Blind People,CHI - Human Factors in Computing Systems,A*,"Many Virtual Reality (VR) locomotion techniques have been proposed, but those explored for and with blind people are often custom-made or require specialized equipment. Consequently, it is unclear how popular techniques can support blind people’s VR locomotion, blocking access to most VR experiences. We implemented three popular techniques — Arm Swinging, Linear Movement (joystick-based steering), and Point &amp; Teleport — with minor adaptations for accessibility. We conducted a study with 14 blind participants consisting of navigation tasks with these techniques and a semi-structured interview. We found no differences in overall performance (e.g., completion time), but contrasting preferences. Findings highlight the challenges and advantages of each technique and participants’ strategies. We discuss, among others, how augmenting the techniques enabled blind people to navigate in VR, the greater control of movement of Arm Swinging, the simplicity and familiarity of Linear Movement, and the potential for efficiency and for scanning the environment of Point &amp; Teleport.",Navigation; Visual Impairment; Virtual Reality.; Arm Swinging; Linear Movement; Point and Teleport,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Stacked Retargeting: Combining Redirected Walking and Hand Redirection to Expand Haptic Retargeting's Coverage,CHI - Human Factors in Computing Systems,A*,"We present Stacked Retargeting—combining haptic retargeting and redirected walking—to maximise the use of passive proxy objects for VR haptics. Haptic retargeting work to date has considered stationary reaching and grasping interactions, and this inherently limits a proxy object’s scope. We consider exactly where this reaching and grasping occurs from, to increase the potential of each proxy. We present (a) a staged approach to implementing Stacked Retargeting, (b) five redirected walking approaches that enable users to arrive anywhere at the site of interaction, and (c) a usability magnitude estimation evaluation of these techniques. We demonstrate how Stacked Retargeting can meaningfully increase the practical use of proxy objects for VR haptics without degrading the user experience.",Virtual Reality; Haptics; Redirection,Keywords,True,
ACM DL,conferencePaper,2024,The Effect of Spatial Audio on Curvature Gains in VR Redirected Walking,CHI - Human Factors in Computing Systems,A*,"Redirected walking (RDW) is a technique that allows users to navigate larger physical spaces in virtual reality (VR) environments by manipulating the users’ view of the virtual world. In this study, we investigate the effect of adding spatial audio elements to curvature gains in RDW aiming to increase the perceptual threshold for the manipulation and allowing for higher levels of unnoticed redirection. We conducted a user study (n = 18), evaluating perceptual thresholds across conditions with and without spatial audio elements across different curvature gains. We found that spatial audio can significantly increase thresholds with a large effect size. This finding indicates the value of spatial audio for RDW. It could facilitate higher levels of redirection, while maintaining a convincing experience, leading to more freedom to navigate virtual environments in even smaller physical spaces.",virtual reality; spatial audio; redirected walking; curvature gains,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,'I Call Upon a Friend': Virtual Reality-Based Supports for Cognitive Reappraisal Identified through Co-designing with Adolescents,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers great promise to expand delivery models for therapeutic interventions to help adolescents develop adaptive emotion regulation skills. Cognitive reappraisal (CR) is an emotion regulation skill that involves changing your thinking to improve your emotional state. However, adolescents face developmental and implementation barriers to do CR successfully. To better understand adolescents’ (15-18 years) lived experience of CR challenges and how they envision VR could support their skills learning and transfer to everyday life, we ran three co-design workshops (N=69). Our research weaves together the workshop findings with prior literature to identify directions for future VR-based CR interventions. From our study results, we generated design strategies leveraging best practices of existing research: embedded and embodied scaffolds, providing different points of view, and externalizing the inner self. To illustrate these strategies in practice, we show how each would work in a challenging emotional scenario identified by adolescents.",participatory design; co-design; mental health; adolescents; emotion regulation; cognitive reappraisal,Title_Abstract,True,
ACM DL,conferencePaper,2024,Stairway to Heaven: A Gamified VR Journey for Breath Awareness,CHI - Human Factors in Computing Systems,A*,"Gamification and virtual reality (VR) are increasingly being explored for their potential to enhance mindful practices and well-being. We further explore the potential of gamification and VR for breath awareness and mindfulness, and contribute Stairway to Heaven, a VR artifact that combines gamification with respiratory sensor biofeedback to cultivate mindful awareness of breathing. In our mixed-method study with 21 participants, we evaluated the usability and effectiveness of our artifact in promoting breathing frequencies between 4 and 10 breaths per minute (BPM). We integrate breath-driven teleportation as a virtual locomotion technique (VLT) using respiratory biofeedback to gamify progression through a virtual wilderness. Additionally, we supplement our design with a mindfulness audio guide. The results of our user study showcase the potential of combining actionable gamification and VR, guided mindfulness, and breath-driven VLT to foster slow breathing self-regulation successfully.",Virtual Reality; Gamification; Mindfulness; Breathing Training; Respiration Sensor,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Designing for Human Operations on the Moon: Challenges and Opportunities of Navigational HUD Interfaces,CHI - Human Factors in Computing Systems,A*,"Future crewed missions to the Moon will face significant environmental and operational challenges, posing risks to the safety and performance of astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs) have proven effective in providing intuitive navigational support on Earth, the design of novel human-spaceflight solutions typically relies on costly and time-consuming analogue deployments, leaving the potential use of lunar HUDs largely under-explored. This paper explores an alternative approach by simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR) representation of the lunar environment. In evaluating these concepts with astronauts and other aerospace experts (n=25), our mixed methods study demonstrates the efficacy of simulated analogues in facilitating rapid design assessments of early-stage HUD solutions. We illustrate this by elaborating key design challenges and guidelines for future lunar HUDs. In reflecting on the limitations of our approach, we propose directions for future design exploration of human-machine interfaces for the Moon.",virtual reality; augmented reality; human factors; head-up display; astronaut; human space flight; human-system exploration; lunar exploration,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,WAVE: Anticipatory Movement Visualization for VR Dancing,CHI - Human Factors in Computing Systems,A*,"Dance games are one of the most popular game genres in Virtual Reality (VR), and active dance communities have emerged on social VR platforms such as VR Chat. However, effective instruction of dancing in VR or through other computerized means remains an unsolved human-computer interaction problem. Existing approaches either only instruct movements partially, abstracting away nuances, or require learning and memorizing symbolic notation. In contrast, we investigate how realistic, full-body movements designed by a professional choreographer can be instructed on the fly, without prior learning or memorization. Towards this end, we describe the design and evaluation of WAVE, a novel anticipatory movement visualization technique where the user joins a group of dancers performing the choreography with different time offsets, similar to spectators making waves in sports events. In our user study (N=36), the participants more accurately followed a choreography using WAVE, compared to following a single model dancer.",VR; dance game; dance instruction,Abstract,True,
ACM DL,conferencePaper,2024,Watch This! Observational Learning in VR Promotes Better Far Transfer than Active Learning for a Fine Psychomotor Task,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) holds great potential for psychomotor training, with existing applications using almost exclusively a ‘learning-by-doing’ active learning approach, despite the possible benefits of incorporating observational learning. We compared active learning (n=26) with different variations of observational learning in VR for a manual assembly task. For observational learning, we considered three levels of visual similarity between the demonstrator avatar and the user, dissimilar (n=25), minimally similar (n=26), or a self-avatar (n=25), as similarity has been shown to improve learning. Our results suggest observational learning can be effective in VR when combined with ‘hands-on’ practice and can lead to better far skill transfer to real-world contexts that differ from the training context. Furthermore, we found self-similarity in observational learning can be counterproductive when focusing on a manual task, and skills decay quickly without further training. We discuss these findings and derive design recommendations for future VR training.",Virtual Reality; Psychomotor; Skills Training; Active; Avatar Similarity; Observational,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems,CHI - Human Factors in Computing Systems,A*,"Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements—referred to as motion guidance. In motion guidance, the “feedforward’’ provides instructional cues of the motions that are to be performed, whereas the “feedback’’ provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR.",Extended Reality; Visualization; Design Space; Motion Guidance,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Metrics of Motor Learning for Analyzing Movement Mapping in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) techniques can modify how physical body movements are mapped to the virtual body. However, it is unclear how users learn such mappings and, therefore, how the learning process may impede interaction. To understand and quantify the learning of the techniques, we design new metrics explicitly for VR interactions based on the motor learning literature. We evaluate the metrics in three object selection and manipulation tasks, employing linear-translational and nonlinear-rotational gains and finger-to-arm mapping. The study shows that the metrics demonstrate known characteristics of motor learning similar to task completion time, typically with faster initial learning followed by more gradual improvements over time. More importantly, the metrics capture learning behaviors that task completion time does not. We discuss how the metrics can provide new insights into how users adapt to movement mappings and how they can help analyze and improve such techniques.",Interaction techniques; beyond-real interactions; motor adaptation; visual-motor mismatches,Title_Abstract,True,
ACM DL,conferencePaper,2024,WieldingCanvas: Interactive Sketch Canvases for Freehand Drawing in VR,CHI - Human Factors in Computing Systems,A*,"Sketching in Virtual Reality (VR) is challenging mainly due to the absence of physical surface support and virtual depth perception cues, which induce high cognitive and sensorimotor load. This paper presents WieldingCanvas, an interactive VR sketching platform that integrates canvas manipulations to draw lines and curves in 3D. Informed by real-life examples of two-handed creative activities, WieldingCanvas interprets users’ spatial gestures to move, swing, rotate, transform, or fold a virtual canvas, whereby users simply draw primitive strokes on the canvas, which are turned into finer and more sophisticated shapes via the manipulation of the canvas. We evaluated the capability and user experience of WieldingCanvas with two studies where participants were asked to sketch target shapes. A set of freehand sketches of high aesthetic qualities were created, and the results demonstrated that WieldingCanvas can assist users with creating 3D sketches.",Virtual Reality; freehand drawing; interactive canvas; two-handed interaction,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Beyond the Blink: Investigating Combined Saccadic &amp; Blink-Suppressed Hand Redirection in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"In pursuit of hand redirection techniques that are ever more tailored to human perception, we propose the first algorithm for hand redirection in virtual reality that makes use of saccades, i.e., fast ballistic eye movements that are accompanied by the perceptual phenomenon of change blindness. Our technique combines the previously proposed approaches of gradual hand warping and blink-suppressed hand redirection with the novel approach of saccadic redirection in one unified yet simple algorithm. We compare three variants of the proposed Saccadic &amp; Blink-Suppressed Hand Redirection (SBHR) technique with the conventional approach to redirection in a psychophysical study (N = 25). Our results highlight the great potential of our proposed technique for comfortable redirection by showing that SBHR allows for significantly greater magnitudes of unnoticeable redirection while being perceived as significantly less intrusive and less noticeable than commonly employed techniques that only use gradual hand warping.",virtual reality; change blindness; eye blinks; saccades; detection thresholds; hand redirection,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Big or Small, It’s All in Your Head: Visuo-Haptic Illusion of Size-Change Using Finger-Repositioning",CHI - Human Factors in Computing Systems,A*,"Haptic perception of physical sizes increases the realism and immersion in Virtual Reality (VR). Prior work rendered sizes by exerting pressure on the user’s fingertips or employing tangible, shape-changing devices. These interfaces are constrained by the physical shapes they can assume, making it challenging to simulate objects growing larger or smaller than the perceived size of the interface. Motivated by literature on pseudo-haptics describing the strong influence of visuals over haptic perception, this work investigates modulating the perception of size beyond this range. We developed a fixed-sized VR controller leveraging finger-repositioning to create a visuo-haptic illusion of dynamic size-change of handheld virtual objects. Through two user studies, we found that with an accompanying size-changing visual context, users can perceive virtual object sizes up to &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX 44.2%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name=""chi24-364-inline1"" type=""svg""/&gt;&lt;/Formula&gt; smaller to &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX 160.4%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 2&lt;/AltText&gt;&lt;File name=""chi24-364-inline2"" type=""svg""/&gt;&lt;/Formula&gt; larger than the perceived size of the device. Without the accompanying visuals, a constant size (&lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX 141.4%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 3&lt;/AltText&gt;&lt;File name=""chi24-364-inline3"" type=""svg""/&gt;&lt;/Formula&gt; of device size) was perceived.",pseudo-haptics; cross-modal integration; perceptual illusion; visuo-haptic perception,Abstract,True,
ACM DL,conferencePaper,2024,Flicker Augmentations: Rapid Brightness Modulation for Real-World Visual Guidance using Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Providing attention guidance, such as assisting in search tasks, is a prominent use for Augmented Reality. Typically, this is achieved by graphically overlaying geometrical shapes such as arrows. However, providing visual guidance can cause side effects such as attention tunnelling or scene occlusions, and introduce additional visual clutter. Alternatively, visual guidance can adjust saliency but this comes with different challenges such as hardware requirements and environment dependent parameters. In this work we advocate for using flicker as an alternative for real-world guidance using Augmented Reality. We provide evidence for the effectiveness of flicker from two user studies. The first compared flicker against alternative approaches in a highly controlled setting, demonstrating efficacy (N = 28). The second investigated flicker in a practical task, demonstrating feasibility with higher ecological validity (N = 20). Finally, our discussion highlights the opportunities and challenges when using flicker to provide real-world visual guidance using Augmented Reality.",augmented reality; eye tracking; gaze; visual guidance; flicker,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,STMG: A Machine Learning Microgesture Recognition System for Supporting Thumb-Based VR/AR Input,CHI - Human Factors in Computing Systems,A*,"AR/VR devices have started to adopt hand tracking, in lieu of controllers, to support user interaction. However, today’s hand input rely primarily on one gesture: pinch. Moreover, current mappings of hand motion to use cases like VR locomotion and content scrolling involve more complex and larger arm motions than joystick or trackpad usage. STMG increases the gesture space by recognizing additional small thumb-based microgestures from skeletal tracking running on a headset. We take a machine learning approach and achieve a 95.1% recognition accuracy across seven thumb gestures performed on the index finger surface: four directional thumb swipes (left, right, forward, backward), thumb tap, and fingertip pinch start and pinch end. We detail the components to our machine learning pipeline and highlight our design decisions and lessons learned in producing a well generalized model. We then demonstrate how these microgestures simplify and reduce arm motions for hand-based locomotion and scrolling interactions.",virtual reality; machine learning; augmented reality; neural networks; microgestures,Keywords,True,
ACM DL,conferencePaper,2024,TriPad: Touch Input in AR on Ordinary Surfaces with Hand Tracking Only,CHI - Human Factors in Computing Systems,A*,"TriPad enables opportunistic touch interaction in Augmented Reality using hand tracking only. Users declare the surface they want to appropriate with a simple hand tap gesture. They can then use this surface at will for direct and indirect touch input. TriPad only involves analyzing hand movements and postures, without the need for additional instrumentation, scene understanding or machine learning. TriPad thus works on a variety of flat surfaces, including glass. It also ensures low computational overhead on devices that typically have a limited power budget. We describe the approach, and report on two user studies. The first study demonstrates the robustness of TriPad’s hand movement interpreter on different surface materials. The second study compares TriPad against direct mid-air AR input techniques on both discrete and continuous tasks and with different surface orientations. TriPad achieves a better speed-accuracy trade-off overall, improves comfort and minimizes fatigue.",augmented reality; touch input; passive surfaces,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Sweating the Details: Emotion Recognition and the Influence of Physical Exertion in Virtual Reality Exergaming,CHI - Human Factors in Computing Systems,A*,"There is great potential for adapting Virtual Reality (VR) exergames based on a user’s affective state. However, physical activity and VR interfere with physiological sensors, making affect recognition challenging. We conducted a study (n=72) in which users experienced four emotion inducing VR exergaming environments (happiness, sadness, stress and calmness) at three different levels of exertion (low, medium, high). We collected physiological measures through pupillometry, electrodermal activity, heart rate, and facial tracking, as well as subjective affect ratings. Our validated virtual environments, data, and analyses are openly available. We found that the level of exertion influences the way affect can be recognised, as well as affect itself. Furthermore, our results highlight the importance of data cleaning to account for environmental and interpersonal factors interfering with physiological measures. The results shed light on the relationships between physiological measures and affective states and inform design choices about sensors and data cleaning approaches for affective VR.",virtual reality; emotion recognition; physiological sensing; affect recognition; psychophysiological correlates; exergaming; high-intensity exercise,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,What You Experience is What We Collect: User Experience Based Fine-Grained Permissions for Everyday Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Everyday Augmented Reality (AR) headsets pose significant privacy risks, potentially allowing prolonged sensitive data collection of both users and bystanders (e.g. members of the public). While users control data access through permissions, current AR systems inherit smartphone permission prompts, which may be less appropriate for all-day AR. This constrains informed choices and risks over-privileged access to sensors. We propose (N=20) a novel AR permission control system that allows better-informed privacy decisions and evaluate it using five mock application contexts. Our system’s novelty lies in enabling users to experience the varying impacts of permission levels on not only a) privacy, but also b) application functionality. This empowers users to better understand what data an application depends on and how its functionalities are impacted by limiting said data. Participants found that our method allows for making better informed privacy decisions, and deemed it more transparent and trustworthy than state-of-the-art AR and smartphone permission systems taken from Android and iOS. Our results offer insights into new and necessary AR permission systems, improving user understanding and control over data access.",Augmented Reality; Privacy; AR sensing,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Kinetic Signatures: A Systematic Investigation of Movement-Based User Identification in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Behavioral Biometrics in Virtual Reality (VR) enable implicit user identification by leveraging the motion data of users’ heads and hands from their interactions in VR. This spatiotemporal data forms a Kinetic Signature, which is a user-dependent behavioral biometric trait. Although kinetic signatures have been widely used in recent research, the factors contributing to their degree of identifiability remain mostly unexplored. Drawing from existing literature, this work systematically examines the influence of static and dynamic components in human motion. We conducted a user study (N&nbsp;=&nbsp;24) with two sessions to reidentify users across different VR sports and exercises after one week. We found that the identifiability of a kinetic signature depends on its inherent static and dynamic factors, with the best combination allowing for 90.91% identification accuracy after one week had passed. Therefore, this work lays a foundation for designing and refining movement-based identification protocols in immersive environments.",virtual reality; usable security; identification; task-driven biometrics; kinetic signatures,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Privacy in Immersive Extended Reality: Exploring User Perceptions, Concerns, and Coping Strategies",CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) technology is changing online interactions, but its granular data collection sensors may be more invasive to user privacy than web, mobile, and the Internet of Things technologies. Despite an increased interest in studying developers’ concerns about XR device privacy, user perceptions have rarely been addressed. We surveyed 464 XR users to assess their awareness, concerns, and coping strategies around XR data in 18 scenarios. Our findings demonstrate that many factors, such as data types and sensitivity, affect users’ perceptions of privacy in XR. However, users’ limited awareness of XR sensors’ granular data collection capabilities, such as involuntary body signals of emotional responses, restricted the range of privacy-protective strategies they used. Our results highlight a need to enhance users’ awareness of data privacy threats in XR, design privacy-choice interfaces tailored to XR environments, and develop transparent XR data practices.",Augmented Reality; Mixed Reality; Virtual Reality; Extended Reality; User privacy; Privacy Perception; Privacy-Seeking Strategies,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Socially Late, Virtually Present: The Effects of Transforming Asynchronous Social Interactions in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Social Virtual Reality (VR) typically entails users interacting in real time. However, asynchronous Social VR presents the possibility of combining the convenience of asynchronous communication with the high presence of VR. Because the tools to easily record and replay VR social interactions are fairly new, scholars have not yet examined how users perceive asynchronous VR social interactions, and how nonverbal transformations of recorded interactions influence user behavior. In this work, we study nonverbal transformations of group interactions around proxemics and gaze and present results from an exploratory user study (N=128) investigating their effects. We found that the combination of spatial accommodation and added gaze increases social presence, perceived attention, and mutual gaze. Results also showed an inverse relationship between interpersonal distance and perceived levels of dominance and threat of the recorded group. Finally, we outline implications for educators and virtual meeting organizers to incorporate these transformations into real-world scenarios.",Virtual Reality; Social Interaction; Proxemics; Eye Gaze; Transformed Social Interaction,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,The Effects of False but Stable Heart Rate Feedback on Cybersickness and User Experience in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers a compelling and immersive experience; however, cybersickness (or VR sickness) stands as a significant obstacle to its widespread adoption. When a user experiences cybersickness, one’s physical condition deteriorates with various symptoms, often accompanied by an increased and destabilized heart rate and even altered perception of one’s state. In this paper, we propose to provide “False but Stable Heart rate (FSH)” feedback through auditory and vibrotactile stimulation to reversely induce a stably perceived heart rate and, thereby, alleviate cybersickness while navigating a sickness-inducing VR content. The validation of the human experiment confirmed the intended effect in a statistically significant way. Furthermore, it was found that the lesser compatible FSH feedback had a more substantial sickness reduction effect but distracted the user with the reduced immersive experience. The compatible FSH feedback still showed moderate sickness reduction with the maintained sense of presence and immersion.",Cybersickness; Cognitive Distraction; False Heart Rate,Title_Abstract,True,
ACM DL,conferencePaper,2024,Was it Real or Virtual? Confirming the Occurrence and Explaining Causes of Memory Source Confusion between Reality and Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Source confusion occurs when individuals attribute a memory to the wrong source (e.g., confusing a picture with an experienced event). Virtual Reality (VR) represents a new source of memories particularly prone to being confused with reality. While previous research identified causes of source confusion between reality and other sources (e.g., imagination, pictures), there is currently no understanding of what characteristics specific to VR (e.g., immersion, presence) could influence source confusion. Through a laboratory study (n=29), we 1) confirm the existence of VR source confusion with current technology, and 2) present a quantitative and qualitative exploration of factors influencing VR source confusion. Building on the Source Monitoring Framework, we identify VR characteristics and assumptions about VR capabilities (e.g., poor rendering) that are used to distinguish virtual from real memories. From these insights, we reflect on how the increasing realism of VR could leave users vulnerable to memory errors and perceptual manipulations.",Virtual Reality; Memory; Source Confusion; Source Misattribution,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,“I’d rather drink in VRChat”: Understanding Drinking in Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Drinking in social VR has become popular, yet little is known about how users perceive and experience alcohol consumption while immersed in virtual spaces with others, as well as its potential harm and negative effects on their offline and online lives. To better understand this emerging phenomenon from the perspective of both drinkers and non-drinkers, we analyzed public discussions from the r/VRchat online community on users’ perceptions, and experiences with alcohol consumption in social VR. Heavy drinking is prevalent. We find that VR drinkers feel less intoxicated, which makes them drink more without being aware of it. Anti-cybersickness designs may affect users’ perception of vertigo, even if the vertigo is not caused by VR. We discuss how affordances that support meaningful activities (i.e., sense of presence, embodiment, and social interactions) exacerbate alcohol abuse. We propose implications for the design of safer social VR experiences for both drinkers and non-drinkers.",Virtual Reality; social VR; social interaction; alcohol intoxication; drinking,Title_Keywords,True,
ACM DL,conferencePaper,2024,Blended Whiteboard: Physicality and Reconfigurability in Remote Mixed Reality Collaboration,CHI - Human Factors in Computing Systems,A*,"The whiteboard is essential for collaborative work. To preserve its physicality in remote collaboration, Mixed Reality (MR) can blend real whiteboards across distributed spaces. Going beyond reality, MR can further enable interactions like panning and zooming in a virtually reconfigurable infinite whiteboard. However, this reconfigurability conflicts with the sense of physicality. To address this tension, we introduce Blended Whiteboard, a remote collaborative MR system enabling reconfigurable surface blending across distributed physical whiteboards. Blended Whiteboard supports a unique collaboration style, where users can sketch on their local whiteboards but also reconfigure the blended space to facilitate transitions between loosely and tightly coupled work. We describe design principles inspired by proxemics; supporting users in changing between facing each other and being side-by-side, and switching between navigating the whiteboard synchronously and independently. Our work shows exciting benefits and challenges of combining physicality and reconfigurability in the design of distributed MR whiteboards.",mixed reality; avatars; proxemics; remote collaboration; f-formations; 3C collaboration model,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,From Real to Virtual: Exploring Replica-Enhanced Environment Transitions along the Reality-Virtuality Continuum,CHI - Human Factors in Computing Systems,A*,"Recent Head-Mounted Displays enable users to perceive the real environment using a video-based see-through mode and the fully virtual environment within a single display. Leveraging these advancements, we present a generic concept to seamlessly transition between the real and virtual environment, with the goal of supporting users in engaging with and disengaging from any real environment into Virtual Reality. This transition process uses a digital replica of the real environment and incorporates various stages of Milgram’s Reality-Virtuality Continuum, along with visual transitions that facilitate gradual navigation between them. We implemented the overall transition concept and four object-based transition techniques. The overall transition concept and four techniques were evaluated in a qualitative user study, focusing on user experience, the use of the replica and visual coherence. The results of the user study show, that most participants stated that the replica facilitates the cognitive processing of the transition and supports spatial orientation.",Augmented Reality; Virtual Reality; User Study; Cross-Reality; Transitions; Augmented Virtuality; Replica; Visual Coherence,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,SwitchSpace: Understanding Context-Aware Peeking Between VR and Desktop Interfaces,CHI - Human Factors in Computing Systems,A*,"Cross-reality tasks, like creating or consuming virtual reality (VR) content, often involve inconvenient or distracting switches between desktop and VR. An initial formative study explores cross-reality switching habits, finding most switches are momentary “peeks” between interfaces, with specific habits determined by current context. The results inform a design space for context-aware “peeking” techniques that allow users to view or interact with desktop from VR, and vice versa, without fully switching. We implemented a set of peeking techniques and evaluated them in two levels of a cross-reality task: one requiring only viewing, and another requiring input and viewing. Peeking techniques made task completion faster, with increased input accuracy and reduced perceived workload.",Virtual Reality; interaction techniques; transitional interfaces; controlled experiments,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Volumetric Hybrid Workspaces: Interactions with Objects in Remote and Co-located Telepresence,CHI - Human Factors in Computing Systems,A*,"Volumetric telepresence aims to create a shared space, allowing people in local and remote settings to collaborate seamlessly. Prior telepresence examples typically have asymmetrical designs, with volumetric capture in one location and objects in one format. In this paper, we present a volumetric telepresence mixed reality system that supports real-time, symmetrical, multi-user, partially distributed interactions, using objects in multiple formats, across multiple locations. We align two volumetric environments around a common spatial feature to create a shared workspace for remote and co-located people using objects in three formats: physical, virtual, and volumetric. We conducted a study with 18 participants over 6 sessions, evaluating how telepresence workspaces support spatial coordination and hybrid communication for co-located and remote users undertaking collaborative tasks. Our findings demonstrate the successful integration of remote spaces, effective use of proxemics and deixis to support negotiation, and strategies to manage interactivity in hybrid workspaces.",mixed reality; augmented reality; collaboration; telepresence; volumetric capture; workspace awareness; partially distributed teams,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Connecting Home: Human-Centric Setup Automation in the Augmented Smart Home,CHI - Human Factors in Computing Systems,A*,"Controlling smart homes via vendor-specific apps on smartphones is cumbersome. Augmented Reality (AR) offers a promising alternative by enabling direct interactions with Internet of Things (IoT) devices. However, using AR for smart home control requires knowledge of each device’s 3D position. In this paper, we introduce and evaluate three concepts for identifying IoT device positions with varying degrees of automation. Our mixed-methods laboratory study with 28 participants revealed that, despite being recognized as the most efficient option, the majority of participants opted against a fast, fully automated detection, favoring a balance between efficiency and perceived autonomy and control. We link this decision to psychological needs grounded in self-determination theory and discuss the strengths and weaknesses of each alternative, motivating a user-adaptive solution. Additionally, we observed a “wow-effect” in response to AR interaction for smart homes, suggesting potential benefits of a human-centric approach to the smart home of the future.",Augmented Reality; Smart Home; Self-Determination Theory; Laboratory Experiment,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Engaging recently incarcerated and gang affiliated Black and Latino/a young adults in designing social collocated applications for mixed reality smart glasses through community-based participatory design workshops.,CHI - Human Factors in Computing Systems,A*,"Involving Black and Latina/o communities early and often in emerging technology design can make innovation more democratic, address bias, and reduce harm against these marginalized groups. To the best of our knowledge, no work has examined how recently incarcerated and gang affiliated young adults conceptualize mixed reality (MR) use for social collocated scenarios based on their everyday interactions and meaning-making. To explore this topic, we used a design-based implementation research (DBIR) and community-based participatory design (CBPD) approach to elicit social-technical insights grounded in the personal and critical perspectives of these youth. We find participants frequently grounded design ideas as embodied design elements to surface intangible and invisible qualities such as emotions and reflections on lived experiences, namely criticizing institutional structures that have maintained exclusionary practices against them. We discuss how DBIR and CBPD can uncover larger societal issues impacting marginalized communities through emerging technology design, and we contribute design recommendations for social collocated interactions in MR.",,Title_Abstract,True,
ACM DL,conferencePaper,2024,Interactive Shape Sonification for Tumor Localization in Breast Cancer Surgery,CHI - Human Factors in Computing Systems,A*,"About 20 percent of patients undergoing breast-conserving surgery require reoperation due to cancerous tissue remaining inside the breast. Breast cancer localization systems utilize auditory feedback to convey the distance between a localization probe and a small marker (seed) implanted into the breast tumor prior to surgery. However, no information on the location of the tumor margin is provided. To reduce the reoperation rate by improving the usability and accuracy of the surgical task, we developed an auditory display using shape sonification to assist with tumor margin localization. Accuracy and usability of the interactive shape sonification were determined on models of the female breast in three user studies with both breast surgeons and non-clinical participants. The comparative studies showed a significant increase in usability (p&lt;0.05) and localization accuracy (p&lt;0.001) of the shape sonification over the auditory feedback currently used in surgery.",Augmented reality; Sonification; Auditory display; Breast cancer localization; Breast cancer surgery; Computer assisted interventions; Lumpectomy; Shape sonification; Surgical sonification,Keywords,True,
ACM DL,conferencePaper,2024,A Change of Scenery: Transformative Insights from Retrospective VR Embodied Perspective-Taking of Conflict With a Close Other,CHI - Human Factors in Computing Systems,A*,"Close relationships are irreplaceable social resources, yet prone to high-risk conflict. Building on findings from the fields of HCI, virtual reality, and behavioral therapy, we evaluate the unexplored potential of retrospective VR-embodied perspective-taking to fundamentally influence conflict resolution in close others. We develop a biographically-accurate Retrospective Embodied Perspective-Taking system (REPT) and conduct a mixed-methods evaluation of its influence on close others’ reflection and communication, compared to video-based reflection methods currently used in therapy (treatment as usual, or TAU). Our key findings provide evidence that REPT was able to significantly improve communication skills and positive sentiment of both partners during conflict, over TAU. The qualitative data also indicated that REPT surpassed basic perspective-taking by exclusively stimulating users to embody and reflect on both their own and their partner’s experiences at the same level. In light of these findings, we provide implications and an agenda for social embodiment in HCI design: conceptualizing the use of ‘embodied social cognition,’ and envisioning socially-embodied experiences as an interactive context.",virtual reality; communication; reflection; behavior change; perspective-taking; embodiment; empathy; mixed-methods; social cognition; close relationships; conflict,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST&nbsp;lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality.",augmented reality; large language model; text simplification,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploration of Foot-based Text Entry Techniques for Virtual Reality Environments,CHI - Human Factors in Computing Systems,A*,"Foot-based input can serve as a supplementary or alternative approach to text entry in virtual reality (VR). This work explores the feasibility and design of foot-based techniques that are hands-free. We first conducted a preliminary study to assess foot-based text entry in standing and seated positions with tap and swipe input approaches. The findings showed that foot-based text input was feasible, with the possibility for performance and usability improvements. We then developed three foot-based techniques, including two tap-based techniques (FeetSymTap and FeetAsymTap) and one swipe-based technique (FeetGestureTap), and evaluated their performance via another user study. The results show that the two tap-based techniques supported entry rates of 11.12 WPM and 10.80 WPM, while the swipe-based technique led to 9.16 WPM. Our findings provide a solid foundation for the future design and implementation of foot-based text entry in VR and have the potential to be extended to MR and AR.",virtual reality; text entry; hands-free interaction; foot-based interaction,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Extended Reality,CHI - Human Factors in Computing Systems,A*,"Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment. However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions. We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases. We develop a computational approach to automatically generate test cases based on the authored CAP and the user’s context history. Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements. We evaluated Fast-Forward Reality in a user study (N=12). Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability.",Extended Reality; Validation; Context-Aware Policy; Unit Test,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality,CHI - Human Factors in Computing Systems,A*,"With the proliferation of VR and a metaverse on the horizon, many multi-user activities are migrating to the VR world, calling for effective collaboration support. As one key feature, traditional collaborative systems provide users with undo mechanics to reverse errors and other unwanted changes. While undo has been extensively researched in this domain and is now considered industry standard, it is strikingly absent for VR systems in research and industry. This work addresses this research gap by exploring different undo techniques for basic object manipulation in different collaboration modes in VR. We conducted a study involving 32 participants organized in teams of two. Here, we studied users’ performance and preferences in a tower stacking task, varying the available undo techniques and their mode of collaboration. The results suggest that users desire and use undo in VR and that the choice of the undo technique impacts users’ performance and social connection.",Virtual Reality; CSCW; Undo; Connectedness; Multi-User; SocialVR,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,On the Benefits of Image-Schematic Metaphors when Designing Mixed Reality Systems,CHI - Human Factors in Computing Systems,A*,"A Mixed Reality (MR) system encompasses various aspects, such as visualization and spatial registration of user interface elements, user interactions and interaction feedback. Image-schematic metaphors (ISMs) are universal knowledge structures shared by a wide range of users. They hold a theoretical promise of facilitating greater ease of learning and use for interactive systems without costly adaptations. This paper investigates whether image-schematic metaphors (ISMs) can improve user learning, by comparing an existing MR instruction authoring system with or without ISM enhancements. In a user study with 32 participants, we found that the ISM-enhanced system significantly improved task performance, learnability and mental efficiency compared to the baseline. Participants also rated the ISM-enhanced system significantly higher in terms of perspicuity, efficiency, and novelty. These results empirically demonstrate multiple benefits of ISMs when integrated into the design of this MR system and encourage further studies to explore the wider applicability of ISMs in user interface design.",Mixed Reality; Image Schema; Instruction Authoring,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Predicting the Noticeability of Dynamic Virtual Elements in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"While Virtual Reality (VR) systems can present virtual elements such as notifications anywhere, designing them so they are not missed by or distracting to users is highly challenging for content creators. To address this challenge, we introduce a novel approach to predict the noticeability of virtual elements. It computes the visual saliency distribution of what users see, and analyzes the temporal changes of the distribution with respect to the dynamic virtual elements that are animated. The computed features serve as input for a long short-term memory (LSTM) model that predicts whether a virtual element will be noticed. Our approach is based on data collected from 24 users in different VR environments performing tasks such as watching a video or typing. We evaluate our approach (n = 12), and show that it can predict the timing of when users notice a change to a virtual element within 2.56 sec compared to a ground truth, and demonstrate the versatility of our approach with a set of applications. We believe that our predictive approach opens the path for computational design tools that assist VR content creators in creating interfaces that automatically adapt virtual elements based on noticeability.",Mixed Reality; Virtual Reality; Computational Interaction,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Towards Building Condition-Based Cross-Modality Intention-Aware Human-AI Cooperation under VR Environment,CHI - Human Factors in Computing Systems,A*,"To address critical challenges in effectively identifying user intent and forming relevant information presentations and recommendations in VR environments, we propose an innovative condition-based multi-modal human-AI cooperation framework. It highlights the intent tuples (intent, condition, intent prompt, action prompt) and 2-Large-Language-Models (2-LLMs) architecture. This design, utilizes “condition” as the core to describe tasks, dynamically match user interactions with intentions, and empower generations of various tailored multi-modal AI responses. The architecture of 2-LLMs separates the roles of intent detection and action generation, decreasing the prompt length and helping with generating appropriate responses. We implemented a VR-based intelligent furniture purchasing system based on the proposed framework and conducted a three-phase comparative user study. The results conclusively demonstrate the system’s superiority in time efficiency and accuracy, intention conveyance improvements, effective product acquisitions, and user satisfaction and cooperation preference. Our framework provides a promising approach towards personalized and efficient user experiences in VR.",Virtual Reality; Action Generation; Human-AI Cooperation; Intention Detection,Keywords,True,
ACM DL,conferencePaper,2024,Screenless Interactive Tabletop Gaming with Capacitive Surface Sensing,CHI - Human Factors in Computing Systems,A*,"Many interactive systems that support tabletop games either augment the experience with additional elements or transform game components into digital counterparts, e.g., using mixed reality. However, as many users prefer tangible game elements, digital augmentations can disrupt the immersion they seek to enhance, often due to the complexity of the hardware used. Responding to this challenge, we designed a screenless interactive tabletop system with capacitive sensing. The system is suitable for novice players and provides automatic score-keeping. Our method eliminates the need for external sensors and retains all original game pieces intact. We evaluated our system in a study with a forest planting game (n = 20). Gameplay with our system exhibited shorter turn duration, and participants adopted more effective strategies than in traditional gameplay. These results underscore the potential of screenless interactive tabletops to amplify the gaming experience without causing distractions.",Machine Learning; 3D Printing; Board Games; Tangibles; Touchscreen; Capacitive Sensing,Abstract,True,
ACM DL,conferencePaper,2024,Human I/O: Towards a Unified Approach to Detecting Situational Impairments,CHI - Human Factors in Computing Systems,A*,"Situationally Induced Impairments and Disabilities (SIIDs) can significantly hinder user experience in contexts such as poor lighting, noise, and multi-tasking. While prior research has introduced algorithms and systems to address these impairments, they predominantly cater to specific tasks or environments and fail to accommodate the diverse and dynamic nature of SIIDs. We introduce Human I/O, a unified approach to detecting a wide range of SIIDs by gauging the availability of human input/output channels. Leveraging egocentric vision, multimodal sensing and reasoning with large language models, Human I/O achieves a 0.22 mean absolute error and a 82% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios. Furthermore, while the core focus of our work is on the detection of SIIDs rather than the creation of adaptive user interfaces, we showcase the efficacy of our prototype via a user study with 10 participants. Findings suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs, paving the way for more adaptive and accessible interactive systems in the future.",augmented reality; context awareness; large language models; situational impairments; multimodal sensing,Keywords,True,
ACM DL,conferencePaper,2024,A Design Space for Vision Augmentations and Augmented Human Perception using Digital Eyewear,CHI - Human Factors in Computing Systems,A*,"Head-mounted displays were originally introduced to directly present computer-generated information to the human eye. More recently, the potential to use this kind of technology to support human vision and augment human perception has become actively pursued with applications such as compensating for visual impairments or aiding unimpaired vision. Unfortunately, a systematic analysis of the field is missing. Within this work, we close that gap by presenting a design space for vision augmentations that allows research to systematically explore the field of digital eyewear for vision aid and how it can augment the human visual system. We test our design space against currently available solutions and conceptually develop new solutions. The design space and findings can guide future development and can lead to a consistent categorisation of the many existing approaches.",mixed reality; augmented reality; accessibility; design space; human augmentation; visual impairments; sensory augmentation; augmented human; vision augmentation,Keywords,True,
ACM DL,conferencePaper,2024,RASSAR: Room Accessibility and Safety Scanning in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"The safety and accessibility of our homes is critical to quality of life and evolves as we age, become ill, host guests, or experience life events such as having children. Researchers and health professionals have created assessment instruments such as checklists that enable homeowners and trained experts to identify and mitigate safety and access issues. With advances in computer vision, augmented reality (AR), and mobile sensors, new approaches are now possible. We introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues such as an inaccessible table height or unsafe loose rugs using LiDAR and real-time computer vision. We present findings from three studies: a formative study with 18 participants across five stakeholder groups to inform the design of RASSAR, a technical performance evaluation across ten homes demonstrating state-of-the-art performance, and a user study with six stakeholders. We close with a discussion of future AI-based indoor accessibility assessment tools, RASSAR’s extensibility, and key application scenarios.",Augmented Reality; Accessibility; Computer Vision; Object Detection; Indoor Accessibility Auditing,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring an Extended Reality Floatation Tank Experience to Reduce the Fear of Being in Water,CHI - Human Factors in Computing Systems,A*,"People with a fear of being in water rarely engage in water activities and hence miss out on the associated health benefits. Prior research suggested virtual exposure to treat fears. However, when it comes to a fear of being in water, virtual water might not capture water’s immersive qualities, while real water can pose safety risks. We propose extended reality to combine both advantages: We conducted a study (N=12) where participants with a fear of being in water interacted with playful water-inspired virtual reality worlds while floating inside a floatation tank. Our findings, supported quantitatively by heart rate variability and qualitatively by interviews, suggest that playful extended reality could mitigate fear responses in an entertaining way. We also present insights for the design of future systems that aim to help people with a fear of being in water and other phobias by using the best of the virtual and physical worlds.",virtual reality; extended reality; exposure therapy; fear of being in water; floatation tank; flotation pod; phobia; water,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,An Iterative Participatory Design Approach to Develop Collaborative Augmented Reality Activities for Older Adults in Long-Term Care Facilities,CHI - Human Factors in Computing Systems,A*,"Over four million older adults living in long-term care (LTC) communities experience loneliness, adversely impacting their health. Increased contact with friends and family is an evidence-based intervention to reduce loneliness, but in-person visits are not always possible. Augmented Reality (AR)-based telepresence activities can offer viable alternatives with increased immersion and presence compared to video calls. However, its feasibility as an interaction technology for older adults is not known. In this paper, we detail the design of two dyadic collaborative AR activities that accommodate diminished physical and cognitive abilities of older adults. The findings include a general design framework based on an iterative participatory design focusing on preferred activities, modes of interaction, and overall AR experience of eight older adults, two family members, and five LTC staff. Results demonstrate the potential of collaborative AR as an effective means of interaction for older adults with their family, if designed to cater to their needs.",accessibility; older adults; collaborative augmented reality; iterative participatory design; long term care settings,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,"Lessons From Working in the Metaverse: Challenges, Choices, and Implications from a Case Study",CHI - Human Factors in Computing Systems,A*,"Although the metaverse workspace has the potential to solve some of the drawbacks of remote work while maintaining its benefits, there are few real-world cases of adopting the metaverse as a legitimate workspace and fewer subsequent studies on how to design and operate the metaverse workspace. Thus, questions exist about the organizational or sociotechnical challenges that may emerge and how decisions are made when adopting and operating the metaverse workspace in a real-world setting. To answer such questions, we scrutinized the startup company Zigbang, which has completely replaced their physical office with Soma— a metaverse platform they developed where thousands of people work and other cooperative companies have moved in as tenants. By conducting field observations and semi-structured interviews with various workers and Zigbang's stakeholders, we identify essential design challenges and decisions when adopting a metaverse workspace and highlight the key takeaways learned from the company's trials and errors.",Metaverse; Virtual Environment; Future of Work; Remote Work; Case Study; Metaverse Workspace,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration,CHI - Human Factors in Computing Systems,A*,"Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces.",mixed reality; collaboration; scale; f-formations; space and place,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,An Empirical Study on Oculus Virtual Reality Applications: Security and Privacy Perspectives,ICSE - International Conference on Software Engineering,A*,"Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology. On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS (e.g., Android). As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps. On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics (e.g., hand-tracking and face-tracking data). Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities. Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps. In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods. Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps. We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis. We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps. Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps. Based on these findings, we make suggestions for the future development of VR apps.",metaverse; security and privacy; static analysis; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Machine Learning in Metaverse Security: Current Solutions and Future Challenges,CSUR - Computing Surveys,A*,"The Metaverse, positioned as the next frontier of the Internet, has the ambition to forge a virtual shared realm characterized by immersion, hyper-spatiotemporal dynamics, and self-sustainability. Recent technological strides in AI, Extended Reality, 6G, and blockchain propel the Metaverse closer to realization, gradually transforming it from science fiction into an imminent reality. Nevertheless, the extensive deployment of the Metaverse faces substantial obstacles, primarily stemming from its potential to infringe on privacy and be susceptible to security breaches, whether inherent in its underlying technologies or arising from the evolving digital landscape. Metaverse security provisioning is poised to confront various foundational challenges owing to its distinctive attributes, encompassing immersive realism, hyper-spatiotemporally, sustainability, and heterogeneity. This article undertakes a comprehensive study of the security and privacy challenges facing the Metaverse, leveraging machine learning models for this purpose. In particular, our focus centers on an innovative distributed Metaverse architecture characterized by interactions across 3D worlds. Subsequently, we conduct a thorough review of the existing cutting-edge measures designed for Metaverse systems while also delving into the discourse surrounding security and privacy threats. As we contemplate the future of Metaverse systems, we outline directions for open research pursuits in this evolving landscape.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain; Digital Twin; Extended Reality; Generative AI; Machine Learning; Metaverse Security,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Extended Reality (XR) Toward Building Immersive Solutions: The Key to Unlocking Industry 4.0,CSUR - Computing Surveys,A*,"When developing XR applications for Industry 4.0, it is important to consider the integration of visual displays, hardware components, and multimodal interaction techniques that are compatible with the entire system. The potential use of multimodal interactions in industrial applications has been recognized as a significant factor in enhancing humans' ability to perform tasks and make informed decisions. To offer a comprehensive analysis of the current advancements in industrial XR, this review presents a structured tutorial that provides answers to the following research questions: (R.Q.1) What are the similarities and differences between XR technologies, including augmented reality (AR), mixed reality (MR), Augmented Virtuality (AV), and virtual reality (VR) under Industry 4.0 consideration? (R.Q.2) What types of visual displays and hardware devices are needed to present XR for Industry 4.0? (R.Q.3) How did the multimodal interaction in XR perceive and relate to Industry 4.0? (R.Q.4) How have modern adaptations of XR technologies dealt with the theme of Industry 4.0? (R.Q.5) How can XR technologies in Industry 4.0 develop their services and usages to be more solution-inclusive? This review showcases various instances that demonstrate XR's potential to transform how humans interact with the physical world in Industry 4.0. These advancements can increase productivity, reduce costs, and enhance safety.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",4IR; and augmented virtuality (AV); augmented reality (AR); Extended reality (XR); Industry 4.0; mixed reality (MR); virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A Review of Olfactory Display Designs for Virtual Reality Environments,CSUR - Computing Surveys,A*,"The field of Virtual Reality continues to evolve to provide an ever-greater sense of immersion to the user. However, VR experiences are still primarily constrained through the human senses of vision and audition, with some interest in haptic (mainly vibrotactile) applications. Only recently have olfactory displays—technologies that generate and deliver scent stimuli—been examined to provide the sense of smell to the human olfactory organ in virtual environments. This article presents a classification and review of olfactory-enhanced virtual reality systems, particularly those that deployed a Head-mounted Display or Cave Automatic Virtual Environment system. In addition, the article provides a discussion of the various technological and design challenges for developing an olfactory display suitable for enhancing virtual reality experiences. Finally, the article proposes future perspectives on the field and includes a table summarizing the characteristics and features of the reviewed systems. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",immersion; multisensory; odor; olfaction; presence; smell; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The First Principles: Setting the Context for a Safe and Secure Metaverse,CSUR - Computing Surveys,A*,"The metaverse delivered through converged and amalgamated technologies holds promise. No wonder technology heavyweights, large corporates, research organizations and businesses cutting across industry verticals are racing to put in place a metaverse-first strategy. The bets on consumers rapidly migrating from traditional social networks and collaborative applications to more immersive digital experiences have been placed. However, the transition is not expected to be seamless. Privacy, safety and security concerns abound in the early versions of the metaverse. Increased regulatory oversight and diverse national laws threaten to derail the hype around the metaverse. It is increasingly clear that the final iteration of the metaverse will need to assuage the concerns of individual users while addressing complex legal and regulatory requirements. Thus, a multi-perspective approach needs to be adopted to help set the agenda for the evolution of the metaverse. This research paper examines the different aspects and challenges which the future metaverse will need to address. A set of “first principles” are formulated, which if implemented will lead to the development of an equitable, inclusive, safe and secure metaverse. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",first principles for the metaverse; Metaverse; metaverse security,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Secure and Trustworthy Artificial Intelligence-extended Reality (AI-XR) for Metaverses,CSUR - Computing Surveys,A*,"Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalized experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies such as augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users' privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AR; Metaverse; MR; robust ML; secure ML; trustworthy ML; VR; XR,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,“Are you feeling sick?” – A systematic literature review of cybersickness in virtual reality,CSUR - Computing Surveys,A*,"Cybersickness (CS), also known as visually induced motion sickness (VIMS), is a condition that can affect individuals when they interact with virtual reality (VR) technology. This condition is characterized by symptoms such as nausea, dizziness, headaches, eye fatigue, and so on, and can be caused by a variety of factors. Finding a feasible solution to reduce the impact of CS is extremely important as it will greatly enhance the overall user experience and make VR more appealing to a wider range of people. We have carefully compiled a list of 223 highly pertinent studies to review the current state of research on the most essential aspects of CS. We have provided a novel taxonomy that encapsulates various aspects of CS measurement techniques found in the literature. We have proposed a set of CS mitigation guidelines for both developers and users. We have also discussed various CS-inducing factors and provided a taxonomy that tries to capture the same. Overall, our work provides a comprehensive overview of the current state of research in CS with a particular emphasis on different measurement techniques and CS mitigation strategies, identifies research gaps in the literature, and provides recommendations for future research in the field. © 2024 Association for Computing Machinery. All rights reserved.",cause; cybersickness; guidelies; measurement; mitigation; systematic review; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,From Digital Media to Empathic Spaces: A Systematic Review of Empathy Research in Extended Reality Environments,CSUR - Computing Surveys,A*,"Recent advances in extended reality (XR) technologies have enabled new and increasingly realistic empathy tools and experiences. In XR, all interactions take place in different spatial contexts, all with different features, affordances, and constraints. We present a systematic literature survey of recent work on empathy in XR. As a result, we contribute a research roadmap with three future opportunities and six open questions in XR-enabled empathy research across both physical and virtual spaces. © 2023 Copyright held by the owner/author(s).",empathy; Extended reality (XR); human-computer interaction (HCI); metaverse; spatiality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Evaluation of XR Applications: A Tertiary Review,CSUR - Computing Surveys,A*,"Extended reality (XR) applications—encompassing virtual reality, augmented reality, and mixed reality—are finding their way into multiple domains. Each area has different motivations for employing and different criteria for evaluating XR. Multiple surveys describe XR and its evaluation in particular fields. However, these surveys do not always agree on the definition of XR. This lack of consensus makes it hard to compare and use learnings from XR research across areas. Through a tertiary systematic literature review, we analyzed 81 surveys from several fields to provide a comprehensive summary of the state of XR research regarding the evaluation of XR applications. We seek to understand (i) how is XR defined? (ii) why is XR employed? (iii) how is XR evaluated? (iv) what are the main criticisms and future research paths outlined by the surveys? and (v) how good are the surveys? We present our findings describing XR research in 10 categories. Given our findings, we propose that future research should build upon a solid XR taxonomy and depart from effectiveness into efficiency research—to understand not only if but also how XR achieves the desired outcomes. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",augmented reality; Evaluation; systematic review; tertiary review; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Integration of Sensing, Communication, and Computing for Metaverse: A Survey",CSUR - Computing Surveys,A*,"The metaverse is an Artificial Intelligence (AI)-generated virtual world, in which people can game, work, learn, and socialize. The realization of metaverse not only requires a large amount of computing resources to realize the rendering of the virtual world, but also requires communication resources to realize real-time transmission of massive data to ensure a good user experience. The metaverse is currently moving from fiction to reality with the development of advanced technologies represented by AI, blockchain, extended reality, and Digital Twins (DT). However, due to the shortage of communication as well as computing resources, how to realize secure and efficient data interaction between the virtual and the real is an important issue for the metaverse. In this article, we first discuss the characteristics and architecture of the metaverse and introduce its enabling technologies. To cope with the conflict between limited resources and user demands, the article next introduces an Integrated Sensing, Communication, and Computing (SCC) technology and describes its basic principles and related characteristics of SCC. After that, solutions based on SCC in the metaverse scenarios are summarized and relevant lessons are summarized. Finally, we discuss some research challenges and open issues.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication and computing integration; digital twins; edge computing; Metaverse; sensing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Deceived by Immersion: A Systematic Analysis of Deceptive Design in Extended Reality,CSUR - Computing Surveys,A*,"The well-established deceptive design literature has focused on conventional user interfaces. With the rise of extended reality (XR), understanding deceptive design's unique manifestations in this immersive domain is crucial. However, existing research lacks a full, cross-disciplinary analysis that analyzes how XR technologies enable new forms of deceptive design. Our study reviews the literature on deceptive design in XR environments. We use thematic synthesis to identify key themes. We found that XR's immersive capabilities and extensive data collection enable subtle and powerful manipulation strategies. We identified eight themes outlining these strategies and discussed existing countermeasures. Our findings show the unique risks of deceptive design in XR, highlighting implications for researchers, designers, and policymakers. We propose future research directions that explore unintentional deceptive design, data-driven manipulation solutions, user education, and the link between ethical design and policy regulations.  Copyright © 2024 held by the owner/author(s).",augmented reality; dark pattern; Deceptive design; extended reality; mixed reality; user manipulation; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Soft Delivery: Survey on a New Paradigm for Wireless and Mobile Multimedia Streaming,CSUR - Computing Surveys,A*,"The increasing demand for video streaming services is the key driver of modern wireless and mobile communications. Although many studies have designed digital-based delivery schemes to send video content over wireless and mobile networks, significant quality degradation, known as cliff and leveling effects, often occurs owing to fluctuating channel characteristics. In this article, we present a comprehensive summary of soft delivery, which is a new paradigm for wireless and mobile video streaming and discuss the future directions of soft delivery. Existing studies found that introducing multi-dimensional cosine transform, human vision system, and graph signal processing can make soft delivery schemes more effective in untethered immersive experiences, including virtual reality and volumetric media, than digital-based delivery schemes. In addition, this study finds that soft delivery has the potential to be a new standard to deliver deep neural network models and tactile information over wireless and mobile networks. © 2023 Copyright held by the owner/author(s).",extended reality; hybrid digital-analog delivery; Soft delivery,Abstract_Keywords,True,
Scopus,journalPaper,2024,A Scoping Survey on Cross-reality Systems,CSUR - Computing Surveys,A*,"Immersive technologies such as Virtual Reality (VR) and Augmented Reality (AR) empower users to experience digital realities. Known as distinct technology classes, the lines between them are becoming increasingly blurry with recent technological advancements. New systems enable users to interact across technology classes or transition between them - referred to as cross-reality systems. Nevertheless, these systems are not well understood. Hence, in this article, we conducted a scoping literature review to classify and analyze cross-reality systems proposed in previous work. First, we define these systems by distinguishing three different types. Thereafter, we compile a literature corpus of 306 relevant publications, analyze the proposed systems, and present a comprehensive classification, including research topics, involved environments, and transition types. Based on the gathered literature, we extract nine guiding principles that can inform the development of cross-reality systems. We conclude with research challenges and opportunities. © 2023 held by the owner/author(s).",augmented reality; augmented virtuality; bystander inclusion; collaboration; Cross-reality systems; reality-virtuality continuum; transitional interfaces; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Economic Systems in the Metaverse: Basics, State of the Art, and Challenges",CSUR - Computing Surveys,A*,"Economic systems play pivotal roles in the metaverse. However, we have not yet found an overview that systematically introduces economic systems for the metaverse. Therefore, we review the state-of-the-art solutions, architectures, and systems related to economic systems. When investigating those state-of-the-art studies, we keep two questions in mind: (1) What is the framework of economic systems in the context of the metaverse? and (2) What activities would economic systems engage in the metaverse? This article aims to disclose insights into the economic systems that work for both the current and the future metaverse. To have a clear overview of the economic system framework, we mainly discuss the connections among three fundamental elements in the metaverse, i.e., digital creation, digital assets, and the digital trading market. After that, we elaborate on each topic of the proposed economic system framework. Those topics include incentive mechanisms, monetary systems, digital wallets, decentralized finance activities, and cross-platform interoperability for the metaverse. For each topic, we mainly discuss three questions: (a) the rationale of this topic, (b) why the metaverse needs this topic, and (c) how this topic will evolve in the metaverse. Through this overview, we wish readers can better understand what economic systems the metaverse needs and the insights behind the economic activities in the metaverse.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",blockchain; cross-metaverse interoperability; cryptocurrency; decentralized finance; economic system; incentive mechanism; Metaverse; non-fungible tokens,Title_Abstract_Keywords,True,
ACM DL,journalPaper,2024,RE Methods for Virtual Reality Software Product Development: A Mapping Study,TOSEM - Transactions on Software Engineering and Methodology,A*,"Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",industrial practices; requirements elicitation; Software requirements; virtual reality,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,TouchInsight: Uncertainty-aware Rapid Touch and Text Input for Mixed Reality from Egocentric Vision,UIST - Symposium on User Interface Software and Technology,A*,"While passive surfaces offer numerous benefits for interaction in mixed reality, reliably detecting touch input solely from head-mounted cameras has been a long-standing challenge. Camera specifics, hand self-occlusion, and rapid movements of both head and fingers introduce considerable uncertainty about the exact location of touch events. Existing methods have thus not been capable of achieving the performance needed for robust interaction. In this paper, we present a real-time pipeline that detects touch input from all ten fingers on any physical surface, purely based on egocentric hand tracking. Our method TouchInsight comprises a neural network to predict the moment of a touch event, the finger making contact, and the touch location. TouchInsight represents locations through a bivariate Gaussian distribution to account for uncertainties due to sensing inaccuracies, which we resolve through contextual priors to accurately infer intended user input. We first evaluated our method offline and found that it locates input events with a mean error of 6.3&nbsp;mm, and accurately detects touch events (F1 = 0.99) and identifies the finger used (F1 = 0.96). In an online evaluation, we then demonstrate the effectiveness of our approach for a core application of dexterous touch input: two-handed text entry. In our study, participants typed 37.0 words per minute with an uncorrected error rate of 2.9% on average.",Bayesian inference; egocentric hand tracking; language models; mixed reality; text entry; Touch detection; uncertainty estimation,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Augmented Object Intelligence with XR-Objects,UIST - Symposium on User Interface Software and Technology,A*,"Seamless integration of physical objects as interactive digital entities remains a challenge for spatial computing. This paper explores Augmented Object Intelligence&nbsp; (AOI) in the context of XR, an interaction paradigm that aims to blur the lines between digital and physical by equipping real-world objects with the ability to interact as if they were digital, where every object has the potential to serve as a portal to digital functionalities. Our approach utilizes real-time object segmentation and classification, combined with the power of Multimodal Large Language Models (MLLMs), to facilitate these interactions without the need for object pre-registration. We implement the AOI concept in the form of XR-Objects, an open-source prototype system that provides a platform for users to engage with their physical environment in contextually relevant ways using object-based context menus. This system enables analog objects to not only convey information but also to initiate digital actions, such as querying for details or executing tasks. Our contributions are threefold: (1) we define the AOI concept and detail its advantages over traditional AI assistants, (2) detail the XR-Objects&nbsp; system’s open-source design and implementation, and (3) show its versatility through various use cases and a user study.",augmented objects; augmented reality; context menus; extended reality; mixed reality; spatial computing; user interfaces,Keywords,True,
ACM DL,conferencePaper,2024,Predicting the Limits: Tailoring Unnoticeable Hand Redirection Offsets in Virtual Reality to Individuals' Perceptual Boundaries,UIST - Symposium on User Interface Software and Technology,A*,"Many illusion and interaction techniques in Virtual Reality (VR) rely on Hand Redirection (HR), which has proved to be effective as long as the introduced offsets between the position of the real and virtual hand do not noticeably disturb the user experience. Yet calibrating HR offsets is a tedious and time-consuming process involving psychophysical experimentation, and the resulting thresholds are known to be affected by many variables—limiting HR’s practical utility. As a result, there is a clear need for alternative methods that allow tailoring HR to the perceptual boundaries of individual users. We conducted an experiment with 18 participants combining movement, eye gaze and EEG data to detect HR offsets Below, At, and Above individuals’ detection thresholds. Our results suggest that we can distinguish HR At and Above from no HR. Our exploration provides a promising new direction with potentially strong implications for the broad field of VR illusions.",detection thresholds; EEG; eye gaze; hand movement; hand redirection; Virtual reality; VR illusions,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Augmented Breathing via Thermal Feedback in the Nose,UIST - Symposium on User Interface Software and Technology,A*,"We propose, engineer, and study a novel method to augment the feeling of breathing—enabling interactive applications to let users feel like they are inhaling more/less air (perceived nasal airflow). We achieve this effect by cooling or heating the nose in sync with the user’s inhalation. Our illusion builds on the physiology of breathing: we perceive our breath predominantly through the cooling of our nasal cavities during inhalation. This is why breathing in a “fresh” cold environment feels easier than in a “stuffy” hot environment, even when the inhaled volume is the same. Our psychophysical study confirmed that our in-nose temperature stimulation significantly influenced breathing perception in both directions: making it feel harder &amp; easier to breathe. Further, we found that &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX sim 90 ,%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name=""uist24-115-inline1"" type=""svg""/&gt;&lt;/Formula&gt; of the trials were described as a change in perceived airflow/breathing, while only &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX sim 8 ,%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 2&lt;/AltText&gt;&lt;File name=""uist24-115-inline2"" type=""svg""/&gt;&lt;/Formula&gt; as temperature. Following, we engineered a compact device worn across the septum that uses Peltier elements. We illustrate the potential of this augmented breathing in interactive contexts, such as for virtual reality (e.g., rendering ease of breathing crisp air or difficulty breathing with a deteriorated gas mask) and everyday interactions (e.g., in combination with a relaxation application or to alleviate the perceived breathing resistance when wearing a mask).",Breathing; Perception; Respiration; Thermal; Trigeminal,Abstract,True,
ACM DL,conferencePaper,2024,Thermal In Motion: Designing Thermal Flow Illusions with Tactile and Thermal Interaction,UIST - Symposium on User Interface Software and Technology,A*,"This study presents a novel method for creating moving thermal sensations by integrating the thermal referral illusion with tactile motion. Conducted through three experiments on human forearms, the first experiment examined the impact of temperature and thermal actuator placement on perceived thermal motion, finding the clearest perception with a centrally positioned actuator under both hot and cold conditions. The second experiment identified the speed thresholds of perceived thermal motion, revealing a wider detectable range in hot conditions (1.8 cm/s to 9.5cm/s) compared to cold conditions (2.4cm/s to 5.0cm/s). Finally, we integrated our approach into virtual reality (VR) to assess its feasibility through two interaction scenarios. Our results shed light on the comprehension of thermal perception and its integration with tactile cues, promising significant advancements in incorporating thermal motion into diverse thermal interfaces for immersive VR experiences.",Haptics; Thermal Feedback; Thermal Masking; Thermal Motion; Thermal Referral; Vibration-induced Thermal Illusions; VR,Abstract,True,
ACM DL,conferencePaper,2024,VisCourt: In-Situ Guidance for Interactive Tactic Training in Mixed Reality,UIST - Symposium on User Interface Software and Technology,A*,"In team sports like basketball, understanding and executing tactics—coordinated plans of movements among players—are crucial yet complex, requiring extensive practice. These tactics require players to develop a keen sense of spatial and situational awareness. Traditional coaching methods, which mainly rely on basketball tactic boards and video instruction, often fail to bridge the gap between theoretical learning and the real-world application of tactics, due to shifts in view perspectives and a lack of direct experience with tactical scenarios. To address this challenge, we introduce VisCourt, a Mixed Reality (MR) tactic training system, in collaboration with a professional basketball team. To set up the MR training environment, we employed semi-automatic methods to simulate realistic 3D tactical scenarios and iteratively designed visual in-situ guidance. This approach enables full-body engagement in interactive training sessions on an actual basketball court and provides immediate feedback, significantly enhancing the learning experience. A user study with athletes and enthusiasts shows the effectiveness and satisfaction with VisCourt in basketball training and offers insights for the design of future SportsXR training systems.",Immersive Training; In-Situ Visualization; Mixed Reality; SportsXR,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,EVE: Enabling Anyone to Train Robots using Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task requires expensive trajectory data where a trained human annotator moves a physical robot to train it. Consequently, only those with access to robots produce demonstrations to train robots. In this work, we remove this restriction with EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations, without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study (N = 14, D = 30) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching—physically moving a physical robot—in completion time, usability, motion intent communication, enjoyment, and preference (meanp = 0.30). EVE allows users to train robots for personalized tasks, such as sorting desk supplies, organizing ingredients, or setting up board games. We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.",augmented reality; demonstration collection; robotics,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,avaTTAR: Table Tennis Stroke Training with Embodied and Detached Visualization in Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"Table tennis stroke training is a critical aspect of player development. We designed a new augmented reality (AR) system, avaTTAR, for table tennis stroke training. The system provides both “on-body” (first-person view) and “detached” (third-person view) visual cues, enabling users to visualize target strokes and correct their attempts effectively with this dual perspectives setup. By employing a combination of pose estimation algorithms and IMU sensors, avaTTAR&nbsp; captures and reconstructs the 3D body pose and paddle orientation of users during practice, allowing real-time comparison with expert strokes. Through a user study, we affirm avaTTAR&nbsp;’s capacity to amplify player experience and training results.",Augmented Reality; Motor Learning; Table Tennis,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending,UIST - Symposium on User Interface Software and Technology,A*,"There is increased interest in using generative AI to create 3D spaces for Virtual Reality (VR) applications. However, today’s models produce artificial environments, falling short of supporting collaborative tasks that benefit from incorporating the user’s physical context. To generate environments that support VR telepresence, we introduce SpaceBlender, a novel pipeline that utilizes generative AI techniques to blend users’ physical surroundings into unified virtual spaces. This pipeline transforms user-provided 2D images into context-rich 3D environments through an iterative process consisting of depth estimation, mesh alignment, and diffusion-based space completion guided by geometric priors and adaptive text prompts. In a preliminary within-subjects study, where 20 participants performed a collaborative VR affinity diagramming task in pairs, we compared SpaceBlender with a generic virtual environment and a state-of-the-art scene generation framework, evaluating its ability to create virtual spaces suitable for collaboration. Participants appreciated the enhanced familiarity and context provided by SpaceBlender but also noted complexities in the generative environments that could detract from task focus. Drawing on participant feedback, we propose directions for improving the pipeline and discuss the value and design of blended spaces for different scenarios.",generative AI; VR telepresence,Abstract,True,
ACM DL,conferencePaper,2024,SituationAdapt: Contextual UI Optimization in Mixed Reality with Situation Awareness via LLM Reasoning,UIST - Symposium on User Interface Software and Technology,A*,"Mixed Reality is increasingly used in mobile settings beyond controlled home and office spaces. This mobility introduces the need for user interface layouts that adapt to varying contexts. However, existing adaptive systems are designed only for static environments. In this paper, we introduce SituationAdapt, a system that adjusts Mixed Reality UIs to real-world surroundings by considering environmental and social cues in shared settings. Our system consists of perception, reasoning, and optimization modules for UI adaptation. Our perception module identifies objects and individuals around the user, while our reasoning module leverages a Vision-and-Language Model to assess the placement of interactive UI elements. This ensures that adapted layouts do not obstruct relevant environmental cues or interfere with social norms. Our optimization module then generates Mixed Reality interfaces that account for these considerations as well as temporal constraints. For evaluation, we first validate our reasoning module’s capability of assessing UI contexts in comparison to human expert users. In an online user study, we then establish SituationAdapt’s capability of producing context-aware layouts for Mixed Reality, where it outperformed previous adaptive layout methods. We conclude with a series of applications and scenarios to demonstrate SituationAdapt’s versatility.",Adaptive User Interfaces; Large Language Models.; Mixed Reality,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Desk2Desk: Optimization-based Mixed Reality Workspace Integration for Remote Side-by-side Collaboration,UIST - Symposium on User Interface Software and Technology,A*,"Mixed Reality enables hybrid workspaces where physical and virtual monitors are adaptively created and moved to suit the current environment and needs. However, in shared settings, individual users’ workspaces are rarely aligned and can vary significantly in the number of monitors, available physical space, and workspace layout, creating inconsistencies between workspaces which may cause confusion and reduce collaboration. We present Desk2Desk, an optimization-based approach for remote collaboration in which the hybrid workspaces of two collaborators are fully integrated to enable immersive side-by-side collaboration. The optimization adjusts each user’s workspace in layout and number of shared monitors and creates a mapping between workspaces to handle inconsistencies between workspaces due to physical constraints (e.g. physical monitors). We show in a user study how our system adaptively merges dissimilar physical workspaces to enable immersive side-by-side collaboration, and demonstrate how an optimization-based approach can effectively address dissimilar physical layouts.",gesture retargeting; mixed reality; remote collaboration; shared workspace,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Exploring the Effects of Sensory Conflicts on Cognitive Fatigue in VR Remappings,UIST - Symposium on User Interface Software and Technology,A*,"Virtual reality (VR) is found to present significant cognitive challenges due to its immersive nature and frequent sensory conflicts. This study systematically investigates the impact of sensory conflicts induced by VR remapping techniques on cognitive fatigue, and unveils their correlation. We utilized three remapping methods (haptic repositioning, head-turning redirection, and giant resizing) to create different types of sensory conflicts, and measured perceptual thresholds to induce various intensities of the conflicts. Through experiments involving cognitive tasks along with subjective and physiological measures, we found that all three remapping methods influenced the onset and severity of cognitive fatigue, with visual-vestibular conflict having the greatest impact. Interestingly, visual-experiential/memory conflict showed a mitigating effect on cognitive fatigue, emphasizing the role of novel sensory experiences. This study contributes to a deeper understanding of cognitive fatigue under sensory conflicts and provides insights for designing VR experiences that align better with human perceptual and cognitive capabilities.",Cognitive Fatigue; Cognitive Load; Sensory Conflict; VR Remapping,Abstract,True,
ACM DL,conferencePaper,2024,VirtualNexus: Enhancing 360-Degree Video AR/VR Collaboration with Environment Cutouts and Virtual Replicas,UIST - Symposium on User Interface Software and Technology,A*,"Asymmetric AR/VR collaboration systems bring a remote VR user to a local AR user’s physical environment, allowing them to communicate and work within a shared virtual/physical space. Such systems often display the remote environment through 3D reconstructions or 360° videos. While 360° cameras stream an environment in higher quality, they lack spatial information, making them less interactable. We present VirtualNexus, an AR/VR collaboration system that enhances 360° video AR/VR collaboration with environment cutouts and virtual replicas. VR users can define cutouts of the remote environment to interact with as a world-in-miniature, and their interactions are synchronized to the local AR perspective. Furthermore, AR users can rapidly scan and share 3D virtual replicas of physical objects using neural rendering. We demonstrated our system’s utility through 3 example applications and evaluated our system in a dyadic usability test. VirtualNexus extends the interaction space of 360° telepresence systems, offering improved physical presence, versatility, and clarity in interactions.",Computer Mediated Communication; Virtual/Augmented Reality,Keywords,True,
ACM DL,conferencePaper,2024,Personal Time-Lapse,UIST - Symposium on User Interface Software and Technology,A*,"Our bodies are constantly in motion—from the bending of arms and legs to the less conscious movement of breathing, our precise shape and location change constantly. This can make subtler developments (e.g., the growth of hair, or the healing of a wound) difficult to observe. Our work focuses on helping users record and visualize this type of subtle, longer-term change. We present a mobile tool that combines custom 3D tracking with interactive visual feedback and computational imaging to capture personal time-lapse, which approximates longer-term video of the subject (typically, part of the capturing user’s body) under a fixed viewpoint, body pose, and lighting condition. These personal time-lapses offer a powerful and detailed way to track visual changes of the subject over time. We begin with a formative study that examines what makes personal time-lapse so difficult to capture. Building on our findings, we motivate the design of our capture tool, evaluate this design with users, and demonstrate its effectiveness in a variety of challenging examples.",Camera-based UIs; Graphics / 3D; Virtual/Augmented Reality,Keywords,True,
ACM DL,conferencePaper,2024,TouchpadAnyWear: Textile-Integrated Tactile Sensors for Multimodal High Spatial-Resolution Touch Inputs with Motion Artifacts Tolerance,UIST - Symposium on User Interface Software and Technology,A*,"This paper presents TouchpadAnyWear, a novel family of textile-integrated force sensors capable of multi-modal touch input, encompassing micro-gesture detection, two-dimensional (2D) continuous input, and force-sensitive strokes. This thin (&lt;1.5&nbsp;mm) and conformal device features high spatial resolution sensing and motion artifact tolerance through its unique capacitive sensor architecture. The sensor consists of a knitted textile compressive core, sandwiched by stretchable silver electrodes, and conductive textile shielding layers on both sides. With a high-density sensor pixel array (25/cm2), TouchpadAnyWear can detect touch input locations and sizes with millimeter-scale spatial resolution and a wide range of force inputs (0.05&nbsp;N to 20&nbsp;N). The incorporation of miniature polymer domes, referred to as “poly-islands”, onto the knitted textile locally stiffens the sensing areas, thereby reducing motion artifacts during deformation. These poly-islands also provide passive tactile feedback to users, allowing for eyes-free localization of the active sensing pixels. Design choices and sensor performance are evaluated using in-depth mechanical characterization. Demonstrations include an 8-by-8 grid sensor as a miniature high-resolution touchpad and a T-shaped sensor for thumb-to-finger micro-gesture input. User evaluations validate the effectiveness and usability of TouchpadAnyWear in daily interaction contexts, such as tapping, forceful pressing, swiping, 2D cursor control, and 2D stroke-based gestures. This paper further discusses potential applications and explorations for TouchpadAnyWear in wearable smart devices, gaming, and augmented reality devices.",Capacitive; Fabrication; Gesture Recognition; High Resolution; Motion Artifacts; Multimodal; Printing; Soft Wearable; Tactile Display; Textile; Touch Sensor,Abstract,True,
ACM DL,conferencePaper,2024,Gait Gestures: Examining Stride and Foot Strike Variation as an Input Method While Walking,UIST - Symposium on User Interface Software and Technology,A*,"Walking is a cyclic pattern of alternating footstep strikes, with each pair of steps forming a stride, and a series of strides forming a gait. We conduct a systematic examination of different kinds of intentional variations from a normal gait that could be used as input actions without interrupting overall walking progress. A design space of 22 candidate Gait Gestures is generated by adapting previous standing foot input actions and identifying new actions possible in a walking context. A formative study (n=25) examines movement easiness, social acceptability, and walking compatibility with foot movement logging to calculate temporal and spatial characteristics. Using a categorization of these results, 7 gestures are selected for a wizard-of-oz prototype demonstrating an AR interface controlled by Gait Gestures for ordering food and audio playback while walking. As a technical proof-of-concept, a gait gesture recognizer is developed and tested using the formative study data.",foot-based gesture; interaction technique; mixed reality; walking,Keywords,True,
ACM DL,conferencePaper,2024,EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras,UIST - Symposium on User Interface Software and Technology,A*,"In augmented and virtual reality (AR/VR) experiences, a user’s arms and hands can provide a convenient and tactile surface for touch input. Prior work has shown on-body input to have significant speed, accuracy, and ergonomic benefits over in-air interfaces, which are common today. In this work, we demonstrate high accuracy, bare hands (i.e., no special instrumentation of the user) skin input using just an RGB camera, like those already integrated into all modern XR headsets. Our results show this approach can be accurate, and robust across diverse lighting conditions, skin tones, and body motion (e.g., input while walking). Finally, our pipeline also provides rich input metadata including touch force, finger identification, angle of attack, and rotation. We believe these are the requisite technical ingredients to more fully unlock on-skin interfaces that have been well motivated in the HCI literature but have lacked robust and practical methods.",AR/VR; Computer Vision; On-Body Computing; Touch Surfaces and Touch Interaction,Abstract,True,
ACM DL,conferencePaper,2024,SIM2VR: Towards Automated Biomechanical Testing in VR,UIST - Symposium on User Interface Software and Technology,A*,"Automated biomechanical testing has great potential for the development of VR applications, as initial insights into user behaviour can be gained in silico early in the design process. In particular, it allows prediction of user movements and ergonomic variables, such as fatigue, prior to conducting user studies. However, there is a fundamental disconnect between simulators hosting state-of-the-art biomechanical user models and simulators used to develop and run VR applications. Existing user simulators often struggle to capture the intricacies of real-world VR applications, reducing ecological validity of user predictions. In this paper, we introduce sim2vr, a system that aligns user simulation with a given VR application by establishing a continuous closed loop between the two processes. This, for the first time, enables training simulated users directly in the same VR application that real users interact with. We demonstrate that sim2vr can predict differences in user performance, ergonomics and strategies in a fast-paced, dynamic arcade game. In order to expand the scope of automated biomechanical testing beyond simple visuomotor tasks, advances in cognitive models and reward function design will be needed.",automated testing; biomechanical simulation; deep reinforcement learning; interaction design; virtual reality; VR application; VR development; VR simulation alignment,Keywords,True,
ACM DL,conferencePaper,2024,"Hands-on, Hands-off: Gaze-Assisted Bimanual 3D Interaction",UIST - Symposium on User Interface Software and Technology,A*,"Extended Reality (XR) systems with hand-tracking support direct manipulation of objects with both hands. A common interaction in this context is for the non-dominant hand (NDH) to orient an object for input by the dominant hand (DH). We explore bimanual interaction with gaze through three new modes of interaction where the input of the NDH, DH, or both hands is indirect based on Gaze+Pinch. These modes enable a new dynamic interplay between our hands, allowing flexible alternation between and pairing of complementary operations. Through applications, we demonstrate several use cases in the context of 3D modelling, where users exploit occlusion-free, low-effort, and fluid two-handed manipulation. To gain a deeper understanding of each mode, we present a user study on an asymmetric rotate-translate task. Most participants preferred indirect input with both hands for lower physical effort, without a penalty on user performance. Otherwise, they preferred modes where the NDH oriented the object directly, supporting preshaping of the hand, which is more challenging with indirect gestures. The insights gained are of relevance for the design of XR interfaces that aim to leverage eye and hand input in tandem.",3D manipulation; bimanual interaction; eye-tracking; gaze input; virtual reality,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,GradualReality: Enhancing Physical Object Interaction in Virtual Reality via Interaction State-Aware Blending,UIST - Symposium on User Interface Software and Technology,A*,"We present GradualReality, a novel interface enabling a Cross Reality experience that includes gradual interaction with physical objects in a virtual environment and supports both presence and usability. Daily Cross Reality interaction is challenging as the user’s physical object interaction state is continuously changing over time, causing their attention to frequently shift between the virtual and physical worlds. As such, presence in the virtual environment and seamless usability for interacting with physical objects should be maintained at a high level. To address this issue, we present an Interaction State-Aware Blending approach that (i) balances immersion and interaction capability and (ii) provides a fine-grained, gradual transition between virtual and physical worlds. The key idea includes categorizing the flow of physical object interaction into multiple states and designing novel blending methods that offer optimal presence and sufficient physical awareness at each state. We performed extensive user studies and interviews with a working prototype and demonstrated that GradualReality provides better Cross Reality experiences compared to baselines.",Adaptive System; Context-Awareness; Cross Reality; Mixed Reality,Title_Keywords,True,
ACM DL,conferencePaper,2024,StegoType: Surface Typing from Egocentric Cameras,UIST - Symposium on User Interface Software and Technology,A*,"Text input is a critical component of any general purpose computing system, yet efficient and natural text input remains a challenge in AR and VR. Headset based hand-tracking has recently become pervasive among consumer VR devices and affords the opportunity to enable touch typing on virtual keyboards. We present an approach for decoding touch typing on uninstrumented flat surfaces using only egocentric camera-based hand-tracking as input. While egocentric hand-tracking accuracy is limited by issues like self occlusion and image fidelity, we show that a sufficiently diverse training set of hand motions paired with typed text can enable a deep learning model to extract signal from this noisy input. Furthermore, by carefully designing a closed-loop data collection process, we can train an end-to-end text decoder that accounts for natural sloppy typing on virtual keyboards. We evaluate our work with a user study (n=18) showing a mean online throughput of 42.4 WPM with an uncorrected error rate (UER) of 7% with our method compared to a physical keyboard baseline of 74.5 WPM at 0.8% UER, showing progress towards unlocking productivity and high throughput use cases in AR/VR.",augmented reality; hand-tracking; mixed reality; text input; virtual reality,Keywords,True,
ACM DL,conferencePaper,2024,Eye-Hand Movement of Objects in Near Space Extended Reality,UIST - Symposium on User Interface Software and Technology,A*,"Hand-tracking in Extended Reality (XR) enables moving objects in near space with direct hand gestures, to pick, drag and drop objects in 3D. In this work, we investigate the use of eye-tracking to reduce the effort involved in this interaction. As the eyes naturally look ahead to the target for a drag operation, the principal idea is to map the translation of the object in the image plane to gaze, such that the hand only needs to control the depth component of the operation. We have implemented four techniques that explore two factors: the use of gaze only to move objects in X-Y vs. extra refinement by hand, and the use of hand input in the Z axis to directly move objects vs. indirectly via a transfer function. We compared all four techniques in a user study (N=24) against baselines of direct and indirect hand input. We detail user performance, effort and experience trade-offs and show that all eye-hand techniques significantly reduce physical effort over direct gestures, pointing toward effortless drag-and-drop for XR environments.",drag&amp; drop; eye-tracking; gaze interaction; near space; object manipulation,Title_Abstract,True,
ACM DL,conferencePaper,2024,Computational Trichromacy Reconstruction: Empowering the Color-Vision Deficient to Recognize Colors Using Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"We propose an assistive technology that helps individuals with Color Vision Deficiencies (CVD) to recognize/name colors. A dichromat’s color perception is a reduced two-dimensional (2D) subset of a normal trichromat’s three dimensional color (3D) perception, leading to confusion when visual stimuli that appear identical to the dichromat are referred to by different color names. Using our proposed system, CVD individuals can interactively induce distinct perceptual changes to originally confusing colors via a computational color space transformation. By combining their original 2D precepts for colors with the discriminative changes, a three dimensional color space is reconstructed, where the dichromat can learn to resolve color name confusions and accurately recognize colors. Our system is implemented as an Augmented Reality (AR) interface on smartphones, where users interactively control the rotation through swipe gestures and observe the induced color shifts in the camera view or in a displayed image. Through psychophysical experiments and a longitudinal user study, we demonstrate that such rotational color shifts have discriminative power (initially confusing colors become distinct under rotation) and exhibit structured perceptual shifts dichromats can learn with modest training. The AR App is also evaluated in two real-world scenarios (building with lego blocks and interpreting artistic works); users all report positive experience in using the App to recognize object colors that they otherwise could not.",Assistive Technology; Augmented Reality; Color Naming; Color Recognition; Color Vision Deficiency; Dichromat,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,VRCopilot: Authoring 3D Layouts with Generative AI Models in VR,UIST - Symposium on User Interface Software and Technology,A*,"Immersive authoring provides an intuitive medium for users to create 3D scenes via direct manipulation in Virtual Reality (VR). Recent advances in generative AI have enabled the automatic creation of realistic 3D layouts. However, it is unclear how capabilities of generative AI can be used in immersive authoring to support fluid interactions, user agency, and creativity. We introduce VRCopilot, a mixed-initiative system that integrates pre-trained generative AI models into immersive authoring to facilitate human-AI co-creation in VR. VRCopilot presents multimodal interactions to support rapid prototyping and iterations with AI, and intermediate representations such as wireframes to augment user controllability over the created content. Through a series of user studies, we evaluated the potential and challenges in manual, scaffolded, and automatic creation in immersive authoring. We found that scaffolded creation using wireframes enhanced the user agency compared to automatic creation. We also found that manual creation via multimodal specification offers the highest sense of creativity and agency.",Generative AI; Human-AI Co-creation; Virtual Reality,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,AniCraft: Crafting Everyday Objects as Physical Proxies for Prototyping 3D Character Animation in Mixed Reality,UIST - Symposium on User Interface Software and Technology,A*,"We introduce AniCraft, a mixed reality system for prototyping 3D character animation using physical proxies crafted from everyday objects. Unlike existing methods that require specialized equipment to support the use of physical proxies, AniCraft only requires affordable markers, webcams, and daily accessible objects and materials. AniCraft allows creators to prototype character animations through three key stages: selection of virtual characters, fabrication of physical proxies, and manipulation of these proxies to animate the characters. This authoring workflow is underpinned by diverse physical proxies, manipulation types, and mapping strategies, which ease the process of posing virtual characters and mapping user interactions with physical proxies to animated movements of virtual characters. We provide a range of cases and potential applications to demonstrate how diverse physical proxies can inspire user creativity. User experiments show that our system can outperform traditional animation methods for rapid prototyping. Furthermore, we provide insights into the benefits and usage patterns of different materials, which lead to design implications for future research.",3D Animation; Mixed Reality; Physical Proxy; Rigged Character,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Fiery Hands: Designing Thermal Glove through Thermal and Tactile Integration for Virtual Object Manipulation,UIST - Symposium on User Interface Software and Technology,A*,"We present a novel approach to render thermal and tactile feedback to the palm and fingertips through thermal and tactile integration. Our approach minimizes the obstruction of the palm and inner side of the fingers and enables virtual object manipulation while providing localized and global thermal feedback. By leveraging thermal actuators positioned strategically on the outer palm and back of the fingers in interplay with tactile actuators, our approach exploits thermal referral and tactile masking phenomena. Through a series of user studies, we validate the perception of localized thermal sensations across the palm and fingers, showcasing the ability to generate diverse thermal patterns. Furthermore, we demonstrate the efficacy of our approach in VR applications, replicating diverse thermal interactions with virtual objects. This work represents significant progress in thermal interactions within VR, offering enhanced sensory immersion at an optimal energy cost.",Thermal and Haptic Interfaces; Thermal Gloves; Thermal Illusions; Thermal Referral; Virtual Reality,Keywords,True,
ACM DL,conferencePaper,2024,Flip-Pelt: Motor-Driven Peltier Elements for Rapid Thermal Stimulation and Congruent Pressure Feedback in Virtual Reality,UIST - Symposium on User Interface Software and Technology,A*,"This study introduces ""Flip-Pelt,"" a motor-driven peltier device designed to provide rapid thermal stimulation and congruent pressure feedback in virtual reality (VR) environments. Our system incorporates eight motor-driven peltier elements, allowing for the flipping of preheated or cooled elements to the opposite side. In evaluating the Flip-Pelt device, we assess user ability to distinguish between heat/cold sources by their patterns and stiffness, and its impact on enhancing haptic experiences in VR content that involves contact with various thermal sources. Our findings demonstrate that rapid thermal stimulation and congruent pressure feedback provided by Flip-Pelt enhance the recognition accuracy of thermal patterns and the stiffness of virtual objects. These features also improve haptic experiences in VR scenarios through their temporal congruency between tactile and thermal stimuli. Additionally, we discuss the scalability of the Flip-Pelt system to other body parts by proposing design prototypes.",Multimodal Haptics; Thermal Feedback; Virtual Reality,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Hydroptical Thermal Feedback: Spatial Thermal Feedback Using Visible Lights and Water,UIST - Symposium on User Interface Software and Technology,A*,"We control the temperature of materials in everyday interactions, recognizing temperature’s important influence on our bodies, minds, and experiences. However, thermal feedback is an under-explored modality in human-computer interaction partly due to its limited temporal (slow) and spatial (small-area and non-moving) capabilities. We introduce hydroptical thermal feedback, a spatial thermal feedback method that works by applying visible lights on body parts in water. Through physical measurements and psychophysical experiments, our results show: (1) Humans perceive thermal sensations when visible lights are cast on the skin under water, and perceived warmth is greater for lights with shorter wavelengths, (2) temporal capabilities, (3) apparent motion (spatial) of warmth and coolness sensations, and (4) hydroptical thermal feedback can support the perceptual illusion that the water itself is warmer. We propose applications, including virtual reality (VR), shared water experiences, and therapies. Overall, this paper contributes hydroptical thermal feedback as a novel method, empirical results demonstrating its unique capabilities, proposed applications, and design recommendations for using hydroptical thermal feedback. Our method introduces controlled, spatial thermal perceptions to water experiences.",Thermal display; Thermal feedback; Visible light; WaterHCI,Abstract,True,
ACM DL,conferencePaper,2024,SonoHaptics: An Audio-Haptic Cursor for Gaze-Based Object Selection in XR,UIST - Symposium on User Interface Software and Technology,A*,"We introduce SonoHaptics, an audio-haptic cursor for gaze-based 3D object selection. SonoHaptics addresses challenges around providing accurate visual feedback during gaze-based selection in Extended Reality&nbsp;(XR), e.&nbsp;g., lack of world-locked displays in no- or limited-display smart glasses and visual inconsistencies. To enable users to distinguish objects without visual feedback, SonoHaptics employs the concept of cross-modal correspondence in human perception to map visual features of objects (color, size, position, material) to audio-haptic properties (pitch, amplitude, direction, timbre). We contribute data-driven models for determining cross-modal mappings of visual features to audio and haptic features, and a computational approach to automatically generate audio-haptic feedback for objects in the user’s environment. SonoHaptics provides global feedback that is unique to each object in the scene, and local feedback to amplify differences between nearby objects. Our comparative evaluation shows that SonoHaptics enables accurate object identification and selection in a cluttered scene without visual feedback.",Computational Interaction; Extended Reality; Gaze-based Selection; Haptics; Multimodal Feedback; Sonification,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,Auptimize: Optimal Placement of Spatial Audio Cues for Extended Reality,UIST - Symposium on User Interface Software and Technology,A*,"Spatial audio in Extended Reality (XR) provides users with better awareness of where virtual elements are placed, and efficiently guides them to events such as notifications, system alerts from different windows, or approaching avatars. Humans, however, are inaccurate in localizing sound cues, especially with multiple sources due to limitations in human auditory perception such as angular discrimination error and front-back confusion. This decreases the efficiency of XR interfaces because users misidentify from which XR element a sound is coming from. To address this, we propose Auptimize, a novel computational approach for placing XR sound sources, which mitigates such localization errors by utilizing the ventriloquist effect. Auptimize disentangles the sound source locations from the visual elements and relocates the sound sources to optimal positions for unambiguous identification of sound cues, avoiding errors due to inter-source proximity and front-back confusion. Our evaluation shows that Auptimize decreases spatial audio-based source identification errors compared to playing sound cues at the paired visual-sound locations. We demonstrate the applicability of Auptimize for diverse spatial audio-based interactive XR scenarios.",audio perception; computational interaction; Extended Reality,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,SonifyAR: Context-Aware Sound Generation in Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"Sound plays a crucial role in enhancing user experience and immersiveness in Augmented Reality (AR). However, current platforms lack support for AR sound authoring due to limited interaction types, challenges in collecting and specifying context information, and difficulty in acquiring matching sound assets. We present SonifyAR, an LLM-based AR sound authoring system that generates context-aware sound effects for AR experiences. SonifyAR expands the current design space of AR sound and implements a Programming by Demonstration (PbD) pipeline to automatically collect contextual information of AR events, including virtual-content-semantics and real-world context. This context information is then processed by a large language model to acquire sound effects with Recommendation, Retrieval, Generation, and Transfer methods. To evaluate the usability and performance of our system, we conducted a user study with eight participants and created five example applications, including an AR-based science experiment, and an assistive application for low-vision AR users.",Augmented Reality; Authoring Tool; Mixed Reality; Sound,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2024,LoopBot: Representing Continuous Haptics of Grounded Objects in Room-scale VR,UIST - Symposium on User Interface Software and Technology,A*,"In room-scale virtual reality, providing continuous haptic feedback from touching grounded objects, such as walls and handrails, has been challenging due to the user’s walking range and the required force. In this study, we propose LoopBot, a novel technique to provide continuous haptic feedback from grounded objects using only a single user-following robot. Specifically, LoopBot is equipped with a loop-shaped haptic prop attached to an omnidirectional robot that scrolls to cancel out the robot’s displacement, giving the user the haptic sensation that the prop is actually fixed in place, or “grounded.” We first introduce the interaction design space of LoopBot and, as one of its promising interaction scenarios, implement a prototype for the experience of walking while grasping handrails. A performance evaluation shows that scrolling the prop cancels 77.5% of the robot’s running speed on average. A preliminary user test (N = 10) also shows that the subjective realism of the experience and the sense of the virtual handrails being grounded were significantly higher than when the prop was not scrolled. Based on these findings, we discuss possible further development of LoopBot.",Encountered-type haptic device; Room-scale virtual reality; Wheeled robot,Abstract_Keywords,True,
ACM DL,conferencePaper,2024,JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets,UIST - Symposium on User Interface Software and Technology,A*,"We propose JetUnit, a water-based VR haptic system designed to produce force feedback with a wide spectrum of intensities and frequencies through water jets. The key challenge in designing this system lies in optimizing parameters to enable the haptic device to generate force feedback that closely replicates the most intense force produced by direct water jets while ensuring the user remains dry. In this paper, we present the key design parameters of the JetUnit wearable device determined through a set of quantitative experiments and a perception study. We further conducted a user study to assess the impact of integrating our haptic solutions into virtual reality experiences. The results revealed that, by adhering to the design principles of JetUnit, the water-based haptic system is capable of delivering diverse force feedback sensations, significantly enhancing the immersive experience in virtual reality.",force feedback; haptics; VR; water jets,Title_Abstract,True,
ACM DL,conferencePaper,2024,Selfrionette: A Fingertip Force-Input Controller for Continuous Full-Body Avatar Manipulation and Diverse Haptic Interactions,UIST - Symposium on User Interface Software and Technology,A*,"We propose Selfrionette, a controller that uses fingertip force input to drive avatar movements in virtual reality (VR). This system enables users to interact with virtual objects and walk in VR using only fingertip force, overcoming physical and spatial constraints. Additionally, by fixing users’ fingers, it provides users with counterforces equivalent to the applied force, allowing for diverse and wide dynamic range haptic feedback by adjusting the relationship between force input and virtual movement. To evaluate the effectiveness of the proposed method, this paper focuses on hand interaction as a first step. In User Study 1, we measured usability and embodiment during reaching tasks under Selfrionette, body tracking, and finger tracking conditions. In User Study 2, we investigated whether users could perceive haptic properties such as weight, friction, and compliance under the same conditions as User Study 1. Selfrionette was found to be comparable to body tracking in realism of haptic interaction, enabling embodied avatar experiences even in limited spatial conditions.",avatar manipulation; embodiment; force-based input; haptics,Abstract,True,
ACM DL,conferencePaper,2024,"SpinShot: Optimizing Both Physical and Perceived Force Feedback of Flywheel-Based, Directional Impact Handheld Devices",UIST - Symposium on User Interface Software and Technology,A*,"Real-world impact, such as hitting a tennis ball and a baseball, generates instantaneous, directional impact forces. However, current ungrounded force feedback technologies, such as air jets and propellers, can only generate directional impulses that are 10x-10,000x weaker. We present SpinShot, a flywheel-based device with a solenoid-actuated stopper capable of generating directional impulse of 22Nm in 1ms, which is more than 10x stronger than prior ungrounded directional technologies. Furthermore, we present a novel force design that reverses the flywheel immediately after the initial impact, to significantly increase the perceived magnitude. We conducted a series of two formative, perceptual studies (n=16, 18), followed by a summative user experience study (n=16) that compared SpinShot vs. moving mass (solenoid) and vs. air jets in a VR baseball hitting game. Results showed that SpinShot significantly improved realism, immersion, magnitude (p &lt; .01) compared to both baselines, but significantly reduced comfort vs. air jets primarily due to the 2.9x device weight. Overall, SpinShot was preferred by 63-75% of the participants.",Flywheel; Handheld Device; Haptic; Impact Force; Perceptual Design; Ungrounded Force Feedback; Virtual Reality,Keywords,True,
ACM DL,conferencePaper,2024,CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision,UIST - Symposium on User Interface Software and Technology,A*,"Cooking is a central activity of daily living, supporting independence as well as mental and physical health. However, prior work has highlighted key barriers for people with low vision (LV) to cook, particularly around safely interacting with tools, such as sharp knives or hot pans. Drawing on recent advancements in computer vision (CV), we present CookAR, a head-mounted AR system with real-time object affordance augmentations to support safe and efficient interactions with kitchen tools. To design and implement CookAR, we collected and annotated the first egocentric dataset of kitchen tool affordances, fine-tuned an affordance segmentation model, and developed an AR system with a stereo camera to generate visual augmentations. To validate CookAR, we conducted a technical evaluation of our fine-tuned model as well as a qualitative lab study with 10 LV participants for suitable augmentation design. Our technical evaluation demonstrates that our model outperforms the baseline on our tool affordance dataset, while our user study indicates a preference for affordance augmentations over the traditional whole object augmentations.",accessibility; affordance segmentation; augmented reality; visual augmentation,Keywords,True,
IEEE,conferencePaper,2024,Context-Relevant Locations as an Alternative to the Place Illusion in Augmented Reality,VR - International Symposium Virtual Reality,A*,"Presence is a powerful aspect of Virtual Reality (VR). However, there has been no consensus on how to achieve presence in Augmented Reality (AR) or whether it exists at all. The Place Illusion, a key component in presence as defined in VR, cannot be obtained in AR as there is no way to make the user feel as though they are transported somewhere else when they are limited to what they can physically see in front of them. However, recently it has been argued that coherence or congruence are important parts of the Place and Plausibility Illusions. The implication for AR is that the AR content might invoke a higher Plausibility Illusion if it is consistent with the physical place the content is situated in. In this study, we define the concept of a Context-Relevant Location (CRL), a physical place that is congruent with the experience. We present a study with a between-subjects design that allowed users to interact with AR objects in a CRL and in a generic environment. The results indicate that presence was higher in the CRL setting than the generic environment, contribute to the debate about providing a concrete description of presence-like phenomena in AR, and posit that CRLs play a similar role to the Place Illusion in an AR setting.",Augmented reality; context-relevant location; plausibility; presence,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,"Lifter for VR Headset: Enhancing Immersion, Presence, Flow, and Alleviating Mental and Physical Fatigue during Prolonged Use",VR - International Symposium Virtual Reality,A*,"The virtual reality (VR) headset is still relatively heavy, causing a significant physical and mental burden and negatively affecting the VR user experience, particularly during extended periods of use. In this paper, we present a prototype design of the “Lifter,” which utilizes a counterbalanced wire-pulley mechanism to partially relieve the weight of the VR headset (between 50% and 85%). The human subject study has confirmed that the Lifter relieved not only physical fatigue but also significantly improved mental burden, sense of immersion, presence, and flow (perception of time passing) during prolonged usage (30 minutes or more).",Head-Mounted Display; Headset Weight; Weight Reduction,Abstract,True,
IEEE,conferencePaper,2024,"MeetingBenji: Tackling Cynophobia with Virtual Reality, Gamification, and Biofeedback",VR - International Symposium Virtual Reality,A*,"Phobias, particularly animal phobias like cynophobia (fear of dogs), disrupt the lives of those affected by, for instance, limiting outdoor activities. While virtual reality exposure therapy (VRET) has emerged as a potential treatment for this phobia, these efforts have been limited by high dropout rates and a lack of ability to handle stressful situations in people who suffer from cynophobia. Inspired by these challenges, we present MeetingBenji, a VRET system for cynophobia that uses (i) gamification to enhance motivation and engagement, and (ii) biofeedback to facilitate self-control and reduce physiological responses. In a study (N=10) that compared the effects of displaying dogs in 3D scenes and 360º videos using the Behavioral Approach Test (BAT) – in which participants are increasingly exposed to the source of phobia – participants reported a high level of immersion to the exposure sequence. Further, they reported feeling more anxiety with 3D content than 360º video (60%), lower heart rates in the presence of biofeedback (between 1.71% and 7.46%), and improved self-control across the three exposure levels. They appreciated our gamified elements – completing all exposure levels. This study suggests that VRET with gamification and biofeedback is an effective approach to stimulate the habituation of people with cynophobia.",biofeedback; cynophobia; gamification; VR exposure therapy,Title_Abstract,True,
IEEE,conferencePaper,2024,iStrayPaws: Immersing in a Stray Animal's World through First-Person VR to Bridge Human-Animal Empathy,VR - International Symposium Virtual Reality,A*,"While Virtual Reality Perspective-Taking (VRPT) demonstrates its efficiency in inducing empathy, its application primarily focuses on vulnerable humans, not animals. Existing animal-related works mainly targets farm animals and wildlife. In this work, we focus on stray animals and introduce iStrayPaws, a VRPT system that simulates stray animals’ challenging lives. The system offers users an immersive first-person journey into the world of stray animals encountering different difficulties like inclement weather, hunger, and illnesses. Enriched with audio-visual and kinesthetic design, the system seeks to deepen users’ understanding of stray animals’ life and foster profound emotional connections. To evaluate the system, a user study was conducted, which showed that VRPT recipients exhibited significant improvement in both state and trait empathy compared to traditional method. Our research not only delivers a novel, accessible, and interactive animal empathy experience but also provides innovative solutions for addressing stray animal issues and advancing broader animal welfare work.",Embodied Experience; Empathy; Hand Mocap; Stray Animals; Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Exploring Presence in Interactions with LLM-Driven NPCs: A Comparative Study of Speech Recognition and Dialogue Options,VR - International Symposium Virtual Reality,A*,"Combining modern technologies like large-language models (LLMs), speech-to-text, and text-to-speech can enhance immersion in virtual reality (VR) environments. However, challenges exist in effectively implementing LLMs and educating users. This paper explores implementing LLM-powered virtual social actors and facilitating user communication. We developed a murder mystery game where users interact with LLM-based non-playable characters (NPCs) through interrogation, clue-gathering, and exploration. Two versions were tested: one using speech recognition and another with traditional dialog boxes. While both provided similar social presence, users felt more immersed with speech recognition but found it overwhelming, while the dialog version was more challenging. Slow NPC response times were a source of frustration, highlighting the need for faster generation or better masking for a seamless experience.",Immersive systems; Large Language Models (LLM); NPC; Presence; Social Actors; Speech Recognition; VR,Abstract,True,
IEEE,conferencePaper,2024,Effects of Different Tracker-driven Direction Sources on Continuous Artificial Locomotion in VR,VR - International Symposium Virtual Reality,A*,"Continuous artificial locomotion in VR typically involves users selecting their direction using controller input, with the forward direction determined by the Head, Hands, or less commonly, the Hip. The effects of these different sources on user experience are under-explored, and Feet have not been used as a direction source. To address these gaps, we compared these direction sources, including a novel Feet-based technique. A user study with 22 participants assessed these methods in terms of performance, preference, motion sickness, and sense of presence. Our findings indicate high levels of presence and minimal motion sickness across all methods. Performance differences were noted in one task, where the Head outperformed the Hand. The Hand method was the least preferred, feeling less natural and realistic. The Feet method was found to be more natural than the Head and more realistic than the Hip. This study enhances understanding of direction sources in VR locomotion and introduces Feet-based direction as a viable alternative.",Continuous Locomotion; User Studies; Virtual Reality,Keywords,True,
IEEE,conferencePaper,2024,Influence of Rotation Gains on Unintended Positional Drift during Virtual Steering Navigation in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Unintended Positional Drift (UPD) is a phenomenon that occurs during navigation in Virtual Reality (VR). It is characterized by the user’s unconscious or unintentional physical movements in the workspace while using a locomotion technique (LT) that does not require physical displacement (e.g., steering, teleportation). Recent work showed that some factors, such as the LT used and the type of trajectory, can influence UPD. However, little is known about the influence of rotation gains (commonly used in redirection-based LTs) on UPD during navigation in VR. In this paper, we conducted two user studies to assess the influence of rotation gains on UPD. In the first study, participants had to perform consecutive turns in a corridor virtual environment. In the second study, participants had to explore a large office floor and collect spheres freely. We compared the conditions between rotation gains and without gains, and we also varied the turning angle to perform the turns while considering factors such as sensitivity to cybersickness and the learning effect. We found that rotation gains and lower turning angles decreased UPD during the first study, but the presence of rotation gains increased UPD in the second study. This work contributes to the understanding of UPD, which tends to be an overlooked topic and discusses the design implications of these results for improving navigation in VR.",Locomotion Techniques; Rotation Gains; Unintended Positional Drift; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Semi-Automated Guided Teleportation through Immersive Virtual Environments,VR - International Symposium Virtual Reality,A*,"Immersive knowledge spaces like museums or cultural sites are often explored by traversing pre-defined paths that are curated to unfold a specific educational narrative. To support this type of guided exploration in VR, we present a semi-automated, hands-free path traversal technique based on teleportation that features a slow-paced interaction workflow targeted at fostering knowledge acquisition and maintaining spatial awareness. In an empirical user study with 34 participants, we evaluated two variations of our technique, differing in the presence or absence of intermediate teleportation points between the main points of interest along the route. While visiting additional intermediate points was objectively less efficient, our results indicate significant benefits of this approach regarding the user’s spatial awareness and perception of interface dependability. However, the user’s perception of flow, presence, attractiveness, perspicuity, and stimulation did not differ significantly. The overall positive reception of our approach encourages further research into semi-automated locomotion based on teleportation and provides initial insights into the design space of successful techniques in this domain.",3D Navigation; 3D User Interfaces; Guided Navigation; Guided Tour; Head-Mounted Display; Teleportation; Virtual Reality,Keywords,True,
IEEE,conferencePaper,2024,The Effects of Electrical Stimulation of Ankle Tendons on Redirected Walking with the Gradient Gain,VR - International Symposium Virtual Reality,A*,"As a redirected walking technique, a method has been proposed to enable users to walk in an undulating virtual space even in a flat physical environment by setting the slope of the floor in the virtual environment to be different from that in the physical environment without causing discomfort. However, the slope range in which discrepancies between visual and proprioceptive sensations are not perceived is limited, restricting the slopes that can be presented. In this study, we proposed redirected walking using electrical stimulation of the Achilles and tibialis anterior muscle tendons, extending the applicable slope range of redirected walking without compromising the natural gait sensation. Electrical stimulation of the ankle tendons affects the proprioceptive sensation and gives the illusion of tilting in the standing posture, expanding the applicable slope range. Two experiments showed that the proposed method improved the experience of uphill and downhill walking in terms of the range of the virtual slope where a high naturalness of gait and a high congruency of visual and proprioceptive sensations are maintained. Notably, electrical stimulation of the Achilles tendons significantly improved the naturalness of the walking experience during virtual downhill walking, which has been considered more challenging in previous studies.",Electrical stimulation of ankle tendons; Locomotion technique; Redirected walking; Transcutaneous electrical stimulation; Virtual reality,Keywords,True,
IEEE,conferencePaper,2024,Neural Motion Tracking: Formative Evaluation of Zero Latency Rendering,VR - International Symposium Virtual Reality,A*,"Low motion-to-photon latencies between physical movement and rendering updates are crucial for an immersive virtual reality (VR) experience and to avoidusers’ discomfort and sickness. Current methods aim to minimize the delay between the motion measurement and rendering at the cost of increasing technical complexity and possibly decreasing accuracy. By relying on capturing physical motion, these strategies will, by nature, not result in zero latency rendering or will be based on prediction and resulting uncertainty. This paper presents and evaluates a novel alternative and proof of principle for VR motion tracking that could enable motion-to-photon latencies of zero and below zero in time. We termed our concept Neural Motion Tracking, which we define as the sensing and assessment of motion through human neural activation of the somatic nervous system. In contrast to measuring physical activity, the key principle is that we aim to utilize the physiological timeframe between a user’s intention and the execution of motion. We aim to foresee upcoming motion ahead of the physical movement, by sampling preceding electromyographic signals before the muscle activation. The electromechanical delay (EMD) between potential change in the muscle activation and actual physical movement opens a gap in which measurement can be taken and evaluated before the physical motion. In a first proof of principle, we evaluated the concept with two activities, arm bending and head rotation, measured with a binary activation measure. Our results indicate that it is possible to predict movement and update a rendering up to 2&nbsp;ms before its physical execution, which is assessed by optical tracking after approximately 4&nbsp;ms. However, to make the best use of this advantage, electromyography (EMG) sensor data should be as high quality as possible (i.e., low noise and from muscle-near electrodes). Our results empirically quantify this characteristic for the first time when compared to state-of-the-art optical tracking systems for VR. We discuss our results and potential pathways to motivate further work toward marker- and latency-less motion tracking.",augmented reality; electromyography; latency; mixed reality; tracking; Virtual reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Investigation of Redirection Algorithms in Small Tracking Spaces,VR - International Symposium Virtual Reality,A*,"In virtual reality, redirected walking lets users walk in larger virtual spaces than the physical tracking space set aside for their movements. This benefits immersion and spatial navigation compared to virtual locomotion techniques such as teleportation or joystick control. Different algorithms have tried to optimise redirected walking. These algorithms have been tested in simulation in large spaces and with small user studies. However, few studies have looked at the user experience of these algorithms in small tracking spaces. We conducted a user study to compare the performance of different redirected walking algorithms in a small tracking space of 3.5m x 3.5m. Three algorithms were chosen based on their approaches to redirection – Reset Only, Steer to Centre and Alignment Based Redirection Control. 36 people participated in the study. It was found users preferred Reset Only in the tracking space. Reset Only redirects users less and is easier to implement than Steer to Centre or Alignment Based Redirection Control. Additionally, Reset Only had similar performance to Steer to Centre and better task performance than Alignment Based Redirection Control despite resetting users more often. Based on these findings, we provide guidelines for developers working in small tracking spaces.",locomotion; redirected walking; user experience; user study; virtual reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Interactive Multi-GPU Light Field Path Tracing Using Multi-Source Spatial Reprojection,VR - International Symposium Virtual Reality,A*,"Path tracing combined with multiview displays enables progress towards achieving ultrarealistic virtual reality. However, multiview displays based on light field technology impose a heavy workload for real-time graphics due to the large number of views to be rendered. In order to achieve low latency performance, computational effort can be reduced by path tracing only some views (source views), and synthesizing the remaining views (target views) through spatial reprojection, which reuses path traced pixels from source views to target views. Deciding the number of source views with respect to the computational resources is not trivial, since spatial reprojection introduces dependencies in the otherwise trivially parallel rendering pipeline and path tracing multiple source views increases the computation time. In this paper, we demonstrate how to reach near-perfect linear multi-GPU scalability through a coarse-grained distribution of the light field path tracing workload. Our multi-source method path traces a single source view per GPU, which helps decreasing the number of dependencies. Reducing dependencies reduces the overhead of image transfers and G-Buffers rasterization used for spatial reprojection. In a node of 4 × RTX A6000 GPUs, given 4 source views, we reach a light field rendering frequency of 3–19 Hz, which corresponds to interactive rate. On four test scenes, we outperform state-of-the-art multi-GPU light field path tracing pipelines, achieving a speedup of 1.65 × up to 4.63 × for 1D light fields of dimension 100 × 1, each view having a resolution of 768 × 432, and 1.51 × up to 3.39 × for 2D stereo near-eye light fields of size 12 × 6 (left eye: 6 × 6 views and right eye: 6 × 6 views), 1024 × 1024 per view.",Dependencies; Multiview; Parallel Rendering; View Synthesis,Abstract,True,
IEEE,conferencePaper,2024,Exploring Visual Conditions in Virtual Reality for the Teleoperation of Robots,VR - International Symposium Virtual Reality,A*,"In the teleoperation of robots, the absence of proprioception means that visual information plays a crucial role. Previous research has investigated methods to offer optimal vantage points to operators during teleoperation, with virtual reality (VR) being proposed as a mechanism to give the operator intuitive control over the viewpoint for improved visibility and interaction. However, the most effective perspective for robot operation and the optimal portrayal of the robot within the virtual environment remain unclear. This paper examines the impact of various visual conditions on users’ efficiency and preference in controlling a simulated robot via VR. We present a user study that compares two operating perspectives and three robot appearances. The findings indicate mixed user preferences and highlight distinct advantages associated with each perspective and appearance combination. We conclude with recommendations on selecting the most beneficial perspective and appearance based on specific application requirements.",3D User Interfaces; Teleoperation; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Choose Your Reference Frame Right: An Immersive Authoring Technique for Creating Reactive Behavior,VR - International Symposium Virtual Reality,A*,"Immersive authoring enables content creation for virtual environments without a break of immersion. To enable immersive authoring of reactive behavior for a broad audience, we present modulation mapping, a simplified visual programming technique. To evaluate the applicability of our technique, we investigate the role of reference frames in which the programming elements are positioned, as this can affect the user experience. Thus, we developed two interface layouts: ""surround-referenced"" and ""object-referenced"". The former positions the programming elements relative to the physical tracking space, and the latter relative to the virtual scene objects. We compared the layouts in an empirical user study (n = 34) and found the surround-referenced layout faster, lower in task load, less cluttered, easier to learn and use, and preferred by users. Qualitative feedback, however, revealed the object-referenced layout as more intuitive, engaging, and valuable for visual debugging. Based on the results, we propose initial design implications for immersive authoring of reactive behavior by visual programming. Overall, modulation mapping was found to be an effective means for creating reactive behavior by the participants.",Empirical Evaluation; Immersive Authoring; Spatial Reference Frames; Virtual Reality; Visual Programming,Keywords,True,
IEEE,conferencePaper,2024,Motion Passwords,VR - International Symposium Virtual Reality,A*,"This paper introduces “Motion Passwords”, a novel biometric authentication approach where virtual reality users verify their identity by physically writing a chosen word in the air with their hand controller. This method allows combining three layers of verification: knowledge-based password input, handwriting style analysis, and motion profile recognition. As a first step towards realizing this potential, we focus on verifying users based on their motion profiles. We conducted a data collection study with 48 participants, who performed over 3800 Motion Password signatures across two sessions. We assessed the effectiveness of feature-distance and similarity-learning methods for motion-based verification using the Motion Passwords as well as specific and uniform ball-throwing signatures used in previous works. In our results, the similarity-learning model was able to verify users with the same accuracy for both signature types. This demonstrates that Motion Passwords, even when applying only the motion-based verification layer, achieve reliability comparable to previous methods. This highlights the potential for Motion Passwords to become even more reliable with the addition of knowledge-based and handwriting style verification layers. Furthermore, we present a proof-of-concept Unity application demonstrating the registration and verification process with our pretrained similarity-learning model. We publish our code, the Motion Password dataset, the pretrained model, and our Unity prototype on https://github.com/cschell/MoPs",Authentication; Biometrics; Extended Reality; Verification,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Out-Of-Virtual-Body Experiences: Virtual Disembodiment Effects on Time Perception in VR,VR - International Symposium Virtual Reality,A*,"This paper presents a novel experiment investigating the relationship between virtual disembodiment and time perception in Virtual Reality (VR). Recent work demonstrated that the absence of a virtual body in a VR application changes the perception of time. However, the effects of simulating an out-of-body experience (OBE) in VR on time perception are still unclear. We designed an experiment with two types of virtual disembodiment techniques based on viewpoint gradual transition: a virtual body’s behind view and facing view transitions. We investigated their effects on forty-four participants in an interactive scenario where a lamp was repeatedly activated and time intervals were estimated. Our results show that, while both techniques elicited a significant virtual disembodiment perception, time duration estimations in the minute range were only shorter in the facing view compared to the eye view condition. We believe that reducing agency in the facing view is a key factor in the time perception alteration. This provides first steps towards a novel approach to manipulating time perception in VR, with potential applications for mental health treatments such as schizophrenia or depression and for improving our understanding of the relation between body, virtual body, and time.",Avatar; Disembodiment; Embodiment; Plausibility; Presence; Time Perception; Virtual Body; Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Some Times Fly: The Effects of Engagement and Environmental Dynamics on Time Perception in Virtual Reality,VR - International Symposium Virtual Reality,A*,"An hour spent with friends seems shorter than an hour waiting for a medical appointment. Many physiological and psychological factors, such as body temperature and emotions, have been shown to correlate with our subjective perception of time. Experiencing virtual reality (VR) has been observed to make users significantly underestimate the duration. This paper explores the effect of virtual environment characteristics on time perception, focusing on two key parameters: user engagement and environmental dynamics. We found that increased presence and interaction with the environment significantly decreased the users’ estimation of the VR experience duration. Furthermore, while a dynamic environment lacks significance in shifting perception toward one specific direction, that is, underestimation or overestimation of the durations, it significantly distorts perceived temporal length. Exploiting these two factors’ influence smartly constitutes a powerful tool in designing intelligent and adaptive virtual environments that can reduce stress, alleviate boredom, and improve well-being by adjusting the pace at which we experience the passage of time.",Environmental Dynamics; Time Perception; User Engagement; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Enhancing VR Sketching with a Dynamic Shape Display,VR - International Symposium Virtual Reality,A*,"Sketching on virtual objects in Virtual Reality (VR) can be challenging due to the lack of a physical surface that constrains the movement and provides haptic feedback for contact and movement. While using a flat physical drawing surface has been proposed, it creates a significant discrepancy between the physical and virtual surfaces when sketching on non-planar virtual objects. We propose using a dynamic shape display that physically mimics the shape of a virtual surface, allowing users to sketch on a virtual surface as if they are sketching on a physical object’s surface. We demonstrate this using VRScroll, a shape-changing device that features seven independently controlled flaps to imitate the shape of a virtual surface automatically. Our user study showed that participants exhibited higher precision when tracing simple shapes with the dynamic shape display and produced clearer sketches. We also provided several design implications for dynamic shape displays aimed at enabling precise sketching in VR.",dynamic shape display; on-surface interactions; virtual reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Simulating Object Weight in Virtual Reality: The Role of Absolute Mass and Weight Distributions,VR - International Symposium Virtual Reality,A*,"Weight interfaces enable users of Virtual Reality (VR) to perceive the weight of virtual objects, significantly enhancing realism and enjoyment. While research on these systems primarily focused on their implementation, little attention has been given to determining the weight to be rendered by them: As the perceived weight of objects is influenced not only by their absolute mass, but also by their weight distribution and prior expectations, it is currently unknown which simulated mass provides the most realistic representation of a given object. We conducted a study, in which 30 participants chose the best fitting weight for a virtual object in 54 experimental trials. Across these trials, we systematically varied the virtual objects’ visual mass (three levels), their weight distribution (six levels), and the position of the physical mass on the grip (three levels). Our Bayesian analysis suggests that the visual weight distribution of objects does not affect which absolute physical mass best represents them, whereas the position of the provided physical mass does. Additionally, participants overweighted virtual objects with lower visual mass while underweighting objects with higher visual mass. We discuss how these findings can be leveraged by designers of weight interfaces and VR experiences to optimize realism.",multisensory integration; virtual reality; weight interfaces; weight perception; weight simulation,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Enriching Industrial Training Experience in Virtual Reality with Pseudo-Haptics and Vibrotactile Stimulation,VR - International Symposium Virtual Reality,A*,"Virtual Reality (VR) technology facilitates effective, flexible, and safe industrial training for novice technicians when on-site training is not feasible. However, previous research has shown that training in VR may be less successful than traditional learning approaches in real-world settings, and haptic interaction may be the key to improving virtual training. In this study, we integrated pseudo-haptic feedback from motion delay with vibrotactile stimulation to enhance the sense of presence, enjoyment, and the perception of physical properties in VR, which may be crucial for achieving faithful simulations. The impact of combined haptic support was assessed in a complex industrial training procedure completing a variety of tasks such as component assembly and cleaning. The results indicate that vibrotactile cues are beneficial for presence and enjoyment, whereas pseudo-haptic illusions effectively enable kinesthetic sensations. Furthermore, multimodal haptic feedback that mixed the two yielded the most advantageous outcomes. Our findings highlight the potential of the pseudo-haptic and vibrotactile fusion in industrial training scenarios, presenting practical implications of the state-of-the-art haptic technologies for virtual learning.",Haptics; Industrial training; Multimodal interaction; User study; Virtual reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Investigating the Impact of Odors and Visual Congruence on Motion Sickness in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Motion sickness is a prevalent side effect of exposure to virtual reality (VR). Previous work found that pleasant odors can be effective in alleviating symptoms of motion sickness such as nausea. However, it is unknown whether pleasant odors that do not match the anticipated scent of the virtual environment are also effective as they could, in turn, amplify symptoms such as disorientation. Therefore, we conducted a study with 24 participants experiencing a pleasant odor (rose) and an unpleasant odor (garlic) while being immersed in a virtual environment involving either virtual roses or garlic. We found that participants had lower motion sickness when experiencing the rose odor, however, only in the rose environment. Accordingly, we also showed that the sense of disorientation was lower for the rose odor, however, only while being immersed in the rose environment. Results indicate that whether pleasant odors are effective in alleviating motion sickness symptoms depends on the visual appearance of the virtual environment. We discuss possible explanations for such effects to occur. Our work contributes to the goal of mitigating visually induced motion sickness in VR.",motion sickness; odor; olfaction; virtual reality; visually induced motion sickness,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Generative Terrain Authoring with Mid-air Hand Sketching in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Terrain generation and authoring in Virtual Reality (VR) offers unique benefits, including 360-degree views, improved spatial perception, immersive and intuitive design experience and natural input modalities. Yet even in VR it can be challenging to integrate natural input modalities, preserve artistic controls and lower the effort of landscape prototyping. To tackle these challenges, we present our VR-based terrain generation and authoring system, which utilizes hand tracking and a generative model to allow users to quickly prototype natural landscapes, such as mountains, mesas, canyons and volcanoes. Via positional hand tracking and hand gesture detection, users can use their hands to draw mid-air strokes to indicate desired shapes for the landscapes. A Conditional Generative Adversarial Network trained by using real-world terrains and their height maps then helps to generate a realistic landscape which combines features of training data and the mid-air strokes. In addition, users can use their hands to further manipulate their mid-air strokes to edit the landscapes. In this paper, we explore this design space and present various scenarios of terrain generation. Additionally, we evaluate our system across a diverse user base that varies in VR experience and professional background. The study results indicate that our system is feasible, user-friendly and capable of fast prototyping.",Generative Terrain Authoring; Hand Gesture Control; Hand Sketching; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,How Different Is the Perception of Vibrotactile Texture Roughness in Augmented versus Virtual Reality?,VR - International Symposium Virtual Reality,A*,"Wearable haptic devices can modify the haptic perception of an object touched directly by the finger in a portable and unobtrusive way. In this paper, we investigate whether such wearable haptic augmentations are perceived differently in Augmented Reality (AR) vs. Virtual Reality (VR) and when touching with a virtual hand instead of one’s own hand. We first designed a system for real-time rendering of vibrotactile virtual textures without constraints on hand movements, integrated with an immersive visual AR/VR headset. We then conducted a psychophysical study with 20 participants to evaluate the haptic perception of virtual roughness textures on a real surface touched directly with the finger (1) without visual augmentation, (2) with a realistic virtual hand rendered in AR, and (3) with the same virtual hand in VR. On average, participants overestimated the roughness of haptic textures when touching with their real hand alone and underestimated it when touching with a virtual hand in AR, with VR in between. Exploration behaviour was also slower in VR than with real hand alone, although subjective evaluation of the texture was not affected. We discuss how the perceived visual delay of the virtual hand may produce this effect.",Augmented Reality; Haptic Perception; Psychophysical Study; Roughness Textures; Virtual Hands; Virtual Reality; Wearable Haptics,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,TeenWorlds: Supporting Emotional Expression for Teenagers with their Parents and Peers through a Collaborative VR Experience,VR - International Symposium Virtual Reality,A*,"Adolescence is a period of growth and exploration, marked by influential relationships with peers and parents. These relationships are essential for teenagers’ well-being, highlighting the need to support their interpersonal interactions. Emotional expression is key in resolving conflicts that can frequently arise. This paper investigates the potential of TeenWorlds, a Virtual Reality (VR) application, to facilitate emotional expression and shared understanding among teenagers and their peers and parents. In our study, teenagers, accompanied by either a peer or a parent (total n=42), used TeenWorlds to visually represent their emotions during a shared conflict, discuss them, and collaborate on a joint VR drawing. Our findings indicate that TeenWorlds can foster communication, reflection, and strengthen interpersonal relationships. However, notable differences were observed in interactions with peers versus parents. We contribute insights into designing VR systems that support reflective experiences and meaningful family interactions, ultimately enhancing the well-being of adolescents, parents, and families.",adolescent; CCI; collaboration; emotional expression; family; parents; reflection; teenager; Virtual Reality; youth,Abstract_Keywords,True,
IEEE,conferencePaper,2024,HistoLab VR: A User Elicitation Study Exploring the Potential of Virtual Reality Game-based Learning for Hazard Awareness,VR - International Symposium Virtual Reality,A*,"Occupational medicine is a vital field for workplace safety and health but often encounters challenges in engaging students and effectively communicating subtle yet critical workplace hazards. To tackle these issues, we developed HistoLab VR, a Virtual Reality (VR) game that immerses participants in a histology lab environment based on real-world practice. Our comprehensive user study with 17 students and experts assessed the game’s impact on hazard awareness, interest in occupational medicine, and user experience through quantitative and qualitative measures. Our findings show that HistoLab VR not just immersed participants in a relatable histology lab worker experience but that it effectively raised awareness about subtle hazards and conveyed the inherent stress of the job. We discuss our results and highlight the potential of VR as a valuable educational tool for occupational medicine training.",anxiety; education; ergonomics; hazard awareness; histology laboratory; occupational medicine; serious games.; workplace,Title_Abstract,True,
IEEE,conferencePaper,2024,Game-Based Motivation: Enhancing Learning with Achievements in a Customizable Virtual Reality Environment,VR - International Symposium Virtual Reality,A*,"Digital learning experiences that promote interactive learning and engagement are becoming increasingly relevant. Educational games can be used to create an engaging learning atmosphere that allows knowledge acquisition through hands-on activities. Combining it with virtual reality (VR) allows users to interact with virtual environments, leading to a highly immersive learning experience. In this study, we explore how game achievements impact motivation and learning in a customizable VR learning environment. Using an A/B test involving 50 students, we utilized an interactive wave simulation to assess motivation, engagement, and the overall learning experience. Data collection involved standardized questionnaires, along with tracking interaction time and interactions within the virtual environment. The findings revealed that users who earned game achievements to unlock customization features felt significantly more accomplished when they mastered challenges and obtained all achievements. However, it was observed that adding achievements could also create pressure on students, leading to feelings of embarrassment when facing task failures. While achievements have the potential to enhance engagement and motivation, their excessive use may lead to distractions, anxiety, and reduced overall engagement. It shows that is crucial to find a good balance in employing game achievements within educational environments to ensure they contribute positively to the learning experience without causing undue stress or deterring learners.",customizable learning; immersive learning; interactive simulations; STEM education; virtual reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Hands or Controllers? How Input Devices and Audio Impact Collaborative Virtual Reality,VR - International Symposium Virtual Reality,A*,"Advancing virtual reality technologies are enabling real-time virtual-face to virtual-face communication. Hand tracking systems that are integrated into Head-Mounted Displays (HMD) enable users to directly interact with their environments and with each other using their hands as opposed to using controllers. Due to the novelties of these technologies our understanding of how they impact our interactions is limited. In this paper, we investigate the consequences of using different interaction control systems, hand tracking or controllers, when interacting with others in a virtual environment. We design and implement NASA’s Survival on the Moon teamwork evaluation exercise in virtual reality (VR) and test for effects with and without allowing verbal communication. We evaluate social presence, perceived comprehension, team cohesion, group synergy, task workload, as well as task performance and duration. Our findings reveal that audio communication significantly enhances social presence, perceived comprehension, and team cohesion, but it also increases effort workload and negatively impacts group synergy. The choice of interaction control systems has limited impact on various aspects of virtual collaboration in this scenario, although participants using hand tracking reported lower effort workload, while participants using controllers reported lower mental workload in the absence of audio.",avatars; collaboration; Communication; gestures,Title_Abstract,True,
IEEE,conferencePaper,2024,Exploring User Placement for VR Remote Collaboration in a Constrained Passenger Space,VR - International Symposium Virtual Reality,A*,"Extended Reality (XR) offers the potential to transform the passenger experience by allowing users to inhabit varied virtual spaces for entertainment, work or social interaction, whilst escaping the constrained transit environment. XR allows remote collaborators to feel like they are together and enables them to perform complex 3D tasks. However, the social and physical constraints of the passenger space pose unique challenges to productive and socially acceptable collaboration. Using a collaborative VR puzzle task, we examined the effects of five different f-formations of collaborator placement and orientation in an interactive workspace on social presence, task workload, and implications for social acceptability. Our quantitative and qualitative results showed that face-to-face formations were preferred for tasks with a high need for verbal communication but may lead to social collisions, such as inadvertently staring at a neighbouring passenger, or physical intrusions, such as gesturing in another passenger’s personal space. More restrictive f-formations, however, were preferred for passenger use as they caused fewer intrusions on other passengers’ visual and physical space.",Collaboration; Constrained Spaces; Mixed Reality; Passenger Spaces; Social Acceptability; Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Stand Alone or Stay Together: An In-situ Experiment of Mixed-Reality Applications in Embryonic Anatomy Education,VR - International Symposium Virtual Reality,A*,"Where traditional media and methods reach their limits in anatomy education, mixed-reality (MR) environments can provide effective learning support because of their high interactivity and spatial visualization capabilities. However, the underlying design and pedagogical requirements are as diverse as the technologies themselves. This paper examines the effectiveness of individual- and collaborative learning environments for anatomy education, using embryonic heart development as an example. Both applications deliver the same content using identical visualizations and hardware but differ in interactivity and pedagogical approach. The environments were evaluated in a user study with medical students (n = 90) during their examination phase, assessing usability, user experience, social interaction/co-presence, cognitive load, and personal preference. Additionally, we conducted a knowledge test before and after an MR learning session to determine educational effects compared to a conventional anatomy seminar. Results indicate that the individual learning environment was generally preferred. However, no significant difference in learning effectiveness could be shown between the conventional approach and the MR applications. This suggests that both can effectively complement traditional seminars despite their different natures. Our study contributes to understanding how different MR settings could be tailored for anatomical education.",Collaborative Learning; Immersive Learning Environments; Individual Adaptive Learning; Medical Education; Mixed Reality,Keywords,True,
IEEE,conferencePaper,2024,Contextual Matching Between Learning and Testing Within VR Does Not Always Enhance Memory Retrieval,VR - International Symposium Virtual Reality,A*,"Episodic memory is influenced by environmental contexts, such as location and auditory stimuli. The most well-known effect is the reinstatement effect, which refers to the phenomenon where contextual matching between learning and testing enhances memory retrieval. Previous studies have investigated whether the reinstatement effect can be observed within immersive virtual environments. However, only a limited number of studies have reported a significant reinstatement effect using virtual reality, while most have failed to detect it. In this study, we re-examined the reinstatement effect using 360-degree video-based virtual environments. Specifically, we carefully selected virtual environments to elicit different emotional responses, which has been suggested as a key factor in inducing a robust reinstatement effect in the physical world. Surprisingly, we found a significant reversed reinstatement effect with a large effect size. This counter-intuitive result suggests that contextual congruence does not necessarily enhance memory and may even interfere with it. This outcome may be explained by the retrieval-induced forgetting phenomenon, but further exploration is needed. This finding is particularly important for virtual reality-based, educational applications and highlights the need for a deeper understanding of the complex interactions between memory and contextual cues within virtual environments.",360-degree video; environmental context-dependent memory; reinstatement; retrieval-induced forgetting; virtual reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Toward Facilitating Search in VR With the Assistance of Vision Large Language Models,VR - International Symposium Virtual Reality,A*,"While search is a common need in Virtual Reality (VR) applications, current approaches are cumbersome, often requiring users to type on a mid-air keyboard using controllers in VR or remove VR equipment to search on a computer. We first conducted a literature review and a formative study, identifying six common search needs: knowing about one object, knowing about the object’s partial details, knowing objects with environmental context, knowing about interactions with objects, and finding objects within field of view (FOV) and out of FOV in the VR scene. Informed by these needs, we designed technology probes that leveraged recent advances in Vision Large Language Models and conducted a probe-based study with users to elicit feedback. Based on the findings, we derived design principles for VR designers and developers to consider when designing a user-friendly search interface in VR. While prior work about VR search tended to address specific aspects of search, our work contributes design considerations aimed at enhancing the ease of search in VR and potential future directions.",participatory design; Virtual reality; vision large language model; VR search,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Evaluating Gaze Interactions within AR for Nonspeaking Autistic Users,VR - International Symposium Virtual Reality,A*,"Nonspeaking autistic individuals often face significant inclusion barriers in various aspects of life, mainly due to a lack of effective communication means. Specialized computer software, particularly delivered via Augmented Reality (AR), offers a promising and accessible way to improve their ability to engage with the world. While research has explored near-hand interactions within AR for this population, gaze-based interactions remain unexamined. Given the fine motor skill requirements and potential for fatigue associated with near-hand interactions, there is a pressing need to investigate the potential of gaze interactions as a more accessible option. This paper presents a study investigating the feasibility of eye gaze interactions within an AR environment for nonspeaking autistic individuals. We utilized the HoloLens 2 to create an eye gaze-based interactive system, enabling users to select targets either by fixating their gaze for a fixed period or by gazing at a target and triggering selection with a physical button (referred to as a ‘clicker’). We developed a system called HoloGaze that allows a caregiver to join an AR session to train an autistic individual in gaze-based interactions as appropriate. Using HoloGaze, we conducted a study involving 14 nonspeaking autistic participants. The study had several phases, including tolerance testing, calibration, gaze training, and interacting with a complex interface: a virtual letterboard. All but one participant were able to wear the device and complete the system’s default eye calibration; 10 participants completed all training phases that required them to select targets using gaze only or gaze-click. Interestingly, the 7 users who chose to continue to the testing phase with gaze-click were much more successful than those who chose to continue with gaze alone. We also report on challenges and improvements needed for future gaze-based interactive AR systems for this population. Our findings pave the way for new opportunities for specialized AR solutions tailored to the needs of this under-served and under-researched population.",assistive technology; augmented reality; eye tracking; nonspeaking autistic people,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Exploring Immersive Debriefing in Virtual Reality Training: A Comparative Study,VR - International Symposium Virtual Reality,A*,"Simulation and debriefing are two essential and inseparable phases of virtual reality training. With the widespread adoption of these training tools, it is crucial to define the best pedagogical approaches for trainers and learners to maximize their effectiveness. However, despite their educational benefits, virtual reality-specific debriefing methods remain underexplored in research. This article proposes an architecture and interface for an all-in-one immersive debriefing module that is adaptable to different types of training, including a complete system for recording, replaying, and redoing actions. A study with 36 participants compared this immersive debriefing system with traditional discussion-based and video-supported debriefing. Participants were divided into three groups to evaluate the effectiveness of each method. The results showed no significant differences between these debriefing methods across several criteria, such as satisfaction, motivation, or information retention. Immersive debriefing is as usable and retentive as traditional or video debriefing in this context. The next step will be to evaluate the redo system in other training courses involving more dynamic scenarios.",Debriefing; Immersive Learning; Simulation; Trainer; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,"A Critical Review of Virtual and Extended Reality Immersive Police Training: Application Areas, Benefits &amp; Vulnerabilities",VR - International Symposium Virtual Reality,A*,"Virtual and Extended Reality (VR/XR) headsets have promised to enhance police training through the delivery of immersive simulations able to be conducted anywhere, anytime. However, little consideration has been given to reviewing the evidenced benefits and potential issues posed by XR police training. In this paper, we summarise the evidenced usage and benefits of XR police training through a formative targeted literature review (n=41 publications). We then reflect on the prospective technical, security, social and legal issues posed by XR police training, identifying four areas where issues or vulnerabilities exist: training content, trainees and trainers, systems and devices, and state and institutional stakeholders. We highlight significant concerns around e.g. the validity of training; the psychological impact and risks of trauma; the safety and privacy risks posed to trainees and trainers; and the risks to policing institutions. We aim to encourage end-user communities (e.g. police forces) to more openly reflect on the risks of immersive training, so we can ultimately move towards transparent, validated, trusted training that is evidenced to improve policing outcomes.",Extended Reality; Police Training; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,The Impact of Task-Responsibility on User Experience and Behaviour under Asymmetric Knowledge Conditions,VR - International Symposium Virtual Reality,A*,"Virtual Reality presents a promising tool for knowledge transfer, allowing users to learn in different environments and with the help of three-dimensional visualizations. At the same time, having to learn new ways of interacting with their environment can present a significant hurdle for novice users. When users enter a virtual space to receive knowledge from a more experienced person, the question arises as to whether they benefit from learning VR-specific interaction techniques instead of letting the expert take over some or all interactions. Based on related work about expert-novice interaction in virtual spaces, this paper presents a user study comparing three different distributions of interaction responsibilities between participants and an expert user. The Role-Based interaction mode gives the expert the full interaction responsibility. The Shared interaction mode gives both users the same interaction capabilities, allowing them to share the responsibility of interacting with the virtual space. Finally, the Parallel interaction mode gives participants full interaction responsibility, while the expert can provide guidance through oral communication and visual demonstration. Our results indicate that assuming interaction responsibility led to higher task loads but also increased the participant’s engagement and feeling of presence. For most participants, sharing interaction responsibilities with the expert represented the best trade-off between engagement and challenge. While we did not measure a significant increase in learning success, participant comments indicated that they also paid more attention to details when assuming more interaction responsibility.",3D User Interfaces; Collaboration; Head-Mounted Display; Instruction; Knowledge-Transfer; Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Evaluating the effects of Situated and Embedded Visualisation in Augmented Reality Guidance for Isolated Medical Assistance,VR - International Symposium Virtual Reality,A*,"One huge advantage of Augmented Reality (AR) is its numerous possibilities of displaying information in the physical world, especially when applying Situated Analytics (SitA). AR devices and their respective interaction techniques allow for supplementary guidance to assist an operator carrying out complex procedures such as medical diagnosis and surgery, for instance. Their usage promotes user autonomy by presenting relevant information when the operator may not necessarily possess expert knowledge of every procedure and may also not have access to external help such as in a remote or isolated situation (e.g., International Space Station, middle of an ocean, desert). In this paper, we propose a comparison of two different forms of AR visualisation: An embedded visualisation and a situated projected visualisation, with the aim to assist operators with the most appropriate visualisation format when carrying out procedures (medical in our case). To evaluate these forms of visualisation, we carried out an experiment involving 23 participants possessing latent/novice medical knowledge. These participant profiles were representative of operators who are medically trained yet do not apply their knowledge every day (e.g., an astronaut in orbit or a sailor out at sea). We discuss our findings which include the advantages of embedded visualised information in terms of precision compared to situated projected information with the accompanying limitations in addition to future improvements to our proposition. We conclude with the prospects of our work, notably the continuation and possibility of evaluating our proposition in a less controlled and real context in collaboration with our national space agency.",AR Guidance; Augmented Reality; Immersive Analytics; Isolated Situation; Medical Assistance; Procedure Execution; Situated Analytics; Situated Visualisation,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,An Evaluation of Targeting Methods in Spatial Computing Interfaces with Visual Distractions,VR - International Symposium Virtual Reality,A*,"In modern spatial computing devices, users are confronted with diverse methods for object selection, including eye gaze (cf. Apple Vision Pro), hand gestures (cf. Microsoft HoloLens 2), touch gestures (cf. Google Glass Enterprise Edition 2), and external controllers (cf. Magic Leap 2). Although there are a plethora of empirical studies on which selection techniques perform best, a common limiting factor stems from the partly artificial setups. These typically exclude practical influences such as visual distraction. In this paper, we present a user study comparing two hand-based and two gaze-based state-of-the-art selection methods, using the HoloLens 2. We extended a traditional Fitts’ law-inspired study design by incorporating a visual task that simulates changes in the user interface after a successful selection. Without a visual task, gaze-based techniques were on average faster than hand-based techniques. This performance gain was eliminated (for head gaze) or even reversed (for eye gaze) when the visual task was active. These findings underscore the value of continued practice-oriented research of targeting methods in virtual environments.",Augmented Reality; Interaction Techniques; Selection,Keywords,True,
IEEE,conferencePaper,2024,Evaluation of AR Pattern Guidance Methods for a Surface Cleaning Task,VR - International Symposium Virtual Reality,A*,"Cleanroom cleaning is a surface coverage task where the pattern should be followed correctly, and the entire surface should be covered. We investigate the efficacy of augmented reality (AR) by implementing various pattern guidance designs to enhance a cleanroom cleaning task. We developed an AR guidance system for cleaning procedures and evaluated four distinct pattern guidance methods: (1) breadcrumbs, (2) examples, (3) middle lines, and (4) outlines. We vary the instructions on the entire surface or as a single step. To measure performance, accuracy, and user satisfaction associated with each guidance method, we conducted a large-scale (n=864) between-subjects study. Our findings indicate that single step instructions proved to be more intuitive and efficient than full instructions, especially for the breadcrumbs. We also discussed the implications of our results for the development of AR applications for surface coverage and pattern optimization.",Augmented Reality; Motion control.; Pattern guidance,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Editing Immersive Recordings: An Elicitation Study,VR - International Symposium Virtual Reality,A*,"Immersive recordings capture virtual reality interactions and are used in various contexts such as education and entertainment. However, there has been only limited research on requirements and techniques for editing such recordings. We interviewed expert editors of video recordings to understand their workflows, familiarised them with immersive recordings, and asked them about what editing challenges and capabilities they can envision for immersive recordings. The experts identified several functionalities they considered relevant for editing, including viewer placement, control over the viewer’s size, support for live and asynchronous collaboration, and different transition types.",Immersive Editing; Immersive Recordings; Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Towards an Avatar Customization System for Semi-realistic Ethnically-diverse Virtual Reality Avatars,VR - International Symposium Virtual Reality,A*,"Due to the Proteus effect, in which people modify their behaviour based on their avatar, participant avatar representation is an important factor in virtual reality (VR) studies. We develop an open source prototype avatar customization system that enables quick customization of semi-realistic, ethnically-diverse avatars. The prototype provides options for customizing body and face shape, hairstyle, glasses, religious clothing, and skin, eye, and hair colour. The prototype generates avatar assets that are fully rigged and textured for incorporation into VR study code, and it serves as a step towards designing more inclusive VR research studies.",avatar customization; Proteus effect; virtual avatars,Title_Abstract,True,
IEEE,conferencePaper,2024,"Comparing Tracking Accuracy in Standalone MR-HMDs: Apple Vision Pro, Hololens 2, Meta Quest 3, and Pico 4 Pro",VR - International Symposium Virtual Reality,A*,"Modern Mixed Reality Head-Mounted Displays (MR-HMDs) can track user movements across large spaces without external markers. This study evaluates the tracking accuracy and the loop closure capabilities of four commercially available MR-HMDs across four distinct scenarios. We found consistent tracking performance in well-lit and expansive environments for all devices. Tracking accuracy remained stable even in outdoor nighttime conditions. Furthermore, most HMDs demonstrated effective error correction during loop closure, with errors in non-loop scenarios consistently exceeding those in loop scenarios.",Mixed Reality; Tracking Accuracy; Visual Inertial Odometry,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Exploring Alternative Text Input Modalities in Virtual Reality: A Comparative Study,VR - International Symposium Virtual Reality,A*,"Text input in Virtual Reality (VR) is crucial for communication, search, and productivity. We compared four keyboard designs for VR text entry, leveraging the flexibility and the tracking options of a 3D environment. We used the Dvorak layout to control for experience differences. The designs were: (a) a floating keyboard with touch input, (b) a keyboard attached on the back of the hand with touch input, (c) a floating keyboard with eye tracking and pinch input, and (d) a keyboard laid out over a rolling shape with touch input. Designs (b), (c), and (d) can move in 3D space, while design (a) is static. Design (d) had similar efficiency to design (a) but with better usability and lower Physical Demand. Design (b) led to higher Physical Demand, Effort, and Frustration. Design (c) had lower Physical Demand but higher Mental Demand, Effort, and error rates. Typing speeds averaged 6.51 WPM (1.24% error rate) for (a), 5.56 WPM (3.82% error rate) for (b), 5.33 WPM (1.43% error rate) for (c), and 6.70 WPM (1.64% error rate) for (d).",Interface design; Text input; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,A Study on the Effectiveness of Augmented Reality Signal-Integrated Camera Monitor Systems for Safe Lane Changing,VR - International Symposium Virtual Reality,A*,"This study investigates the effectiveness of augmented reality (AR) signals in camera monitor systems (CMS) for enhancing safety during lane changes. Seventy participants used seven side mirror conditions, including traditional side mirrors and six CMS conditions with and without AR signals. Results showed that CMS with AR signals significantly reduced the number of collisions and reaction time compared to CMS without AR signals.",Augmented Reality; Camera Monitor Systems; Driving Safety; Driving Simulation; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Exploring Influencers' and Users' Experiences in Douyin's Virtual Reality Live-Streaming,VR - International Symposium Virtual Reality,A*,"VR live-streaming has become an emerging part on Douyin. This study aims to explore the technical modes, content strategies, user experiences in Douyin‘s VR live-streaming. Through interviews and focus groups, we found that VR technology is recognized by influencers and has become an essential part of their creative practice. For some influencers, VR technology is a key factor in enhancing audience engagement and immersive experiences, although technical literacy barriers may arise when setting up VR scenes. We also provide dimensions for improving and developing user adoption and experience of VR technology in social media environments.",Douyin; Influencers; Live-Streaming; Users; Virtual Reality,Title_Keywords,True,
IEEE,conferencePaper,2024,Digital Eyes: Social Implications of XR EyeSight,VR - International Symposium Virtual Reality,A*,"The EyeSight feature, introduced with the new Apple Vision Pro XR headset, promises to revolutionize user interaction by simulating real human eye expressions on a digital display. This feature could enhance XR devices’ social acceptability and social presence when communicating with others outside the XR experience. In this pilot study, we explore the implications of the EyeSight feature by examining social acceptability, social presence, emotional responses, and technology acceptance. Eight participants engaged in conversational tasks in three conditions to contrast experiencing the Apple Vision Pro with EyeSight, the Meta Quest 3 as a reference XR headset, and a face-to-face setting. Our preliminary findings indicate that while the EyeSight feature improves perceptions of social presence and acceptability compared to the reference headsets, it does not match the social connectivity of direct human interactions.",Extended Reality; Social Acceptability; Social Presence,Keywords,True,
IEEE,conferencePaper,2024,SOLDAR: Supporting Low-Volume PCB Prototyping Using Collaborative Robots and Augmented Reality,VR - International Symposium Virtual Reality,A*,"Printed circuit boards (PCBs) are fundamental to modern electronics and are present in almost every electronic device. However, despite their ubiquity, current PCB assembly methods can be time-consuming and lack flexibility for one-off designs. This poster investigates how low-volume PCB prototyping can be enhanced by integrating collaborative robots (cobots) and Augmented Reality (AR). Specifically, we introduce SOLDAR, a system that facilitates the soldering of electronic through-hole components on PCBs. By using a cobot for optimal PCB positioning and AR glasses for step-by-step guidance, SOLDAR aims to streamline the assembly process. The expected outcomes are increased efficiency, reduced assembly time, and greater flexibility for low-volume PCB prototyping designs. To validate these hypotheses, user experiments are necessary.",Augmented Reality; Collaborative Robots; Prototyping,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Earscape: A VR Auditory Educational Escape Room,VR - International Symposium Virtual Reality,A*,"According to the World Health Organisation’s World Report on Hearing, there is a strong need to provide better education on hearing loss from a young age. This project aims to educate the Danish young population (13 to 17-year-olds) about the hearing sense through an educational multiplayer virtual reality-based escape room with the benefits of educational escape rooms. In collaboration with relevant audiologist stakeholders, this project follows an iterative process of design, implementation, and evaluation of the application. The developed solution will undergo several user studies in the following months.",Educational Escape Room; Hearing Loss; Multiplayer; Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Haptic and Auditory Feedback on Immersive Media in Virtual Reality,VR - International Symposium Virtual Reality,A*,"In Virtual Reality (VR), visual and auditory sensations are effectively leveraged to create immersive experiences. However, touch is significantly underutilized in immersive media. We enhance the VR image viewing experience by integrating haptic and auditory feedback into 3D environments constructed from immersive media. We address the challenges of utilizing depth maps from various image formats to create intractable environments. The VR experience is enhanced using vibrohaptic feedback and audio cues triggered by controller collisions with haptic materials.",Auditory Feedback; Haptic Feedback; Immersive Media; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,From Ground to Sky: Flying-motion Generation via Motion Dataset Adaptation,VR - International Symposium Virtual Reality,A*,"We conducted a study utilizing a lightweight generative network to create flying motions. The existing datasets used for training did not include any data on flying motions. Therefore, we selected certain classes from the existing motion datasets and transformed these motions to resemble flying actions. By training the existing generative network with the modified dataset, we were able to generate motions that closely resemble flying. The results of this study demonstrate the potential for generating flying motions. The generation of flying motions for human avatars is expected to be a critical technology not only in 3D animation or game industry but also in virtual environments, enabling users to experience various activities through their avatars.",Avatar motion; dataset; Virtual Reality,Keywords,True,
IEEE,conferencePaper,2024,Walking of uphill slopes in immersive virtual environments,VR - International Symposium Virtual Reality,A*,"We explore three visual manipulation techniques aiming to create a realistic feeling of walking an uphill slope while in reality being on flat ground. The techniques are based on real physical visual perception and consist of modification of height and display of virtual shoes, modification of speed, and modification of view pitch. Quantitative and qualitative evaluation indicated that modification of speed, and pitch contributed to user discomfort, as well as a general increase in discomfort correlating with the slope’s increasing inclination. However, height manipulation was well received and can be used in future projects for more realistic landscape.",human factors; Virtual reality; walking,Keywords,True,
IEEE,conferencePaper,2024,Enhanced Wayfinding Insights Through VR and Eye-Tracking Analysis,VR - International Symposium Virtual Reality,A*,"This paper presents a novel method for evaluating wayfinding within a public building to provide meaningful insights for stakeholders. Our approach features unique methods for both data collection and evaluation, with a holistic digital capture of the entire virtual environment experienced by participants, maintained in an interactive format for in-depth analysis. We also captured and output data in point cloud formats, raw data text files, and task-specific metrics, which support interactive replays of participants’ experiences. We developed algorithms to extract meaningful insights from the raw data based on assumptions about wayfinding characteristics. The contribution is a flexible framework that can be easily adapted for future projects with adjustable variables to suit specific applications.",gaze tracking; point cloud; signage; unreal engine; virtual reality; wayfinding,Keywords,True,
IEEE,conferencePaper,2024,Pipelining Processors for Decomposing Character Animation,VR - International Symposium Virtual Reality,A*,"This paper presents an openly available implementation of a modular pipeline architecture for character animation. It effectively decomposes frequently necessary processing steps into dedicated character processors, such as copying data from various motion sources, applying inverse kinematics, or scaling the character. Processors can easily be parameterized, extended (e.g., with AI), and freely arranged or even duplicated in any order necessary, greatly reducing side effects and fostering fine-tuning, maintenance, and reusability of the complex interplay of real-time animation steps.",Agents; Avatars; Embodiment; Extended Reality.; Humanoid Characters; Open-Source; Virtual Humans; Virtual Reality,Keywords,True,
IEEE,conferencePaper,2024,Study of inpainting based on generative AI for noise-canceling HMDs,VR - International Symposium Virtual Reality,A*,"Entering a small space such as an elevator or a crowded train with a stranger can cause discomfort and suffocation. This is because the stranger is invading the individual’s personal space. However, it is difficult to maintain an appropriate interpersonal distance from others at all times in various situations. Therefore, a noise-canceling HMD [2][3] that uses AR to change the size of the person in the field of vision has been proposed as a means of reducing noise such as discomfort caused by inappropriate interpersonal distance. In this paper, we propose an improvement method using generative AI for background completion in noise-canceling HMDs.",Augmented Reality; Noise-canceling HMD,Keywords,True,
IEEE,conferencePaper,2024,A Comparison between Vibrotactile Error-correction Feedback on Upper and Lower Body in the VR Snowboard Balancing Task,VR - International Symposium Virtual Reality,A*,"This study investigated the effect of vibrotactile stimulus location on the balancing task in virtual reality (VR). Using a virtual snowboarding system with wearable haptic devices, we conducted a between-subject user study comparing the effectiveness of two different body locations–upper body (UB; torso vibrations) and lower body (LB; ankle vibrations). The real-time vibrotactile balance-correction feedback was generated by the Center of Pressure (CoP) calculated from the sensor array on insoles. The initial results showed that UB feedback is better than LB to improve users’ balance ability.",balancing; center-of-pressure; Vibrotactile wearables; virtual reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,The MASTER XR Platform for Robotics Training in Manufacturing,VR - International Symposium Virtual Reality,A*,"The MASTER project introduces an open Extended Reality (XR) platform designed to enhance human-robot collaboration and train workers in robotics within manufacturing settings. It includes modules for creating safe workspaces, intuitive robot programming, and user-friendly human-robot interactions (HRI), including eye-tracking technologies. The development of the platform is supported by two open calls targeting technical SMEs and educational institutes to enhance and test its functionalities. By employing the learning-by-doing methodology and integrating effective teaching principles, the MASTER platform aims to provide a comprehensive learning environment, preparing students and professionals for the complexities of flexible and collaborative manufacturing settings.",Extended Reality (XR); Eye Tracking; Human-Robot Collaboration; Industry 4.0; Manufacturing; Robotics; Worker Training,Abstract_Keywords,True,
IEEE,conferencePaper,2024,VR4UrbanDev: An Immersive Virtual Reality Experience for Energy Data Visualization,VR - International Symposium Virtual Reality,A*,"In this demonstration paper, we present our interactive virtual reality (VR) experience, which has been designed to facilitate interaction with energy-related information. This experience consists of two main modes: the world in miniature for large-scale and first-person for real-world scale visualizations. Additionally, we presented our approach to potential target groups in interviews. The results of these interviews can help developers for future implementation considering the requirements of each group.",Building Information Modeling; Data Visualization; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,UXR-kit: An Ideation Kit and Method for Collaborative and User-Centered Design about Extended Reality systems.,VR - International Symposium Virtual Reality,A*,"Emerging kits and methods about Extended Reality (XR) systems are mainly centered on the prototyping phase. The ideation phase, which comes before prototyping, is currently still under-explored. In this work, we propose UXR-kit: a toolkit and a method for the co-design of ideas for XR systems. UXR-kit is based on an approach inspired by design studios and generative techniques and highlights the specificities of XR systems. Results from an experimental study suggest that UXR-kit allows the emergence of ideas for XR designs through both World-In-Miniature representations and first-person representations at scale 1:1.",Design toolkit; Extended Reality; Ideation; Mixed Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Cultural Windows: Towards Immersive Journeys into Global Living Spaces,VR - International Symposium Virtual Reality,A*,"“Cultural Windows” is a research initiative aimed at enhancing cross-cultural understanding through immersive extended reality (XR) experiences. The project deploys AR and VR platforms to allow users to explore diverse living spaces, bridging the gap between preconceived notions and the actual appearance of these spaces. By using 3D scanning to create accurate models of culturally significant objects and integrating them into immersive systems, the project provides insights into the use of immersive technologies in cultural education, promoting engagement with global living designs.",Cross-Cultural Visualization; Cultural Awareness in Design; Extended Reality (XR),Abstract_Keywords,True,
IEEE,conferencePaper,2024,A Volumetric Video Application to Enhance Museum Experiences,VR - International Symposium Virtual Reality,A*,"Volumetric video (VV) is an emerging 3D format that allows the integration of real people into XR (extended reality) applications. Recent cost-effective AI-based methods have enabled VV capture using single handheld cameras or mobile phones. This study addresses the quality, integration, and acceptance of AI-based VV content creation in an augmented reality (AR) application designed to enhance museum experiences. The main result reveals that, although the current VV quality is lower than professional standards, users still find significant added value and enjoy its immersive experience.",,Abstract,True,
IEEE,conferencePaper,2024,Effectiveness of Adaptive Difficulty Settings on Self-efficacy in VR Exercise,VR - International Symposium Virtual Reality,A*,"The difficulty is a fundamental factor of the user’s motivation and engagement in some tasks. Dynamic difficulty adjustment (DDA) systems provide users with an optimal level of challenge. Previously, some studies developed a DDA system that can set the task’s difficulty to any level. However, these studies lack the investigation of the influence of the difficulty levels on the psychological aspect. For this purpose, we consider a difficulty setting that consists of stepwise difficulty levels (e.g., hard, normal, and easy) set to adapt to each user’s skill and evaluate it using self-efficacy. In the experiment, we employ a Kendama task in a VR space where the difficulty level can be easily adjusted. The result shows that the difficulty levels in our method can be set according to the user’s skill. Moreover, we experimentally clarify a strong correlation between successful experiences in imagination and the enhancement of self-efficacy in the difficulty setting, which means that adapting difficulty levels to the user’s skill has the potential to enhance self-efficacy effectively.",Difficulty adjustment; Self-efficacy; User experience; Virtual reality,Keywords,True,
IEEE,conferencePaper,2024,Investigation of Simulator Sickness in Walking with Multiple Locomotion Technologies in Virtual Reality,VR - International Symposium Virtual Reality,A*,"With the increasing development of Virtual Reality, locomotion has become an essential component of interaction in VR. Currently, various locomotion technologies have been developed to provide users with a natural walking experience in virtual environments. However, the multiple walking techniques impact users’ walking experience in different ways. Simulator sickness is a common issue in VR experiences. Since different walking methods may influence simulator sickness differently, we conducted a user study to evaluate simulator sickness in walking with three relevant walking methods: real walking, arm-swing, and omnidirectional treadmill, and the results indicated that these three walking methods caused different levels of simulator sickness, and people perceived stronger sickness when they walked on the omnidirectional treadmill.",Locomotion Technologies; Natural Walking Techniques; Simulator Sickness; Virtual Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Exploring an XR Indoor Navigation System for Remote Collaboration,VR - International Symposium Virtual Reality,A*,"While collaboration in shared extended reality spaces has been extensively explored, larger environments like entire floors or buildings have garnered less attention. To address this gap, spatial navigation and collaboration across realities must be made possible so that users can find each other and foster shared spatial understanding independent from reality. Current developments target either navigation or collaboration but lack the combination. In this poster, we present an extended reality remote collaboration system using an augmented reality (AR) based indoor navigation for on-site and a Building Information Model (BIM) of the physical environment Virtual Reality (VR) system for remote users. We conducted a user study with ten participants (five pairs) to gather initial insights into the system’s usability and preferences for collaborative tools. The results offer initial insights into creating shared spatial understanding across realities. Our work contributes to a collaborative XR navigation system for extensive shared spaces.",AR; indoor navigation; remote; VR; XR collaboration,Abstract,True,
IEEE,conferencePaper,2024,Wheel-Based Attachable Footwear for VR: Challenges and Opportunities in Seated Walking-in-Place Locomotion,VR - International Symposium Virtual Reality,A*,"This poster explores the potential of Cybershoes, a foot-based consumer input device, used with a swivel chair to enable seated walking-in-place (WIP) locomotion in virtual reality (VR). Through a qualitative study with 12 participants, we investigated the effects of Cybershoes on user comfort, presence, motion sickness, and overall experience during various sightseeing tasks. Our findings reveal both opportunities and challenges for Cybershoes as a seated-WIP solution. Participants perceived Cybershoes as more natural for navigation compared to handheld controllers, with most reporting reduced motion sickness. However, challenges included perceived slower movement speed, ergonomic issues, and limited action detection. Our work also highlights Cybershoes’ potential beyond gaming, including applications in exercise, professional training, remote work, and accessibility.",input.; locomotion; seated walking; shoes; virtual travel; VR; wheel,Abstract,True,
IEEE,conferencePaper,2024,White Lies in Virtual Reality: Impact on Enjoyment and Fatigue,VR - International Symposium Virtual Reality,A*,"This study examined the impact of a ""white lie"" designed to boost motivation during virtual reality exercise on enjoyment and mental fatigue. Participants engaged in a ball-throwing or ball-targeting task and were randomly assigned to groups with or without the white lie. Results indicated that both groups experienced similar levels of enjoyment and fatigue, suggesting the white lie had minimal effect on these factors. All participants, regardless of group, reported high levels of enjoyment, with 17 out of 18 indicating they had fun, no significant differences in mental fatigue were found between groups while participants generally favored the white lie. However, the positive experience across all participants highlights the potential of Virtual Reality for promoting exercise engagement.",Enjoyment; Fatigue; Virtual Reality; White Lies,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Dynamic Difficulty Adjustment in Virtual Reality Exergaming to Regulate Exertion Levels via Heart Rate Monitoring,VR - International Symposium Virtual Reality,A*,"By regulating exertion levels, Dynamic difficulty adjustment (DDA) has the potential to enhance user experience and optimize exercise in Virtual Reality (VR) exergames. This pilot study assesses the effectiveness of adjusting the difficulty of gameplay challenges based on heart rate (HR) data to control the intensity of physical activity in VR exergaming. Observational results from 13 participants indicate that the HR-based DDA more effectively maintained target heart rate zones compared to randomized adjustments. Improved perceived exertion, and increased enjoyment underlines the potential of this approach for VR-based exercise and rehabilitation programs.",dynamic difficulty adjustment; heart rate; virtual reality exergames,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Rendering diffraction Phenomena on rough surfaces in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Wave-optical phenomena, such as diffraction, significantly impact the visual appearance of surfaces. Despite their importance, wave-optical reflection models are rare and computationally expensive. Recently, we presented a real-time model that accounts for diffraction-induced color shifts and speckle. Given that diffraction phenomena are highly dependent on illumination and viewing directions, as well as stereoscopic vision, we developed a VR demo to evaluate the new model. This demo shows the substantial impact of diffraction on the appearance of rough surfaces, particularly in stereoscopic viewing.",Diffraction; Modeling; Predictive Rendering; Virtual Reality,Title_Keywords,True,
IEEE,conferencePaper,2024,Supporting Wildfire Evacuation Preparedness through a Virtual Reality Simulation,VR - International Symposium Virtual Reality,A*,This demo presents a virtual reality simulation of a wildfire evacuation. Players are tasked with going through a home environment and collecting items they believe they would need and want to take if they were under an evacuation notice. The experience is playable on the Meta Quest 2 headset.,Evacuation; Training; Virtual Reality; Wildfires,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Row your boat in VR and solve thinking exercises on the way: The Brain-Row Challenge,VR - International Symposium Virtual Reality,A*,"In this demo, we showcase Brain-Row Challenge. Brain-Row Challenge is a research prototype for dual-task training in Virtual Reality (VR). Dual-task training combines a mental and a physical task. This training is relevant in neurodegenerative diseases, especially in Parkinson’s disease. The user is rowing with a Concept 2 ergometer over a Nordic lake, must follow a marked route and answers multiple-choice questions by rowing through gates. Steering is done with an inertial measurement unit that is attached to the handlebar. The VR experience can also be compared to a less immersive representation of the rowing course on a TV screen.",Dual Tasking; Ergometers; Excer Game; Medical Application,Abstract,True,
IEEE,conferencePaper,2024,ChronoShore: Diegetic Temporal Exploration in a Simulated Virtual Coast Environment,VR - International Symposium Virtual Reality,A*,"This paper introduces ChronoShore, an immersive virtual reality (VR) experience designed to explore diegetic time manipulation mechanics within a semi-realistic coastal environment. Traditional 2D video scrubbing methods fall short in immersive settings, particularly for understanding time-bound processes such as simulations of geology or biology. ChronoShore addresses this by allowing users to interact with celestial bodies to dynamically control and experience the passage of time, currently showcasing different weather events and atmospheric phenomena.",simulation; Time manipulation; virtual reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Hands-On Plant Root System Reconstruction in Virtual Reality,VR - International Symposium Virtual Reality,A*,"VRoot is an immersive extended reality reconstruction tool for root system architectures from 3D volumetric scans of soil columns. We have conducted a laboratory user study to assess the performance of new users with our software in comparison to established software. We utilize a plant model to derive a synthetic root architecture, providing a baseline for reconstruction. This demo showcases the processes and techniques contributing to exact and efficient manual root architecture reconstruction in Virtual Reality. The extraction task typically is the sparse graph-structure extraction from a 3D magnetic-resonance imaging (MRI) data set. We visualize the RSA directly within the MRI and offer selection-set-based methods of adapting and augmenting the root architecture. This application is in productive use at our partner institute, where it is used to analyze complex root images.",3D imaging; root reconstruction; virtual reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Off-The-Shelf: Exploring 3D Arrangements of See-Through Masks to Switch between Virtual Environments,VR - International Symposium Virtual Reality,A*,"This demo explores prioritization techniques to arrange see-through masks in virtual reality (VR). The oval masks show live previews of different virtual environments (VEs) and allow for seamless teleportation into a corresponding VE by putting the mask on the face. Each environment includes a mini-game (e.g., basketball and archery) in which the user has to perform a small task. The arrangement of the masks changes depending on a calculated rating, which considers the time since the game was last played and the game score. We envision this system to help users to multitask in VR. For example, to control multiple characters in VR games, to experience multi-strand (nonlinear) narratives, and to supervise semi-autonomous agents in different VEs.",Mask; Multiverse; Transitions; Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2024,EcoDive: Enhancing Presence and Ambient Environmental Awareness in a Virtual Reality Experience for Underwater Marine Debris Collection,VR - International Symposium Virtual Reality,A*,"This paper presents a VR-based serious game. The game aims to raise awareness about ocean pollution by immersing players in a virtual underwater world where they collect trash to prevent coral bleaching and save marine life. Despite their efforts, players inevitably face game over, highlighting the futility of merely collecting trash and underscoring the need to prevent waste from entering oceans. The game uses various diegetic feedback mechanisms and enhanced user presence features to deepen emotional engagement and promote pro-environmental behavior.",Coral Bleaching; Diegetic Feedback; Environmental Awareness; Environmental Education; Marine Conservation; Ocean Pollution; Pro-Environmental Behavior; Serious Game; Virtual Reality (VR),Title_Keywords,True,
IEEE,conferencePaper,2024,GazeLock: Gaze- and Lock Pattern-Based Authentication,VR - International Symposium Virtual Reality,A*,"Password entry is common authentication approach in Extended Reality (XR) applications for its simplicity and familiarity, but it faces challenges in public and dynamic environments due to its cumbersome nature and susceptibility to observation attacks. Manual password input can be disruptive and prone to theft through shoulder surfing or surveillance. While alternative knowledge-based approaches exist, they often require complex physical gestures and are impractical for frequent public use. We present GazeLock, an eye-tracking and lock pattern-based authentication method. This method aims to provide an easy-to-learn and efficient alternative by leveraging familiar lock patterns operated through gaze. It ensures resilience to external observation, as physical interaction is unnecessary and eyes are obscured by the headset. Its hands-free, discreet nature makes it suitable for secure public use. We demonstrate this method by simulating the unlocking of a smart lock via an XR headset, showcasing its potential applications and benefits in real-world scenarios.",Authentication; Extended Reality (XR); Eye Tracking; Gaze-based Interaction,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Us Xtended - Tracking and Sensing through Embedded and Embodied Design in Virtual Reality,VR - International Symposium Virtual Reality,A*,"This short paper presents an embodied and embedded design method via biometric data tracking on the example of the virtual reality prototype Us Xtended. Users are taken through different immersive worlds and their task is to manipulate the environments via a certain type of physiological interaction (i.e. heart rate, gaze, voice, cognitive load). By employing biofeedback, the system tailors the immersive environment via audiovisual and haptic stimuli to user’s psycho-physiological responses and reflects them on its scale which is part of the virtual environment. By recording their voice, users can self-assess their own affects. In the finale, users stand in a pastiche-like world filled with different artifacts of psycho-physiological evaluations they co-created with the biofeedback system throughout their journey.",affect; biometrics; embedded and embodied design; psycho-physiology; self-quantification; virtual reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Make America Great Again and Again: How to Adapt Interactive Installation Art for Virtual Reality,VR - International Symposium Virtual Reality,A*,"Make America Great Again and Again features a large, fluttering American flag accompanied by the Star-Spangled Banner, with sixty small screens displaying one-minute video clips in sequence. This one-hour loop continues until participants upload their own videos, transforming the flag into a collage of visitor selfies. By providing a public sphere for local visitors, this interactive art project encourages them to share their opinions on this controversial issue. To capture global perspectives on the topic, the project was adapted into a virtual reality environment using the metaverse platform Styly. This paper outlines the process of converting the installation into virtual reality artwork.",Installation art; Interactive art; VR conversion,Title_Abstract,True,
IEEE,conferencePaper,2024,Real-Time Scent Prediction and Release for Video Games,VR - International Symposium Virtual Reality,A*,"This demo explores the use of computer vision technologies for the integration of scent in video games and interactive applications. We present an extendable system that is domain-independent and allows for customization and debugging based on the targeted game. Using Minecraft as a case study, we optimized the system configuration and evaluated its performance. Our aim is to advance the exploration of scent integration in gaming and inspire future designs for olfactory experiences.",Computer Vision; Scent Integration; Virtual Reality,Keywords,True,
IEEE,conferencePaper,2024,ICELab Demo: an industrial digital-twin and simulator in VR,VR - International Symposium Virtual Reality,A*,"In this demo we present an application featuring the integration of Virtual Reality (VR) technologies with the demonstration laboratory (ICELab) built around Industry 4.0/5.0 concepts. In particular, we showcase a digital twin of the real laboratory that allows the user to explore its environment in VR and interact with the different machinery to obtain several data and information.",Computer Graphics; Cyber-Physical Factory; Digital Twin,Abstract,True,
IEEE,conferencePaper,2024,"Travel Speed, Spatial Awareness, And Implications for Egocentric Target-Selection-Based Teleportation - A Replication Design",VR - International Symposium Virtual Reality,A*,"Virtual travel in Virtual Reality experiences is common, offering users the ability to explore expansive virtual spaces. Various interfaces exist for virtual travel, with speed playing a crucial role in user experience and spatial awareness. Teleportation-based interfaces provide instantaneous transitions, whereas continuous and semi-continuous methods vary in speed and control. Prior research by Bowman et al. highlighted the impact of travel speed on spatial awareness demonstrating that instantaneous travel can lead to user disorientation. However, additional cues, such as visual target selection, can aid in reorientation. This study replicates and extends Bowman’s experiment, investigating the influence of travel speed and visual target cues on spatial orientation.",,Abstract,True,
IEEE,conferencePaper,2024,Walking &gt; Walking-in-Place &gt; Flying/Steering &gt; Teleportation? Designing Locomotion Research for Replication and Extension,VR - International Symposium Virtual Reality,A*,"In this abstract, we discuss the demand for replication and extension efforts related to two seminal studies focused on virtual reality (VR) locomotion interfaces, initially centered around a VR implementation of the Visual Cliff, often referred to as Virtual Pit. The original experiments by Slater et al. (1995) and Usoh et al. (1999) compared different locomotion methods, including Real Walking, Walking-in-Place, and Flying/Steering, with a focus on presence and ease of use. We discuss the importance of these studies for the field, motivate replication efforts focused on these studies, discuss potential confounding factors, and present considerations for a concerted effort to reproduce the findings with state-of-the-art VR systems and measures, extensions to locomotion methods like Teleportation, and means to support future replications and extensions.",locomotion; presence; replication; steering; teleportation; user study; Virtual reality; walking,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Fade-to-Black Duration in Egocentric Target-Selection-Based Teleport - A Replication Design,VR - International Symposium Virtual Reality,A*,"Fade-to-black animations are a commonly used technique to visualize transitions during teleportation. However, their duration varies across different implementations and has not been extensively researched. This abstract details a study design to understand how the level of environmental detail affects the preferred duration of fade-to-black animations. We propose a within-subject study, comparing participants’ preferred duration across three virtual environments with varying levels of detail. We discuss improvements to the task design of an existing study. Other than the level of environmental detail, we motivate research into the effects of different tasks (i.e. hurried or calm) on the preferred duration.",locomotion; replication; teleportation; transitions; user study; Virtual reality,Keywords,True,
IEEE,conferencePaper,2024,Generative Multi-Modal Artificial Intelligence for Dynamic Real-Time Context-Aware Content Creation in Augmented Reality,VR - International Symposium Virtual Reality,A*,"We introduce a framework that uses generative Artificial Intelligence (AI) for dynamic and context-aware content creation in Augmented Reality (AR). By integrating Vision Language Models (VLMs), our system detects and understands the physical space around the user, recommending contextually relevant objects. These objects are transformed into 3D models using a text-to-3D generative AI techniques, allowing for real-time content inclusion within the AR space. This approach enhances user experience by enabling intuitive customization through spoken commands, while reducing costs and improving accessibility to advanced AR interactions. The framework’s vision and language capabilities support the generation of comprehensive and context-specific 3D objects.",3D object generation; Augmented reality; generative AI; vision language models,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2024,A Generative Framework for Low-Cost Result Validation of Machine Learning-as-a-Service Inference,AsiaCCS - Asia Conference on Computer and Communications Security,A,"The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as Augmented/Virtual Reality, integrity verification of the outsourced ML tasks is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time integrity validation of ML-as-a-Service (MLaaS) inference. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergence measurements to identify, with a high likelihood, if the service model is under attack. Fides also offers a re-classification functionality that predicts the original class whenever an attack is identified. We devised a generative adversarial network framework for training the attack detection and re-classification models. The evaluation shows that Fides achieves an accuracy of up to 98% for attack detection and 94% for re-classification.",edge computing; machine learning as a service; result verification; trusted execution environment; verifiable computing,Abstract,True,
Scopus,conferencePaper,2024,SoK: Data Privacy in Virtual Reality,PETS - International Symposium on Privacy Enhancing Technologies,A,"The adoption of virtual reality (VR) technologies has rapidly gained momentum in recent years as companies around the world begin to position the so-called “metaverse” as the next major medium for accessing and interacting with the internet. While consumers have become accustomed to a degree of data harvesting on the web, the real-time nature of data sharing in the metaverse indicates that privacy concerns are likely to be even more prevalent in the new “Web 3.0.” Research into VR privacy has demonstrated that a plethora of sensitive personal information is observable by various would-be adversaries from just a few minutes of telemetry data. On the other hand, we have yet to see VR parallels for many privacy-preserving tools aimed at mitigating threats on conventional platforms. This paper aims to systematize knowledge on the landscape of VR privacy threats and countermeasures by proposing a comprehensive taxonomy of data attributes, protections, and adversaries based on the study of 74 collected publications. We complement our qualitative discussion with a statistical analysis of the risk associated with various data sources inherent to VR in consideration of the known attacks and defenses. By focusing on highlighting the clear outstanding opportunities, we hope to motivate and guide further research into this increasingly important field.",,Title_Abstract,True,
Scopus,journalPaper,2024,<italic>xr-droid</italic>: A Benchmark Dataset for AR/VR and Security Applications,TODSC -Transactions on Dependable and Secure Computing,A,"The development of metaverses and virtual worlds on various platforms, including mobile devices, has led to the growth of applications in virtual reality (VR) and augmented reality (AR) in recent years. This application growth is paralleled by a growth of interest in analyzing and understanding AR/VR applications from security and performance standpoints. Despite this growing interest, benchmark datasets are lacking to facilitate this research pursuit. In this paper, we collect a dataset that consists of 408 diverse AR/VR applications from the Google Play Store and acquire various data modalities associated with those applications standardized in the form of seven features: control flow graphs, strings, functions, permissions, API calls, hexdump, and metadata. We highlight various research endeavors (applications) that can benefit from our dataset for each data modality. IEEE",Android; AR; Benchmark testing; Dataset; Hardware; Internet; Operating systems; Security; Security Applications; Solid modeling; Three-dimensional displays; VR,Abstract,True,
Scopus,journalPaper,2024,Time to Think the Security of WiFi-Based Behavior Recognition Systems,TODSC -Transactions on Dependable and Secure Computing,A,"Behavior recognition plays an essential role in numerous behavior-driven applications (e.g., virtual reality and smart home) and even in the security-critical applications (e.g., security surveillance and elder healthcare). Recently, WiFi-based behavior recognition (WBR) technique stands out among many behavior recognition techniques due to its advantages of being non-intrusive, device-free, and ubiquitous. However, existing WBR research mainly focuses on improving the recognition precision, while rarely studying the security aspects. In this article, we reveal that WBR systems are vulnerable to manipulating physical signals. For instance, our observation shows that WiFi signals can be changed by jamming signals. By exploiting the vulnerability, we propose two approaches to generate physically online adversarial samples to perform untargeted attack and targeted attack, respectively. The effectiveness of these attacks are extensively evaluated over four real-world WBR systems. The experiment results show that our attack approaches can achieve 80% and 60% success rates for untargeted attack and targeted attack in physical world, respectively. We also show that our attack approaches can be generalized to other WiFi-based sensing applications, such as user authentication.  © 2004-2012 IEEE.",Adversarial sample; behavior recognition; genetic algorithm; WiFi,Abstract,True,
Scopus,journalPaper,2024,Dangers Behind Charging VR Devices: Hidden Side Channel Attacks via Charging Cables,TOIFS - Transactions on Information Forensics and Security,A,"Virtual reality (VR), offering 3D visuals and stereophonic sounds, significantly enhances users' immersive experiences and has become a milestone in the era of the metaverse. However, due to the limited battery capacity of VR devices, it is common for users to rely on charging cables, which serve the dual purpose of power supply and audio output, to recharge their VR devices while in use. In this study, we propose an inconspicuous and stealthy side channel attack, coined as LineTalker, which can unveil visual-related and audio-related activities from VR devices during the charging process. The insight behind LineTalker is rooted in the observation that visual-related activities (e.g., 3D image rendering) are power-intensive and result in fluctuations in the current strength of the cable's power supply line, which can be leveraged as side channel information. Similarly, audio-related activities (e.g., playing music) leave traces on the cable's audio output line. Rather than providing a user with a compromised charging cable (i.e., embedding a current sensor) to measure the current strength, to make the attack less conspicuous, LineTalker employs the Hall effect to indirectly access side channel information. This is achieved by capturing magnetic signals using a Hall sensor placed near the target cable in a contactless manner. Experimental results demonstrate that LineTalker achieves an overall accuracy of 94.60% and 64.38% in inferring user activities in VR devices with intrusive and non-intrusive attack manners, respectively.  © 2005-2012 IEEE.",charging cable; non-intrusive attack; privacy inference; side channel attack; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,An Anti-Disguise Authentication System Using the First Impression of Avatar in Metaverse,TOIFS - Transactions on Information Forensics and Security,A,"Metaverse is a vast virtual world parallel to the physical world, where the user acts as an avatar to enjoy various services that break through the temporal and spatial limitations of the physical world. Metaverse allows users to create arbitrary digital appearances as their own avatars by which an adversary may disguise his/her avatar to fraud others. In this paper, we propose an anti-disguise authentication method that draws on the idea of the first impression from the physical world to recognize an old friend. Specifically, the first meeting scenario in the metaverse is stored and recalled to help the authentication between avatars. To prevent the adversary from replacing and forging the first impression, we construct a chameleon-based signcryption mechanism and design a ciphertext authentication protocol to ensure the public verifiability of encrypted identities. The security analysis shows that the proposed signcryption mechanism meets not only the security requirement but also the public verifiability. Besides, the ciphertext authentication protocol has the capability of defending against the replacing and forging attacks on the first impression. Extensive experiments show that the proposed avatar authentication system is able to achieve anti-disguise authentication at a low storage consumption on the blockchain.  © 2005-2012 IEEE.",anti-disguise; authentication; avatar; Metaverse,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Inclusion of individuals with autism spectrum disorder in Software Engineering,IST - Information and Software Technology,A,"Context: Software Engineering is dedicated to the systematic and efficient development of software, which necessitates the active participation of all team members and a recognition of their unique skills and abilities, including those with autism spectrum disorders (ASD). The inclusion of individuals with ASD presents new perspectives, yet there is a lack of systematic evidence regarding the primary obstacles and potential benefits associated with their inclusion. Objective: This paper aims to identify, characterize, and describe barriers, facilitators, and methodological proposals described by the community to include individuals with ASD in the discipline of Software Engineering. Methods: We conducted a comprehensive systematic multivocal mapping study to evaluate the existing evidence on the inclusion of individuals with ASD in Software Engineering. Results: We obtained 34 primary studies from which we identified the main facilitators of motivation to learn new skills, attention to detail, and the ability to report and visualize patterns. In contrast, the main barriers detected were communication, a lack of neurodivergent computational thinking, and sensory integration. Additionally, we identified and classified four categories of proposals that allowed the inclusion of individuals with ASD: (i) using virtual reality, (ii) creating more inclusive workspaces, (iii) encouraging neurodivergent computational thinking, and (iv) improving social skills. Conclusions: This study identifies the principal elements that ought to be taken into consideration when allocating tasks and roles to individuals with ASD in software development. © 2024 Elsevier B.V.",Autism spectrum disorders; Software engineering; Systematic multivocal mapping study,Abstract,True,
Scopus,journalPaper,2024,SENEM: A software engineering-enabled educational metaverse,IST - Information and Software Technology,A,"Context: The term metaverse refers to a persistent, virtual, three-dimensional environment where individuals may communicate, engage, and collaborate. One of the most multifaceted and challenging use cases of the metaverse is education, where educators and learners may require multiple technical, social, psychological, and interaction instruments to accomplish their learning objectives. While the characteristics of the metaverse might nicely fit the problem's needs, our research points out a noticeable lack of knowledge into (1) the specific requirements that an educational metaverse should actually fulfill to let educators and learners successfully interact towards their objectives and (2) how to design an appropriate educational metaverse for both educators and learners. Objective: In this paper, we aim to bridge this knowledge gap by proposing SENEM, a novel software engineering-enabled educational metaverse. We first elicit a set of functional requirements that an educational metaverse should fulfill. Method: In this respect, we conduct a literature survey to extract the currently available knowledge on the matter discussed by the research community, and afterward, we assess and complement such knowledge through semi-structured interviews with educators and learners. Upon completing the requirements elicitation stage, we then build our prototype implementation of SENEM, a metaverse that makes available to educators and learners the features identified in the previous stage. Finally, we evaluate the tool in terms of learnability, efficiency, and satisfaction through a Rapid Iterative Testing and Evaluation research approach, leading us to the iterative refinement of our prototype. Results: Through our survey strategy, we extracted nine requirements that guided the tool development that the study participants positively evaluated. Conclusion: Our study reveals that the target audience appreciates the elicited design strategy. Our work has the potential to form a solid contribution that other researchers can use as a basis for further improvements. © 2024 The Author(s)",Human-centered studies; Metaverse engineering; Software engineering in practice; Virtual learning environments,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Collaborative software design and modeling in virtual reality,IST - Information and Software Technology,A,"Context: Software engineering is becoming more and more distributed. Developers and other stakeholders are often located in different locations, departments, and countries and operating within different time zones. Most online software design and modeling tools are not adequate for distributed collaboration since they do not support awareness and lack features for effective communication. Objective: The aim of our research is to support distributed software design activities in Virtual Reality (VR). Method: Using design science research methodology, we design and evaluate a tool for collaborative design in VR. We evaluate the collaboration efficiency and recall of design information when using the VR software design environment compared to a non-VR software design environment. Moreover, we collect the perceptions and preferences of users to explore the opportunities and challenges that were incurred by using the VR software design environment. Results: We find that there is no significant difference in the efficiency and recall of design information when using the VR compared to the non-VR environment. Furthermore, we find that developers are more satisfied with collaboration in VR. Conclusion: The results of our research and similar studies show that working in VR is not yet faster or more efficient than working on standard desktops. It is very important to improve the interface in VR (gestures with haptics, keyboard and voice input), as confirmed by the difference in results between the first and second evaluation. © 2023 Elsevier B.V.",Collaboration; Immersion; Software development; Software modeling; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Emerging technologies in higher education assessment and feedback practices: A systematic literature review,JSS - Journal of Systems and Software,A,"The use of Emerging Technologies, such as Artificial Intelligence (AI), Learning Analytics (LA) and Extended Reality (XR) applications, in higher education has proliferated in recent times, as these technologies are considered to have a significant impact on the future of postsecondary teaching and learning. We wanted to find out the emerging technologies used in computing education, its evaluation and effectiveness, and limitations and gaps for future research. We carried out a Systematic Literature Review study on the use of Emerging Technologies in higher education computing education to identify the state of the art in the use of these three groups of technologies for assessment and feedback practices. After systematic search and filtering from a search pool of 3038 studies published between 2016 and 2021, we selected 38 articles for detailed meta-analysis. Our findings reveal that 71% of the reviewed studies are journal articles, 50% studies focus on learning analytics, and the majority of the studies employ quantitative approaches. The results from this systematic review suggest that XR technologies have received least attention to date in computing education (amongst the emerging technologies considered for the review) and there is a lack of frameworks for design, evaluation and use of emerging technologies in higher education. The findings of this review will be beneficial for researchers and educators to obtain an in-depth understanding of the main areas of application of emerging technologies in higher education computing education, an inventory of emerging technology tools used for assessment and feedback, effectiveness indicators, and evaluation approaches that have been used. For evidence-based guidance on future assessment and feedback practices using emerging technologies, we also present a brief research agenda, drawing attention to the need to trial more XR, focus on formative assessment and feedback practices, better understand impact of human-centric issues and take more thoughtful consideration of ethics in the use of emerging technologies in computing education. © 2024 The Author(s)",Artificial intelligence; Assessment; Emerging technologies; Extended reality; Feedback; Learning analytics; Systematic literature review,Abstract_Keywords,True,
Scopus,journalPaper,2024,The influence of the city metaphor and its derivates in software visualization,JSS - Journal of Systems and Software,A,"Context: The city metaphor is widely used in software visualization to represent complex systems as buildings and structures, providing an intuitive way for developers to understand software components. Various software visualization tools have utilized this approach. Objective: Identify the influence of the city metaphor on software visualization research, determine its state-of-the-art status, and identify derived tools and their main characteristics. Method: Conduct a systematic mapping study of 406 publications that reference the first paper on the use of the city metaphor in software visualization and/or the main paper of the CodeCity tool. Analyze the 168 publications from which valuable information could be extracted, and build a complete categoric analysis. Results: The field has grown considerably, with an increasing number of publications since 2001, and a changing research community with evolving interconnections between groups. Researchers have developed more tools that support the city metaphor, but less than 50% of the tools were referenced in their papers. Moreover, 85% of the tools did not use extended reality environments, indicating an opportunity for further exploration. Conclusion: The study demonstrates the active and continually growing presence of the city metaphor in research and its impact on software visualization and its derivatives. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2024",City metaphor; Extended reality; Software comprehension; Software visualization; State of the art; Systematic mapping study; Visualizations,Abstract_Keywords,True,
Scopus,journalPaper,2024,Stripe Sensitive Convolution for Omnidirectional Image Dehazing,TVCG - Transactions on Visualization and Computer Graphics,A,"The haze in a scenario may affect the 360 photo/video quality and the immersive 360° virtual reality (VR) experience. The recent single image dehazing methods, to date, have been only focused on plane images. In this work, we propose a novel neural network pipeline for single omnidirectional image dehazing. To create the pipeline, we build the first hazy omnidirectional image dataset, which contains both synthetic and real-world samples. Then, we propose a new stripe sensitive convolution (SSConv) to handle the distortion problems due to the equirectangular projections. The SSConv calibrates distortion in two steps: 1) extracting features using different rectangular filters and, 2) learning to select the optimal features by a weighting of the feature stripes (a series of rows in the feature maps). Subsequently, using SSConv, we design an end-to-end network that jointly learns haze removal and depth estimation from a single omnidirectional image. The estimated depth map is leveraged as the intermediate representation and provides global context and geometric information to the dehazing module. Extensive experiments on challenging synthetic and real-world omnidirectional image datasets demonstrate the effectiveness of SSConv, and our network attains superior dehazing performance. The experiments on practical applications also demonstrate that our method can significantly improve the 3-D object detection and 3-D layout performances for hazy omnidirectional images. © 1995-2012 IEEE.",Omnidirectional image dehazing; omnidirectional image depth estimation; stripe sensitive convolution; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,VR-HandNet: A Visually and Physically Plausible Hand Manipulation System in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—This study aims to allow users to perform dexterous hand manipulation of objects in virtual environments with handheld VR controllers. To this end, the VR controller is mapped to the virtual hand and the hand motions are dynamically synthesized when the virtual hand approaches an object. At each frame, given the information about the virtual hand, VR controller input, and hand-object spatial relations, the deep neural network determines the desired joint orientations of the virtual hand model in the next frame. The desired orientations are then converted into a set of torques acting on hand joints and applied to a physics simulation to determine the hand pose at the next frame. The deep neural network, named VR-HandNet, is trained with a reinforcement learning-based approach. Therefore, it can produce physically plausible hand motion since the trial-and-error training process can learn how the interaction between hand and object is performed under the environment that is simulated by a physics engine. Furthermore, we adopted an imitation learning paradigm to increase visual plausibility by mimicking the reference motion datasets. Through the ablation studies, we validated the proposed method is effectively constructed and successfully serves our design goal. A live demo is demonstrated in the supplementary video. © 2024 IEEE Computer Society. All rights reserved.",Hand manipulation; physics-based animation; reinforcement learning; virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,ClockRay: A Wrist-Rotation Based Technique for Occluded-Target Selection in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Target selection is one of essential operation made available by interaction techniques in virtual reality (VR) environments. However, effectively positioning or selecting occluded objects is under-investigated in VR, especially in the context of high-density or a high-dimensional data visualization with VR. In this paper, we propose ClockRay, an occluded-object selection technique that can maximize the intrinsic human wrist rotation skills through the integration of emerging ray selection techniques in VR environments. We describe the design space of the ClockRay technique and then evaluate its performance in a series of user studies. Drawing on the experimental results, we discuss the benefits of ClockRay compared to two popular ray selection techniques – RayCursor and RayCasting. Our findings can inform the design of VR-based interactive visualization systems for high-density data. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",3D data visualization; disambiguation; object selection; RayCasting; Virtual reality; wrist rotation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,An Examination of the Relationship between Visualization Media and Consumer Product Evaluation,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual product presentations that rely on static images and text are often insufficient to communicate all the information that is necessary to accurately evaluate a product. Technologies such as Virtual Reality (VR) or Augmented Reality (AR) have enabled more sophisticated representation methods, but certain product characteristics are difficult to assess and may result in perceptual differences when a product is evaluated in different visual media. In this article, we report two case studies in which a group of participants evaluated three designs of two product typologies (i.e., a desktop telephone and a coffee maker) as presented in three different visual media (i.e., photorealistic renderings, AR, and VR for the first case study; and photographs, a non-immersive virtual environment, and AR for the second case study) using eight semantic scales. An inferential statistical method using Aligned Rank Transform (ART) proceedings was applied to determine perceptual differences between groups. Our results show that in both cases product attributes in Jordan's physio-pleasure category are the most affected by the presentation media. The socio-pleasure category was also affected for the case of the coffee makers. The level of immersion afforded by the medium significantly affects product evaluation. © 1995-2012 IEEE.",and virtual realities; Artificial; augmented; consumer products; perception and psychophysics; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Redirected Walking on Omnidirectional Treadmill,TVCG - Transactions on Visualization and Computer Graphics,A,"—Redirected walking (RDW) and omnidirectional treadmill (ODT) are two effective solutions to the natural locomotion interface in virtual reality. ODT fully compresses the physical space and can be used as the integration carrier of all kinds of devices. However, the user experience varies in different directions of ODT, and the premise of interaction between users and integrated devices is a good match between virtual and real objects. RDW technology uses visual cues to guide the user’s location in physical space. Based on this principle, combining RDW technology with ODT to guide the user’s walking direction through visual cues can effectively improve user experience on ODT and make full use of various devices integrated on ODT. This paper explores the novel prospects of combining RDW technology with ODT and formally puts forward the concept of O-RDW (ODT-based RDW). Two baseline algorithms, i.e., OS2MD (ODT-based steer to multi-direction), and OS2MT (ODT-based steer to multi-target), are proposed to combine the merits of both RDW and ODT. With the help of the simulation environment, this paper quantitatively analyzes the applicable scenarios of the two algorithms and the influence of several main factors on the performance. Based on the conclusions of the simulation experiments, the two O-RDW algorithms are successfully applied in the practical application case of multi-target haptic feedback. Combined with the user study, the practicability and effectiveness of O-RDW technology in practical use are further verified. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Device integration haptic feedback locomotion interfaces; omnidirectional treadmill; redirected walking,Abstract,True,
Scopus,journalPaper,2024,Real-Time Multi-Map Saliency-Driven Gaze Behavior for Non-Conversational Characters,TVCG - Transactions on Visualization and Computer Graphics,A,"—Gaze behavior of virtual characters in video games and virtual reality experiences is a key factor of realism and immersion. Indeed, gaze plays many roles when interacting with the environment; not only does it indicate what characters are looking at, but it also plays an important role in verbal and non-verbal behaviors and in making virtual characters alive. Automated computing of gaze behaviors is however a challenging problem, and to date none of the existing methods are capable of producing close-to-real results in an interactive context. We therefore propose a novel method that leverages recent advances in several distinct areas related to visual saliency, attention mechanisms, saccadic behavior modelling, and head-gaze animation techniques. Our approach articulates these advances to converge on a multi-map saliency-driven model which offers real-time realistic gaze behaviors for non-conversational characters, together with additional user-control over customizable features to compose a wide variety of results. We first evaluate the benefits of our approach through an objective evaluation that confronts our gaze simulation with ground truth data using an eye-tracking dataset specifically acquired for this purpose. We then rely on subjective evaluation to measure the level of realism of gaze animations generated by our method, in comparison with gaze animations captured from real actors. Our results show that our method generates gaze behaviors that cannot be distinguished from captured gaze animations. Overall, we believe that these results will open the way for more natural and intuitive design of realistic and coherent gaze animations for real-time applications. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Animation dataset eye-tracking data gaze behavior; neural networks; simulation,Abstract,True,
Scopus,journalPaper,2024,A Comparative Evaluation of Optical See-Through Augmented Reality in Surgical Guidance,TVCG - Transactions on Visualization and Computer Graphics,A,"During traditional surgeries, planning and instrument guidance is displayed on an external screen. Recent developments of augmented reality (AR) techniques can overcome obstacles including hand-eye discoordination and heavy mental load. Among these AR technologies, optical see-through (OST) schemes with stereoscopic displays can provide depth perception and retain the physical scene for safety considerations. However, limitations still exist in certain AR systems and the influence of these factors on surgical performance is yet to explore. To this end, experiments of multi-scale surgical tasks were carried out to compare head-mounted display (HMD) AR and autostereoscopic image overlay (AIO) AR, concerning objective performance and subjective evaluation. To solely analyze effects brought by display techniques, the tracking system in each included display system was identical and similar tracking accuracy was proved by a preliminary experiment. Focus and context rendering was utilized to enhance in-situ visualization for surgical guidance. Latency values of all display systems were assessed and a delay experiment proved the latency differences had no significant impact on user performance. Results of multi-scale surgical tasks showed that HMD outperformed in detailed operations probably due to stable resolution along the depth axis, while AIO had better performance in larger-scale operations for better depth perception. This article helps point out the critical limitations of current OST AR techniques and potentially promotes the progress of AR applications in surgical guidance. © 2023 IEEE.",Augmented reality; autostereoscopic image overlay; head-mounted display; optical see-through; surgical guidance,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Sensory Attenuation With a Virtual Robotic Arm Controlled Using Facial Movements,TVCG - Transactions on Visualization and Computer Graphics,A,"—When humans generate stimuli voluntarily, they perceive the stimuli more weakly than those produced by others, which is called sensory attenuation (SA). SA has been investigated in various body parts, but it is unclear whether an extended body induces SA. This study investigated the SA of audio stimuli generated by an extended body. SA was assessed using a sound comparison task in a virtual environment. We prepared robotic arms as extended bodies, and the robotic arms were controlled by facial movements. To evaluate the SA of robotic arms, we conducted two experiments. Experiment 1 investigated the SA of the robotic arms under four conditions. The results showed that robotic arms manipulated by voluntary actions attenuated audio stimuli. Experiment 2 investigated the SA of the robotic arm and innate body under five conditions. The results indicated that the innate body and robotic arm induced SA, while there were differences in the sense of agency between the innate body and robotic arm. Analysis of the results indicated three findings regarding the SA of the extended body. First, controlling the robotic arm with voluntary actions in a virtual environment attenuates the audio stimuli. Second, there were differences in the sense of agency related to SA between extended and innate bodies. Third, the SA of the robotic arm was correlated with the sense of body ownership. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",human augmentation; robotic arm; Sensory attenuation; virtual reality,Keywords,True,
Scopus,journalPaper,2024,An Evaluation of View Rotation Techniques for Seated Navigation in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Head tracking is commonly used in VR applications to allow users to naturally view 3D content using physical head movement, but many applications also support turning with hand-held controllers. Controller and joystick controls are convenient for practical settings where full 360-degree physical rotation is not possible, such as when the user is sitting at a desk. Though controller-based rotation provides the benefit of convenience, previous research has demonstrated that virtual or joystick-controlled view rotation to have drawbacks of sickness and disorientation compared to physical turning. To combat such issues, researchers have considered various techniques such as speed adjustments or reduced field of view, but data is limited on how different variations for joystick rotation influences sickness and orientation perception. Our studies include different variations of techniques such as joystick rotation, resetting, and field-of-view reduction. We investigate trade-offs among different techniques in terms of sickness and the ability to maintain spatial orientation. In two controlled experiments, participants traveled through a sequence of rooms and were tested on spatial orientation, and we also collected subjective measures of sickness and preference. Our findings indicate a preference by users towards directly-manipulated joystick-based rotations compared to user-initiated resetting and minimal effects of technique on spatial awareness. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Human-centered computing; human-computer interaction; virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,MobileSky: Real-Time Sky Replacement for Mobile AR,TVCG - Transactions on Visualization and Computer Graphics,A,"We present MobileSky, the first automatic method for real-time high-quality sky replacement for mobile AR applications. The primary challenge of this task is how to extract sky regions in camera feed both quickly and accurately. While the problem of sky replacement is not new, previous methods mainly concern extraction quality rather than efficiency, limiting their application to our task. We aim to provide higher quality, both spatially and temporally consistent sky mask maps for all camera frames in real time. To this end, we develop a novel framework that combines a new deep semantic network called FSNet with novel post-processing refinement steps. By leveraging IMU data, we also propose new sky-aware constraints such as temporal consistency, position consistency, and color consistency to help refine the weakly classified part of the segmentation output. Experiments show that our method achieves an average of around 30 FPS on off-the-shelf smartphones and outperforms the state-of-the-art sky replacement methods in terms of execution speed and quality. In the meantime, our mask maps appear to be visually more stable across frames. Our fast sky replacement method enables several applications, such as AR advertising, art making, generating fantasy celestial objects, visually learning about weather phenomena, and advanced video-based visual effects. To facilitate future research, we also create a new video dataset containing annotated sky regions with IMU data. © 2023 IEEE.",Mobile augmented reality; semantic segmentation; sky replacement,Keywords,True,
Scopus,journalPaper,2024,Real-Time High-Quality Computer-Generated Hologram Using Complex-Valued Convolutional Neural Network,TVCG - Transactions on Visualization and Computer Graphics,A,"—Holographic displays are ideal display technologies for virtual and augmented reality because all visual cues are provided. However, real-time high-quality holographic displays are difficult to achieve because the generation of high-quality computer-generated hologram (CGH) is inefficient in existing algorithms. Here, complex-valued convolutional neural network (CCNN) is proposed for phase-only CGH generation. The CCNN-CGH architecture is effective with a simple network structure based on the character design of complex amplitude. A holographic display prototype is set up for optical reconstruction. Experiments verify that state-of-the-art performance is achieved in terms of quality and generation speed in existing end-to-end neural holography methods using the ideal wave propagation model. The generation speed is three times faster than HoloNet and one-sixth faster than Holo-encoder, and the Peak Signal to Noise Ratio (PSNR) is increased by 3 dB and 9 dB, respectively. Real-time high-quality CGHs are generated in 1920 × 1072 and 3840 × 2160 resolutions for dynamic holographic displays. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",augmented reality; Holography; neural models; virtual,Abstract_Keywords,True,
Scopus,journalPaper,2024,Studying the Influence of Multisensory Stimuli on a Firefighting Training Virtual Environment,TVCG - Transactions on Visualization and Computer Graphics,A,"—How we perceive and experience the world around us is inherently multisensory. Most of the Virtual Reality (VR) literature is based on the senses of sight and hearing. However, there is a lot of potential for integrating additional stimuli into Virtual Environments (VEs), especially in a training context. Identifying the relevant stimuli for obtaining a virtual experience that is perceptually equivalent to a real experience will lead users to behave the same across environments, which adds substantial value for several training areas, such as firefighters. In this article, we present an experiment aiming to assess the impact of different sensory stimuli on stress, fatigue, cybersickness, Presence and knowledge transfer of users during a firefighter training VE. The results suggested that the stimulus that significantly impacted the user’s response was wearing a firefighter’s uniform and combining all sensory stimuli under study: heat, weight, uniform, and mask. The results also showed that the VE did not induce cybersickness and that it was successful in the task of transferring knowledge. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Biofeedback; computer graphics; professional training; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Path Tracing in 2D, 3D, and Physicalized Networks",TVCG - Transactions on Visualization and Computer Graphics,A,"It is common to advise against using 3D to visualize abstract data such as networks, however Ware and Mitchell's 2008 study showed that path tracing in a network is less error prone in 3D than in 2D. It is unclear, however, if 3D retains its advantage when the 2D presentation of a network is improved using edge-routing, and when simple interaction techniques for exploring the network are available. We address this with two studies of path tracing under new conditions. The first study was preregistered, involved 34 users, and compared 2D and 3D layouts that the user could rotate and move in virtual reality with a handheld controller. Error rates were lower in 3D than in 2D, despite the use of edge-routing in 2D and the use of mouse-driven interactive highlighting of edges. The second study involved 12 users and investigated data physicalization, comparing 3D layouts in virtual reality versus physical 3D printouts of networks augmented with a Microsoft HoloLens headset. No difference was found in error rate, but users performed a variety of actions with their fingers in the physical condition which can inform new interaction techniques. © 1995-2012 IEEE.",3D printing; augmented reality; data physicalization; graph visualization; path finding; path following; tangible,Abstract_Keywords,True,
Scopus,journalPaper,2024,StyleVR: Stylizing Character Animations With Normalizing Flows,TVCG - Transactions on Visualization and Computer Graphics,A,"—The significance of artistry in creating animated virtual characters is widely acknowledged, and motion style is a crucial element in this process. There has been a long-standing interest in stylizing character animations with style transfer methods. However, this kind of models can only deal with short-term motions and yield deterministic outputs. To address this issue, we propose a generative model based on normalizing flows for stylizing long and aperiodic animations in the VR scene. Our approach breaks down this task into two sub-problems: motion style transfer and stylized motion generation, both formulated as the instances of conditional normalizing flows with multi-class latent space. Specifically, we encode high-frequency style features into the latent space for varied results and control the generation process with style-content labels for disentangled edits of style and content. We have developed a prototype, StyleVR, in Unity, which allows casual users to apply our method in VR. Through qualitative and quantitative comparisons, we demonstrate that our system outperforms other methods in terms of style transfer as well as stochastic stylized motion generation. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Character animation motion eneration st le transfer; normalizing flow; virtual reality,Keywords,True,
Scopus,journalPaper,2024,Interactive Virtual Ankle Movement Controlled by Wrist sEMG Improves Motor Imagery: An Exploratory Study,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual reality (VR) techniques can significantly enhance motor imagery training by creating a strong illusion of action for central sensory stimulation. In this article, we establish a precedent by using surface electromyography (sEMG) of contralateral wrist movement to trigger virtual ankle movement through an improved data-driven approach with a continuous sEMG signal for fast and accurate intention recognition. Our developed VR interactive system can provide feedback training for stroke patients in the early stages, even if there is no active ankle movement. Our objectives are to evaluate: 1) the effects of VR immersion mode on body illusion, kinesthetic illusion, and motor imagery performance in stroke patients; 2) the effects of motivation and attention when utilizing wrist sEMG as a trigger signal for virtual ankle motion; 3) the acute effects on motor function in stroke patients. Through a series of well-designed experiments, we have found that, compared to the 2D condition, VR significantly increases the degree of kinesthetic illusion and body ownership of the patients, and improves their motor imagery performance and motor memory. When compared to conditions without feedback, using contralateral wrist sEMG signals as trigger signals for virtual ankle movement enhances patients' sustained attention and motivation during repetitive tasks. Furthermore, the combination of VR and feedback has an acute impact on motor function. Our exploratory study suggests that the sEMG-based immersive virtual interactive feedback provides an effective option for active rehabilitation training for severe hemiplegia patients in the early stages, with great potential for clinical application.  © 1995-2012 IEEE.",motor imagery; sEMG-based virtual feedback; VR-based stroke rehabilitation training,Abstract,True,
Scopus,journalPaper,2024,Visual Illusion Created by a Striped Pattern Through Augmented Reality for the Prevention of Tumbling on Stairs,TVCG - Transactions on Visualization and Computer Graphics,A,"A fall on stairs can be a dangerous accident. An important indicator of falling risk is the foot clearance, which is the height of the foot when ascending stairs or the distance of the foot from the step when descending. We developed an augmented reality system with a holographic lens using a visual illusion to improve the foot clearance on stairs. The system draws a vertical striped pattern on the stair riser as the participant ascends the stairs to create the illusion that the steps are higher than the actual steps, and draws a horizontal striped pattern on the stair tread as the participant descends the stairs to create the illusion of narrower stairs. We experimentally evaluated the accuracy of the system and fitted a model to determine the appropriate stripe thickness. Finally, participants ascended and descended stairs before, during, and after using the augmented reality system. The foot clearance significantly improved, not only while the participants used the system but also after they used the system compared with before.  © 1995-2012 IEEE.",Human-computer interaction; user interfaces; virtual and augmented reality; virtual device interfaces,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,VR Blowing: A Physically Plausible Interaction Method for Blowing Air in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"This article introduces an interaction method allowing virtual reality (VR) users to interact with virtual objects by blowing air. The proposed method allows users to interact with virtual objects in a physically plausible way by recognizing the intensity of the wind generated by the user's actual wind blowing activity in the physical world. This is expected to provide immersed VR experience since it enables users to interact with virtual objects in the same way they do in the real world. Three experiments were carried out to develop and improve this method. In the first experiment, we collected the user's blowing data and used it to model a formula to estimate the speed of the wind from the sound waves obtained through a microphone. In the second experiment, we investigated how much gain can be applied to the formula obtained in the first experiment. The aim is to reduce the lung capacity required to generate wind without compromising physical plausibility. In the third experiment, the advantages and disadvantages of the proposed method compared to the controller-based method were investigated in two scenarios of blowing a ball and a pinwheel. According to the experimental results and participant interview, participants felt a stronger sense of presence and found the VR experience more fun with the proposed blowing interaction method. © 1995-2012 IEEE.",Blowing control; human-computer interaction; immersion; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Specular Path Generation and Near-Reflective Diffraction in Interactive Acoustical Simulations,TVCG - Transactions on Visualization and Computer Graphics,A,"Most systems for simulating sound propagation in a virtual environment for interactive applications use ray- or path-based models of sound. With these models, the 'early' (low-order) specular reflection paths play a key role in defining the 'sound' of the environment. However, the wave nature of sound, and the fact that smooth objects are approximated by triangle meshes, pose challenges for creating realistic approximations of the reflection results. Existing methods which produce accurate results are too slow to be used in most interactive applications with dynamic scenes. This paper presents a method for reflections modeling called spatially sampled near-reflective diffraction (SSNRD), based on an existing approximate diffraction model, Volumetric Diffraction and Transmission (VDaT). The SSNRD model addresses the challenges mentioned above, produces results accurate to within 1-2 dB on average compared to edge diffraction, and is fast enough to generate thousands of paths in a few milliseconds in large scenes. This method encompasses scene geometry processing, path trajectory generation, spatial sampling for diffraction modeling, and a small deep neural network (DNN) to produce the final response of each path. All steps of the method are GPU-accelerated, and NVIDIA RTX real-time ray tracing hardware is used for spatial computing tasks beyond just traditional ray tracing. © 1995-2012 IEEE.",Acoustics; graph and tree search strategies; neural nets; parallel algorithms; raytracing; virtual reality,Keywords,True,
Scopus,journalPaper,2024,PalmEx: Adding Palmar Force-Feedback for 3D Manipulation With Haptic Exoskeleton Gloves,TVCG - Transactions on Visualization and Computer Graphics,A,"—Haptic exoskeleton gloves are a widespread solution for providing force-feedback in Virtual Reality (VR), especially for 3D object manipulations. However, they are still lacking an important feature regarding in-hand haptic sensations: the palmar contact. In this paper, we present PalmEx, a novel approach which incorporates palmar force-feedback into exoskeleton gloves to improve the overall grasping sensations and manual haptic interactions in VR. PalmEx’s concept is demonstrated through a self-contained hardware system augmenting a hand exoskeleton with an encountered palmar contact interface – physically encountering the users’ palm. We build upon current taxonomies to elicit PalmEx’s capabilities for both the exploration and manipulation of virtual objects. We first conduct a technical evaluation optimising the delay between the virtual interactions and their physical counterparts. We then empirically evaluate PalmEx’s proposed design space in a user study (n=12) to assess the potential of a palmar contact for augmenting an exoskeleton. Results show that PalmEx offers the best rendering capabilities to perform believable grasps in VR. PalmEx highlights the importance of the palmar stimulation, and provides a low-cost solution to augment existing high-end consumer hand exoskeletons. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Artefact; encountered-type of haptic device; ETHD; exoskeleton; haptics; on-demand; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,STTAR: Surgical Tool Tracking Using Off-the-Shelf Augmented Reality Head-Mounted Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"The use of Augmented Reality (AR) for navigation purposes has shown beneficial in assisting physicians during the performance of surgical procedures. These applications commonly require knowing the pose of surgical tools and patients to provide visual information that surgeons can use during the performance of the task. Existing medical-grade tracking systems use infrared cameras placed inside the Operating Room (OR) to identify retro-reflective markers attached to objects of interest and compute their pose. Some commercially available AR Head-Mounted Displays (HMDs) use similar cameras for self-localization, hand tracking, and estimating the objects' depth. This work presents a framework that uses the built-in cameras of AR HMDs to enable accurate tracking of retro-reflective markers without the need to integrate any additional electronics into the HMD. The proposed framework can simultaneously track multiple tools without having previous knowledge of their geometry and only requires establishing a local network between the headset and a workstation. Our results show that the tracking and detection of the markers can be achieved with an accuracy of 0.09±0.06mm on lateral translation, 0.42±0.32mm on longitudinal translation and 0.80±0.39° for rotations around the vertical axis. Furthermore, to showcase the relevance of the proposed framework, we evaluate the system's performance in the context of surgical procedures. This use case was designed to replicate the scenarios of k-wire insertions in orthopedic procedures. For evaluation, seven surgeons were provided with visual navigation and asked to perform 24 injections using the proposed framework. A second study with ten participants served to investigate the capabilities of the framework in the context of more general scenarios. Results from these studies provided comparable accuracy to those reported in the literature for AR-based navigation procedures. © 1995-2012 IEEE.",Augmented reality; computer-assisted medical procedures; navigation; tracking,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"The Effects of Auditory, Visual, and Cognitive Distractions on Cybersickness in Virtual Reality",TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness (CS) is one of the challenges that has hindered the widespread adoption of Virtual Reality (VR). Consequently, researchers continue to explore novel means to mitigate the undesirable effects associated with this affliction, one that may require a combination of remedies as opposed to a solitary stratagem. Inspired by research probing into the use of distractions as a means to control pain, we investigated the efficacy of this countermeasure against CS, studying how the introduction of temporally time-gated distractions affects this malady during a virtual experience featuring active exploration. Downstream of this, we studied how other aspects of the VR experience are affected by this intervention. We discuss the results of a between-subjects study manipulating the presence, sensory modality, and nature of periodic and short-lived (5-12 seconds) distractor stimuli across four experimental conditions: 1) no-distractors (ND); 2) auditory distractors (AD); 3) visual distractors (VD); 4) cognitive distractors (CD). Two of these conditions (VD and AD) formed a yoked control design wherein every matched pair of 'seers' and 'hearers' was periodically exposed to distractors that were identical in terms of content, temporality, duration, and sequence. In the CD condition, each participant had to periodically perform a 2-back working memory task, the duration and temporality of which was matched to distractors presented in each matched pair of the yoked conditions. These three conditions were compared to a baseline control group featuring no distractions. Results indicated that the reported sickness levels were lower in all three distraction groups in comparison to the control group. The intervention also increased the amount of time users were able to endure the VR simulation and avoided causing detriments to spatial memory and virtual travel efficiency. Overall, it appears that it may be possible to make users less consciously aware and bothered by the symptoms of CS, thereby reducing its perceived severity.  © 1995-2012 IEEE.",Active exploration; cybersickness; distractions; electrodermal activity; information recall; pain reduction; spatial memory; virtual motion; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Hold Tight: Identifying Behavioral Patterns During Prolonged Work in VR Through Video Analysis,TVCG - Transactions on Visualization and Computer Graphics,A,"VR devices have recently been actively promoted as tools for knowledge workers and prior work has demonstrated that VR can support some knowledge worker tasks. However, only a few studies have explored the effects of prolonged use of VR such as a study observing 16 participants working in VR and a physical environment for one work-week each and reporting mainly on subjective feedback. As a nuanced understanding of participants' behavior in VR and how it evolves over time is still missing, we report on the results from an analysis of 559 hours of video material obtained in this prior study. Among other findings, we report that (1) the frequency of actions related to adjusting the headset reduced by 46% and the frequency of actions related to supporting the headset reduced by 42% over the five days; (2) the HMD was removed 31% less frequently over the five days but for 41% longer periods; (3) wearing an HMD is disruptive to normal patterns of eating and drinking, but not to social interactions, such as talking. The combined findings in this work demonstrate the value of long-term studies of deployed VR systems and can be used to inform the design of better, more ergonomic VR systems as tools for knowledge workers.  © 1995-2012 IEEE.",future of work; long-term; office work; productivity work; prolonged use; video-analysis; virtual reality,Keywords,True,
Scopus,journalPaper,2024,Breaking the Isolation: Exploring the Impact of Passthrough in Shared Spaces on Player Performance and Experience in VR Exergames,TVCG - Transactions on Visualization and Computer Graphics,A,"VR exergames offer an engaging solution to combat sedentary behavior and promote physical activity. However, challenges emerge when playing these games in shared spaces, particularly due to the presence of bystanders. VR's passthrough functionality enables players to maintain awareness of their surrounding environment while immersed in VR gaming, rendering it a promising solution to improve users' awareness of the environment. This study investigates the passthrough's impact on player performance and experiences in shared spaces, involving an experiment with 24 participants that examines Space (Office vs. Corridor) and Passthrough Function (With vs. Without). Results reveal that Passthrough enhances game performance and environmental awareness while reducing immersion. Players prefer an open area to an enclosed room, whether with or without Passthrough, finding it more socially acceptable. Additionally, Passthrough appears to encourage participation among players with higher self-consciousness, potentially alleviating their concerns about being observed by bystanders. Our findings provide valuable insights for designing VR experiences in shared spaces, underscoring the potential of VR's passthrough to enhance user experiences and promote VR adoption in these environments.  © 1995-2012 IEEE.",Exergames; Immersion; Passthrough; Shared Spaces; Social Acceptability; Virtual Reality,Keywords,True,
Scopus,journalPaper,2024,MOUNT: Learning 6DoF Motion Prediction Based on Uncertainty Estimation for Delayed AR Rendering,TVCG - Transactions on Visualization and Computer Graphics,A,"The delay of rendering on AR devices requires prediction of head motion using sensor data acquired tens of even one hundred milliseconds ago to avoid misalignment between the virtual content and the physical world, where the misalignment will lead to a sense of time latency and dizziness for users. To solve the problem, we propose a method for the 6DoF motion prediction to compensate for the time latency. Compared with traditional hand-crafted methods, our method is based on deep learning, which has better motion prediction ability to deal with complex human motion. In particular, we propose a MOtion UNcerTainty encode decode network (MOUNT) that estimates the uncertainty of input data and predicts the uncertainty of output motion to improve the prediction accuracy and smoothness. Experiments on the EuRoC and our collected dataset demonstrate that our method significantly outperforms the traditional method and greatly improves AR visual effects. © 1995-2012 IEEE.",learning environments; learning technologies; Virtual and augmented reality,Keywords,True,
Scopus,journalPaper,2024,The Differential Effects of Multisensory Attentional Cues on Task Performance in VR Depending on the Level of Cognitive Load and Cognitive Capacity,TVCG - Transactions on Visualization and Computer Graphics,A,"As the utilization of VR is expanding across diverse fields, research on devising attentional cues that could optimize users' task performance in VR has become crucial. Since the cognitive load imposed by the context and the individual's cognitive capacity are representative factors that are known to determine task performance, we aimed to examine how the effects of multisensory attentional cues on task performance are modulated by the two factors. For this purpose, we designed a new experimental paradigm in which participants engaged in dual (N-back, visual search) tasks under different levels of cognitive load while an attentional cue (visual, tactile, or visuotactile) was presented to facilitate search performance. The results showed that multi-sensory attentional cues are generally more effective than uni-sensory cues in enhancing task performance, but the benefit of multi-sensory cues changes according to the level of cognitive load and the individual's cognitive capacity; the amount of benefit increases as the cognitive load is higher and the cognitive capacity is lower. The findings of this study provide practical implications for designing attentional cues to enhance VR task performance, considering both the complexity of the VR context and users' internal characteristics.  © 1995-2012 IEEE.",Attentional Cue; Cognitive Capacity; Cognitive Load; Multisensory; Performance; Virtual Reality,Keywords,True,
Scopus,journalPaper,2024,Instant Segmentation and Fitting of Excavations in Subsurface Utility Engineering,TVCG - Transactions on Visualization and Computer Graphics,A,"Using augmented reality for subsurface utility engineering (SUE) has benefited from recent advances in sensing hardware, enabling the first practical and commercial applications. However, this progress has uncovered a latent problem â the insufficient quality of existing SUE data in terms of completeness and accuracy. In this work, we present a novel approach to automate the process of aligning existing SUE databases with measurements taken during excavation works, with the potential to correct the deviation from the as-planned to as-built documentation, which is still a big challenge for traditional workers at sight. Our segmentation algorithm performs infrastructure segmentation based on the live capture of an excavation on site. Our fitting approach correlates the inferred position and orientation with the existing digital plan and registers the as-planned model into the as-built state. Our approach is the first to circumvent tedious postprocessing, as it corrects data online and on-site. In our experiments, we show the results of our proposed method on both synthetic data and a set of real excavations.  © 1995-2012 IEEE.",3D Models; Augmented Reality; Geometric Constraints; Infrastructure; Localization; Segmentation,Abstract_Keywords,True,
Scopus,journalPaper,2024,RedirectedDoors+: Door-Opening Redirection with Dynamic Haptics in Room-Scale VR,TVCG - Transactions on Visualization and Computer Graphics,A,"RedirectedDoors is a visuo-haptic door-opening redirection technique in VR, and it has shown promise in its ability to efficiently compress the physical space required for a room-scale VR experience. However, its previous implementation has only supported laboratory experiments with a single door opening at a fixed location. To significantly expand this technique for room-scale VR, we have developed RedirectedDoors+, a robot-based system that permits consecutive door-opening redirection with haptics. Specifically, our system is mainly achieved with the use of three components: (1) door robots, a small number of wheeled robots equipped with a doorknob-like prop, (2) a robot-positioning algorithm that arbitrarily positions the door robots to provide the user with just-in-time haptic feedback during door opening, and (3) a user-steering algorithm that determines the redirection gain for every instance of door opening to keep the user away from the boundary of the play area. Results of simulated VR exploration in six virtual environments reveal our system's performance relative to user walking speed, paths, and number of door robots, from which we derive its usage guidelines. We then conduct a user study ($N=12$) in which participants experience a walkthrough application using the actual system. The results demonstrate that the system is able to provide haptic feedback while redirecting the user within a limited play area. © 1995-2012 IEEE.",Encounter-type haptic device; Redirected Walking; Virtual reality; Visuo-haptic redirection,Keywords,True,
Scopus,journalPaper,2024,"Assessing Depth Perception in VR and Video See-Through AR: A Comparison on Distance Judgment, Performance, and Preference",TVCG - Transactions on Visualization and Computer Graphics,A,"Spatial User Interfaces along the Reality-Virtuality continuum heavily depend on accurate depth perception. However, current display technologies still exhibit shortcomings in the simulation of accurate depth cues, and these shortcomings also vary between Virtual or Augmented Reality (VR, AR: eXtended Reality (XR) for short). This article compares depth perception between VR and Video See-Through (VST) AR. We developed a digital twin of an existing office room where users had top erform five depth-dependent tasks in VR and VST AR. Thirty-two participants took part in a user study using a 1 Ã - 4 within-subjects design. Our results reveal higher misjudgment rates in VST AR due to conflicting depth cues between virtual and physical content. Increased head movements observed in participants were interpreted as a compensatory response to these conflicting cues. Furthermore, a longer task completion time in the VST AR condition indicates a lower task performance in VST AR. Interestingly, while participants rated the VR condition as easier and contrary to the increased misjudgments and lower performance with the VST AR display, a majority still expressed a preference for the VST AR experience. We discuss and explain these findings with the high visual dominance and referential power of the physical content in the VST AR condition, leading to a higher spatial presence and plausibility. © 1995-2012 IEEE.",AR; Depth perception; egocentric distance judgment; task performance; user preference; video see-through; VR,Abstract,True,
Scopus,journalPaper,2024,Omnidirectional Virtual Visual Acuity: A User-Centric Visual Clarity Metric for Virtual Reality Head-Mounted Displays and Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Users' perceived image quality of virtual reality head-mounted displays (VR HMDs) is determined by multiple factors, including the HMD's structure, optical system, display and render resolution, and users' visual acuity (VA). Existing metrics such as pixels per degree (PPD) have limitations that prevent accurate comparison of different VR HMDs. One of the main limitations is that not all VR HMD manufacturers released the official PPD or details of their HMDs' optical systems. Without these details, developers and users cannot know the precise PPD or calculate it for a given HMD. The other issue is that the visual clarity varies with the VR environment. Our work has identified a gap in having a feasible metric that can measure the visual clarity of VR HMDs. To address this gap, we present an end-to-end and user-centric visual clarity metric, omnidirectional virtual visual acuity (OVVA), for VR HMDs. OVVA extends the physical visual acuity chart into a virtual format to measure the virtual visual acuity of an HMD's central focal area and its degradation in its noncentral area. OVVA provides a new perspective to measure visual clarity and can serve as an intuitive and accurate reference for VR applications sensitive to visual accuracy. Our results show that OVVA is a simple yet effective metric for comparing VR HMDs and environments. © 1995-2012 IEEE.",Frame rate; Head-mounted displays; Measurements; Passthrough; Render resolution; Virtual reality; Visual clarity,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Workspace Guardian: Investigating Awareness of Personal Workspace Between Co-Located Augmented Reality Users,TVCG - Transactions on Visualization and Computer Graphics,A,"As augmented reality (AR) systems proliferate and the technology gets smaller and less intrusive, we imagine a future where many AR users will interact in the same physical locations (e.g., in shared work places and public spaces). While previous research has explored AR collaboration in these spaces, our focus is on co-located but independent work. In this paper, we explore co-located AR user behavior and investigate techniques for promoting awareness of personal workspace boundaries. Specifically, we compare three techniques: showing all virtual content, visualizing bounding box outlines of content, and a self-defined workspace boundary. The findings suggest that a self-defined boundary led to significantly more personal workspace encroachments. © 1995-2012 IEEE.",augmented reality; Mixed reality; three-dimensional displays,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"PLUME: Record, Replay, Analyze and Share User Behavior in 6DoF XR Experiences",TVCG - Transactions on Visualization and Computer Graphics,A,"From education to medicine to entertainment, a wide range of industrial and academic fields now utilize eXtended Reality (XR) technologies. This diversity and growing use are boosting research and leading to an increasing number of XR experiments involving human subjects. The main aim of these studies is to understand the user experience in the broadest sense, such as the user cognitive and emotional states. Behavioral data collected during XR experiments, such as user movements, gaze, actions, and physiological signals constitute precious assets for analyzing and understanding the user experience. While they contribute to overcome the intrinsic flaws of explicit data such as post-experiment questionnaires, the required acquisition and analysis tools are costly and challenging to develop, especially for 6DoF (Degrees of Freedom) XR experiments. Moreover, there is no common format for XR behavioral data, which restrains data-sharing, and thus hinders wide usages across the community, replicability of studies, and the constitution of large datasets or meta-analysis. In this context, we present PLUME, an open-source software toolbox (PLUME Recorder, PLUME Viewer, PLUME Python) that allows for the exhaustive record of XR behavioral data (including synchronous physiological signals), their offline interactive replay and analysis (with a standalone application), and their easy sharing due to our compact and interoperable data format. We believe that PLUME can greatly benefit the scientific community by making the use of behavioral and physiological data available for the greatest, contributing to the reproducibility and replicability of XR user studies, enabling the creation of large datasets, and contributing to a deeper understanding of user experience. © 1995-2012 IEEE.",Data Collection; Extended Reality; Human-Computer Interaction; Physiological Signals; Quality of Experience; User Behavior; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Investigating the Effects of Avatarization and Interaction Techniques on Near-field Mixed Reality Interactions with Physical Components,TVCG - Transactions on Visualization and Computer Graphics,A,"Mixed reality (MR) interactions feature users interacting with a combination of virtual and physical components. Inspired by research investigating aspects associated with near-field interactions in augmented and virtual reality (AR & VR), we investigated how avatarization, the physicality of the interacting components, and the interaction technique used to manipulate a virtual object affected performance and perceptions of user experience in a mixed reality fundamentals of laparoscopic peg-transfer task wherein users had to transfer a virtual ring from one peg to another for a number of trials. We employed a 3 (Physicality of pegs) X 3 (Augmented Avatar Representation) X 2 (Interaction Technique) multi-factorial design, manipulating the physicality of the pegs as a between-subjects factor, the type of augmented self-avatar representation, and the type of interaction technique used for object-manipulation as within-subjects factors. Results indicated that users were significantly more accurate when the pegs were virtual rather than physical because of the increased salience of the task-relevant visual information. From an avatar perspective, providing users with a reach envelope-extending representation, though useful, was found to worsen performance, while co-located avatarization significantly improved performance. Choosing an interaction technique to manipulate objects depends on whether accuracy or efficiency is a priority. Finally, the relationship between the avatar representation and interaction technique dictates just how usable mixed reality interactions are deemed to be.  © 1995-2012 IEEE.",Interactions in MR; Mixed Reality; Self-Avatars; Tangible entities,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Molecular Docking Improved with Human Spatial Perception Using Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Adaptive steered molecular dynamics (ASMD) is a computational biophysics method in which an external force is applied to a selected set of atoms or a specific reaction coordinate to induce a particular molecular motion. Virtual reality (VR) based methods for protein-ligand docking are beneficial for visualizing on-the-fly interactive molecular dynamics and performing promising docking trajectories. In this paper, we propose a novel method to guide ASMD with optimal trajectories collected from human experiences using interactive molecular dynamics in virtual reality (iMD-VR). We also explain the benefits of using VR as a tool for expediting the process of ligand binding, outlining an experimental protocol that enables iMD-VR users to guide Amprenavir into and out of the binding pockets of HIV-1 protease and recreate their respective crystallographic binding poses within 5 minutes. Later, we discuss our analysis of the results from iMD-VR-assisted ASMD simulation and assess its performance compared to a standard ASMD simulation. From the accuracy point of view, our proposed method calculates higher Potential Mean Force (PMF) values consistently relative to a standard ASMD simulation with an almost twofold increase in all the experiments. Finally, we describe the novelty of the research and discuss results showcasing a faster and more effective convergence of the ligand to the protein's binding site as compared to a standard molecular dynamics simulation, proving the effectiveness of VR in the field of drug discovery. Future work includes the development of an artificial intelligence algorithm capable of predicting optimal binding trajectories for many protein-ligand pairs, as well as the required force needed to steer the ligand to follow the said trajectory. © 1995-2012 IEEE.",Molecular Docking; Molecular Dynamics Simulation; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Virtual Reality Self Co-Embodiment: An Alternative to Mirror Therapy for Post-Stroke Upper Limb Rehabilitation,TVCG - Transactions on Visualization and Computer Graphics,A,"We present Virtual Reality Self Co-embodiment, a new method for post-stroke upper limb rehabilitation. It is inspired by mirror therapy, where the patient's healthy arm is involved in recovering the affected arm's motion. By tracking the user's head, wrists, and fingers' positions, our new approach allows the handicapped arm to control a digital avatar in order to pursue a reaching task. We apply the concept of virtual co-embodiment to use the information from the unaffected arm and complete the affected limb's impaired motion, which is our added unique feature. This requires users to mechanically involve the incapacitated area as much as possible, prioritizing actual movement rather than the sole imagination of it. As a result, subjects will see a seemingly normally functional virtual arm primarily controlled by their handicapped extremity, but with the constant support of their healthy limb's motion. Our experiment compares the task execution performance and embodiment perceived when interacting with both mirror therapy and our proposed technique. We found that our approach's provided sense of ownership is mildly impacted by users' motion planning response times, which mirror therapy does not exhibit. We also observed that mirror therapy's sense of ownership is moderately affected by the subject's proficiency while executing the assigned task, which our new method did not display. The results indicate that our proposed method provides similar embodiment and rehabilitation capabilities to those perceived from existing mirror therapy. This experiment was performed in healthy individuals to have an unbiased comparison of how mirror therapy's and VRSelfCo's task performance and degree of virtual embodiment compare, but future work explores the possibility of applying this new approach to actual post-stroke patients. © 1995-2012 IEEE.",Human-centered computing; motor imagery; rehabilitation techniques; stroke recovery; user interfaces; virtual embodiment; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via Transformer-Based 360° Image Outpainting,TVCG - Transactions on Visualization and Computer Graphics,A,"360° images, with a field-of-view (FoV) of 180°× 360°, provide immersive and realistic environments for emerging virtual reality (VR) applications, such as virtual tourism, where users desire to create diverse panoramic scenes from a narrow FoV photo they take from a viewpoint via portable devices. It thus brings us to a technical challenge: 'How to allow the users to freely create diverse and immersive virtual scenes from a narrow FoV image with a specified viewport?' To this end, we propose a transformer-based 360° image outpainting framework called Dream360, which can generate diverse, high-fidelity, and high-resolution panoramas from user-selected viewports, considering the spherical properties of 360° images. Compared with existing methods, e.g., [3], which primarily focus on inputs with rectangular masks and central locations while overlooking the spherical property of 360° images, our Dream360 offers higher outpainting flexibility and fidelity based on the spherical representation. Dream360 comprises two key learning stages: (I) codebook-based panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN learns a sphere-specific codebook from spherical harmonic (SH) values, providing a better representation of spherical data distribution for scene modeling. The frequency-aware refinement matches the resolution and further improves the semantic consistency and visual fidelity of the generated results. Our Dream360 achieves significantly lower Frechet Inception Distance (FID) scores and better visual fidelity than existing methods. We also conducted a user study involving 15 participants to interactively evaluate the quality of the generated results in VR, demonstrating the flexibility and superiority of our Dream360 framework. © 1995-2012 IEEE.",360 image outpainting; virtual scene creation; vision transformer,Abstract,True,
Scopus,journalPaper,2024,Swift-Eye: Towards Anti-blink Pupil Tracking for Precise and Robust High-Frequency Near-Eye Movement Analysis with Event Cameras,TVCG - Transactions on Visualization and Computer Graphics,A,"Eye tracking has shown great promise in many scientific fields and daily applications, ranging from the early detection of mental health disorders to foveated rendering in virtual reality (VR). These applications all call for a robust system for high-frequency near-eye movement sensing and analysis in high precision, which cannot be guaranteed by the existing eye tracking solutions with CCD/CMOS cameras. To bridge the gap, in this paper, we propose Swift-Eye, an offline precise and robust pupil estimation and tracking framework to support high-frequency near-eye movement analysis, especially when the pupil region is partially occluded. Swift-Eye is built upon the emerging event cameras to capture the high-speed movement of eyes in high temporal resolution. Then, a series of bespoke components are designed to generate high-quality near-eye movement video at a high frame rate over kilohertz and deal with the occlusion over the pupil caused by involuntary eye blinks. According to our extensive evaluations on EV-Eye, a large-scale public dataset for eye tracking using event cameras, Swift-Eye shows high robustness against significant occlusion. It can improve the IoU and F1-score of the pupil estimation by 20% and 12.5% respectively, compared with the second-best competing approach, when over 80% of the pupil region is occluded by the eyelid. Lastly, it provides continuous and smooth traces of pupils in extremely high temporal resolution and can support high-frequency eye movement analysis and a number of potential applications, such as mental health diagnosis, behaviour-brain association, etc. The implementation details and source codes can be found at https://github.com/ztysdu/Swift-Eye.  © 1995-2012 IEEE.",event camera; Eye tracking; feature fusion,Abstract,True,
Scopus,journalPaper,2024,Research Trends in Virtual Reality Music Concert Technology: A Systematic Literature Review,TVCG - Transactions on Visualization and Computer Graphics,A,"Advances in virtual reality (VR) technology have sparked novel avenues of growth in the musical domain. Following the COVID-19 pandemic, the rise of VR technology has led to growing interest in VR music concerts as an alternative to traditional live concerts. These virtual settings can provide immersion like attending real concerts for physically distant audiences and performers, and also can offer new creative possibilities. VR music concert research is still in its infancy, and advances in technologies such as multimodal devices are rapidly expanding the diversity of research, requiring a unified understanding of the field. To identify trends in VR music concert technology, we conducted a PRISMA-based systematic literature review covering the period from 2018 to 2023. After a thorough screening process, a total of 27 papers were selected for review. The studies were classified and analyzed based on the research topic (audience, performer, concert venue), interaction type (user-environment, user-user), and hardware used (head-mounted display, additional hardware). Furthermore, we categorized the evaluation metrics into user experience, usability, and performance. Our review contributes to advancing the understanding of recent developments in VR music concert technology, shedding light on the diversification and potential of this emerging field. © 1995-2012 IEEE.",Evaluation Metric; Interaction; Music Concert; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Designing and Evaluating a VR Lobby for a Socially Enriching Remote Opera Watching Experience,TVCG - Transactions on Visualization and Computer Graphics,A,"The latest social VR technologies have enabled users to attend traditional media and arts performances together while being geographically removed, making such experiences accessible despite budget, distance, and other restrictions. In this work, we aim at improving the way remote performances are shared by designing and evaluating a VR theatre lobby which serves as a space for users to gather, interact, and relive the common experience of watching a virtual opera. We conducted an initial test with experts ($\mathrm{N}=10$, i.e., designers and opera enthusiasts) in pairs using our VR lobby prototype, developed based on the theoretical lobby design concept. A unique aspect of our experience is its highly realistic representation of users in the virtual space. The test results guided refinements to the VR lobby structure and implementation, aiming to improve the user experience and align it more closely with the social VR lobby's intended purpose. With the enhanced prototype, we ran a between-subject controlled study ($\mathrm{N}=40$) to compare the user experience in the social VR lobby between individuals and paired participants. To do so, we designed and validated a questionnaire to measure the user experience in the VR lobby. Results of our mixed-methods analysis, including interviews, questionnaire results, and user behavior, reveal the strength of our social VR lobby in connecting with other users, consuming the opera in a deeper manner, and exploring new possibilities beyond what is common in real life. All supplemental materials are available at https://github.com/cwi-dis/IEEEVR2024-VRLobby.  © 1995-2012 IEEE.",Collaborative interaction; Empirical studies in HCI; Perfoming arts; User studies; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,Embodying a self-avatar with a larger leg: its impacts on motor control and dynamic stability,TVCG - Transactions on Visualization and Computer Graphics,A,"Several studies have shown that users of immersive virtual reality can feel high levels of embodiment in self-avatars that have different morphological proportions than those of their actual bodies. Deformed and unrealistic morphological modifications are accepted by embodied users, underlying the adaptability of one's mental map of their body (body schema) in response to incoming sensory feedback. Before initiating a motor action, the brain uses the body schema to plan and sequence the necessary movements. Therefore, embodiment in a self-avatar with a different morphology, such as one with deformed proportions, could lead to changes in motor planning and execution. In this study, we aimed to measure the effects on movement planning and execution of embodying a self-avatar with an enlarged lower leg on one side. Thirty participants embodied an avatar without any deformations, and with an enlarged dominant or non-dominant leg, in randomized order. Two different levels of embodiment were induced, using synchronous or asynchronous visuotactile stimuli. In each condition, participants performed a gait initiation task. Their center of mass and center of pressure were measured, and the margin of stability (MoS) was computed from these values. Their perceived level of embodiment was also measured, using a validated questionnaire. Results show no significant changes on the biomechenical variables related to dynamic stability. Embodiment scores decreased with asynchronous stimuli, without impacting the measures related to stability. The body schema may not have been impacted by the larger virtual leg. However, deforming the self-avatar's morphology could have important implications when addressing individuals with impaired physical mobility by subtly influencing action execution during a rehabilitation protocol. © 1995-2012 IEEE.","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, Augmented, and Virtual Realities; H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/Methodology",Abstract,True,
Scopus,journalPaper,2024,Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual Reality: Robustness and User Experience,TVCG - Transactions on Visualization and Computer Graphics,A,"Eye tracking is routinely being incorporated into virtual reality (VR) systems. Prior research has shown that eye tracking data, if exposed, can be used for re-identification attacks [14]. The state of our knowledge about currently existing privacy mechanisms is limited to privacy-utility trade-off curves based on data-centric metrics of utility, such as prediction error, and black-box threat models. We propose that for interactive VR applications, it is essential to consider user-centric notions of utility and a variety of threat models. We develop a methodology to evaluate real-time privacy mechanisms for interactive VR applications that incorporate subjective user experience and task performance metrics. We evaluate selected privacy mechanisms using this methodology and find that re-identification accuracy can be decreased to as low as 14% while maintaining a high usability score and reasonable task performance. Finally, we elucidate three threat scenarios (black-box, black-box with exemplars, and white-box) and assess how well the different privacy mechanisms hold up to these adversarial scenarios. This work advances the state of the art in VR privacy by providing a methodology for end-to-end assessment of the risk of re-identification attacks and potential mitigating solutions. f  © 1995-2012 IEEE.",eye tracking; privacy; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Exploring Audio Interfaces for Vertical Guidance in Augmented Reality via Hand-Based Feedback,TVCG - Transactions on Visualization and Computer Graphics,A,"This research proposes an evaluation of pitch-based sonification methods via user experiments in real-life scenarios, specifically vertical guidance, with the aim of standardizing the use of audio interfaces in AR in guidance tasks. Using literature on assistive technology for people who are blind or visually impaired, we aim to generalize their applicability to a broader population and for different use cases. We propose and test sonification methods for vertical guidance in a series of hand-navigation assessments with users without visual feedback. Including feedback from a visually impaired expert in digital accessibility, results (N=19) outlined that methods that do not rely on memorizing pitch had the most promising accuracy and self-reported workload performances. Ultimately, we argue for audio AR's ability to enhance user performance in different scenarios, from video games to finding objects in a pantry. © 1995-2012 IEEE.",assistive technologies; Audio interfaces; augmented and mixed reality,Title_Keywords,True,
Scopus,journalPaper,2024,Exploring Bimanual Haptic Feedback for Spatial Search in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Spatial search tasks are common and crucial in many Virtual Reality (VR) applications. Traditional methods to enhance the performance of spatial search often employ sensory cues such as visual, auditory, or haptic feedback. However, the design and use of bimanual haptic feedback with two VR controllers for spatial search in VR remains largely unexplored. In this work, we explored bimanual haptic feedback with various combinations of haptic properties, where four types of bimanual haptic feedback were designed, for spatial search tasks in VR. Two experiments were designed to evaluate the effectiveness of bimanual haptic feedback on spatial direction guidance and search in VR. The results from the first experiment reveal that our proposed bimanual haptic schemes significantly enhanced the recognition of spatial directions in terms of accuracy and speed compared to spatial audio feedback. The second experiment's findings suggest that the performance of bimanual haptic feedback was comparable to or even better than the visual arrow, especially in reducing the angle of head movement and enhancing searching targets behind the participants, which was supported by subjective feedback as well. Based on these findings, we have derived a set of design recommendations for spatial search using bimanual haptic feedback in VR.  © 1995-2012 IEEE.",Bimanual Haptic Feedback; Controllers; Spatial Search; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,PetPresence: Investigating the Integration of Real-World Pet Activities in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"For VR interaction, the home environment with complicated spatial setup and dynamics may hinder the VR user experience. In particular, pets' movement may be more unpredictable. In this paper, we investigate the integration of real-world pet activities into immersive VR interaction. Our pilot study showed that the active pet movements, especially dogs, could negatively impact users' performance and experience in immersive VR. We proposed three different types of pet integration, namely semitransparent real-world portal, non-interactive object in VR, and interactive object in VR. We conducted the user study with 16 pet owners and their pets. The results showed that compared to the baseline condition without any pet-integration technique, the approach of integrating the pet as interactive objects in VR yielded significantly higher participant ratings in perceived realism, joy, multisensory engagement, and connection with their pets in VR. © 1995-2012 IEEE.",Distractions; Haptics; Pet; Presence; Virtual Reality,Title_Keywords,True,
Scopus,journalPaper,2024,Spatial Contraction Based on Velocity Variation for Natural Walking in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) offers an immersive 3D digital environment, but enabling natural walking sensations without the constraints of physical space remains a technological challenge. Previous VR locomotion methods, including game controller, teleportation, treadmills, walking-in-place, and redirected walking (RDW), have made strides towards overcoming this challenge. However, these methods also face limitations such as possible unnaturalness, additional hardware requirements, or motion sickness risks. This paper introduces Spatial Contraction (SC), an innovative VR locomotion method inspired by the phenomenon of Lorentz contraction in Special Relativity. Similar to the Lorentz contraction, our SC contracts the virtual space along the user's velocity direction in response to velocity variation. The virtual space contracts more when the user's speed is high, whereas minimal or no contraction happens at low speeds. We provide a virtual space transformation method for spatial contraction and optimize the user experience in smoothness and stability. Through SC, VR users can effectively traverse a longer virtual distance with a shorter physical walking. Different from locomotion gains, the spatial contraction effect is observable by the user and aligns with their intentions, so there is no inconsistency between the user's proprioception and visual perception. SC is a general locomotion method that has no special requirements for VR scenes. The experimental results of our live user studies in various virtual scenarios demonstrate that SC has a significant effect in reducing both the number of resets and the physical walking distance users need to cover. Furthermore, experiments have also demonstrated that SC has the potential for integration with existing locomotion techniques such as RDW. © 1995-2012 IEEE.",locomotion; redirected walking; spatial contraction; velocity; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Learning Middle-Latitude Cyclone Formation up in the Air: Student Learning Experience, Outcomes, and Perceptions in a CAVE-Enabled Meteorology Class",TVCG - Transactions on Visualization and Computer Graphics,A,"Cave Automatic Virtual Environment (CAVE) is a virtual reality (VR) environment that has not been fully studied due to its high cost and complexity in system integration. Previous CAVE-related studies mainly focused on comparing its effectiveness with other learning media, such as textbooks, desktop VR, or head-mounted display (HMD) VR. In this study, through the utilization of CAVE in a meteorology class, we concentrated on CAVE itself, measured how CAVE impacted learners' learning outcomes before and after using CAVE in an actual ongoing undergraduate-level class, and investigated how learners perceived their learning experiences. Quantitative data were collected to examine the students' knowledge acquisition and learning experience. We also triangulated the quantitative results with qualitative data from the interviews regarding learners' perceptions of the CAVE-enabled class and their knowledge mastery. The results indicated that their learning outcomes increased through learning with CAVE and that their perceptions of immersion, presence, and engagement significantly correlated with each other. The interview results showed a great fondness of and satisfaction with the learning experience, group collaboration, and effectiveness of the CAVE-enabled class from the learners. We also learned that the learners' learning experiences in CAVE could be further improved if we provided them with more learner-environment interaction, offered them a better sense of immersion, and reduced cybersickness. Implications of these findings are discussed. © 1995-2012 IEEE.",CAVE; learning expereince; learning outcomes; meteorology; student perceptions,Abstract,True,
Scopus,journalPaper,2024,Empowering Persons with Autism Through Cross-Reality and Conversational Agents,TVCG - Transactions on Visualization and Computer Graphics,A,"Autism Spectrum Disorder is a neurodevelopmental condition that can affect autonomy and independence. Our research explores the integration of Cross-Reality and Conversational Agents for Autistic persons to improve ability and confidence in everyday life situations. We combine two technologies of the Virtual-Real continuum. User experiences unfold from the simulation of tasks in VR to the execution of similar tasks supported by AR in the real world. A speech-based Conversational Agent is integrated with both VR and AR. It provides contextualized help, promotes generalization, and stimulates users to apply what they learned in the virtual space. The paper presents the approach and describes an empirical study involving 17 young Autistic persons. © 1995-2012 IEEE.",Accessibility technologies; Augmented reality; Conversational agents; Human computer interaction; Interactive learning environments; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,Expressive Talking Avatars,TVCG - Transactions on Visualization and Computer Graphics,A,"Stylized avatars are common virtual representations used in VR to support interaction and communication between remote collaborators. However, explicit expressions are notoriously difficult to create, mainly because most current methods rely on geometric markers and features modeled for human faces, not stylized avatar faces. To cope with the challenge of emotional and expressive generating talking avatars, we build the Emotional Talking Avatar Dataset which is a talking-face video corpus featuring 6 different stylized characters talking with 7 different emotions. Together with the dataset, we also release an emotional talking avatar generation method which enables the manipulation of emotion. We validated the effectiveness of our dataset and our method in generating audio based puppetry examples, including comparisons to state-of-the-art techniques and a user study. Finally, various applications of this method are discussed in the context of animating avatars in VR. © 1995-2012 IEEE.",Computer graphics; Graphics systems and interfaces; HCI design and evaluation methods; Human computer interaction (HCI); Human-centered computing; User studies; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,"In Case of Doubt, One Follows One's Self: The Implicit Guidance of the Embodied Self-Avatar",TVCG - Transactions on Visualization and Computer Graphics,A,"The sense of embodiment in virtual reality (VR) is commonly understood as the subjective experience that one's physical body is substituted by a virtual counterpart, and is typically achieved when the avatar's body, seen from a first-person view, moves like one's physical body. Embodiment can also be experienced in other circumstances (e.g., in third-person view) or with imprecise or distorted visuo-motor coupling. It was moreover observed, in various cases of small or progressive temporal and spatial manipulations of avatars' movements, that participants may spontaneously follow the movement shown by the avatar. The present work investigates whether, in some specific contexts, participants would follow what their avatar does even when large movement discrepancies occur, thereby extending the scope of understanding of the self-avatar follower effect beyond subtle changes of motion or speed manipulations. We conducted an experimental study in which we introduced uncertainty about which movement to perform at specific times and analyzed participants' movements and subjective feedback after their avatar showed them an incorrect movement. Results show that, when in doubt, participants were influenced by their avatar's movements, leading them to perform that particular error twice more often than normal. Importantly, results of the embodiment score indicate that participants experienced a dissociation with their avatar at those times. Overall, these observations not only demonstrate the possibility of provoking situations in which participants follow the guidance of their avatar for large motor distortions, despite their awareness about the avatar movement disruption and on the possible influence it had on their choice, and, importantly, exemplify how the cognitive mechanism of embodiment is deeply rooted in the necessity of having a body. © 1995-2012 IEEE.",self-avatar follower effect; sense of agency; sense of body ownership; virtual embodiment; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Stepping into the Right Shoes: The Effects of User-Matched Avatar Ethnicity and Gender on Sense of Embodiment in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"In many consumer virtual reality (VR) applications, users embody predefined characters that offer minimal customization options, frequently emphasizing storytelling over user choice. We explore whether matching a user's physical characteristics, specifically ethnicity and gender, with their virtual self-avatar affects their sense of embodiment in VR. We conducted a $2\times 2$ within-subjects experiment ($\mathrm{n}=32$) with a diverse user population to explore the impact of matching or not matching a user's self-avatar to their ethnicity and gender on their sense of embodiment. Our results indicate that matching the ethnicity of the user and their self-avatar significantly enhances sense of embodiment regardless of gender, extending across various aspects, including appearance, response, and ownership. We also found that matching gender significantly enhanced ownership, suggesting that this aspect is influenced by matching both ethnicity and gender. Interestingly, we found that matching ethnicity specifically affects self-location while matching gender specifically affects one's body ownership.  © 1995-2012 IEEE.",avatars; diversity; sense of embodiment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Revisiting Walking-in-Place by Introducing Step-Height Control, Elastic Input, and Pseudo-Haptic Feedback",TVCG - Transactions on Visualization and Computer Graphics,A,"Walking-in-place (WIP) is a locomotion technique that enables users to 'walk infinitely' through vast virtual environments using walking-like gestures within a limited physical space. This article investigates alternative interaction schemes for WIP, addressing successively the control, input, and output of WIP. First, we introduce a novel height-based control to increase advanced speed. Second, we introduce a novel input system for WIP based on elastic and passive strips. Third, we introduce the use of pseudo-haptic feedback as a novel output for WIP meant to alter walking sensations. The results of a series of user studies show that height and frequency based control of WIP can facilitate higher virtual speed with greater efficacy and ease than in frequency-based WIP. Second, using an upward elastic input system can result in a stable virtual speed control, although excessively strong elastic forces may impact the usability and user experience. Finally, using a pseudo-haptic approach can improve the perceived realism of virtual slopes. Taken together, our results suggest that, for future VR applications, there is value in further research into the use of alternative interaction schemes for walking-in-place. © 1995-2012 IEEE.",elastic input; locomotion; passive haptics; pseudo-haptics; virtual reality; Walking-in-place,Keywords,True,
Scopus,journalPaper,2024,BiRD: Using Bidirectional Rotation Gain Differences to Redirect Users during Back-and-forth Head Turns in Walking,TVCG - Transactions on Visualization and Computer Graphics,A,"Redirected walking (RDW) facilitates user navigation within expansive virtual spaces despite the constraints of limited physical spaces. It employs discrepancies between human visual-proprioceptive sensations, known as gains, to enable the remapping of virtual and physical environments. In this paper, we explore how to apply rotation gain while the user is walking. We propose to apply a rotation gain to let the user rotate by a different angle when reciprocating from a previous head rotation, to achieve the aim of steering the user to a desired direction. To apply the gains imperceptibly based on such a Bidirectional Rotation gain Difference (BiRD), we conduct both measurement and verification experiments on the detection thresholds of the rotation gain for reciprocating head rotations during walking. Unlike previous rotation gains which are measured when users are turning around in place (standing or sitting), BiRD is measured during users' walking. Our study offers a critical assessment of the acceptable range of rotational mapping differences for different rotational orientations across the user's walking experience, contributing to an effective tool for redirecting users in virtual environments. © 1995-2012 IEEE.",detection thresholds; Redirected walking; rotation gain; simulator sickness; virtual reality,Keywords,True,
Scopus,journalPaper,2024,Investigating Personalization Techniques for Improved Cybersickness Prediction in Virtual Reality Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"In recent cybersickness research, there has been a growing interest in predicting cybersickness using real-time physiological data such as heart rate, galvanic skin response, eye tracking, postural sway, and electroencephalogram. However, the impact of individual factors such as age and gender, which are pivotal in determining cybersickness susceptibility, remains unknown in predictive models. Our research seeks to address this gap, underscoring the necessity for a more personalized approach to cybersickness prediction to ensure a better, more inclusive virtual reality experience. We hypothesize that a personalized cybersickness prediction model would outperform non-personalized models in predicting cybersickness. Evaluating this, we explored four personalization techniques: 1) data grouping, 2) transfer learning, 3) early shaping, and 4) sample weighing using an open-source cybersickness dataset. Our empirical results indicate that personalized models significantly improve prediction accuracy. For instance, with early shaping, the Deep Temporal Convolutional Neural Network (DeepTCN) model achieved a 69.7% reduction in RMSE compared to its non-personalized version. Our study provides evidence of personalization techniques' benefits in improving cybersickness prediction. These findings have implications for developing personalized cybersickness prediction models tailored to individual differences, which can be used to develop personalized cybersickness reduction techniques in the future. © 1995-2012 IEEE.",Cybersickness; Cybersickness Personalization; Cybersickness Prediction; Deep Learning; Early Shaping; Machine Learning; Transfer Learning,Title_Abstract,True,
Scopus,journalPaper,2024,CAEVR: Biosignals-Driven Context-Aware Empathy in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"There is little research on how Virtual Reality (VR) applications can identify and respond meaningfully to users' emotional changes. In this paper, we investigate the impact of Context-Aware Empathic VR (CAEVR) on the emotional and cognitive aspects of user experience in VR. We developed a real-time emotion prediction model using electroencephalography (EEG), electrodermal activity (EDA), and heart rate variability (HRV) and used this in personalized and generalized models for emotion recognition. We then explored the application of this model in a context-aware empathic (CAE) virtual agent and an emotion-adaptive (EA) VR environment. We found a significant increase in positive emotions, cognitive load, and empathy toward the CAE agent, suggesting the potential of CAEVR environments to refine user-agent interactions. We identify lessons learned from this study and directions for future work. © 1995-2012 IEEE.",context-aware; emotion; empathy; metaverse; physiology; virtual agents; VR,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Evaluating Text Reading Speed in VR Scenes and 3D Particle Visualizations,TVCG - Transactions on Visualization and Computer Graphics,A,"This work reports how text size and other rendering conditions affect reading speeds in a virtual reality environment and a scientific data analysis application. Displaying text legibly yet space-efficiently is a challenging problem in immersive displays. Effective text displays that enable users to read at their maximum speed must consider the variety of virtual reality (VR) display hardware and possible visual exploration tasks. We investigate how text size and display parameters affect reading speed and legibility in three state-of-the-art VR displays: two head-mounted displays and one CAVE. In our perception experiments, we establish limits where reading speed declines as the text size approaches the so-called critical print sizes (CPS) of individual displays, which can inform the design of uniform reading experiences across different VR systems. We observe an inverse correlation between display resolution and CPS. Yet, even in high-fidelity VR systems, the measured CPS was larger than in comparable physical text displays, highlighting the value of increased VR display resolutions in certain visualization scenarios. Our findings indicate that CPS can be an effective metric for evaluating VR display usability. Additionally, we evaluate the effects of text panel placement, orientation, and occlusion-reducing rendering methods on reading speeds in generic volumetric particle visualizations. Our study provides insights into the trade-off between text representation and legibility in cluttered immersive environments with specific suggestions for visualization designers and highlight areas for further research. © 1995-2012 IEEE.",Human-Computer Interaction; Perception; Scientific Visualization; Text Representation; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Low-Latency Ocular Parallax Rendering and Investigation of Its Effect on Depth Perception in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"With a demand for an immersive experience in virtual/augmented reality (VR/AR) displays, recent efforts have incorporated eye states, such as focus and fixation, into display graphics. Among these, ocular parallax, a small parallax generated by eye rotation, has received considerable attention for its impact on depth perception. However, the substantial latency of head-mounted displays (HMDs) has made it challenging to accurately assess its true effect during free eye movements. To address this issue, we propose a high-speed (360 Hz) and low-latency (4.8 ms) ocular parallax rendering system with a custom-built eye tracker. Using this proposed system, we conducted an investigation to determine the latency requirements necessary for achieving perceptually stable ocular parallax rendering. Our findings indicate that, in binocular viewing, ocular parallax rendering is perceived as significantly less stable than conventional rendering when the latency exceeds 43.72 ms at 1.3 D and 21.50 ms at 2.0 D. We also evaluated the effects of ocular parallax rendering on binocular fusion and monocular depth perception under free viewing conditions. The results demonstrated that ocular parallax rendering can enhance binocular fusion but has a limited impact on depth perception under monocular viewing conditions when latency is minimized. © 1995-2012 IEEE.",binocular fusion; depth perception; eye's front-nodal-point tracking; low-latency feedback system; Ocular parallax,Title_Abstract,True,
Scopus,journalPaper,2024,"Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million Motion Capture Recordings from 105,000 XR Users",TVCG - Transactions on Visualization and Computer Graphics,A,"Extended reality (XR) devices such as the Meta Quest and Apple Vision Pro have seen a recent surge in attention, with motion tracking atelemetrya data lying at the core of nearly all XR and metaverse experiences. Researchers are just beginning to understand the implications of this data for security, privacy, usability, and more, but currently lack large-scale human motion datasets to study. The BOXRR-23 dataset contains 4,717,215 motion capture recordings, voluntarily submitted by 105,852 XR device users from over 50 countries. BOXRR-23 is over 200 times larger than the largest existing motion capture research dataset and uses a new, highly efficient and purpose-built XR Open Recording (XROR) file format. © 1995-2012 IEEE.",big data; Dataset; extended reality; motion capture; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,PreVR: Variable-Distance Previews for Higher-Order Disocclusion in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"The paper introduces PreVR, a method for allowing the user of a VR application to preview a virtual environment (VE) around any number of corners. This way the user can gain line of sight to any part of the VE, no matter how distant or how heavily occluded it is. PreVR relies on a multiperspective visualization that implements a higher-order disocclusion effect with piecewise linear rays that bend multiple times as needed to reach the visualization target. PreVR was evaluated in a user study (N = 88) that investigates four points on the VR interface design continuum defined by the maximum disocclusion order d. In a first control condition (CC0), d = 0, corresponds to conventional VR exploration with no preview capability. In a second control condition (CC1), d = 1, corresponds to the prior art approach of giving the user a preview around the first corner. In a first experimental condition (EC3), d = 3, so PreVR provided up to third-order disocclusion. In a second experimental condition (ECN), d was not capped, so PreVR could provide a disocclusion effect of any order, as needed to reach any location in the VE. Participants searched for a stationary target, for a dynamic target moving on a random continuous trajectory, and for a transient dynamic target that appeared at random locations in the maze and disappeared 5s later. The study quantified VE exploration efficiency with four metrics: viewpoint translation, view direction rotation, number of teleportations, and task completion time. Results show that the previews afforded by PreVR bring a significant VE exploration efficiency advantage. ECN outperforms EC3, CC1, and CC0 for all metrics and all tasks, and EC3 frequently outperforms CC1 and CC0. © 1995-2012 IEEE.",Disocclusion; navigation; virtual reality; visualization,Keywords,True,
Scopus,journalPaper,2024,Redirection Strategy Switching: Selective Redirection Controller for Dynamic Environment Adaptation,TVCG - Transactions on Visualization and Computer Graphics,A,"In this paper, we present the Selective Redirection Controller (SRC), which selects the optimal redirection controller based on the physical and virtual environment in Redirected Walking (RDW). The primary advantage of SRC over existing controllers is its dynamic switching among four different redirection controllers (S2C, TAPF, ARC, and SRL) based on the user's environment, as opposed to using a single fixed controller throughout the experience. By switching between redirection controllers based on the context around the user, SRC aims to optimize the advantages of each redirection strategy. The SRC model is trained using reinforcement learning to dynamically and instantaneously switch redirection controllers based on the user's environment. We evaluated the performance of SRC against traditional redirection controllers through simulations and user studies conducted in various physical and virtual environments. The findings indicate that SRC reduces the number of resets significantly compared to traditional redirection controllers. Heat map visualization was utilized during the development process to analyze which redirection controller SRC chooses based on the different environments around the user. SRC alternates between redirection techniques based on the user's environment, maximizing the advantages of each strategy for a superior RDW experience. © 1995-2012 IEEE.",Redirected walking; Reinforcement learning; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,IntenSelect+: Enhancing Score-Based Selection in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Object selection in virtual environments is one of the most common and recurring interaction tasks. Therefore, the used technique can critically influence a system's overall efficiency and usability. IntenSelect is a scoring-based selection-by-volume technique that was shown to offer improved selection performance over conventional raycasting in virtual reality. This initial method, however, is most pronounced for small spherical objects that converge to a point-like appearance only, is challenging to parameterize, and has inherent limitations in terms of flexibility. We present an enhanced version of IntenSelect called IntenSelect+ designed to overcome multiple shortcomings of the original IntenSelect approach. In an empirical within-subjects user study with 42 participants, we compared IntenSelect+ to IntenSelect and conventional raycasting on various complex object configurations motivated by prior work. In addition to replicating the previously shown benefits of IntenSelect over raycasting, our results demonstrate significant advantages of IntenSelect+ over IntenSelect regarding selection performance, task load, and user experience. We, therefore, conclude that IntenSelect+ is a promising enhancement of the original approach that enables faster, more precise, and more comfortable object selection in immersive virtual environments.  © 1995-2012 IEEE.",3D Interaction; 3D User Interfaces; IntenSelect; Score-Based Selection; Selection; Temporal Selection; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The Effects of Secondary Task Demands on Cybersickness in Active Exploration Virtual Reality Experiences,TVCG - Transactions on Visualization and Computer Graphics,A,"Active exploration in virtual reality (VR) involves users navigating immersive virtual environments, going from one place to another. While navigating, users often engage in secondary tasks that require attentional resources, as in the case of distracted driving. Inspired by research generally studying the effects of task demands on cybersickness (CS), we investigated how the attentional demands specifically associated with secondary tasks performed during exploration affect CS. Downstream of this, we studied how increased attentional demands from secondary tasks affect spatial memory and navigational performance. We discuss the results of a multi-factorial between-subjects study, manipulating a secondary task's demand across two levels and studying its effects on CS in two different sickness-inducing levels of an exploration experience. The secondary task's demand was manipulated by parametrically varying n in an aural n-back working memory task and the provocativeness of the experience was manipulated by varying how frequently users experienced a yaw-rotational reorientation effect during the exploration. Results revealed that increases in the secondary task's demand increased sickness levels, also resulting in a higher temporal onset rate, especially when the experience was not already highly sickening. Increased attentional demand from the secondary task also vitiated navigational performance and spatial memory. Overall, increased demands from secondary tasks performed during navigation produce deleterious effects on the VR experience. © 1995-2012 IEEE.",Active Exploration; Cybersickness; Electrodermal Activity; Secondary Task Demand; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Robust Dual-Modal Speech Keyword Spotting for XR Headsets,TVCG - Transactions on Visualization and Computer Graphics,A,"While speech interaction finds widespread utility within the Extended Reality (XR) domain, conventional vocal speech keyword spotting systems continue to grapple with formidable challenges, including suboptimal performance in noisy environments, impracticality in situations requiring silence, and susceptibility to inadvertent activations when others speak nearby. These challenges, however, can potentially be surmounted through the cost-effective fusion of voice and lip movement information. Consequently, we propose a novel vocal-echoic dual-modal keyword spotting system designed for XR headsets. We devise two different modal fusion approches and conduct experiments to test the system's performance across diverse scenarios. The results show that our dual-modal system not only consistently outperforms its single-modal counterparts, demonstrating higher precision in both typical and noisy environments, but also excels in accurately identifying silent utterances. Furthermore, we have successfully applied the system in real-time demonstrations, achieving promising results. The code is available at https://github.com/caizhuojiang/VE-KWS. © 1995-2012 IEEE.",extended reality; keyword spotting; multimodal interaction; Speech interaction,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Exploring the Influence of Virtual Avatar Heads in Mixed Reality on Social Presence, Performance and User Experience in Collaborative Tasks",TVCG - Transactions on Visualization and Computer Graphics,A,"In Mixed Reality (MR), users' heads are largely (if not completely) occluded by the MR Head-Mounted Display (HMD) they are wearing. As a consequence, one cannot see their facial expressions and other communication cues when interacting locally. In this paper, we investigate how displaying virtual avatars' heads on-top of the (HMD-occluded) heads of participants in a Video See-Through (VST) Mixed Reality local collaborative task could improve their collaboration as well as social presence. We hypothesized that virtual heads would convey more communicative cues (such as eye direction or facial expressions) hidden by the MR HMDs and lead to better collaboration and social presence. To do so, we conducted a between-subject study ($\mathrm{n}=88$) with two independent variables: the type of avatar (CartoonAvatar/RealisticAvatar/NoAvatar) and the level of facial expressions provided (HighExpr/LowExpr). The experiment involved two dyadic communication tasks: (i) the 20-question game where one participant asks questions to guess a hidden word known by the other participant and (ii) a urban planning problem where participants have to solve a puzzle by collaborating. Each pair of participants performed both tasks using a specific type of avatar and facial animation. Our results indicate that while adding an avatar's head does not necessarily improve social presence, the amount of facial expressions provided through the social interaction does have an impact. Moreover, participants rated their performance higher when observing a realistic avatar but rated the cartoon avatars as less uncanny. Taken together, our results contribute to a better understanding of the role of partial avatars in local MR collaboration and pave the way for further research exploring collaboration in different scenarios, with different avatar types or MR setups. © 1995-2012 IEEE.",Avatar Representation; Collaboration; Mixed Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Visual Cue Based Corrective Feedback for Motor Skill Training in Mixed Reality: A Survey,TVCG - Transactions on Visualization and Computer Graphics,A,"When learning a motor skill it is helpful to get corrective feedback from an instructor. This will support the learner to execute the movement correctly. With modern technology, it is possible to provide this feedback via mixed reality. In most cases, this involves visual cues to help the user understand the corrective feedback. We analyzed recent research approaches utilizing visual cues for feedback in mixed reality. The scope of this article is visual feedback for motor skill learning, which involves physical therapy, exercise, rehabilitation etc. While some of the surveyed literature discusses therapeutic effects of the training, this article focuses on visualization techniques. We categorized the literature from a visualization standpoint, including visual cues, technology and characteristics of the feedback. This provided insights into how visual feedback in mixed reality is applied in the literature and how different aspects of the feedback are related. The insights obtained can help to better adjust future feedback systems to the target group and their needs. This article also provides a deeper understanding of the characteristics of the visual cues in general and promotes future, more detailed research on this topic. © 1995-2012 IEEE.",Human-centered computing; interaction techniques; virtual and augmented reality; visualization; visualization techniques and methodologies,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Task and Environment-Aware Virtual Scene Rearrangement for Enhanced Safety in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Emerging VR applications have revolutionized user experiences by immersing individuals in digitally crafted environments. However, fully immersive experiences introduce new challenges, notably the risk of physical hazards when users are unaware of their surroundings. Existing solutions, including guardian spaces and locomotion systems, present trade-offs that either disrupt the immersive experience or risk inducing motion sickness. To address these challenges, we propose a novel approach that dynamically rearranges VR scenes according to users' physical spaces, seamlessly embedding physical constraints and interaction tasks into the virtual environment. We design a computational model to optimize the rearranged scene through a cost function, ensuring collision-free interactions while maintaining visual fidelity and the goal of interaction tasks. The experiments demonstrate improvements in user experience and safety, presenting an innovative solution to harmonize physical and virtual environments in VR applications.  © 1995-2012 IEEE.",scene rearrangement; scene synthesis; VR safety,Title,True,
Scopus,journalPaper,2024,'May I Speak?': Multi-Modal Attention Guidance in Social VR Group Conversations,TVCG - Transactions on Visualization and Computer Graphics,A,"In this paper, we present a novel multi-modal attention guidance method designed to address the challenges of turn-taking dynamics in meetings and enhance group conversations within virtual reality (VR) environments. Recognizing the difficulties posed by a confined field of view and the absence of detailed gesture tracking in VR, our proposed method aims to mitigate the challenges of noticing new speakers attempting to join the conversation. This approach tailors attention guidance, providing a nuanced experience for highly engaged participants while offering subtler cues for those less engaged, thereby enriching the overall meeting dynamics. Through group interview studies, we gathered insights to guide our design, resulting in a prototype that employs light as a diegetic guidance mechanism, complemented by spatial audio. The combination creates an intuitive and immersive meeting environment, effectively directing users' attention to new speakers. An evaluation study, comparing our method to state-of-the-art attention guidance approaches, demonstrated significantly faster response times ($p < 0.001$), heightened perceived conversation satisfaction ($p < 0.001$), and preference ($p < 0.001$) for our method. Our findings contribute to the understanding of design implications for VR social attention guidance, opening avenues for future research and development. © 1995-2012 IEEE.",Attention Guidance; Group Conversations; Multi-modal Interaction; Social VR; Turn-taking,Abstract,True,
Scopus,journalPaper,2024,Visuo-Haptic VR and AR Guidance for Dental Nerve Block Education,TVCG - Transactions on Visualization and Computer Graphics,A,"The inferior alveolar nerve block (IANB) is a dental anesthetic injection that is critical to the performance of many dental procedures. Dental students typically learn to administer an IANB through videos and practice on silicone molds and, in many dental schools, on other students. This causes significant stress for both the students and their early patients. To reduce discomfort and improve clinical outcomes, we created an anatomically informed virtual reality headset-based educational system for the IANB. It combines a layered 3D anatomical model, dynamic visual guidance for syringe position and orientation, and active force feedback to emulate syringe interaction with tissue. A companion mobile augmented reality application allows students to step through a visualization of the procedure on a phone or tablet. We conducted a user study to determine the advantages of preclinical training with our IANB simulator. We found that in comparison to dental students who were exposed only to traditional supplementary study materials, dental students who used our IANB simulator were more confident administering their first clinical injections, had less need for syringe readjustments, and had greater success in numbing patients. © 1995-2012 IEEE.",augmented reality; health care information systems; mixed reality; Virtual reality; visualization techniques,Abstract_Keywords,True,
Scopus,journalPaper,2024,Comparing Synchronous and Asynchronous Task Delivery in Mixed Reality Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Asynchronous digital communication is a widely applied and well-known form of information exchange. Most pieces of technology make use of some variation of asynchronous communication systems, be it messaging or email applications. This allows recipients to process digital messages immediately (synchronous) or whenever they have time (asynchronous), meaning that purely digital interruptions can be mitigated easily. Mixed Reality systems have the potential to not only handle digital interruptions but also interruptions in physical space, e.g., caused by co-workers in workspaces or learning environments. However, the benefits of such systems previously remained untested in the context of Mixed Reality. We conducted a user study (N=26) to investigate the impact that the timing of task delivery has on the participants' performance, workflow, and emotional state. Participants had to perform several cognitively demanding tasks in a Mixed Reality workspace. Inside the virtual workspace, we simulated in-person task delivery either during tasks (i.e., interrupting the participant) or between tasks (i.e., delaying the interruption). Our results show that delaying interruptions has a significant impact on subjective metrics like the perceived performance and workload. © 1995-2012 IEEE.",Evaluation; Interruptions; Mixed Reality; Task focus; Workspaces,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Corr-Track: Category-Level 6D Pose Tracking with Soft-Correspondence Matrix Estimation,TVCG - Transactions on Visualization and Computer Graphics,A,"Category-level pose tracking methods can continuously track the pose of objects without requiring any prior knowledge of the specific shape of the tracked instance. This makes them advantageous in augmented reality and virtual reality applications. The key challenge is how to train neural networks to accurately predict the poses of objects they have never seen before and exhibit strong generalization performance. We propose a novel category-level 6D pose tracking method Corr-Track, which is capable of accurately tracking objects belonging to the same category from depth video streams. Our approach utilizes direct soft correspondence constraints to train a neural network, which estimates bidirectional soft correspondences between sparsely sampled point clouds of objects in two frames. We first introduce a soft correspondence matrix for pose tracking tasks and establish effective constraints through direct spatial point-to-point correspondence representations in the sparse point cloud correspondence matrix. We propose the 'point cloud expansion' strategy to address the 'point cloud shrinkage' problem resulting from soft correspondences. This strategy ensures that the corresponding point cloud accurately reproduces the shape of the target point cloud, leading to precise pose tracking results. We evaluated our approach on the NOCS-REAL275 and Wild6D dataset and observed superior performance compared to previous methods. Additionally, we conducted cross-category experiments that further demonstrated its generalization capability. © 1995-2012 IEEE.",Category-level object pose estimation; object tracking,Abstract,True,
Scopus,journalPaper,2024,Towards Co-Operative Beaming Displays: Dual Steering Projectors for Extended Projection Volume and Head Orientation Range,TVCG - Transactions on Visualization and Computer Graphics,A,"Existing near-eye displays (NEDs) have trade-offs related to size, weight, computational resources, battery life, and body temperature. A recent paradigm, beaming display, addresses these trade-offs by separating the NED into a steering projector (SP) for image presentation and a passive headset worn by the user. However, the beaming display has issues with the projection area of a single SP and has severe limitations on the head orientation and pose that the user can move. In this study, we distribute dual steering projectors in the scene to extend the head orientation and pose of the beaming display by coordinating the dual projections on a passive headset. For cooperative control of each SP, we define a geometric model of the SPs and propose a calibration and projection control method designed for multiple projectors. We present implementations of the system along with evaluations showing that the precision and delay are 1.8 5.7 mm and 14.46 ms, respectively, at a distance of about 1 m from the SPs. From this result, our prototype with multiple SPs can project images in the projection area (formula Presented) of the passive headset while extending the projectable head orientation. Furthermore, as applications of cooperative control by multiple SPs, we show the possibility of multiple users, improving dynamic range and binocular presentation. © 1995-2012 IEEE.",Augmented reality; Near-eye display; Projectors,Keywords,True,
Scopus,journalPaper,2024,Eye-Hand Typing: Eye Gaze Assisted Finger Typing via Bayesian Processes in AR,TVCG - Transactions on Visualization and Computer Graphics,A,"Nowadays, AR HMDs are widely used in scenarios such as intelligent manufacturing and digital factories. In a factory environment, fast and accurate text input is crucial for operators' efficiency and task completion quality. However, the traditional AR keyboard may not meet this requirement, and the noisy environment is unsuitable for voice input. In this article, we introduce Eye-Hand Typing, an intelligent AR keyboard. We leverage the speed advantage of eye gaze and use a Bayesian process based on the information of gaze points to infer users' text input intentions. We improve the underlying keyboard algorithm without changing user input habits, thereby improving factory users' text input speed and accuracy. In real-time applications, when the user's gaze point is on the keyboard, the Bayesian process can predict the most likely characters, vocabulary, or commands that the user will input based on the position and duration of the gaze point and input history. The system can enlarge and highlight recommended text input options based on the predicted results, thereby improving user input efficiency. A user study showed that compared with the current HoloLens 2 system keyboard, Eye-Hand Typing could reduce input error rates by 28.31 % and improve text input speed by 14.5%. It also outperformed a gaze-only technique, being 43.05% more accurate and 39.55% faster. And it was no significant compromise in eye fatigue. Users also showed positive preferences. © 1995-2012 IEEE.",Augmented reality; bayesian process; eye-hand coordination; fitts' law; multi-modal interaction; primacy effect; text entry,Keywords,True,
Scopus,journalPaper,2024,Enhancing Tai Chi Training System: Towards Group-Based and Hyper-Realistic Training Experiences,TVCG - Transactions on Visualization and Computer Graphics,A,"In this article, we propose a lightweight and flexible enhanced Tai Chi training system composed of multiple standalone virtual reality (VR) devices. The system aims to enable a hyper-realistic multi-user action training platform at low cost by displaying real-time action guidance trajectories, providing real-world impossible visual effects and functions, and rapidly enhancing movement precision and communication interest for learners. We objectively evaluate participants' action quality at different levels of immersion, including traditional coach guidance (TCG), VR, and mixed reality (MR), along with subjective measures like motion sickness, quality of interaction, social meaning, presence/immersion to comprehensively explore the system's feasibility. The results indicate VR performs the best in training accuracy, but MR provides superior social experience and relatively high accuracy. Unlike TCG, MR offers hyper-realistic hand movement trajectories and Tai Chi social references. Compared with VR, MR provides more realistic avatar companions and a safer environment. In summary, MR balances accuracy and social experience. © 1995-2012 IEEE.",action guidance trajectories; hand movement trajectories analysis; hyper-realistic; social experience; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,100-Phones: A Large VI-SLAM Dataset for Augmented Reality Towards Mass Deployment on Mobile Phones,TVCG - Transactions on Visualization and Computer Graphics,A,"Visual-inertial SLAM (VI-SLAM) is a key technology for Augmented Reality (AR), which allows the AR device to recover its 6-DoF motion in real-time in order to render the virtual content with the corresponding pose. Nowadays, smartphones are still the mainstream devices for ordinary users to experience AR. However the current VI-SLAM methods, although performing well on high-end phones, still face robustness challenges when deployed on a larger stock of mid- and low-end phones. Existing VI-SLAM datasets use either very ideal sensors or only a limited number of devices for data collection, which cannot reflect the capability gaps that VI-SLAM methods need to solve when deployed on a large variety of phone models. This work proposes 100-Phones. the first VI-SLAM dataset covering a wide range of mainstream phones in the market. The dataset consists of 350 sequences collected by 100 different models of phones. Through analysis and experiments on the collected data, we conclude that the quality of visual-inertial data vary greatly among the mainstream phones, and the current open source VI-SLAM methods still have serious robustness issues when it comes to mass deployment on mobile phones. We release the dataset to facilitate the robustness improvement of VI-SLAM and to promote the mass popularization of AR. Project page: https://github.com/zju3dv/100-Phones.  © 1995-2012 IEEE.",Augmented Reality; Benchmark; Dataset; VI-SLAM,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Self-Guided DMT: Exploring a Novel Paradigm of Dance Movement Therapy in Mixed Reality for Children with ASD,TVCG - Transactions on Visualization and Computer Graphics,A,"Children diagnosed with Autism Spectrum Disorder (ASD) often exhibit motor disorders. Dance Movement Therapy (DMT) has shown great potential for improving the motor control ability of children with ASD. However, traditional DMT methods often lack vividness and are difficult to implement effectively. To address this issue, we propose a Mixed Reality DMT approach, utilizing interactive virtual agents. This approach offers immersive training content and multi-sensory feedback. To improve the training performance of children with ASD, we introduce a novel training paradigm featuring a self-guided mode. This paradigm enables the rapid creation of a virtual twin agent of the child with ASD using a single photo to embody oneself, which can then guide oneself during training. We conducted an experiment with the participation of 24 children diagnosed with ASD (or ASD propensity), recording their training performance under various experimental conditions. Through expert rating, behavior coding of training sessions, and statistical analysis, our findings revealed that the use of the twin agent for self-guidance resulted in noticeable improvements in the training performance of children with ASD. These improvements were particularly evident in terms of enhancing movement quality and refining overall target-related responses. Our study holds clinical potential in the field of medical treatment and rehabilitation for children with ASD. © 1995-2012 IEEE.",Autism spectrum disorder; dance movement therapy; mixed reality; self-guided; virtual agent,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,ViComp: Video Compensation for Projector-Camera Systems,TVCG - Transactions on Visualization and Computer Graphics,A,"Projector video compensation aims to cancel the geometric and photometric distortions caused by non-ideal projection surfaces and environments when projecting videos. Most existing projector compensation methods start by projecting and capturing a set of sampling images, followed by an offline compensation model training step. Thus, abundant user effort is required before the users can watch the video. Moreover, the sampling images have little prior knowledge of the video content and may lead to suboptimal results. To address these issues, this paper builds a video compensation system that can online adapt the compensation parameters. Our approach consists of five threads and can perform compensation, projection, capturing, and short-term and long-term model updates in parallel. Due to the parallel mechanism, rather than projecting and capturing hundreds of sampling images and training the model offline, we can directly use the projected and captured video frames for model updates on the fly. To quickly apply to the new environment, we introduce a deep learning-based compensation model that integrates a fixed transformer-based method and a novel CNN-based network. Moreover, for fast convergence and to reduce error accumulation during fine-tuning, we present a strategy that cooperates with short-term and long-term memory model updates. Experiments show that it significantly outperforms state-of-the-art baselines. © 1995-2012 IEEE.",Continuous projection mapping; Projector compensation; Projector-camera system; Spatial augmented reality; Video compensation,Keywords,True,
Scopus,journalPaper,2024,The Utilitarian Virtual Self - Using Embodied Personalized Avatars to Investigate Moral Decision-Making in Semi-Autonomous Vehicle Dilemmas,TVCG - Transactions on Visualization and Computer Graphics,A,"Embodied personalized avatars are a promising new tool to investigate moral decision-making by transposing the user into the 'middle of the action' in moral dilemmas. Here, we tested whether avatar personalization and motor control could impact moral decision-making, physiological reactions and reaction times, as well as embodiment, presence and avatar perception. Seventeen participants, who had their personalized avatars created in a previous study, took part in a range of incongruent (i.e., harmful action led to better overall outcomes) and congruent (i.e., harmful action led to trivial outcomes) moral dilemmas as the drivers of a semi-autonomous car. They embodied four different avatars (counterbalanced - personalized motor control, personalized no motor control, generic motor control, generic no motor control). Overall, participants took a utilitarian approach by performing harmful actions only to maximize outcomes. We found increased physiological arousal (SCRs and heart rate) for personalized avatars compared to generic avatars, and increased SCRs in motor control conditions compared to no motor control. Participants had slower reaction times when they had motor control over their avatars, possibly hinting at more elaborate decision-making processes. Presence was also higher in motor control compared to no motor control conditions. Embodiment ratings were higher for personalized avatars, and generally, personalization and motor control were perceptually positive features. These findings highlight the utility of personalized avatars and open up a range of future research possibilities that could benefit from the affordances of this technology and simulate, more closely than ever, real-life action. © 1995-2012 IEEE.",Human-centered computing-Empirical studies in HCI; Usability testing; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,VR.net: A Real-world Dataset for Virtual Reality Motion Sickness Research,TVCG - Transactions on Visualization and Computer Graphics,A,"Researchers have used machine learning approaches to identify motion sickness in VR experience. These approaches would certainly benefit from an accurately labeled, real-world, diverse dataset that enables the development of generalizable ML models. We introduce 'VR.net', a dataset comprising 165-hour gameplay videos from 100 real-world games spanning ten diverse genres, evaluated by 500 participants. VR.net accurately assigns 24 motion sickness-related labels for each video frame, such as camera/object movement, depth of field, and motion flow. Building such a dataset is challenging since manual labeling would require an infeasible amount of time. Instead, we implement a tool to automatically and precisely extract ground truth data from 3D engines' rendering pipelines without accessing VR games' source code. We illustrate the utility of VR.net through several applications, such as risk factor detection and sickness level prediction. We believe that the scale, accuracy, and diversity of VR.net can offer unparalleled opportunities for VR motion sickness research and beyond.We also provide access to our data collection tool, enabling researchers to contribute to the expansion of VR.net.  © 1995-2012 IEEE.",Machine Learning; Motion Sickness; Virtual Reality,Title_Keywords,True,
Scopus,journalPaper,2024,"Handwriting for Text Input and the Impact of XR Displays, Surface Alignments, and Sentence Complexities",TVCG - Transactions on Visualization and Computer Graphics,A,"Text input is desirable across various eXtended Reality (XR) use cases and is particularly crucial for knowledge and office work. This article compares handwriting text input between Virtual Reality (VR) and Video See-Through Augmented Reality (VST AR), facilitated by physically aligned and mid-air surfaces when writing simple and complex sentences. In a $2\times 2\times 2$ experimental design, 72 participants performed two ten-minute handwriting sessions, each including ten simple and ten complex sentences representing text input in real-world scenarios. Our developed handwriting application supports different XR displays, surface alignments, and handwriting recognition based on digital ink. We evaluated usability, user experience, task load, text input performance, and handwriting style. Our results indicate high usability with a successful transfer of handwriting skills to the virtual domain. XR displays and surface alignments did not impact text input speed and error rate. However, sentence complexities did, with participants achieving higher input speeds and fewer errors for simple sentences (17.85 WPM, 0.51% MSD ER) than complex sentences (15.07 WPM, 1.74% MSD ER). Handwriting on physically aligned surfaces showed higher learnability and lower physical demand, making them more suitable for prolonged handwriting sessions. Handwriting on mid-air surfaces yielded higher novelty and stimulation ratings, which might diminish with more experience. Surface alignments and sentence complexities significantly affected handwriting style, leading to enlarged and more connected cursive writing in both mid-air and for simple sentences. The study also demonstrated the benefits of using XR controllers in a pen-like posture to mimic styluses and pressure-sensitive tips on physical surfaces for input detection. We additionally provide a phrase set of simple and complex sentences as a basis for future text input studies, which can be expanded and adapted. © 1995-2012 IEEE.",AR; digital ink; digital twin; handwriting; mid-air; phrase set; physically aligned; recognition; text input; VR; XR,Abstract,True,
Scopus,journalPaper,2024,Measurement of Empathy in Virtual Reality with Head-Mounted Displays: A Systematic Review,TVCG - Transactions on Visualization and Computer Graphics,A,"We present a systematic review of 111 papers that measure the impact of virtual experiences created through head-mounted displays (HMDs) on empathy. Our goal was to analyze the conditions and the extent to which virtual reality (VR) enhances empathy. To achieve this, we categorized the relevant literature according to measurement methods, correlated human factors, viewing experiences, topics, and participants. Meta-analysis was performed based on categorized themes, and under specified conditions, we found that VR can improve empathy. Emotional empathy increased temporarily after the VR experience and returned to its original level over time, whereas cognitive empathy remained enhanced. Furthermore, while VR did not surpass 2D video in improving emotional empathy, it did enhance cognitive empathy, which is associated with embodiment. Our results are consistent with existing research suggesting differentiation between cognitive empathy (influenced by environmental factors and learnable) and emotional empathy (highly heritable and less variable). Interactivity, target of empathy, and point of view were not found to significantly affect empathy, but participants' age and nationality were found to influence empathy levels. It can be concluded that VR enhances cognitive empathy by immersing individuals in the perspective of others and that storytelling and personal characteristics are more important than the composition of the VR scene. Our findings provide guiding information for creating empathy content in VR and designing experiments to measure empathy. © 1995-2012 IEEE.",empathy; measurement; systematic review; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Modeling the Impact of Head-Body Rotations on Audio-Visual Spatial Perception for Virtual Reality Applications,TVCG - Transactions on Visualization and Computer Graphics,A,"Humans perceive the world by integrating multimodal sensory feedback, including visual and auditory stimuli, which holds true in virtual reality (VR) environments. Proper synchronization of these stimuli is crucial for perceiving a coherent and immersive VR experience. In this work, we focus on the interplay between audio and vision during localization tasks involving natural head-body rotations. We explore the impact of audio-visual offsets and rotation velocities on users' directional localization acuity for various viewing modes. Using psychometric functions, we model perceptual disparities between visual and auditory cues and determine offset detection thresholds. Our findings reveal that target localization accuracy is affected by perceptual audio-visual disparities during head-body rotations, but remains consistent in the absence of stimuli-head relative motion. We then showcase the effectiveness of our approach in predicting and enhancing users' localization accuracy within realistic VR gaming applications. To provide additional support for our findings, we implement a natural VR game wherein we apply a compensatory audio-visual offset derived from our measured psychometric functions. As a result, we demonstrate a substantial improvement of up to 40% in participants' target localization accuracy. We additionally provide guidelines for content creation to ensure coherent and seamless VR experiences.  © 1995-2012 IEEE.",Audio-Visual Spatial Perception; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"StainedSweeper: Compact, Variable-Intensity Light-Attenuation Display with Sweeping Tunable Retarders",TVCG - Transactions on Visualization and Computer Graphics,A,"Light Attenuation Displays (LADs) are a type of Optical See-Through Head-Mounted Display (OST-HMD) that present images by attenuating incoming light with a pixel-wise polarizing color filter. Although LADs can display images in bright environments, there is a trade-off between the number of Spatial Light Modulators (SLMs) and the color gamut and contrast that can be expressed, making it difficult to achieve both high-fidelity image display and a small form factor. To address this problem, we propose StainedSweeper, a LAD that achieves both the wide color gamut and the variable intensity with a single SLM. Our system synchronously controls a pixel-wise Digital Micromirror Device (DMD) and a nonpixel polarizing color filter to pass light when each pixel is the desired color. By sweeping this control at high speed, the human eye perceives images in a time-multiplexed, integrated manner. To achieve this, we develop the OST-HMD design using a reflective Solc filter as a polarized color filter and a color reproduction algorithm based on the optimization of the time-multiplexing matrix for the selected primary color filters. Our proof-of-concept prototype showed that our single SLM design can produce subtractive images with variable contrast and a wider color gamut than conventional LADs. © 1995-2012 IEEE.",augmented reality; Light attenuation display; polarized color filter; see-through display; time-multiplexing,Keywords,True,
Scopus,journalPaper,2024,Investigating Whether the Mass of a Tool Replica Influences Virtual Training Learning Outcomes,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) has emerged as a promising solution to address the pressing concern of transferring know-how in the manufacturing industry. Making an immersive training experience often involves designing an instrumented replica of a tool whose use is to be learned through virtual training. The process of making a replica can alter its mass, making it different from that of the original tool. As far as we know, the influence of this difference on learning outcomes has never been evaluated. To investigate this subject, an immersive training experience was designed with pre and post-training phases under real conditions, dedicated to learning the use of a rotary tool. 80 participants took part in this study, split into three groups: a control group performing the virtual training using a replica with the same mass as the original tool ($\mathrm{m}=100\%$), a second group that used a replica with a lighter mass than the original tool ($\mathrm{m}= 50\%$) and a third group using a replica heavier than the original tool ($\mathrm{m}=150\%$). Despite variations in the mass of the replica used for training, this study revealed that the learning outcomes remained comparable across all groups, while also demonstrating significant enhancements in certain performance measures, including task completion time. Overall, these findings provide useful insights regarding the design of tool replicas for immersive training. © 1995-2012 IEEE.",Prop Design; User Study; Virtual Reality; Virtual Training; Weight Perception,Abstract_Keywords,True,
Scopus,journalPaper,2024,Try This for Size: Multi-Scale Teleportation in Immersive Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"The ability of a user to adjust their own scale while traveling through virtual environments enables them to inspect tiny features being ant-sized and to gain an overview of the surroundings as a giant. While prior work has almost exclusively focused on steering-based interfaces for multi-scale travel, we present three novel teleportation-based techniques that avoid continuous motion flow to reduce the risk of cybersickness. Our approaches build on the extension of known teleportation workflows and suggest specifying scale adjustments either simultaneously with, as a connected second step after, or separately from the user's new horizontal position. The results of a two-part user study with 30 participants indicate that the simultaneous and connected specification paradigms are both suitable candidates for effective and comfortable multi-scale teleportation with nuanced individual benefits. Scale specification as a separate mode, on the other hand, was considered less beneficial. We compare our findings to prior research and publish the executable of our user study to facilitate replication and further analyses.  © 1995-2012 IEEE.",3D Navigation; 3D User Interfaces; Head-Mounted Display; Multi-Scale; Teleportation; Virtual Reality,Title_Keywords,True,
Scopus,journalPaper,2024,A Study on Collaborative Visual Data Analysis in Augmented Reality with Asymmetric Display Types,TVCG - Transactions on Visualization and Computer Graphics,A,"Collaboration is a key aspect of immersive visual data analysis. Due to its inherent benefit of seeing co-located collaborators, augmented reality is often useful in such collaborative scenarios. However, to enable the augmentation of the real environment, there are different types of technology available. While there are constant developments in specific devices, each of these device types provide different premises for collaborative visual data analysis. In our work we combine handheld, optical see-through and video see-through displays to explore and understand the impact of these different device types in collaborative immersive analytics. We conducted a mixed-methods collaborative user study where groups of three performed a shared data analysis task in augmented reality with each user working on a different device, to explore differences in collaborative behaviour, user experience and usage patterns. Both quantitative and qualitative data revealed differences in user experience and usage patterns. For collaboration, the different display types influenced how well participants could participate in the collaborative data analysis, nevertheless, there was no measurable effect in verbal communication.  © 1995-2012 IEEE.",Augmented Reality; Collaboration; Empirical Studies,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,APF-S2T: Steering to Target Redirection Walking Based on Artificial Potential Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"Redirected walking (RDW) enables users to walk naturally within a virtual environment that is larger than the physical environment. Recently, several artificial potential field (APF) and alignment-based redirected controllers have been developed and have been demonstrated to significantly outperform conventional controllers. APF Steer-to-Gradient (APF-S2G) and APF Redirected Walking (APF-RDW) utilize the negative gradient and the total force vector, respectively, which are localized to the user's position. These vectors usually point towards the opposite wall when the user is in corridors, resulting in frequent resets within those regions. This paper introduces the APF Steer-to-Target (APF-S2T), a redirected controller that first finds the target sample point with the lowest score in the user's walkable area in both physical and virtual environments. The score of a sample point is determined by the APF value at the point and the distance from the user's position. The direction from the user's position to the target point is then used as the steering direction for setting RDW gains. We conducted a simulation-based evaluation to compare APF-S2T, APF-S2G, APF-RDW, Visibility Polygon-based alignment (Vis.-Poly.) and Alignment-Optimized controllers in terms of the number of resets and the average distance between resets. The results indicated that APF-S2T significantly outperformed the state-of-the-art controllers. © 1995-2012 IEEE.",artificial potential field; locomotion; Redirected walking; virtual reality,Keywords,True,
Scopus,journalPaper,2024,"Who says you are so sick? An investigation on individual susceptibility to cybersickness triggers using EEG, EGG and ECG",TVCG - Transactions on Visualization and Computer Graphics,A,"In this research paper, we conducted a study to investigate the connection between three objective measures: Electrocardio-gram(EGG), Electrogastrogram (EGG), and Electroencephalogram (EEG), and individuals' susceptibility to cybersickness. Our primary objective was to identify which of these factors plays a central role in causing discomfort when experiencing rotations along three different axes: Roll, Pitch, and Yaw. This study involved 35 participants who were tasked with destroying asteroids using their eye gaze while undergoing passive rotations in four separate sessions. The results, when combined with subjective measurements (specifically, Fast motion sickness questionnaire (FMS) and Simulator sickness questionnaire (SSQ) score), demonstrated that EGG measurements were superior in detecting symptoms associated with nausea. As for ECG measurements, our observations did reveal significant changes in Heart Rate Variability (HRV) parameters. However, we caution against relying solely on ECG as a dependable indicator for assessing the extent of cybersickness. Most notably, EEG signals emerged as a crucial resource for discerning individual differences related to these rotational axes. Our findings were significant not only in the context of periodic activities but also underscored the potential of aperiodic activities in detecting the severity of cybersickness and an individual's susceptibility to rotational triggers.  © 1995-2012 IEEE.",Electroencephalogram; Electrogastrogram; Gybersickness; Individual Susceptibility.Electrocardiogram; Virtual Reality,Keywords,True,
Scopus,journalPaper,2024,Cybersickness Abatement from Repeated Exposure to VR with Reduced Discomfort,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness, or sickness induced by virtual reality (VR), negatively impacts the enjoyment and adoption of the technology. One method that has been used to reduce sickness is repeated exposure to VR, herein Cybersickness Abatement from Repeated Exposure (CARE). However, high sickness levels during repeated exposure may discourage some users from returning. Field of view (FOV) restriction reduces cybersickness by minimizing visual motion in the periphery, but also negatively affects the user's visual experience. This study explored whether CARE that occurs with FOV restriction generalizes to a full FOV experience. Participants played a VR game for up to 20 minutes. Those in the Repeated Exposure Condition played the same VR game on four separate days, experiencing FOV restriction during the first three days and no FOV restriction on the fourth day. Results indicated significant CARE with FOV restriction (Days 1-3). Further, cybersickness on Day 4, without FOV restriction, was significantly lower than that of participants in the Single Exposure Condition, who experienced the game without FOV restriction only on one day. The current findings show that significant CARE can occur while experiencing minimal cybersickness. Results are considered in the context of multiple theoretical explanations for CARE, including sensory rearrangement, adaptation, habituation, and postural control.  © 1995-2012 IEEE.",Adaptation; Cybersickness; Field of View Restriction; Habituation; Repeated Exposure; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Beyond the Wizard of Oz: Negative Effects of Imperfect Machine Learning to Examine the Impact of Reliability of Augmented Reality Cues on Visual Search Performance,TVCG - Transactions on Visualization and Computer Graphics,A,"Despite knowing exactly what an object looks like, searching for it in a person's visual field is a time-consuming and error-prone experience. In Augmented Reality systems, new algorithms are proposed to speed up search time and reduce human errors. However, these algorithms might not always provide 100% accurate visual cues, which might affect users' perceived reliability of the algorithm and, thus, search performance. Here, we examined the detrimental effects of automation bias caused by imperfect cues presented in the Augmented Reality head-mounted display using the YOLOv5 machine learning model. 53 participants in the two groups received either 100% accurate visual cues or 88.9% accurate visual cues. Their performance was compared with the control condition, which did not include any additional cues. The results show how cueing may increase performance and shorten search times. The results also showed that performance with imperfect automation was much worse than perfect automation and that, consistent with automation bias, participants were frequently enticed by incorrect cues. © 1995-2012 IEEE.",Augmented Reality; Automation Bias; Imperfect Cues; Visual Search,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,With or Without You: Effect of Contextual and Responsive Crowds on VR-based Crowd Motion Capture,TVCG - Transactions on Visualization and Computer Graphics,A,"While data is vital to better understand and model interactions within human crowds, capturing real crowd motions is extremely challenging. Virtual Reality (VR) demonstrated its potential to help, by immersing users into either simulated virtual crowds based on autonomous agents, or within motion-capture-based crowds. In the latter case, users' own captured motion can be used to progressively extend the size of the crowd, a paradigm called Record-and-Replay (2R). However, both approaches demonstrated several limitations which impact the quality of the acquired crowd data. In this paper, we propose the new concept of contextual crowds to leverage both crowd simulation and the 2R paradigm towards more consistent crowd data. We evaluate two different strategies to implement it, namely a Replace-Record-Replay (3R) paradigm where users are initially immersed into a simulated crowd whose agents are successively replaced by the user's captured-data, and a Replace-Record-Replay-Responsive (4R) paradigm where the pre-recorded agents are additionally endowed with responsive capabilities. These two paradigms are evaluated through two real-world-based scenarios replicated in VR. Our results suggest that the behaviors observed in VR users with surrounding agents from the beginning of the recording process are made much more natural, enabling 3R or 4R paradigms to improve the consistency of captured crowd datasets.  © 1995-2012 IEEE.",Crowd Simulation; Human Interaction; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,'Where Did My Apps Go?' Supporting Scalable and Transition-Aware Access to Everyday Applications in Head-Worn Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Future augmented reality (AR) glasses empower users to view personal applications and services anytime and anywhere without being restricted by physical locations and the availability of physical screens. In typical everyday activities, people move around to carry out different tasks and need a variety of information on the go. Existing interfaces in AR do not support these use cases well, especially when the number of applications increases. We explore the usability of three world-referenced approaches that move AR applications with users as they transition among different locations, featuring different levels of AR app availability: (1) always using a menu to manually open an app when needed; (2) automatically suggesting a relevant subset of all apps; and (3) carrying all apps with the users to the new location. Through a controlled study and a relatively more ecologically-valid study in AR, we reached better understandings on the performance trade-offs and observed the impact of various everyday contextual factors on these interfaces in more realistic AR settings. Our results shed light on how to better support the mobile information needs of users in everyday life in future AR interfaces.  © 1995-2012 IEEE.",Adaptive interface; augmented reality; automation; glanceable interface; mobile computing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Projection Mapping under Environmental Lighting by Replacing Room Lights with Heterogeneous Projectors,TVCG - Transactions on Visualization and Computer Graphics,A,"Projection mapping (PM) is a technique that enhances the appearance of real-world surfaces using projected images, enabling multiple people to view augmentations simultaneously, thereby facilitating communication and collaboration. However, PM typically requires a dark environment to achieve high-quality projections, limiting its practicality. In this paper, we overcome this limitation by replacing conventional room lighting with heterogeneous projectors. These projectors replicate environmental lighting by selectively illuminating the scene, excluding the projection target. Our contributions include a distributed projector optimization framework designed to effectively replicate environmental lighting and the incorporation of a large-aperture projector, in addition to standard projectors, to reduce high-luminance emitted rays and hard shadows-undesirable factors for collaborative tasks in PM. We conducted a series of quantitative and qualitative experiments, including user studies, to validate our approach. Our findings demonstrate t hat our projector-based lighting system significantly enhancesthe contrast and realism of PM results even under e nvironmental lighting compared to typical lights. Furthermore, our method facilitates a substantial shift in the perceived color mode from the undesirable aperture-color mode, where observers perceive the projected object as self-luminous, to the surface-color mode in PM. © 1995-2012 IEEE.",Augmented reality; cooperative distributed projector optimization; large-aperture projector; projection mapping,Keywords,True,
Scopus,journalPaper,2024,Real-and-Present: Investigating the Use of Life-Size 2D Video Avatars in HMD-Based AR Teleconferencing,TVCG - Transactions on Visualization and Computer Graphics,A,"Augmented Reality (AR) teleconferencing allows spatially distributed users to interact with each other in 3D through agents in their own physical environments. Existing methods leveraging volumetric capturing and reconstruction can provide a high-fidelity experience but are often too complex and expensive for everyday use. Other solutions target mobile and effortless-to-setup teleconferencing on AR Head Mounted Displays (HMD). They directly transplant the conventional video conferencing onto an AR-HMD platform or use avatars to represent remote participants. However, they can only support either a high fidelity or a high level of co-presence. Moreover, the limited Field of View (FoV) of HMDs could further degrade users' immersive experience. To achieve a balance between fidelity and co-presence, we explore using life-size 2D video-based avatars (video avatars for short) in AR teleconferencing. Specifically, with the potential effect of FoV on users' perception of proximity, we first conducted a pilot study to explore the local-user-centered optimal placement of video avatars in small-group AR conversations. With the placement results, we then implement a proof-of-concept prototype of video-avatar-based teleconferencing. We conduct user evaluations with our prototype to verify its effectiveness in balancing fidelity and co-presence. Following the indication in the pilot study, we further quantitatively explore the effect of FoV size on the video avatar's optimal placement through a user study involving more FoV conditions in a VR-simulated environment. We regress placement models to serve as references for computationally determining video avatar placements in such teleconferencing applications on various existing AR HMDs and future ones with bigger FoVs.  © 1995-2012 IEEE.",AR Teleconference; Co-presence; Fidelity; Field of View; Video-Based Avatar,Abstract,True,
Scopus,journalPaper,2024,Inserting Objects into Any Background Images via Implicit Parametric Representation,TVCG - Transactions on Visualization and Computer Graphics,A,"Inserting an object into a background scene has wide applications in image editing and mixed reality. However, existing methods still struggle to seamlessly adapt the object to the background while maintaining its individual characteristics. In this paper, we propose to fine-tune a pre-trained diffusion-based insertion model such that it learns to establish a unique correspondence between a few weights and the target object, given as input few-shot images of an object. A novel individualized feature extraction (IFE) module is designed to extract the individual detail features from few-shot object images. Then, the individual features of the target object, together with the semantic features of the target object and the background context features extracted by the pre-trained image encoders are injected into the cross-attention modules of the latent diffusion model, enabling it to learn the correlation information of the target object and the background scene through the attention mechanism. The weights obtained by fine-tuning implicitly serve as an alternative representation of the target object, with which the object can be easily inserted into any background images. Extensive comparative experiments validate the superiority of the proposed method to the state-of-the-art insertion methods in maintaining the individual details of the inserted object and adapting it to background scenes, including allowing the interaction between the inserted object and the background scene, correctly handling their occlusion relationship, maintaining the consistency of their viewpoints and poses. © 1995-2012 IEEE.",implicit parametric representation; insertion strategy; latent diffusion model; Object insertion,Abstract,True,
Scopus,journalPaper,2024,Multi-User Redirected Walking in Separate Physical Spaces for Online VR Scenarios,TVCG - Transactions on Visualization and Computer Graphics,A,"With the recent rise of Metaverse, online multiplayer VR applications are becoming increasingly prevalent worldwide. However, as multiple users are located in different physical environments, different reset frequencies and timings can lead to serious fairness issues for online collaborative/competitive VR applications. For the fairness of online VR apps/games, an ideal online RDW strategy must make the locomotion opportunities of different users equal, regardless of different physical environment layouts. The existing RDW methods lack the scheme to coordinate multiple users in different PEs, and thus have the issue of triggering too many resets for all the users under the locomotion fairness constraint. We propose a novel multi-user RDW method that is able to significantly reduce the overall reset number and give users a better immersive experience by providing a fair exploration. Our key idea is to first find out the 'bottleneck' user that may cause all users to be reset and estimate the time to reset given the users' next targets, and then redirect all the users to favorable poses during that maximized bottleneck time to ensure the subsequent resets can be postponed as much as possible. More particularly, we develop methods to estimate the time of possibly encountering obstacles and the reachable area for a specific pose to enable the prediction of the next reset caused by any user. Our experiments and user study found that our method outperforms existing RDW methods in online VR applications.  © 1995-2012 IEEE.",fairness; multi-user; online VR; Redirected walking,Abstract,True,
Scopus,journalPaper,2024,Adaptive Complementary Filter for Hybrid Inside-Out Outside-In HMD Tracking With Smooth Transitions,TVCG - Transactions on Visualization and Computer Graphics,A,"Head-mounted displays (HMDs) in room-scale virtual reality are usually tracked using inside-out visual SLAM algorithms. Alternatively, to track the motion of the HMD with respect to a fixed real-world reference frame, an outside-in instrumentation like a motion capture system can be adopted. However, outside-in tracking systems may temporarily lose tracking as they suffer by occlusion and blind spots. A possible solution is to adopt a hybrid approach where the inside-out tracker of the HMD is augmented with an outside-in sensing system. On the other hand, when the tracking signal of the outside-in system is recovered after a loss of tracking the transition from inside-out tracking to hybrid tracking may generate a discontinuity, i.e a sudden change of the virtual viewpoint, that can be uncomfortable for the user. Therefore, hybrid tracking solutions for HMDs require advanced sensor fusion algorithms to obtain a smooth transition. This work proposes a method for hybrid tracking of a HMD with smooth transitions based on an adaptive complementary filter. The proposed approach can be configured with several parameters that determine a trade-off between user experience and tracking error. A user study was carried out in a room-scale virtual reality environment, where users carried out two different tasks while multiple signal tracking losses of the outside-in sensor system occurred. The results show that the proposed approach improves user experience compared to a standard Extended Kalman Filter, and that tracking error is lower compared to a state-of-the-art complementary filter when configured for the same quality of user experience.  © 1995-2012 IEEE.",Head mounted displays; room-scale virtual reality; sensor fusion; tracking technologies,Abstract_Keywords,True,
Scopus,journalPaper,2024,Fumos: Neural Compression and Progressive Refinement for Continuous Point Cloud Video Streaming,TVCG - Transactions on Visualization and Computer Graphics,A,"Point cloud video (PCV) offers watching experiences in photorealistic 3D scenes with six-degree-of-freedom (6-DoF), enabling a variety of VR and AR applications. The user's Field of View (FoV) is more fickle with 6-DoF movement than 3-DoF movement in 360-degree video. PCV streaming is extremely bandwidth-intensive. However, current streaming systems require hundreds of Mbps bandwidth, exceeding the bandwidth capabilities of commodity devices. To save bandwidth, FoV-adaptive streaming predicts a user's FoV and only downloads point cloud data falling in the predicted FoV. But it is difficult to accurately predict the user's FoV even 2a3 seconds before playback due to 6-DoF. Misprediction of FoV or network bandwidth dips results in frequent stalls. To avoid rebuffering, existing systems would cause incomplete FoV and degraded experience, deteriorating the user's quality of experience (QoE). In this paper, we describe Fumos, a novel system that preserves interactive experience by avoiding playback stalls while maintaining high perceptual quality and high compression rate. We find a research gap in inter-frame redundant utilization and progressive mechaism. Fumos has three crucial designs, including (1) Neural compression framework with inter-frame coding, namely N-PCC, which achieves both bandwidth efficiency and high fidelity. (2) Progressive refinement streaming framework that enables continuous playback by incrementally upgrading a fetched portion to a higher quality (3) System-level adaptation that employs Lyapunov optimization to jointly optimize the long-term user QoE. Experimental results demonstrate that Fumos significantly outperforms Draco, achieving an average decoding rate acceleration of over 260Ã- . Moreover, the proposed compression framework N-PCC attains remarkable BD-Rate gains, averaging 91.7% and 51.7% against the state-of-the-art point cloud compression methods G-PCC and V-PCC, respectively.  © 1995-2012 IEEE.",Deep learning; Point cloud compression; Streaming media; User Experience; Virtual Reality (VR),Keywords,True,
Scopus,journalPaper,2024,MusiKeys: Exploring Haptic-to-Auditory Sensory Substitution to Improve Mid-Air Text-Entry,TVCG - Transactions on Visualization and Computer Graphics,A,"Physical QWERTY keyboards are the current standard for performing precision text-entry with extended reality devices. Ideally, there would exist a comparable, self-contained solution that works anywhere, without requiring external keyboards. Unfortunately, when physical keyboards are recreated virtually, we currently lose critical haptic feedback information from the sense of touch, which impedes typing. In this paper, we introduce the MusiKeys Technique, which uses auditory feedback in virtual reality to communicate missing haptic feedback information typists normally receive when using a physical keyboard. To examine this concept, we conducted a user study with 24 participants which encompassed four mid-air virtual keyboards augmented with increasing amounts of feedback information, along with a fifth physical keyboard for reference. Results suggest that providing clicking feedback on key-press and key-release improves typing performance compared to not providing auditory feedback, which is consistent with the literature. We also found that audio can serve as a substitute for information contained in haptic feedback, in that users can accurately perceive the presented information. However, under our specific study conditions, this awareness of the feedback information did not yield significant differences in typing performance. Our results suggest this kind of feedback replacement can be perceived by users but needs more research to tune and improve the specific techniques.  © 1995-2012 IEEE.",Extended reality; Human-computer interaction (HCI); Mid-air text-entry; Sensory substitution; Spatial computing; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,ProtoColVR: Requirements Gathering and Collaborative Rapid Prototyping of VR Training Simulators for Multidisciplinary Teams,TVCG - Transactions on Visualization and Computer Graphics,A,"We present ProtoColVR, a methodology and a plugin designed for gathering requirements and collaborative rapid prototyping of virtual reality training simulators. Our methodology outlines the utilization of current technologies, the involvement of stakeholders during design and development, and the implementation of simulator creation through multiple iterations. We incorporate open-source tools and freely available environments like Twine and Unity to establish a reference implementation for requirements gathering and rapid prototyping. ProtoColVR is the outcome of our collaboration with a hospital and our Navy, and it has undergone testing in a development Jam. From these tests, we have gained valuable insights, including the ability to create functional prototypes within multidisciplinary teams, enhance communication among different roles, and streamline requirements gathering while improving our understanding of the virtualized environment.  © 1995-2012 IEEE.",Methodology; Multidisciplinary; Prototyping; Simulation; Training; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Human Factors at Play: Understanding the Impact of Conditioning on Presence and Reaction Time in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"A prerequisite to improving the presence of a user in mixed reality (MR) is the ability to measure and quantify presence. Traditionally, subjective questionnaires have been used to assess the level of presence. However, recent studies have shown that presence is correlated with objective and systemic human performance measures such as reaction time. These studies analyze the correlation between presence and reaction time when technical factors such as object realism and plausibility of the object's behavior change. However, additional psychological and physiological human factors can also impact presence. It is unclear if presence can be mapped to and correlated with reaction time when human factors such as conditioning are involved. To answer this question, we conducted an exploratory study ($N=60$) where the relationship between presence and reaction time was assessed under three different conditioning scenarios: control, positive, and negative. We demonstrated that human factors impact presence. We found that presence scores and reaction times are significantly correlated (correlation coefficient of 0.64), suggesting that the impact of human factors on reaction time correlates with its effect on presence. In demonstrating that, our study takes another important step toward using objective and systemic measures like reaction time as a presence measure. © 1995-2012 IEEE.",Conditioning; Mixed Reality; Presence,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Field of View Restriction and Snap Turning as Cybersickness Mitigation Tools,TVCG - Transactions on Visualization and Computer Graphics,A,"Multiple tools are available to reduce cybersickness (sickness caused by virtual reality), but past research has not investigated the combined effects of multiple mitigation tools. Field of view (FOV) restriction limits peripheral vision during self-motion, and ample evidence supports its effectiveness for reducing cybersickness. Snap turning involves discrete rotations of the user's perspective without presenting intermediate views, although reports on its effectiveness at reducing cybersickness are limited and equivocal. Both mitigation tools reduce the visual motion that can cause cybersickness. The current study (N = 201) investigated the individual and combined effects of FOV restriction and snap turning on cybersickness when playing a consumer virtual reality game. FOV restriction and snap turning in isolation reduced cybersickness compared to a control condition without mitigation tools. Yet, the combination of FOV restriction and snap turning did not further reduce cybersickness beyond the individual tools in isolation, and in some cases the combination of tools led to cybersickness similar to that in the no mitigation control. These results indicate that caution is warranted when combining multiple cybersickness mitigation tools, which can interact in unexpected ways.  © 1995-2012 IEEE.",Cybersickness; field of view restriction; motion sickness; snap turning; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Examining Effects of Technique Awareness on the Detection of Remapped Hands in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Input remapping techniques have been widely explored to allow users in virtual reality to exceed both their own physical abilities, the limitations of physical space, or to facilitate interactions with real-world objects. Often considered is how these techniques can be applied to achieve maximum utility, but still be undetectable to users to maintain a sense of immersion and presence. Existing psychophysical methods used to determine these detection thresholds have known limitations: they are highly conservative lower bounds for detection and do not account for complex usage of the technique. Our work describes and evaluates a method for estimating detection that reduces these limitations and yields meaningful upper bounds. We present the findings of our work where we apply this method to a well-explored hand motion scaling technique. In wholly unaware cases, we determined that users may detect their hand speed as abnormal at around 3.37 times the normal speed, compared to a scale factor of 1.47 that was estimated using traditional methods when users knew the motion scaling was occurring. A considerable number of participants in unaware cases (12 of 56) never detected their hand speed increasing at all, even at the maximum scale factor of 5.0. The study demonstrates just how conservative the thresholds generated by traditional psychophysical methods can be compared to detection during naive usage, and our method can be modified and applied easily to other techniques. © 1995-2012 IEEE.",human perception; interaction techniques; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A Sense of Urgency on the Sense of Agency: Challenges in Evaluating Agency and Embodiment in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Control over an avatar in virtual reality can improve one's perceived sense of agency and embodiment towards their avatar. Yet, the relationship between control on agency and embodiment remains unclear. This work aims to investigate two main ideas: (1) the effectiveness of currently used metrics in measuring agency and embodiment and (2) the relationship between different levels of control on agency, embodiment, and cognitive performance. To do this, we conducted a between-participants user study with three conditions on agency (n=57). Participants embodied an avatar with one of three types of control (i.e., Low - control over head only, Medium - control over head and torso, or High - control over head, torso, and arms) and completed a Stroop test. Our results indicate that the degree of control afforded to participants impacted their embodiment and cognitive performance but, as expected, could not be detected in the self-reported agency scores. Furthermore, our results elucidated further insights into the relationship between control and embodiment, suggesting potential uncanny valley-like effects. Future work should aim to refine agency measures to better capture the effect of differing levels of control and consider other methodologies to measure agency. © 1995-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,SmoothRide: A Versatile Solution to Combat Cybersickness in Elevation-Altering Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness continues to bar many individuals from taking full advantage of virtual reality (VR) technology. Previous work has established that navigating virtual terrain with elevation changes poses a significant risk in this regard. In this paper, we investigate the effectiveness of three cybersickness reduction strategies on users performing a navigation task across virtual elevation-altering terrain. These strategies include static field of view (FOV) reduction, a flat surface approach that disables terrain collision and maintains constant elevation for users, and SmoothRide, a novel technique designed to dampen a user's perception of vertical motion as they travel. To assess the impact of these strategies, we conducted a within-subjects study involving 61 participants. Each strategy was compared against a control condition, where users navigated across terrain without any cybersickness reduction measures in place. Cybersickness data were collected using the Fast Motion Sickness Scale (FMS) and Simulator Sickness Questionnaire (SSQ), along with galvanic skin response (GSR) data. We measured user presence using the IGroup Presence questionnaire (IPQ) and a Single-Item Presence Scale (SIP). Our findings reveal that users experienced significantly lower levels of cybersickness using SmoothRide or FOV reduction. Presence scores reported on the IPQ were statistically similar between SmoothRide and the control condition. Conversely, terrain flattening had adverse effects on user presence scores, and we could not identify a significant effect on cybersickness compared to the control. We demonstrate that SmoothRide is an effective, lightweight, configurable, and easy-to-integrate tool for reducing cybersickness in simulations featuring elevation-altering terrain. © 1995-2012 IEEE.",Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI; Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,HOIMotion: Forecasting Human Motion during Human-Object Interactions Using Egocentric 3D Object Bounding Boxes,TVCG - Transactions on Visualization and Computer Graphics,A,"We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information. © 1995-2012 IEEE.",augmented reality; graph convolutional network; Human motion forecasting; human-object interaction,Abstract_Keywords,True,
Scopus,journalPaper,2024,Scene-aware Foveated Rendering,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose a new scene-aware foveated rendering method, which incorporates the scene awareness and characteristics of the human visual system into the mapping-based foveated rendering framework. First, we generate the conservative visual importance map that encodes the visual features of the scene, visual acuity, and gaze motion. Second, we construct the pixel size control map using a convolution kernel method. Third, we utilize the pixel size control map to guide the foveated rendering. At last, a temporal coherent refinement strategy is used to maintain the smooth foveated rendering for the adjacent frames. Compared to the state-of-the-art mapping-based foveated rendering methods using the same compression ratio, our method achieves smaller MSE, higher PSNR, and SSIM in the fovea, periphery, salient regions, and the whole image. We also conducted user studies, and the results proved that the perceptual quality of our method has a high visual similarity with the around truth rendered with the full resolution. © 1995-2012 IEEE.",Foveated rendering; Pixel size control; Virtual reality; Visual importance,Keywords,True,
Scopus,journalPaper,2024,Efficient and Accurate Semi-automatic Neuron Tracing with Extended Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Neuron tracing, alternately referred to as neuron reconstruction, is the procedure for extracting the digital representation of the three-dimensional neuronal morphology from stacks of microscopic images. Achieving accurate neuron tracing is critical for profiling the neuroanatomical structure at single-cell level and analyzing the neuronal circuits and projections at whole-brain scale. However, the process often demands substantial human involvement and represents a nontrivial task. Conventional solutions towards neuron tracing often contend with challenges such as non-intuitive user interactions, suboptimal data generation throughput, and ambiguous visualization. In this paper, we introduce a novel method that leverages the power of extended reality (XR) for intuitive and progressive semi-automatic neuron tracing in real time. In our method, we have defined a set of interactors for controllable and efficient interactions for neuron tracing in an immersive environment. We have also developed a GPU-accelerated automatic tracing algorithm that can generate updated neuron reconstruction in real time. In addition, we have built a visualizer for fast and improved visual experience, particularly when working with both volumetric images and 3D objects. Our method has been successfully implemented with one virtual reality (VR) headset and one augmented reality (AR) headset with satisfying results achieved. We also conducted two user studies and proved the effectiveness of the interactors and the efficiency of our method in comparison with other approaches for neuron tracing. © 2024 IEEE.",Extended Reality; Eye Tracking; Human-centered Computing; Neuron Tracing; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The Effects of Depth of Knowledge of a Virtual Agent,TVCG - Transactions on Visualization and Computer Graphics,A,"We explored the impact of depth of knowledge on conversational agents and human perceptions in a virtual reality (VR) environment. We designed experimental conditions with low, medium, and high depths of knowledge in the domain of game development and tested them among 27 game development students. We aimed to understand how the agent's predefined knowledge levels affected the participants' perceptions of the agent and its knowledge. Our findings showed that participants could distinguish between different knowledge levels of the virtual agent. Moreover, the agent's depth of knowledge significantly impacted participants' perceptions of intelligence, rapport, factuality, the uncanny valley effect, anthropomorphism, and willingness for future interaction. We also found strong correlations between perceived knowledge, perceived intelligence, factuality, and willingness for future interactions. We developed design guidelines for creating conversational agents from our data and observations. This study contributes to the human-agent interaction field in VR settings by providing empirical evidence on the importance of tailoring virtual agents' depth of knowledge to improve user experience, offering insights into designing more engaging and effective conversational agents. © 1995-2012 IEEE.",conversational AI; depth of knowledge; knowledge bank; prompt engineering; virtual agent; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Precise Tool to Target Positioning Widgets (TOTTA) in Spatial Environments: A Systematic Review,TVCG - Transactions on Visualization and Computer Graphics,A,"TOTTA outlines the spatial position and rotation guidance of a real/virtual tool (TO) towards a real/virtual target (TA), which is a key task in Mixed reality applications. The task error can have critical consequences regarding safety, performance, and quality, such as surgical implantology or industrial maintenance scenarios. The TOTTA problem lacks a dedicated study and it is scattered in different domains with isolated designs. This work contributes to a systematic review of the TOTTA visual widgets, studying 70 unique designs from 24 papers. TOTTA is commonly guided by the visual overlap -an intuitive, pre-attentive ""collimation""feedback- of simple shaped widgets: Box, 3D Axes, 3D Model, 2D Crosshair, Globe, Tetrahedron, Line, Plane. Our research discovers that TO and TA are often represented with the same shape. They are distinguished by topological elements (e.g. edges/vertices/faces), colors, transparency levels, and added. shapes, widget quantity, and size. Meanwhile some designs provide continuous ""during manipulation feedback""relative to the distance between TO and TA by text, dynamic color, sonification, and amplified graphical visualization. Some approaches trigger discrete ""TA reached feedback""such as color alteration, added sound, TA shape change, and added text. We found the lack of golden standards, including in testing procedures, as current ones are limited to partial sets with different and incomparable setups (different target configurations, avatar, background, etc.). We also found a bias in participants: right-handed, young male, non-color impaired. © 1995-2012 IEEE.",3D positioning; 3D user interface; tool to target manipulation; Virtual environments; widgets,Abstract,True,
Scopus,journalPaper,2024,Frankenstein's Monster in the Metaverse: User Interaction with Customized Virtual Agents,TVCG - Transactions on Visualization and Computer Graphics,A,"Enabled by the latest achievements in artificial intelligence (AI), computer graphics as well as virtual, augmented, and mixed reality (VR/AR/MR), virtual agents are increasingly resembling humans in both their appearance and intelligent behavior. This results in enormous potential for agents to support users in their daily lives, for example in customer service, healthcare, education or the envisioned all-encompassing metaverse. Today's technology would allow users to customize their conversation partners in the metaverse - as opposed to reality - according to their preferences, potentially improving the user experience. On the other hand, there is little research on how reshaping the head of a communication partner might affect the immediate interaction with them. In this paper, we investigate the user requirements for and the effects of agent customization. In a two-stage user study ($N=30$), we collected both self-reported evaluations (e.g., intrinsic motivation) and interaction metrics (e.g., interaction duration and number of tried out items) for the process of agent customization itself as well as data on how users perceived the subsequent human-agent interaction in VR. Our results indicate that users only wish to have full customization for agents in their personal social circle, while for general services, a selection or even a definite assignment of pre-configured agents is sufficient. When customization is offered, attributes such as gender, clothing or hair are subjectively more relevant to users than facial features such as skin or eye color. Although the customization of human interaction partners is beyond our control, customization of virtual agents significantly increases perceived social presence as well as rapport and trust. Further findings on user motivation and agent diversity are discussed in the paper. © 1995-2012 IEEE.",Intelligent virtual agents; personalization; user study,Title_Abstract,True,
Scopus,journalPaper,2024,NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene Understanding,TVCG - Transactions on Visualization and Computer Graphics,A,"In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM). However, a notable gap exists in the existing approaches when it comes to scene understanding. In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations. Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations. Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning. Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking. Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches. Finally, we also show that our approach can be used in augmented reality applications. Project page: https://zju3dv.github.io/nis_slam. © 1995-2012 IEEE.",Implicit representation; Neural dense SLAM; Scene understanding; Semantic segmentation,Abstract,True,
Scopus,journalPaper,2024,Searching Across Realities: Investigating ERPs and Eye-Tracking Correlates of Visual Search in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Mixed Reality allows us to integrate virtual and physical content into users' environments seamlessly. Yet, how this fusion affects perceptual and cognitive resources and our ability to find virtual or physical objects remains uncertain. Displaying virtual and physical information simultaneously might lead to divided attention and increased visual complexity, impacting users' visual processing, performance, and workload. In a visual search task, we asked participants to locate virtual and physical objects in Augmented Reality and Augmented Virtuality to understand the effects on performance. We evaluated search efficiency and attention allocation for virtual and physical objects using event-related potentials, fixation and saccade metrics, and behavioral measures. We found that users were more efficient in identifying objects in Augmented Virtuality, while virtual objects gained saliency in Augmented Virtuality. This suggests that visual fidelity might increase the perceptual load of the scene. Reduced amplitude in distractor positivity ERP, and fixation patterns supported improved distractor suppression and search efficiency in Augmented Virtuality. We discuss design implications for mixed reality adaptive systems based on physiological inputs for interaction. © 1995-2012 IEEE.",Augmented Reality; Augmented Virtuality; EEG; Event-Related Potentials; Eye Tracking; Mixed Reality; Visual Search,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Expressive 3D Facial Animation Generation Based on Local-to-Global Latent Diffusion,TVCG - Transactions on Visualization and Computer Graphics,A,"3D Facial animations, crucial to augmented and mixed reality digital media, have evolved from mere aesthetic elements to potent storytelling media. Despite considerable progress in facial animation of neutral emotions, existing methods still struggle to capture the authenticity of emotions. This paper introduces a novel approach to capture fine facial expressions and generate facial animations using audio synchronization. Our method consists of two key components: First, the Local-to-global Latent Diffusion Model (LG-LDM) tailored for authentic facial expressions, which can integrate audio, time step, facial expressions, and other conditions towards possible encoding of emotionally rich yet latent features in response to possibly noisy raw audio signals. The core of LG-LDM is our carefully designed Facial Denoiser Model (FDM) for aligning the local-to-global animation feature with audio. Second, we redesign an Emotion-centric Vector Quantized-Variational AutoEncoder framework (EVQ-VAE) to finely decode the subtle differences under different emotions and reconstruct the final 3D facial geometry. Our work significantly contributes to the key challenges of emotionally realistic 3D facial animation for audio synchronization and enhances the immersive experience and emotional depth in augmented and mixed reality applications. We provide a reproducibility kit including our code, dataset, and detailed instructions for running the experiments. This kit is available at https://github.com/wangxuanx/Face-Diffusion-Model. © 1995-2012 IEEE.",3D facial animation; diffusion model; speech-driven,Abstract,True,
Scopus,journalPaper,2024,Measuring and Predicting Multisensory Reaction Latency: A Probabilistic Model for Visual-Auditory Integration,TVCG - Transactions on Visualization and Computer Graphics,A,"— Virtual/augmented reality (VR/AR) devices offer both immersive imagery and sound. With those wide-field cues, we can simultaneously acquire and process visual and auditory signals to quickly identify objects, make decisions, and take action. While vision often takes precedence in perception, our visual sensitivity degrades in the periphery. In contrast, auditory sensitivity can exhibit an opposite trend due to the elevated interaural time difference. What occurs when these senses are simultaneously integrated, as is common in VR applications such as 360◦ video watching and immersive gaming? We present a computational and probabilistic model to predict VR users’ reaction latency to visual-auditory multisensory targets. To this aim, we first conducted a psychophysical experiment in VR to measure the reaction latency by tracking the onset of eye movements. Experiments with numerical metrics and user studies with naturalistic scenarios showcase the model’s accuracy and generalizability. Lastly, we discuss the potential applications, such as measuring the sufficiency of target appearance duration in immersive video playback, and suggesting the optimal spatial layouts for AR interface design. © 2024 IEEE.",augmented reality; human perception; reaction latency; Virtual reality; visual-audio,Abstract_Keywords,True,
Scopus,journalPaper,2024,Analysis and Design of Efficient Authentication Techniques for Password Entry with the Qwerty Keyboard for VR Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Authentication in digital security relies heavily on text-based passwords, even with other available methods like biometrics and graphical passwords. While virtual reality (VR) keyboards are typically invisible to onlookers, the presence of inconspicuous sensors, including accelerometers, gyroscopes, and barometers, poses a potential risk of unauthorized observation and recording. Traditional defense shoulder-surfing attack methods typically involve breaking apart the Qwerty layout, which destroys the user's inherent familiarity with the layout. This research addresses the need for secure password entry in VR environments while retaining the Qwerty layout. We explore three keyboard-related position alteration strategies to ensure security while mitigating the decline in user experience. These strategies involve moving the entire keyboard, cursor, and keys. Our theoretical study assesses the effectiveness of these strategies against shoulder-surfing attacks. Two user studies, employing ray-based and position-based text entry methods, respectively, evaluate the practical effectiveness of the three strategies in resisting shoulder-surfing attacks, as well as their impact on typing performance and user experience. Our findings demonstrate that the three strategies achieve shoulder-surfing attack resistance comparable to a random layout keyboard. Moreover, compared to a random layout, the two strategies involving the movement of the entire keyboard and the repositioning of keys support faster entry rates and enhanced user experience. © 1995-2012 IEEE.",keyboard layout; password; text entry; user study; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,HaptoFloater: Visuo-Haptic Augmented Reality by Embedding Imperceptible Color Vibration Signals for Tactile Display Control in a Mid-Air Image,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose HaptoFloater, a low-latency mid-air visuo-haptic augmented reality (VHAR) system that utilizes imperceptible color vibrations. When adding tactile stimuli to the visual information of a mid-air image, the user should not perceive the latency between the tactile and visual information. However, conventional tactile presentation methods for mid-air images, based on camera-detected fingertip positioning, introduce latency due to image processing and communication. To mitigate this latency, we use a color vibration technique; humans cannot perceive the vibration when the display alternates between two different color stimuli at a frequency of 25 Hz or higher. In our system, we embed this imperceptible color vibration into the mid-air image formed by a micromirror array plate, and a photodiode on the fingertip device directly detects this color vibration to provide tactile stimulation. Thus, our system allows for the tactile perception of multiple patterns on a mid-air image in 59.5 ms. In addition, we evaluate the visual-haptic delay tolerance on a mid-air display using our VHAR system and a tactile actuator with a single pattern and faster response time. The results of our user study indicate a visual-haptic delay tolerance of 110.6 ms, which is considerably larger than the latency associated with systems using multiple tactile patterns. © 1995-2012 IEEE.",imperceptible color vibration; LCD displays; mid-air images; Visuo-haptic displays,Title_Abstract,True,
Scopus,journalPaper,2024,A Framework for Multimodal Medical Image Interaction,TVCG - Transactions on Visualization and Computer Graphics,A,"Medical doctors rely on images of the human anatomy, such as magnetic resonance imaging (MRI), to localize regions of interest in the patient during diagnosis and treatment. Despite advances in medical imaging technology, the information conveyance remains unimodal. This visual representation fails to capture the complexity of the real, multisensory interaction with human tissue. However, perceiving multimodal information about the patient's anatomy and disease in real-time is critical for the success of medical procedures and patient outcome. We introduce a Multimodal Medical Image Interaction (MMII) framework to allow medical experts a dynamic, audiovisual interaction with human tissue in three-dimensional space. In a virtual reality environment, the user receives physically informed audiovisual feedback to improve the spatial perception of anatomical structures. MMII uses a model-based sonification approach to generate sounds derived from the geometry and physical properties of tissue, thereby eliminating the need for hand-crafted sound design. Two user studies involving 34 general and nine clinical experts were conducted to evaluate the proposed interaction framework's learnability, usability, and accuracy. Our results showed excellent learnability of audiovisual correspondence as the rate of correct associations significantly improved ($p < 0.001$) over the course of the study. MMII resulted in superior brain tumor localization accuracy ($p < 0.05$) compared to conventional medical image interaction. Our findings substantiate the potential of this novel framework to enhance interaction with medical images, for example, during surgical procedures where immediate and precise feedback is needed. © 1995-2012 IEEE.",Audiovisual feedback; Augmented reality; Brain surgery; Brain tumor; HCI; Human-centered design; Human-computer interaction; Medical image interaction; Medical images; Multimodal interaction; Physical modeling synthesis; Sonification; Surgical navigation; Tumor localization; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Robust Collaborative Visual-Inertial SLAM for Mobile Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Achieving precise real-time localization and ensuring robustness are critical challenges in multi-user mobile AR applications. Leveraging collaborative information to augment tracking accuracy on lightweight devices and fortify overall system robustness emerges as a crucial necessity. In this paper, we propose a robust centralized collaborative multi-agent VI-SLAM system for mobile AR interaction and server-side efficient consistent mapping. The system deploys a lightweight VIO frontend on mobile devices for real-time tracking, and a backend running on a remote server to update multiple submaps. When overlapping areas between submaps across agents are detected, the system performs submap fusion to establish a globally consistent map. Additionally, we propose a map registration and fusion strategy based on covisibility areas for online registration and fusion in multi-agent scenarios. To improve the tracking accuracy of the frontend on agent, we introduce a strategy for updating the global map to the local map at a moderate frequency between the camera-rate pose estimation of the frontend VIO and the low-frequency global map optimization, using a tightly coupled strategy to achieve consistency of the multi-agent frontend poses estimation in the global map. The effectiveness of the proposed method is further confirmed by executing backend mapping on the server and deploying VIO frontends on multiple mobile devices for AR demostration. Additionally, we discuss the scalability of the proposed system by analyzing network traffic, synchronization frequency, and other factors at both the agent and server ends. © 2024 IEEE.",Collaborative; Map fusion; SLAM; Tightly coupled; VIO,Title,True,
Scopus,journalPaper,2024,"Investigating Object Translation in Room-scale, Handheld Virtual Reality",TVCG - Transactions on Visualization and Computer Graphics,A,"— Handheld devices have become an inclusive alternative to head-mounted displays in virtual reality (VR) environments, enhancing accessibility and allowing cross-device collaboration. Object manipulation techniques in 3D space with handheld devices, such as those in handheld augmented reality (AR), have been typically evaluated in table-top scale and we currently lack an understanding of how these techniques perform in larger scale environments. We conducted two studies, each with 30 participants, to investigate how different techniques impact usability and performance for room-scale handheld VR object translations. We compared three translation techniques that are similar to commonly studied techniques in handheld AR: 3DSlide, VirtualGrasp, and Joystick. We also examined the effects of target size, target distance, and user mobility conditions (stationary vs. moving). Results indicated that the Joystick technique, which allowed translation in relation to the user’s perspective, was the fastest and most preferred, without difference in precision. Our findings provide insights for designing room-scale handheld VR systems, with potential implications for mixed reality systems involving handheld devices. © 2024 IEEE.",3D manipulation; Augmented Reality; Handheld VR; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Gesture2Text: A Generalizable Decoder for Word-Gesture Keyboards in XR Through Trajectory Coarse Discretization and Pre-Training,TVCG - Transactions on Visualization and Computer Graphics,A,"Text entry with word-gesture keyboards (WGK) is emerging as a popular method and becoming a key interaction for Extended Reality (XR). However, the diversity of interaction modes, keyboard sizes, and visual feedback in these environments introduces divergent word-gesture trajectory data patterns, thus leading to complexity in decoding trajectories into text. Template-matching decoding methods, such as SHARK2 [32], are commonly used for these WGK systems because they are easy to implement and configure. However, these methods are susceptible to decoding inaccuracies for noisy trajectories. While conventional neural-network-based decoders (neural decoders) trained on word-gesture trajectory data have been proposed to improve accuracy, they have their own limitations: they require extensive data for training and deep-learning expertise for implementation. To address these challenges, we propose a novel solution that combines ease of implementation with high decoding accuracy: a generalizable neural decoder enabled by pre-training on large-scale coarsely discretized word-gesture trajectories. This approach produces a ready-to-use WGK decoder that is generalizable across mid-air and on-surface WGK systems in augmented reality (AR) and virtual reality (VR), which is evident by a robust average Top-4 accuracy of 90.4% on four diverse datasets. It significantly outperforms SHARK2 with a 37.2% enhancement and surpasses the conventional neural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder's size is only 4 MB after quantization, without sacrificing accuracy, and it can operate in real-time, executing in just 97 milliseconds on Quest 3. © 1995-2012 IEEE.",discretization; Pre-trained models; text entry; word-gesture keyboard,Abstract,True,
Scopus,journalPaper,2024,Filtering on the Go: Effect of Filters on Gaze Pointing Accuracy during Physical Locomotion in Extended Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Eye tracking filters have been shown to improve accuracy of gaze estimation and input for stationary settings. However, their effectiveness during physical movement remains underexplored. In this work, we compare common online filters in the context of physical locomotion in extended reality and propose alterations to improve them for on-the-go settings. We conducted a computational experiment where we simulate performance of the online filters using data on participants attending visual targets located in world-, path-, and two head-based reference frames while standing, walking, and jogging. Our results provide insights into the filters' effectiveness and factors that affect it, such as the amount of noise caused by locomotion and differences in compensatory eye movements, and demonstrate that filters with saccade detection prove most useful for on-the-go settings. We discuss the implications of our findings and conclude with guidance on gaze data filtering for interaction in extended reality. © 1995-2012 IEEE.",extended reality; eye tracking; gaze filters; gaze-based pointing; physical locomotion; spatial reference frames,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Should I Evaluate my Augmented Reality System in an Industrial Environment? Investigating the Effects of Classroom and Shop Floor Settings on Guided Assembly,TVCG - Transactions on Visualization and Computer Graphics,A,"—Numerous prior studies have investigated real-time assembly instructions using Augmented Reality (AR). However, most such experiments were conducted in laboratory settings with simplistic assembly tasks, failing to represent real-world industrial conditions. To ascertain to what extent results obtained in a laboratory environment may differ from studies in actual industrial environments, we carried out a user study with 32 manufacturing apprentices. We compared assembly task execution results in two settings, a classroom and an industrial workshop environment. To facilitate the experiments, we developed AR-guided manual assembly systems for simple and more complex assets. Our findings reveal a significantly improved task performance in the industrial workshop, reflected in faster task completion times, fewer errors, and subjectively perceived higher flow. This contradicted participants’ subjective ratings, as they expected to perform better in the classroom environment. Our results suggest that the actual manufacturing environment is critical in evaluating AR systems for real-world industrial applications. © 2024 IEEE Computer Society. All rights reserved.",Augmented reality; augmented reality guidelines; classroom setting; manual assembly; shop floor setting; user study,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,RingGesture: A Ring-Based Mid-Air Gesture Typing System Powered by a Deep-Learning Word Prediction Framework,TVCG - Transactions on Visualization and Computer Graphics,A,"—Text entry is a critical capability for any modern computing experience, with lightweight augmented reality (AR) glasses being no exception. Designed for all-day wearability, a limitation of lightweight AR glass is the restriction to the inclusion of multiple cameras for extensive field of view in hand tracking. This constraint underscores the need for an additional input device. We propose a system to address this gap: a ring-based mid-air gesture typing technique, RingGesture, utilizing electrodes to mark the start and end of gesture trajectories and inertial measurement units (IMU) sensors for hand tracking. This method offers an intuitive experience similar to raycast-based mid-air gesture typing found in VR headsets, allowing for a seamless translation of hand movements into cursor navigation. To enhance both accuracy and input speed, we propose a novel deep-learning word prediction framework, Score Fusion, comprised of three key components: a) a word-gesture decoding model, b) a spatial spelling correction model, and c) a lightweight contextual language model. In contrast, this framework fuses the scores from the three models to predict the most likely words with higher precision. We conduct comparative and longitudinal studies to demonstrate two key findings: firstly, the overall effectiveness of RingGesture, which achieves an average text entry speed of 27.3 words per minute (WPM) and a peak performance of 47.9 WPM. Secondly, we highlight the superior performance of the Score Fusion framework, which offers a 28.2% improvement in uncorrected Character Error Rate over a conventional word prediction framework, Naive Correction, leading to a 55.2% improvement in text entry speed for RingGesture. Additionally, RingGesture received a System Usability Score of 83 signifying its excellent usability. © 2024 IEEE.",augmented reality; language models; Text entry; word prediction,Abstract_Keywords,True,
Scopus,journalPaper,2024,Evaluating Force-based Haptics for Immersive Tangible Interactions with Surface Visualizations,TVCG - Transactions on Visualization and Computer Graphics,A,"Haptic feedback provides an essential sensory stimulus crucial for interaction and analyzing three-dimensional spatio-temporal phenomena on surface visualizations. Given its ability to provide enhanced spatial perception and scene maneuverability, virtual reality (VR) catalyzes haptic interactions on surface visualizations. Various interaction modes, encompassing both mid-air and on-surface interactions - with or without the application of assisting force stimuli - have been explored using haptic force feedback devices. In this paper, we evaluate the use of on-surface and assisted on-surface haptic modes of interaction compared to a no-haptic interaction mode. A force-based haptic stylus is used for all three modalities; the on-surface mode uses collision based forces, whereas the assisted on-surface mode is accompanied by an additional snapping force. We conducted a within-subjects user study involving fundamental interaction tasks performed on surface visualizations. Keeping a consistent visual design across all three modes, our study incorporates tasks that require the localization of the highest, lowest, and random points on surfaces; and tasks that focus on brushing curves on surfaces with varying complexity and occlusion levels. Our findings show that participants took almost the same time to brush curves using all the interaction modes. They could draw smoother curves using the on-surface interaction modes compared to the no-haptic mode. However, the assisted on-surface mode provided better accuracy than the on-surface mode. The on-surface mode was slower in point localization, but the accuracy depended on the visual cues and occlusions associated with the tasks. Finally, we discuss participant feedback on using haptic force feedback as a tangible input modality and share takeaways to aid the design of haptics-based tangible interactions for surface visualizations. © 1995-2012 IEEE.",AR/VR/Immersive; Computer Graphics Techniques; Domain Agnostic; Guidelines; Human-Subjects Quantitative Studies; Interaction Design; Isosurface Techniques; Scalar Field Data; Specialized Input/Display Hardware,Abstract,True,
Scopus,journalPaper,2024,MobiTangibles: Enabling Physical Manipulation Experiences of Virtual Precision Hand-Held Tools' Miniature Control in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"Realistic simulation for miniature control interactions, typically identified by precise and confined motions, commonly found in precision hand-held tools, like calipers, powered engravers, retractable knives, etc., are beneficial for skill training associated with these kinds of tools in virtual reality (VR) environments. However, existing approaches aiming to simulate hand-held tools' miniature control manipulation experiences in VR entail prototyping complexity and require expertise, posing challenges for novice users and individuals with limited resources. Addressing this challenge, we introduce MobiTangibles-proxies for precision hand-held tools' miniature control interactions utilizing smartphone-based magnetic field sensing. MobiTangibles passively replicate fundamental miniature control experiences associated with hand-held tools, such as single-axis translation and rotation, enabling quick and easy use for diverse VR scenarios without requiring extensive technical knowledge. We conducted a comprehensive technical evaluation to validate the functionality of MobiTangibles across diverse settings, including evaluations for electromagnetic interference within indoor environments. In a user-centric evaluation involving 15 participants across bare hands, VR controllers, and MobiTangibles conditions, we further assessed the quality of miniaturized manipulation experiences in VR. Our findings indicate that MobiTangibles outperformed conventional methods in realism and fatigue, receiving positive feedback. © 1995-2012 IEEE.",Hand-held tool; Miniature control; Physical manipulation; Skill training; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,A Multi-aperture Coaxial Projector Balancing Shadow Suppression and Deblurring,TVCG - Transactions on Visualization and Computer Graphics,A,"This paper proposes a projection system that optically removes the cast shadow in projection mapping. Specifically, we realize the large-aperture (LA) projection using a large-format Fresnel lens to suppress cast shadows by condensing the projection light from a wide viewing angle. However, the resolution and contrast of the projected results are significantly degraded by defocus blur, veiling glare, and stray light caused by the aberration of an LA Fresnel lens. To solve the technical problems, we employ two different approaches: optical and digital image processing methods. First, we introduce a residual projector with a typical aperture lens on the same optical axis as the LA projector, projecting the residual (i.e., high-frequency) components attenuated in the LA projection. These projectors play different roles in shadow suppression and blur compensation, both achieved by projecting simultaneously. Secondly, we optimize the pair of projection images that can balance the shadow suppression and deblurring performance of our projection system. We implemented a proof-of-concept prototype and validated the above-mentioned techniques through projection experiments and a user study. © 1995-2012 IEEE.",Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality; Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers,Keywords,True,
Scopus,journalPaper,2024,Exploring the Effect of Viewing Attributes of Mobile AR Interfaces on Remote Collaborative and Competitive Tasks,TVCG - Transactions on Visualization and Computer Graphics,A,"— Mobile devices have the potential to facilitate remote tasks through Augmented Reality (AR) solutions by integrating digital information into the real world. Although prior studies have explored Mobile Augmented Reality (MAR) for co-located collaboration, none have investigated the impact of various viewing attributes that can influence remote task performance, such as target object viewing angles, synchronization styles, or having a secondary small screen showing other users current view in the MAR environment. In this paper, we explore five techniques considering these attributes, specifically designed for two modes of remote tasks: collaborative and competitive. We conducted a user study employing various combinations of those attributes for both tasks. In both instances, results indicate users’ optimal performance and preference for the technique that allows asynchronous viewing of object manipulations on the small screen. Overall, this paper contributes novel techniques for remote tasks in MAR, addressing aspects such as viewing angle and synchronization in object manipulation alongside secondary small-screen interfaces. Additionally, it presents the results of a user study evaluating the effectiveness, usability, and user preference of these techniques in remote settings and offers a set of recommendations for designing and implementing MAR solutions to enhance remote activities. © 2024 IEEE.",Collaborative Task; Competitive Task; Mobile Augmented Reality; Remote Collaboration,Abstract_Keywords,True,
Scopus,journalPaper,2024,Multimodal Feedback Methods for Advancing the Accessibility of Immersive Virtual Reality for People With Balance Impairments Due to Multiple Sclerosis,TVCG - Transactions on Visualization and Computer Graphics,A,"Maintaining balance in immersive virtual reality (VR) environments poses a significant challenge for users, particularly affecting those with pre-existing balance disorders. This study investigates the efficacy of multimodal feedback—comprising auditory, vibrotactile, and visual stimuli—in mitigating balance issues within VR. A sample of 68 participants, divided equally between individuals with balance deficits related to multiple sclerosis and those without, was evaluated. The research explored the impact of various feedback conditions on balance performance. The results demonstrated that the multimodal feedback condition significantly enhanced balance control compared to other conditions, with statistical analysis confirming this improvement (p<.001). These findings underscore the potential of integrated sensory feedback in addressing balance-related difficulties in VR, thereby improving the overall accessibility and user experience for individuals affected by balance impairments. This research contributes valuable insights into optimizing VR environments for enhanced stability and user comfort. © 2024 IEEE.",HMDs; immersive VR; Multimodal feedback; Multiple Sclerosis,Title_Abstract,True,
Scopus,journalPaper,2024,From Avatars to Agents: Self-Related Cues Through Embodiment and Personalization Affect Body Perception in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Our work investigates the influence of self-related cues in the design of virtual humans on body perception in virtual reality. In a $2\times 2$ mixed design, 64 participants faced photorealistic virtual humans either as a motion-synchronized embodied avatar or as an autonomous moving agent, appearing subsequently with a personalized and generic texture. Our results unveil that self-related cues through embodiment and personalization yield an individual and complemented increase in participants' sense of embodiment and self-identification towards the virtual human. Different body weight modification and estimation tasks further showed an impact of both factors on participants' body weight perception. Additional analyses revealed that the participant's body mass index predicted body weight estimations in all conditions and that participants' self-esteem and body shape concerns correlated with different body weight perception results. Hence, we have demonstrated the occurrence of double standards through induced self-related cues in virtual human perception, especially through embodiment. © 1995-2012 IEEE.",agency; body image; body weight perception; self-location; virtual body ownership; Virtual human,Title_Abstract,True,
Scopus,journalPaper,2024,Touching the Ground: Evaluating the Effectiveness of Data Physicalizations for Spatial Data Analysis Tasks,TVCG - Transactions on Visualization and Computer Graphics,A,"Inspired by recent advances in digital fabrication, artists and scientists have demonstrated that physical data encodings (i.e., data physicalizations) can increase engagement with data, foster collaboration, and in some cases, improve data legibility and analysis relative to digital alternatives. However, prior empirical studies have only investigated abstract data encoded in physical form (e.g., laser cut bar charts) and not continuously sampled spatial data fields relevant to climate and medical science (e.g., heights, temperatures, densities, and velocities sampled on a spatial grid). This paper presents the design and results of the first study to characterize human performance in 3D spatial data analysis tasks across analogous physical and digital visualizations. Participants analyzed continuous spatial elevation data with three visualization modalities: (1) 2D digital visualization; (2) perspective-tracked, stereoscopic 'fishtank' virtual reality; and (3) 3D printed data physicalization. Their tasks included tracing paths downhill, looking up spatial locations and comparing their relative heights, and identifying and reporting the minimum and maximum heights within certain spatial regions. As hypothesized, in most cases, participants performed the tasks just as well or better in the physical modality (based on time and error metrics). Additional results include an analysis of open-ended feedback from participants and discussion of implications for further research on the value of data physicalization. All data and supplemental materials are available at https://osf.io/7xdq4/.  © 1995-2012 IEEE.",Data physicalization; evaluation; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,SpatialTouch: Exploring Spatial Data Visualizations in Cross-reality,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose and study a novel cross-reality environment that seamlessly integrates a monoscopic 2D surface (an interactive screen with touch and pen input) with a stereoscopic 3D space (an augmented reality HMD) to jointly host spatial data visualizations. This innovative approach combines the best of two conventional methods of displaying and manipulating spatial 3D data, enabling users to fluidly explore diverse visual forms using tailored interaction techniques. Providing such effective 3D data exploration techniques is pivotal for conveying its intricate spatial structures - often at multiple spatial or semantic scales - across various application domains and requiring diverse visual representations for effective visualization. To understand user reactions to our new environment, we began with an elicitation user study, in which we captured their responses and interactions. We observed that users adapted their interaction approaches based on perceived visual representations, with natural transitions in spatial awareness and actions while navigating across the physical surface. Our findings then informed the development of a design space for spatial data exploration in cross-reality. We thus developed cross-reality environments tailored to three distinct domains: for 3D molecular structure data, for 3D point cloud data, and for 3D anatomical data. In particular, we designed interaction techniques that account for the inherent features of interactions in both spaces, facilitating various forms of interaction, including mid-air gestures, touch interactions, pen interactions, and combinations thereof, to enhance the users' sense of presence and engagement. We assessed the usability of our environment with biologists, focusing on its use for domain research. In addition, we evaluated our interaction transition designs with virtual and mixed-reality experts to gather further insights. As a result, we provide our design suggestions for the cross-reality environment, emphasizing the interaction with diverse visual representations and seamless interaction transitions between 2D and 3D spaces  © 1995-2012 IEEE.",cross reality; immersive visualization; interaction techniques; Spatial data,Abstract,True,
Scopus,journalPaper,2024,Collaborative Forensic Autopsy Documentation and Supervised Report Generation using a Hybrid Mixed-Reality Environment and Generative AI,TVCG - Transactions on Visualization and Computer Graphics,A,"—Forensic investigation is a complex procedure involving experts working together to establish cause of death and report findings to legal authorities. While new technologies are being developed to provide better post-mortem imaging capabilities—including mixed-reality (MR) tools to support 3D visualisation of such data—these tools do not integrate seamlessly into their existing collaborative workflow and report authoring process, requiring extra steps, e.g. to extract imagery from the MR tool and combine with physical autopsy findings for inclusion in the report. Therefore, in this work we design and evaluate a new forensic autopsy report generation workflow and present a novel documentation system using hybrid mixed-reality approaches to integrate visualisation, voice and hand interaction, as well as collaboration and procedure recording. Our preliminary findings indicate that this approach has the potential to improve data management, aid reviewability, and thus, achieve more robust standards. Further, it potentially streamlines report generation and minimise dependency on external tools and assistance, reducing autopsy time and related costs. This system also offers significant potential for education. A free copy of this paper and all supplemental materials are available at https://osf.io/ygfzx. © 2024 IEEE.",documentation; Forensic autopsy; generative AI; mixed reality; report generation,Keywords,True,
Scopus,journalPaper,2024,Is Video Gaming a Cure for Cybersickness? Gamers Experience Less Cybersickness Than Non-Gamers in a VR Self-Motion Task,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness remains a major drawback of Virtual Reality (VR) headsets, as a breadth of stationary experiences with visual self-motion can result in visually-induced motion sickness. However, not everybody experiences the same intensity or type of adverse symptoms. Here we propose that prior experience with virtual environments can predict ones degree of cybersickness. Video gaming can enhance visuospatial abilities, which in-turn relate negatively to cybersickness - meaning that consistently engaging in virtual environments can result in protective habituation effects. In a controlled stationary VR experiment, we found that 'VR-naive' video gamers experienced significantly less cybersickness in a virtual tunnel-travel task and outperformed 'VR-naive' non-video gamers on a visual attention task. These findings strongly motivate the use of non-VR games for training VR cybersickness resilience, with future research needed to further understand the mechanism(s) by which gamers become cybersickness resilient - potentially expanding access to VR for even the most susceptible participants. © 1995-2012 IEEE.",Cybersickness; Habituation; Video Gamers; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Virtual Crowds Rheology: Evaluating the Effect of Character Representation on User Locomotion in Crowds,TVCG - Transactions on Visualization and Computer Graphics,A,"Crowd data is a crucial element in the modeling of collective behaviors, and opens the way to simulation for their study or prediction. Given the difficulty of acquiring such data, virtual reality is useful for simplifying experimental processes and opening up new experimental opportunities. This comes at the cost of the need to assess the biases introduced by the use of this technology. Our paper is part of this effort, and investigates the effect of the graphical representation of a crowd on the behavior of a user immersed within. More specifically, we inspect the virtual navigation through virtual crowds, in terms of travel speeds and local navigation choices as a function of the visual representation of the virtual agents that make up the crowd (simple geometric model, anthropomorphic model or realistic model). Through an experiment in which we ask a user to navigate virtual crowds of varying densities, we show that the effect of the visual representation is limited, but that an anthropomorphic representation offers the best trade-off between computational complexity and ecological validity, even though a more realistic representation can be preferred when user behaviour is studied in more details. Our work leads to clear recommendations on the design of immersive simulations for the study of crowd behavior. © 2024 IEEE. P.",Crowd; Human Interaction; Navigation; Virtual Reality; Visual Representation,Abstract_Keywords,True,
Scopus,journalPaper,2024,Visual Perceptual Confidence: Exploring Discrepancies between Self-reported and Actual Distance Perception in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) systems are widely used, and it is essential to know if spatial perception in virtual environments (VEs) is similar to reality. Research indicates that users tend to underestimate distances in VR. Prior work suggests that actual distance judgments in VR may not always match the users self-reported preference of where they think they most accurately estimated distances. However, no explicit investigation evaluated whether user preferences match actual performance in a spatial judgment task. We used blind walking to explore potential dissimilarities between actual distance estimates and user-selected preferences of visual complexities, VE conditions, and targets. Our findings show a gap between user preferences and actual performance when visual complexities were varied, which has implications for better visual perception understanding, VR applications design, and research in spatial perception, indicating the need to calibrate and align user preferences and true spatial perception abilities in VR. © 1995-2012 IEEE.",distance perception; spatial perception; understanding people; Virtual reality; visual complexity,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Pseudo-walking Sensation by Anteroposterior or Lateral Galvanic Vestibular Stimulation and Synchronous Foot-sole Vibrations,TVCG - Transactions on Visualization and Computer Graphics,A,"The walking sensation is a result of the synthesis of multisensory inputs from various systems. The vestibular system, typically used for detecting acceleration, is a crucial component of the walking sensation. This study investigated the use of galvanic vestibular stimulation(GVS) to enhance the sensation of walking in virtual reality (VR) environments, particularly when users are seated and not engaged in active movements. GVS is a transcutaneous electric stimulation technique to evoke vestibular sensory responses and involves the application of a penetrating current to vestibular afferents. This study revealed that the pseudo-walking sensation can be intensified by applying lateral GVS. However, no difference was observed when it was synchronized with the walking rhythm represented by foot-sole vibration patterns. Furthermore, the study compares the effectiveness of lateral versus anterior-posterior GVS in enhancing walking sensations in VR. The findings provide novel perspectives on enhancing the VR walking experience through vestibular stimulation, even in scenarios in which the user is seated. Authors",Cybersickness; Electrodes; Foot; GVS; Legged locomotion; peripersonal space; Rhythm; Synchronization; Vibrations; walking sensation,Abstract,True,
Scopus,journalPaper,2024,A Real-Time and Interactive Fluid Modeling System for Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Within the realm of mixed reality, the capability to dynamically render environmental effects with high realism plays a crucial role in amplifying user engagement and interaction. Fluid dynamics, in particular, stand out as essential elements for crafting immersive virtual settings. This includes the simulation of phenomena like smoke, fire, and clouds, which are instrumental in enriching the virtual experience. This work showcases a cutting-edge system developed to produce dynamic and interactive fluid effects that mirror real captured data in real-time for mixed reality applications. This innovative system seamlessly incorporates fluid reconstruction alongside velocity estimation processes within the Unity engine environment. Our approach leverages a novel physics-based differentiable rendering technique, grounded in the principles of light transport in participating media, to simulate the intricate behaviors of fluid while ensuring high fidelity in visual appearance. To further enhance realism, we have expanded our framework to include the estimation of velocity fields, addressing the critical need for fluid motion simulation. The practical application of these techniques demonstrates the system’s capacity to offer a robust platform for fluid modeling in mixed reality environments. Through extensive evaluations, we illustrate the effectiveness of our approach in various scenes, underscoring its potential to transform mixed reality content creation by providing developers with the tools to incorporate highly realistic and interactive fluid seamlessly. © 2024 IEEE.",differentiable rendering; fluid modeling; mixed reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,TouchMark: Partial Tactile Feedback Design for Upper Limb Rehabilitation in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"The use of Virtual Reality (VR) technology, especially in medical rehabilitation, has expanded to include tactile cues along with visual stimuli. For patients with upper limb hemiplegia, tangible handles with haptic stimuli could improve their ability to perform daily activities. Traditional VR controllers are unsuitable for patient rehabilitation in VR, necessitating the design of specialized tangible handles with integrated tracking devices. Besides, matching tactile stimulation with corresponding virtual visuals could strengthen users' embodiment (i.e., owning and controlling virtual bodies) in VR, which is crucial for patients' training with virtual hands. Haptic stimuli have been shown to amplify the embodiment in VR, whereas the effect of partial tactile stimulation from tangible handles on embodiment remains to be clarified. This research, including three experiments, aims to investigate how partial tactile feedback of tangible handles impacts users' embodiment, and we proposed a design concept called TouchMark for partial tactile stimuli that could help users quickly connect the physical and virtual worlds. To evaluate users' tactile and comfort perceptions when grasping tangible handles in a non-VR setting, various handles with three partial tactile factors were manipulated in Study 1. In Study 2, we explored the effects of partial feedback using three forms of TouchMark on the embodiment of healthy users in VR, with various tangible handles, while Study 3 focused on similar investigations with patients. These handles were utilized to complete virtual food preparation tasks. The tactile and comfort perceptions of tangible handles and users' embodiment were evaluated in this research using questionnaires and interviews. The results indicate that TouchMark with haptic line and ring forms over no stimulation would significantly enhance users' embodiment, especially for patients. The low-cost and innovative TouchMark approach may assist users, particularly those with limited VR experience, in achieving the embodiment and enhancing their virtual interactive experience. © 1995-2012 IEEE.",agency; body ownership; embodiment; self-location; tactile sensation; Virtual rehabilitation,Title_Abstract,True,
Scopus,journalPaper,2024,Co-Designing Dynamic Mixed Reality Drill Positioning Widgets: A Collaborative Approach with Dentists in a Realistic Setup,TVCG - Transactions on Visualization and Computer Graphics,A,"—Mixed Reality (MR) is proven in the literature to support precise spatial dental drill positioning by superimposing 3D widgets. Despite this, the related knowledge about widget’s visual design and interactive user feedback is still limited. Therefore, this study is contributed to by co-designed MR drill tool positioning widgets with two expert dentists and three MR experts. The results of co-design are two static widgets (SWs): a simple entry point, a target axis, and two dynamic widgets (DWs), variants of dynamic error visualization with and without a target axis (DWTA and DWEP). We evaluated the co-designed widgets in a virtual reality simulation supported by a realistic setup with a tracked phantom patient, a virtual magnifying loupe, and a dentist’s foot pedal. The user study involved 35 dentists with various backgrounds and years of experience. The findings demonstrated significant results; DWs outperform SWs in positional and rotational precision, especially with younger generations and subjects with gaming experiences. The user preference remains for DWs (19) instead of SWs (16). However, findings indicated that the precision positively correlates with the time trade-off. The post-experience questionnaire (NASA-TLX) showed that DWs increase mental and physical demand, effort, and frustration more than SWs. Comparisons between DWEP and DWTA show that the DW’s complexity level influences time, physical and mental demands. The DWs are extensible to diverse medical and industrial scenarios that demand precision. © 2024 IEEE.",co-design; dentistry; Dynamic widgets; mixed reality; precise tool positioning; usability testing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,HuBar: A Visual Analytics Tool to Explore Human Behavior based on fNIRS in AR Guidance Systems,TVCG - Transactions on Visualization and Computer Graphics,A,"The concept of an intelligent augmented reality (AR) assistant has significant, wide-ranging applications, with potential uses in medicine, military, and mechanics domains. Such an assistant must be able to perceive the environment and actions, reason about the environment state in relation to a given task, and seamlessly interact with the task performer. These interactions typically involve an AR headset equipped with sensors which capture video, audio, and haptic feedback. Previous works have sought to facilitate the development of intelligent AR assistants by visualizing these sensor data streams in conjunction with the assistant's perception and reasoning model outputs. However, existing visual analytics systems do not focus on user modeling or include biometric data, and are only capable of visualizing a single task session for a single performer at a time. Moreover, they typically assume a task involves linear progression from one step to the next. We propose a visual analytics system that allows users to compare performance during multiple task sessions, focusing on non-linear tasks where different step sequences can lead to success. In particular, we design visualizations for understanding user behavior through functional near-infrared spectroscopy (fNIRS) data as a proxy for perception, attention, and memory as well as corresponding motion data (acceleration, angular velocity, and gaze). We distill these insights into embedding representations that allow users to easily select groups of sessions with similar behaviors. We provide two case studies that demonstrate how to use these visualizations to gain insights about task performance using data collected during helicopter copilot training tasks. Finally, we evaluate our approach by conducting an in-depth examination of a think-aloud experiment with five domain experts. © 1995-2012 IEEE.",Application Motivated Visualization; AR/VR/Immersive; Image and Video Data; Mobile; Perception & Cognition; Specialized Input/Display Hardware; Temporal Data,Abstract,True,
Scopus,journalPaper,2024,A Testbed for Studying Cybersickness and its Mitigation in Immersive Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness (CS) represents one of the oldest problems affecting Virtual Reality (VR) technology. In an attempt to resolve or at least limit this form of discomfort, an increasing number of mitigation techniques have been proposed by academic and industrial researchers. However, the validation of such techniques is often carried out without grounding on a common methodology, making the comparison between the various works in the state of the art difficult. To address this issue, the present article proposes a novel testbed for studying CS in immersive VR and, in particular, methods to mitigate it. The testbed consists of four virtual scenarios, which have been designed to elicit CS in a targeted and predictable manner. The scenarios, grounded on available literature, support the extraction of objective metrics about user's performance. The testbed additionally integrates an experimental protocol that employs standard questionnaires as well as measurements typically adopted in state-of-the-art practice to assess levels of CS and other subjective aspects regarding User Experience. The article shows a possible use case of the testbed, concerning the evaluation of a CS mitigation technique that is compared with the absence of mitigation as baseline condition.  © 1995-2012 IEEE.",Cybersickness; evaluation; simulator sickness; taxonomy; testbed; virtual environments; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Towards 360 VR Sickness Mitigation: From Virtual Reality Eye-tracking to Visual Communication,TVCG - Transactions on Visualization and Computer Graphics,A,"Most 360 virtual reality (VR) contents have been developed without considering that users could be affected by VR sickness. Accordingly, users&#x0027; <italic>viewing safety</italic> has been steadily highlighted as a critical problem in the VR market. In this study, we investigate a novel VR sickness mitigation framework based on human visual characteristics for the rendered VR content. First, we build a large-scale 360 VR content database termed VRSP360 (VR Sickness and Presence 360) dedicated to the analysis of VR sickness and thoroughly conduct eye-tracking experiments to measure human perception. In the experiment, we observe that the users&#x0027; gaze distribution is highly center-biased when they experience excessive VR sickness. From this observation, we design a foveated filtering framework that limits high-frequency textures in the peripheral view to mitigate VR sickness. Particularly, given the human visual system&#x0027;s (HVS) non-uniform resolution with respect to the fovea, we also adopt the foveation-based filtering method using the trade-off between sickness mitigation and presence conservation, which reduces any loss in perceptual quality despite the filtering. We further demonstrate that our framework can effectively compress visual information by applying foveated compression. In addition, we develop two metrics (visual texture index and perceptual information index) to measure the effective preservation of user-perceived information despite the filtration of peripheral vision textures by our proposed mitigation method. Through rigorous subjective evaluation on both original content and its VR-sickness-mitigated version, we demonstrate that the proposed framework successfully mitigates VR sickness with a reduction rate of <inline-formula><tex-math notation=""LaTeX"">$\sim$</tex-math></inline-formula>19&#x0025; on the proposed dataset. IEEE",360 Saliency; Databases; Filtering; Foveation; Gaze tracking; Human Visual Perception (HVS); Indexes; Motion sickness; Prevention and mitigation; Visualization; VR sickness mitigation,Title_Abstract,True,
Scopus,journalPaper,2024,An Immersive and Interactive VR Dataset to Elicit Emotions,TVCG - Transactions on Visualization and Computer Graphics,A,"Images and videos are widely used to elicit emotions; however, their visual appeal differs from real-world experiences. With virtual reality becoming more realistic, immersive, and interactive, we envision virtual environments to elicit emotions effectively, rapidly, and with high ecological validity. This work presents the first interactive virtual reality dataset to elicit emotions. We created five interactive virtual environments based on corresponding validated 360Â° videos and validated their effectiveness with 160 participants. Our results show that our virtual environments successfully elicit targeted emotions. Compared with the existing methods using images or videos, our dataset allows virtual reality researchers and practitioners to integrate their designs effectively with emotion elicitation settings in an immersive and interactive way. © 1995-2012 IEEE.",Dataset (https//github.com/HighTemplar-wjiang/VR-Dataset-Emotions); Emotions; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Gaze-Contingent Layered Optical See-Through Displays with a Confidence-Driven View Volume,TVCG - Transactions on Visualization and Computer Graphics,A,"The vergence-accommodation conflict (VAC) presents a major perceptual challenge for head-mounted displays with a fixed image plane. Varifocal and layered display designs can mitigate the VAC. However, the image quality of varifocal displays is affected by imprecise eye tracking, whereas layered displays suffer from reduced image contrast as the distance between layers increases. Combined designs support a larger workspace and tolerate some eye-tracking error. However, any layered design with a fixed layer spacing restricts the amount of error compensation and limits the in-focus contrast. We extend previous hybrid designs by introducing confidence-driven volume control, which adjusts the size of the view volume at runtime. We use the eye tracker's confidence to control the spacing of display layers and optimize the trade-off between the display's view volume and the amount of eye tracking error the display can compensate. In the case of high-quality focus point estimation, our approach provides high in-focus contrast, whereas low-quality eye tracking increases the view volume to tolerate the error. We describe our design, present its implementation as an optical-see head-mounted display using a multiplicative layer combination, and present an evaluation comparing our design with previous approaches. © 1995-2012 IEEE.",Gaze-Contingent Layered Display; Optical See-Through Mixed Reality; Vergence-Accommodation Conflict,Keywords,True,
Scopus,journalPaper,2024,Tasks Reflected in the Eyes: Egocentric Gaze-Aware Visual Task Type Recognition in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"With eye tracking finding widespread utility in augmented reality and virtual reality headsets, eye gaze has the potential to recognize users' visual tasks and adaptively adjust virtual content displays, thereby enhancing the intelligence of these headsets. However, current studies on visual task recognition often focus on scene-specific tasks, like copying tasks for office environments, which lack applicability to new scenarios, e.g., museums. In this paper, we propose four scene-agnostic task types for facilitating task type recognition across a broader range of scenarios. We present a new dataset that includes eye and head movement data recorded from 20 participants while they engaged in four task types across 15 360-degree VR videos. Using this dataset, we propose an egocentric gaze-aware task type recognition method, TRCLP, which achieves promising results. Additionally, we illustrate the practical applications of task type recognition with three examples. Our work offers valuable insights for content developers in designing task-aware intelligent applications. Our dataset and source code are available at zhimin-wang.github.io/TaskTypeRecognition.html. © 1995-2012 IEEE.",deep learning; eye tracking; intelligent application; Virtual reality; visual task type recognition,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Depth Perception in Optical See-Through Augmented Reality: Investigating the Impact of Texture Density, Luminance Contrast, and Color Contrast",TVCG - Transactions on Visualization and Computer Graphics,A,"The immersive augmented reality (AR) system necessitates precise depth registration between virtual objects and the real scene. Prior studies have emphasized the efficacy of surface texture in providing depth cues to enhance depth perception across various media, including the real scene, virtual reality, and AR. However, these studies predominantly focus on black-and-white textures, leaving a gap in understanding the effectiveness of colored textures. To address this gap and further explore texture-related factors in AR, a series of experiments were conducted to investigate the effects of different texture cues on depth perception using the perceptual matching method. Findings indicate that the absolute depth error increases with decreasing contrast under black-and-white texture. Moreover, textures with higher color contrast also contribute to enhanced accuracy of depth judgments in AR. However, no significant effect of texture density on depth perception was observed. The findings serve as a theoretical reference for texture design in AR, aiding in the optimization of virtual-real registration processes. © 1995-2012 IEEE.",Augmented reality; depth perception; OST AR; texture contrast; texture density,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,An Exploratory Expert-Study for Multi-Type Haptic Feedback for Automotive Virtual Reality Tasks,TVCG - Transactions on Visualization and Computer Graphics,A,"Previous research has shown that integrating haptic feedback can improve immersion and realism in automotive VR applications. However, current haptic feedback approaches primarily focus on a single feedback type. This means users must switch between devices to experience haptic stimuli for different feedback types, such as grabbing, collision, or weight simulation. This restriction limits the ability to simulate haptics realistically for complex tasks such as maintenance. To address this issue, we evaluated existing feedback devices based on our requirements analysis to determine which devices are most suitable for simulating these three feedback types. Since no suitable haptic feedback system can simulate all three feedback types simultaneously, we evaluated which devices can be combined. Based on that, we devised a new multi-type haptic feedback system combining three haptic feedback devices. We evaluated the system with different feedback-type combinations through a qualitative expert study involving twelve automotive VR experts. The results showed that combining weight and collision feedback yielded the best and most realistic experience. The study also highlighted technical limitations in current grabbing devices. Our findings provide insights into the effectiveness of haptic device combinations and practical boundaries for automotive virtual reality tasks. © 1995-2012 IEEE.",Haptics; Human Computer Interaction; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"""As if it were my own hand"": inducing the rubber hand illusion through virtual reality for motor imagery enhancement",TVCG - Transactions on Visualization and Computer Graphics,A,"—Brain-computer interfaces (BCI) are widely used in the field of disability assistance and rehabilitation, and virtual reality (VR) is increasingly used for visual guidance of BCI-MI (motor imagery). Therefore, how to improve the quality of electroencephalogram (EEG) signals for MI in VR has emerged as a critical issue. People can perform MI more easily when they visualize the hand used for visual guidance as their own, and the Rubber Hand Illusion (RHI) can increase people’s ownership of the prosthetic hand. We proposed to induce RHI in VR to enhance participants’ MI ability and designed five methods of inducing RHI, namely active movement, haptic stimulation, passive movement, active movement mixed with haptic stimulation, and passive movement mixed with haptic stimulation, respectively. We constructed a first-person training scenario to train participants’ MI ability through the five induction methods. The experimental results showed that through the training, the participants’ feeling of ownership of the virtual hand in VR was enhanced, and the MI ability was improved. Among them, the method of mixing active movement and tactile stimulation proved to have a good effect on enhancing MI. Finally, we developed a BCI system in VR utilizing the above training method, and the performance of the participants improved after the training. This also suggests that our proposed method is promising for future application in BCI rehabilitation systems. © 2024 IEEE.",Assistive technologies; Brain-computer interface; Rubber hand illusion; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Cultural Reflections in Virtual Reality: The Effects of User Ethnicity in Avatar Matching Experiences on Sense of Embodiment,TVCG - Transactions on Visualization and Computer Graphics,A,"—Matching avatar characteristics to a user can impact sense of embodiment (SoE) in VR. However, few studies have examined how participant demographics may interact with these matching effects. We recruited a diverse and racially balanced sample of 78 participants to investigate the differences among participant groups when embodying both demographically matched and unmatched avatars. We found that participant ethnicity emerged as a significant factor, with Asian and Black participants reporting lower total SoE compared to Hispanic participants. Furthermore, we found that user ethnicity significantly influences ownership (a subscale of SoE), with Asian and Black participants exhibiting stronger effects of matched avatar ethnicity compared to White participants. Additionally, Hispanic participants showed no significant differences, suggesting complex dynamics in ethnic-racial identity. Our results also reveal significant main effects of matched avatar ethnicity and gender on SoE, indicating the importance of considering these factors in VR experiences. These findings contribute valuable insights into understanding the complex dynamics shaping VR experiences across different demographic groups. © 2024 IEEE.",avatars; diversity; sense of embodiment; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,Exploring and Modeling Directional Effects on Steering Behavior in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"— Steering is a fundamental task in interactive Virtual Reality (VR) systems. Prior work has demonstrated that movement direction can significantly influence user behavior in the steering task, and different interactive environments (VEs) can lead to various behavioral patterns, such as tablets and PCs. However, its impact on VR environments remains unexplored. Given the widespread use of steering tasks in VEs, including menu adjustment and object manipulation, this work seeks to understand and model the directional effect with a focus on barehand interaction, which is typical in VEs. This paper presents the results of two studies. The first study was conducted to collect behavioral data with four categories: movement time, average movement speed, success rate, and reenter times. According to the results, we examined the effect of movement direction and built the SθModel. We then empirically evaluated the model through the data collected from the first study. The results proved that our proposed model achieved the best performance across all the metrics (r2 > 0.95), with more than 15% improvement over the original Steering Law in terms of prediction accuracy. Next, we further validated the SθModel by another study with the change of device and steering direction. Consistent with previous assessments, the model continues to exhibit optimal performance in both predicting movement time and speed. Finally, based on the results, we formulated design recommendations for steering tasks in VEs to enhance user experience and interaction efficiency. © 2024 IEEE.",barehand interaction; head-mounted display; human performance modeling; steering law; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,To use or not to use viewpoint oscillations when walking in VR &#x003F; State of the art and perspectives,TVCG - Transactions on Visualization and Computer Graphics,A,"Viewpoint oscillations are periodic changes in the position and&#x002F;or orientation of the point of view in a virtual environment. They can be implemented in Virtual Reality (VR) walking simulations to make them feel closer to real walking. This is especially useful in simulations where users remain in place because of space or hardware constraints. As for today, it remains unclear what exact benefit they bring to user experience during walking simulations, and with what characteristics they should be implemented. To answer these questions, we conduct a systematic literature review focusing on five main dimensions of user experience (walking sensation, vection, cybersickness, presence and embodiment) and discuss 44 articles from the fields of VR, Vision, and Human-Computer Interaction. Overall, the literature suggests that viewpoint oscillations benefit vection, and with less evidence, walking sensation and presence. As for cybersickness, the literature contains contrasted results. Based on these results, we recommend using viewpoint oscillations in applications that require accurate distance or speed perception, or that aim to provide compelling walking simulations without a walking avatar, and a particular attention should be paid to cybersickness. Taken together, this work gives recommendations for enhancing walking simulations in VR, which may be applied to entertainment, virtual visits, and medical rehabilitation. IEEE",Cybersickness; cybersickness; embodiment; Legged locomotion; Oscillators; presence; Solid modeling; User experience; vection; Viewpoint oscillations; Virtual environments; virtual reality; Visualization; walking sensation,Abstract_Keywords,True,
Scopus,journalPaper,2024,Multiple Self-Avatar Effect: Effects of Using Diverse Self-Avatars on Memory Acquisition and Retention of Sign-Language Gestures,TVCG - Transactions on Visualization and Computer Graphics,A,"This study proposes a new learning method that employs multiple embodied self-avatars during learning, to use the potential benefit of virtual reality (VR) for effective learning and training. In this study, by taking advantage of the benefit of virtual reality (VR), we propose a new learning method that employs multiple embodied self-avatars during learning. Based on the multiple-context effect, which posits that learning in diverse situations can prevent forgetting and enhance memory retention, we conducted a between-participants study under two conditions: the varied avatar condition, in which participants learned sign languages with different self-avatars in six iterations, and the constant avatar condition, in which the same self-avatar was used consistently. We employed sign language as a learning material that naturally draws attention to self-avatars and is suitable for investigating the effects of varying self-avatars. Initially, the varied avatar condition performed worse than the constant avatar condition. However, in a test conducted after one week in the real world, the varied avatar condition showed significantly less forgetting and better retention than the constant avatar condition. Furthermore, our results suggested a positive correlation between the degree of embodiment toward the avatars and the effectiveness of the proposed method. This study presents an innovative design approach for the use of self-avatars in VR-based education. Authors",Avatar; Avatars; context-dependent memory; Cybersickness; embodiment; learning; Task analysis; Testing; Training; Virtual environments; virtual reality; Visualization,Abstract_Keywords,True,
Scopus,journalPaper,2024,VPRF: Visual Perceptual Radiance Fields for Foveated Image Synthesis,TVCG - Transactions on Visualization and Computer Graphics,A,"Neural radiance fields (NeRF) has achieved revolutionary breakthrough in the novel view synthesis task for complex 3D scenes. However, this new paradigm struggles to meet the requirements for real-time rendering and high perceptual quality in virtual reality. In this paper, we propose VPRF, a novel visual perceptual based radiance fields representation method, which for the first time integrates the visual acuity and contrast sensitivity models of human visual system (HVS) into the radiance field rendering framework. Initially, we encode both the appearance and visual sensitivity information of the scene into our radiance field representation. Then, we propose a visual perceptual sampling strategy, allocating computational resources according to the HVS sensitivity of different regions. Finally, we propose a sampling weight-constrained training scheme to ensure the effectiveness of our sampling strategy and improve the representation of the radiance field based on the scene content. Experimental results demonstrate that our method renders more efficiently, with higher PSNR and SSIM in the foveal and salient regions compared to the state-of-the-art FoV-NeRF. The results of the user study confirm that our rendering results exhibit high-fidelity visual perception. © 1995-2012 IEEE.",Contrast sensitivity; Foveated rendering; Virtual reality; Visual perceptual,Abstract_Keywords,True,
Scopus,journalPaper,2024,Classification of Internal and External Distractions in an Educational VR Environment Using Multimodal Features,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual reality (VR) can potentially enhance student engagement and memory retention in the classroom. However, distraction among participants in a VR-based classroom is a significant concern. Several factors, including mind wandering, external noise, stress, etc., can cause students to become internally and/or externally distracted while learning. To detect distractions, single or multi-modal features can be used. A single modality is found to be insufficient to detect both internal and external distractions, mainly because of individual variability. In this work, we investigated multi-modal features: eye tracking and EEG data, to classify the internal and external distractions in an educational VR environment. We set up our educational VR environment and equipped it for multi-modal data collection. We implemented different machine learning (ML) methods, including k-nearest-neighbors (kNN), Random Forest (RF), one-dimensional convolutional neural network - long short-term memory (1 D-CNN-LSTM), and two-dimensional convolutional neural networks (2D-CNN) to classify participants' internal and external distraction states using the multi-modal features. We performed cross-subject, cross-session, and gender-based grouping tests to evaluate our models. We found that the RF classifier achieves the highest accuracy over 83% in the cross-subject test, around 68% to 78% in the cross-session test, and around 90% in the gender-based grouping test compared to other models. SHAP analysis of the extracted features illustrated greater contributions from the occipital and prefrontal regions of the brain, as well as gaze angle, gaze origin, and head rotation features from the eye tracking data. © 1995-2012 IEEE.",EEG; Eye-tracking; Human-centered computing; Machine Learning,Abstract,True,
Scopus,journalPaper,2024,Evaluating and Modeling the Effect of Frame Rate on Steering Performance in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Prior work has shown that frame rate significantly influences user behavior in fast-response tasks in 2D and 3D contexts. However, its impact on a steering task, which involves navigating an object along a path from the start to the end, remains relatively unexplored, especially in the context of virtual reality (VR). This task is considered a typical non-fast-response activity, as it does not demand rapid reactions within a limited time frame. Our work aims to understand and model users&#x0027; steering behavior and predict movement time with different task complexities and frame rates in VR environments. We first conducted a user study to collect user behavior in a steering task with four factors: frame rate, path length, width, and radius of curvature. Based on the results, we then quantified the effects of frame rate and built two predictive models. Our models exhibited the best fit (<inline-formula><tex-math notation=""LaTeX"">$r^{2}\gt 0.957$</tex-math></inline-formula>) and over 17&#x0025; improvement in prediction accuracy for movement time compared to existing models. Our models&#x0027; robustness was further validated by applying them to predict steering performance with different VR tasks and frame rates. The two models keep the best predictability for both movement time and speed. IEEE",Adaptation models; Behavioral sciences; Computational modeling; frame rate; head-mounted display; human performance modeling; Predictive models; Solid modeling; steering law; Task analysis; Three-dimensional displays; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Scene-aware Foveated Neural Radiance Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"Foveated rendering provides an idea for improving the image synthesis performance of neural radiance fields (NeRF) methods. In this paper, we propose a scene-aware foveated neural radiance fields method to synthesize high-quality foveated images in complex VR scenes at high frame rates. Firstly, we construct a multi-ellipsoidal neural representation to enhance the neural radiance field&#x0027;s representation capability in salient regions of complex VR scenes based on the scene content. Then, we introduce a uniform sampling based foveated neural radiance field framework to improve the foveated image synthesis performance with one-pass color inference, and improve the synthesis quality by leveraging the foveated scene-aware objective function. Our method synthesizes high-quality binocular foveated images at the average frame rate of 66 frames per second (FPS) in complex scenes with high occlusion, intricate textures, and sophisticated geometries. Compared with the state-of-the-art foveated NeRF method, our method achieves significantly higher synthesis quality in both the foveal and peripheral regions with 1.41-1.46&#x00D7; speedup. We also conduct a user study to prove that the perceived quality of our method has a high visual similarity with the ground truth IEEE",Foveated Rendering; Geometry; Neural networks; Neural radiance field; Neural Radiance Fields; Real-time systems; Rendering (computer graphics); Streaming media; Three-dimensional displays; Virtual Reality,Keywords,True,
Scopus,journalPaper,2024,Immersive Study Analyzer: Collaborative Immersive Analysis of Recorded Social VR Studies,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) has become an important tool for conducting behavioral studies in realistic, reproducible environments. In this paper, we present ISA, an Immersive Study Analyzer system designed for the comprehensive analysis of social VR studies. For in-depth analysis of participant behavior, ISA records all user actions, speech, and the contextual environment of social VR studies. A key feature is the ability to review and analyze such immersive recordings collaboratively in VR, through support of behavioral coding and user-defined analysis queries for efficient identification of complex behavior. Respatialization of the recorded audio streams enables analysts to follow study participants' conversations in a natural and intuitive way. To support phases of close and loosely coupled collaboration, ISA allows joint and individual temporal navigation, and provides tools to facilitate collaboration among users at different temporal positions. An expert review confirms that ISA effectively supports collaborative immersive analysis, providing a novel and effective tool for nuanced understanding of user behavior in social VR studies. © 1995-2012 IEEE.",Behavior Analysis; Collaborative Immersive Analytics; Social VR Studies,Abstract,True,
Scopus,journalPaper,2024,CompositingVis: Exploring Interactions for Creating Composite Visualizations in Immersive Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Composite visualization represents a widely embraced design that combines multiple visual representations to create an integrated view. However, the traditional approach of creating composite visualizations in immersive environments typically occurs asynchronously outside of the immersive space and is carried out by experienced experts. In this work, we aim to empower users to participate in the creation of composite visualization within immersive environments through embodied interactions. This could provide a flexible and fluid experience with immersive visualization and has the potential to facilitate understanding of the relationship between visualization views. We begin with developing a design space of embodied interactions to create various types of composite visualizations with the consideration of data relationships. Drawing inspiration from people's natural experience of manipulating physical objects, we design interactions based on the combination of 3D manipulations in immersive environments. Building upon the design space, we present a series of case studies showcasing the interaction to create different kinds of composite visualizations in virtual reality. Subsequently, we conduct a user study to evaluate the usability of the derived interaction techniques and user experience of creating composite visualizations through embodied interactions. We find that empowering users to participate in composite visualizations through embodied interactions enables them to flexibly leverage different visualization views for understanding and communicating the relationships between different views, which underscores the potential of several future application scenarios  © 1995-2012 IEEE.",Composite Visualization; Embodied Interaction; Empowerment; Immersive Analytics,Abstract,True,
Scopus,journalPaper,2024,The Least Increasing Aversion (LIA) Protocol: Illustration on Identifying Individual Susceptibility to Cybersickness Triggers,TVCG - Transactions on Visualization and Computer Graphics,A,"This paper introduces the Least Increase aversion (LIA) protocol to investigate the relative impact of factors that may trigger cybersickness. The protocol is inspired by the Subjective Matching methodology (SMT) from which it borrows the incremental construction of a richer VR experience, except that the full-blown target experience may cause undesired discomfort. In the first session, the participant briefly encounter all factors at the maximum level. Then in the second session they start with the minimum level of all factors as a Baseline. Subsequently, we expect the participant to minimize their exposure to the most adverse factors. This approach ranks the factors from mildest to worst and helps detect individual susceptibility to cybersickness triggers.To validate the applicability of LIA protocol, we further evaluate it with an experiment to identify individual susceptibility to three rotational axes (Yaw, Pitch, and Roll). The findings not only confirm the protocol&#x0027;s capability to accurately discern individual rankings of various factors to cybersickness but also indicate that individual susceptibility is more intricate and multifaceted than initially anticipated. Authors",Avatars; Cybersickness; Cybersickness; individual susceptibility; Motion sickness; Protocols; Sensitivity; virtual reality; Visual perception; Visualization,Keywords,True,
Scopus,journalPaper,2024,Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses,TVCG - Transactions on Visualization and Computer Graphics,A,"Human eye gaze plays a significant role in many virtual and augmented reality (VR&#x002F;AR) applications, such as gaze-contingent rendering, gaze-based interaction, or eye-based activity recognition. However, prior works on gaze analysis and prediction have only explored eye-head coordination and were limited to human-object interactions. We first report a comprehensive analysis of eye-body coordination in various humanobject and human-human interaction activities based on four public datasets collected in real-world (MoGaze), VR (ADT), as well as AR (GIMO and EgoBody) environments. We show that in human-object interactions, e.g. pick and place, eye gaze exhibits strong correlations with full-body motion while in human-human interactions, e.g. chat and teach, a person&#x0027;s gaze direction is correlated with the body orientation towards the interaction partner. Informed by these analyses we then present Pose2Gaze &#x2013; a novel eye-body coordination model that uses a convolutional neural network and a spatio-temporal graph convolutional neural network to extract features from head direction and full-body poses, respectively, and then uses a convolutional neural network to predict eye gaze. We compare our method with state-of-theart methods that predict eye gaze only from head movements and show that Pose2Gaze outperforms these baselines with an average improvement of 24.0&#x0025; on MoGaze, 10.1&#x0025; on ADT, 21.3&#x0025; on GIMO, and 28.6&#x0025; on EgoBody in mean angular error, respectively. We also show that our method significantly outperforms prior methods in the sample downstream task of eyebased activity recognition. These results underline the significant information content available in eye-body coordination during daily activities and open up a new direction for gaze prediction. IEEE",Activity recognition; activity recognition; augmented reality; Convolutional neural networks; Correlation; Eye-body coordination; Feature extraction; gaze prediction; Head; human-human interaction; human-object interaction; Task analysis; Virtual environments; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Effects of Eye Vergence and Accommodation on Interactions with Content on an AR Magic-lens Display and its Surroundings,TVCG - Transactions on Visualization and Computer Graphics,A,"Augmented reality (AR) magic-lens (ML) displays, such as handheld devices, offer a convenient and accessible way to enrich our environment using virtual imagery. Several display technologies, including conventional monocular, less common stereoscopic, and varifocal displays, are currently being used. Vergence and accommodation effects on depth perception, as well as vergence&#x2013;accommodation conflict, have been studied, where users interact only with the content on the display. However, little research exists on how vergence and accommodation influence user performance and cognitive-task load when users interact with the content on a display and its surroundings in a short timeframe. Examples of this are validating augmented instructions before making an incision andperforming general hand-eye coordinated tasks such as grasping augmented objects. To improve interactions with future AR displays in such scenarios, we must improve our understanding of this influence. To this end, we conducted two fundamental visual-acuity user studies with 28 and 27 participants, while investigating eye vergence and accommodation distances on four ML displays. Our findings show that minimizing the accommodation difference between the display and its surroundings is crucial when the gaze between the display and its surroundings shifts rapidly. Minimizing the difference in vergence is more important when viewing the display and its surroundings as a single context without shifting the gaze. Interestingly, the vergence&#x2013;accommodation conflict did not significantly affect the cognitive-task load nor play a pivotal role in the accuracy of interactions with AR ML content and its physical surroundings Authors",Augmented Reality—Human-computer interaction— Video see-through display—Vergence-accommodation; Fatigue; Lenses; Optical imaging; Smart phones; Stereo image processing; Task analysis; Visualization,Abstract_Keywords,True,
Scopus,journalPaper,2024,ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering Perceived Texture Roughness in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Extensive research has been done in haptic feedback for texture simulation in virtual reality (VR). However, it is challenging to modify the perceived tactile texture of existing physical objects which usually serve as anchors for virtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a finger-worn haptic device that uses vibratory-pneumatic feedback to modulate (i.e., increase and decrease) the perceived roughness of the material surface contacted by the user&#x0027;s fingerpad while supporting the perceived sensation of other haptic properties (e.g., temperature or stickiness) in MR. Our device includes a silicone-based pneumatic actuator that can lift the user&#x0027;s fingerpad on the physical surface to reduce the contact area for roughness decreasing, and an on-finger vibrator for roughness increasing. The results of our perceptual study showed that the participants could perceive changes in roughness, both increasing and decreasing, compared to the original material surface. We also observed the overlapping roughness ratings among certain haptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived roughness of some materials without any haptic feedback. This suggests the potential to alter the perceived texture of one type of material to another in terms of roughness (e.g., modifying the perceived texture of ceramics as glass). Lastly, a user study of MR experience showed that ViboPneumo could significantly improve the MR user experience, particularly for visual-haptic matching, compared to the condition of a bare finger. We also demonstrated a few application scenarios for ViboPneumo. IEEE",AR; Haptic interfaces; haptic perception; Modulation; pneumatic devices; Rough surfaces; roughness; Surface roughness; Surface texture; Vibrations; Virtual reality; VR; wearable haptics,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A novel AR recoloring technique to enhance operator performance on inspection tasks in Industry 4.0 environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Over the past few years, the manufacturing industry has increasingly embraced Augmented Reality (AR) for inspecting real products, yet faces challenges in visualization modalities. In fact, AR content presentation significantly impacts user performance, especially when virtual object colors lack real-world context. Additionally, the lack of studies in this area compounds uncertainty about visualization effects on user performance in inspection tasks. This study introduces a novel AR recoloring technique to enhance user performance during industrial assembly inspection tasks. This technique automatically recolors virtual components based on their physical counterparts, improving distinctiveness. Experimental comparisons with AR experts and representative users, using objective and subjective metrics, demonstrate the proposed AR recoloring technique enhances task performance and reduces mental burden during inspection activities. This innovative approach outperforms established methods like CAD and random modes, showcasing its potential for advancing AR applications in manufacturing, particularly in the inspection of products. Authors",AR recoloring techniques; Assembly; Augmented Reality; Cognitive load; Color; Image color analysis; Industrial assembly inspection; Industry 4.0; Inspection; Three-dimensional displays; Videos; Visualization; Visualization modalities,Abstract_Keywords,True,
Scopus,journalPaper,2024,Enabling Predictive Redirection Reset Based on Virtual-Real Spatial Probability Density Distributions,TVCG - Transactions on Visualization and Computer Graphics,A,"Redirected walking (RDW) allows users to explore vast virtual spaces by walking in confined real spaces, yet suffers from frequent boundary collisions due to physical constraints. The major solution is to use the reset strategy to steer users away from boundaries. However, most reset methods guide users to fixed spots or follow constant patterns, neglecting spatial features and users&#x0027; movement trends. In this paper, we propose an innovative predictive reset method based on spatial probability density distribution to jointly involve impacts of spatial feature and walking intention for forecasting the user&#x0027;s possible positional distribution, and thereby determines the optimal reset direction by maximizing walking expectation. Given a space, we calculate the stationary layout energy to indicate traveling difficulties of all positions. Meanwhile, we exploit a novel intention inference model to anticipate the probability distribution of the user&#x0027;s presence across adjacent positions. Furthermore, we incorporate the obstacle energy attenuation to predict the obstacle avoidance behaviors. All aforementioned factors are amalgamated into a potential region energy map, and then we integrate energy maps of virtual and real spaces into a fusion energy map to enable the prediction considering both spaces simultaneously. Thus, the optimal reset direction is derived by maximizing the fusion energy. Simulation and user studies are conducted on a broad dataset containing plentiful virtual and real spaces. The results demonstrate that our method effectively reduces the physical collisions and increase the continuous walking distance compared to prevalent reset methods, while exhibiting superior applicability when combined with various RDW controllers. The source code and dataset are available at https:&#x002F;&#x002F;github.com&#x002F;huiyuroy&#x002F;Return2MaxPotentialEnergy. IEEE",Aerospace electronics; Layout; Legged locomotion; Market research; Motion Planning; Obstacle Avoidance; Predictive models; Probability Density; Redirected Walking; Space exploration; Trajectory; Virtual Reality,Keywords,True,
Scopus,journalPaper,2024,Every &#x201C;Body&#x201D; Gets a Say: An Augmented Optimization Metric to Preserve Body Pose during Avatar Adaptation in Mixed/Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"User-Avatar interaction within augmented reality applications is rapidly increasing in frequency. Applications routinely place users in rooms with other, remote users embodied by photorealistic avatars, or require users to work with an avatar of a remote user to complete a task. During these types of interactions, it is often required to modify or redirect the posture of an avatar to achieve goals such as contact with or pointing at an object or maintaining eye gaze with the local user. A key limitation of modern redirection techniques is successfully preserving body posture, a critical component of nonverbal communication. This paper presents a new pose-preserving objective function to be used in the multi-objective optimization of an avatar&#x0027;s kinematic configuration. This objective function not only mimics the correct placement of body joints, but also preserves their orientation in space. We have tested this approach against several commonly used and current state-of-the-art redirection techniques and have found that our new approach achieves a significant reduction in targeted redirection error while simultaneously reducing body posture error. Additionally, human subject testing has shown that our new technique provides both a significantly more natural looking redirection and a significantly more realistic and believable overall body posture. Authors",Avatars; Collaboration; Collaborative Augmented Reality (CAR); MIMICs; Motion capture; Multi-Objective Optimization (MOO); Optimization; Performance evaluation; Pose Preservation; Social Presence; Task analysis,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Proxy Importance Based Haptic Retargeting With Multiple Props in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"In virtual reality applications, in addition to visual feedback, real objects can be used as props for virtual objects to provide passive haptic feedback, which greatly enhances user immersion. Usually, real object props are not one-to-one correspondence with virtual objects. Haptic retargeting technique is proposed to establish the virtual-real correspondence by introducing an offset between the virtual hand and the real hand. Sometimes, the offset is too large to cause user discomfort, and it is necessary to introduce a reset between two haptic retargeting operations to force the virtual hand and the real hand to coincide in order to eliminate the offset. However, too many resets can interfere with this immersion. To address this problem, we propose a haptic retargeting method based on proxy importance calculation using multiple props in virtual reality. The concept of proxy importance for props is introduced first, and then a proxy importance based prop selection and placement method for moving virtual objects are proposed. We also improve the performance of our method by using the props&#x0027; weighted proxy importance strategy for multi-user collaboration. Compared to the state-of-the-art methods, our method significantly reduces the number of resets, the task completion time, hand movement distances, and task load without the cost of cybersickness in the single-user task. In the multi-user collaborative task, our method also achieves significant improvement using the strategy that weights the proxy importance of the props. IEEE",Avatars; Collaboration; Hand redirection; Haptic interfaces; haptic retargeting; perception; reset techniques; Shape; Task analysis; Virtual environments; virtual reality; Visualization,Abstract_Keywords,True,
Scopus,journalPaper,2024,3D Gaussian Splatting as New Era: A Survey,TVCG - Transactions on Visualization and Computer Graphics,A,"3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF). This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few. Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year. We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section. IEEE",3D Gaussian Splatting; computer graphics; generation; Image color analysis; Image reconstruction; manipulation; perception; reconstruction; rendering; Rendering (computer graphics); Reviews; Surveys; Three-dimensional displays; Videos; virtual humans,Abstract,True,
Scopus,journalPaper,2024,Designing a Virtual Toolkit for Analysing Planetary Science Data in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"To understand the surface evolution and potential habitability of other planets we must analyse their geology &#x2013; the 3D structure and chemistry of the rocks that are exposed at the surface. Although rovers capture this 3D structure using stereo camera systems and other instruments, when we present this information to mission scientists for analysis it is generally confined to the 2D plane of a computer screen, and the spatial information is lost at the point when it is needed most. To address this problem, we design, develop, and evaluate a prototype Virtual Environment to present geological data in the 3D form in which it was originally captured, and users are supplied with a toolkit for measurement and annotation of data. We observed that users were inspired by the environment and felt more connected to it because they could move within the data; they valued the tools but did not trust the scale and therefore did not always trust the results. We conclude with recommendations for others working in this application area, and pose a series of questions for future research. IEEE",3D perception; Extraterrestrial measurements; Geology; Instruments; Mars; mixed-method study; Prototypes; Spatial databases; Three-dimensional displays; virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,A Real-time Method for Inserting Virtual Objects into Neural Radiance Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"We present the first real-time method for inserting a rigid virtual object into a neural radiance field (NeRF), which produces realistic lighting and shadowing effects, as well as allows interactive manipulation of the object. By exploiting the rich information about lighting and geometry in a NeRF, our method overcomes several challenges of object insertion in augmented reality. For lighting estimation, we produce accurate and robust incident lighting that combines the 3D spatially-varying lighting from NeRF and an environment lighting to account for sources not covered by the NeRF. For occlusion, we blend the rendered virtual object with the background scene using an opacity map integrated from the NeRF. For shadows, with a precomputed field of spherical signed distance fields, we query the visibility term for any point around the virtual object, and cast soft, detailed shadows onto 3D surfaces. Compared with state-of-the-art techniques, our approach can insert virtual objects into scenes with superior fidelity, and has great potential to be further applied to augmented reality systems. IEEE",all-frequency rendering; augmented reality; Estimation; Geometry; Lighting; Neural radiance field; Real-time systems; Rendering (computer graphics); Runtime; shadow; Task analysis,Abstract_Keywords,True,
Scopus,journalPaper,2024,Multisource Differential Fusion Driven Monocular Endoscope Hybrid 3-D Tracking for Advanced Endoscopic Navigation Surgery,TVCG - Transactions on Visualization and Computer Graphics,A,"Surgical navigation systems involve various technologies of segmentation, calibration, registration, tracking, and visualization. These systems aim to superimpose multisource information in the surgical field and provide surgeons with a composite overlay (augmented-reality) view, improving the operative precision and experience. Surgical 3-D tracking is the key to build these systems. Unfortunately, surgical 3-D tracking is still a challenge to endoscopic and robotic navigation systems and easily gets trapped in image artifacts, tissue deformation, and inaccurate positional (e.g., electromagnetic) sensor measurements. This work explores a new monocular endoscope hybrid 3-D tracking method called spatially constrained adaptive differential evolution that combines two spatial constraints with observation-recall adaptive propagation and observation-based fitness computing for stochastic optimization. Specifically, we spatially constraint inaccurate electromagnetic sensor measurements to the centerline of anatomical tubular structures to keep them physically locating inside the tubes, as well as interpolate these measurements to reduce jitter errors for smooth 3-D tracking. We then propose observation-recall adaptive propagation with fitness computing to precisely fuse the constrained sensor measurements, preoperative images, and endoscopic video sequences for accurate hybrid 3-D tracking. Additionally, we also propose a new marker-free hybrid registration strategy to precisely align positional sensor measurements to preoperative images. Our new framework was evaluated on a large amount of clinical data acquired from various surgical endoscopic procedures, with the experimental results showing that it certainly outperforms current surgical 3-D approaches. In particular, the position and rotation errors were significantly reduced from (6.55, 11.4) to (3.02 mm, 8.54<inline-formula><tex-math notation=""LaTeX"">$^\circ$</tex-math></inline-formula>). IEEE",augmented reality; differential evolution; Endoscopes; endoscopy; Monocular 3-D tracking; multisource fusion; Navigation; Optical imaging; Optical sensors; Robot sensing systems; Surgery; surgical navigation; Tracking,Keywords,True,
Scopus,journalPaper,2024,Effects of Focal Distance on Near-Field Depth Perception and Accommodative Response in a VariFocal Optical See-Through Augmented Reality Display,TVCG - Transactions on Visualization and Computer Graphics,A,"Through a human-subject experiment, we investigated the effects of focal distance on depth perception and accommodative response in an optical see-through augmented reality (AR) display. The display was able to provide focus cues and was rigorously calibrated. The near-field distances ranging between 3 diopters and 1 diopter were considered as target distance. In the experiment, it was found that the perceived depth of a virtual object was significantly biased along with the focal distance of virtual image plane of the display. In addition, the experimental results implied that the perceived depth of a virtual object would be potentially more accurate in the condition where the focal distance of virtual image plane was consistent with the target distance than in the conditions where it could deviate from the target distance. Regarding accommodative response, it was found that the response to a virtual object changed along with the focal distance of virtual image plane as well as the target distance. However, the changing rate depending on target distance was less steep in the conditions where the focal distance could be mismatched with the target distance than in the condition where it was consistent with the target distance. In the consistent condition, the changing rate of accommodative responses to virtual objects were similar to that for their physical counterparts. IEEE",Accommodative response; Adaptive optics; augmented reality; Calibration; depth perception; focal distance; focus cue; Optical imaging; optical see-through head-mounted display; Stereo image processing; stereoscopic display; Task analysis; Three-dimensional displays; vergenceaccommodation conflict; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Recall impairment in a different environmental context from learning is called context-dependent forgetting. Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization. Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory. Conventionally, these methods have been validated by switching between physical rooms. However, in this study, we use immersive virtual environments (IVEs) as the environmental context assisted by virtual reality (VR), which is known for its low cost and high reproducibility compared to traditional manipulation. Whereas most existing studies using VR have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE. Furthermore, we are the first to address decontextualization using VR. Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs. Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs. These findings contribute to the design of IVEs for VR-based applications, particularly in educational settings. Authors",360-degree video; decontextualization effect; Encoding; environmental context-dependent memory; forgetting; Memory management; reinstatement effect; Task analysis; Testing; Time measurement; Urban areas; Virtual environments; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Augmented Reality-based Contextual Guidance through Surgical Tool Tracking in Neurosurgery,TVCG - Transactions on Visualization and Computer Graphics,A,"External ventricular drain (EVD) is a common, yet challenging neurosurgical procedure of placing a catheter into the brain ventricular system that requires prolonged training for surgeons to improve the catheter placement accuracy. In this paper, we introduce NeuroLens, an Augmented Reality (AR) system that provides neurosurgeons with guidance that aides them in completing an EVD catheter placement. NeuroLens builds on prior work in AR-assisted EVD to present a registered hologram of a patient&#x0027;s ventricles to the surgeons, and uniquely incorporates guidance on the EVD catheter&#x0027;s trajectory, angle of insertion, and distance to the target. The guidance is enabled by tracking the EVD catheter. We evaluate NeuroLens via a study with 33 medical students and 9 neurosurgeons, in which we analyzed participants&#x0027; EVD catheter insertion accuracy and completion time, eye gaze patterns, and qualitative responses. Our study, in which NeuroLens was used to aid students and surgeons in inserting an EVD catheter into a realistic phantom model of a human head, demonstrated the potential of NeuroLens as a tool that will aid and educate novice neurosurgeons. On average, the use of NeuroLens improved the EVD placement accuracy of the year 1 students by 39.4&#x0025;, of the year 2<inline-formula><tex-math notation=""LaTeX"">$-$</tex-math></inline-formula>4 students by 45.7&#x0025;, and of the neurosurgeons by 16.7&#x0025;. Furthermore, students who focused more on NeuroLens-provided contextual guidance achieved better results, and novice surgeons improved more than the expert surgeons with NeuroLens&#x0027;s assistance. IEEE",Augmented reality; Biomedical optical imaging; Catheters; contextual guidance; image registration; Image registration; Neurosurgery; neurosurgery; Optical imaging; Phantoms; tool tracking; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Impact of Background, Foreground, and Manipulated Object Rendering on Egocentric Depth Perception in Virtual and Augmented Indoor Environments",TVCG - Transactions on Visualization and Computer Graphics,A,"This research investigated how the similarity of the rendering parameters of background and foreground objects affected egocentric depth perception in indoor virtual and augmented environments. We refer to the similarity of the rendering parameters as visual &#x2018;congruence&#x2019;. Study participants manipulated the depth of a sphere to match the depth of a designated target peg. In the first experiment, the sphere and peg were both virtual, while in the second experiment, the sphere is virtual and the peg is real. In both experiments, depth perception accuracy was found to depend on the levels of realism and congruence between the sphere, pegs, and background. In Experiment 1, realistic backgrounds lead to overestimation of depth, but resulted in underestimation when the background was virtual, and when depth cues were applied to the sphere and target peg. In Experiment 2, background and target pegs were real but matched with the virtual sphere; in comparison to Experiment 1, realistically rendered targets prompted an underestimation and more accuracy with the manipulated object. These findings suggest that congruence can affect distance estimation and the underestimation effect in the AR environment resulted from increased graphical fidelity of the foreground target and background. IEEE",Augmented reality; Biomedical imaging; depth perception; egocentric; Estimation; Legged locomotion; perceptual matching; rendering; Rendering (computer graphics); Task analysis; Virtual environments; virtual reality; Visualization,Keywords,True,
Scopus,journalPaper,2024,Come Look at This: Supporting Fluent Transitions between Tightly and Loosely Coupled Collaboration in Social Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Collaborative work in social virtual reality often requires an interplay of loosely coupled collaboration from different virtual locations and tightly coupled face-to-face collaboration. Without appropriate system mediation, however, transitioning between these phases requires high navigation and coordination efforts. In this paper, we present an interaction system that allows collaborators in virtual reality to seamlessly switch between different collaboration models known from related work. To this end, we present collaborators with functionalities that let them work on individual sub-tasks in different virtual locations, consult each other using asymmetric interaction patterns while keeping their current location, and temporarily or permanently join each other for face-to-face interaction. We evaluated our methods in a user study with 32 participants working in teams of two. Our quantitative results indicate that delegating the target selection process for a long-distance teleport significantly improves placement accuracy and decreases task load within the team. Our qualitative user feedback shows that our system can be applied to support flexible collaboration. In addition, the proposed interaction sequence received positive evaluations from teams with varying VR experiences. IEEE",3D User Interfaces; Collaboration; Collaborative Interfaces; Groupwork; Multi-User Environments; Navigation; Social VR; Task analysis; Teleportation; Three-dimensional displays; Virtual environments; Virtual reality; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Development and Evaluation of a Treadmill-based Video-see-through and Optical-see-through Mixed Reality Systems for Obstacle Negotiation Training,TVCG - Transactions on Visualization and Computer Graphics,A,"Mixed reality (MR) technologies have a high potential to enhance obstacle negotiation training beyond the capabilities of existing physical systems. Despite such potential, the feasibility of using MR for obstacle negotiation on typical training treadmill systems and its effects on obstacle negotiation performance remains largely unknown. This research bridges this gap by developing an MR obstacle negotiation training system deployed on a treadmill, and implementing two MR systems with a video see-through (VST) and an optical see-through (OST) Head Mounted Displays (HMDs). We investigated the obstacle negotiation performance with virtual and real obstacles. The main outcomes show that the VST MR system significantly changed the parameters of the leading foot in cases of Box obstacle (approximately 22 cm to 30 cm for stepping over 7cm-box), which we believe was mainly attributed to the latency difference between the HMDs. In the condition of OST MR HMD, users tended to not lift their trailing foot for virtual obstacles (approximately 30 cm to 25 cm for stepping over 7cm-box). Our findings indicate that the low-latency visual contact with the world and the user&#x0027;s body is a critical factor for visuo-motor integration to elicit obstacle negotiation. IEEE",Belts; Foot; Legged locomotion; Resists; Training; Virtual environments; Visualization,Title_Abstract,True,
Scopus,journalPaper,2024,Unified Cross-Structural Motion Retargeting for Humanoid Characters,TVCG - Transactions on Visualization and Computer Graphics,A,"Motion retargeting for animation characters has potential applications in fields such as animation production and virtual reality. However, current methods either assume that the source and target characters have the same skeletal structure, or require designing and training specific model architectures for each structure. In this paper, we aim to address the challenge of motion retargeting across previously unseen skeletal structures with a unified dynamic graph network. The proposed approach utilizes a dynamic graph transformation module to dynamically transfer latent motion features to different structures. We also take into consideration for intricate hand movements and model both torso and hand joints as graphs in a unified manner for whole-body motion retargeting. Our model allows the use of motion data from different structures to train a unified model and learns cross-structural motion retargeting in an unsupervised manner with unpaired data. Experimental results demonstrate the superiority of the proposed method in terms of data efficiency and performance on both seen and unseen structures. IEEE",Animation; Character animation; Decoding; deep learning; Dynamics; Kinematics; motion retargeting; Optimization; Task analysis; Training,Abstract,True,
Scopus,journalPaper,2024,"A Comparison of Virtual Reality Menu Archetypes: Raycasting, Direct Input, and Marking Menus",TVCG - Transactions on Visualization and Computer Graphics,A,"We contribute an analysis of the prevalence and relative performance of archetypal VR menu techniques. An initial survey of 108 menu interfaces in 84 popular commercial VR applications establishes common design characteristics. These characteristics motivate the design of raycast, direct, and marking menu archetypes, and a two-experiment comparison of their relative performance with one and two levels of hierarchy using 8 or 24 items. With a single-level menu, direct input is the fastest interaction technique in general, and is unaffected by number of items. With a two-level hierarchical menu, marking is fastest regardless of item number. Menus using raycasting, the most common menu interaction technique, were among the slowest of the tested menus but were rated most consistently usable. Using the combined results, we provide design and implementation recommendations with applications to general VR menu design. IEEE",direct input; interaction techniques; marking menu; menus; raycasting; virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,AudioGest: Gesture-based Interaction for Virtual Reality using Audio Devices,TVCG - Transactions on Visualization and Computer Graphics,A,"Current virtual reality (VR) system takes gesture interaction based on camera, handle and touch screen as one of the mainstream interaction methods, which can provide accurate gesture input for it. However, limited by application forms and the volume of devices, these methods cannot extend the interaction area to such surfaces as walls and tables. To address the above challenge, we propose AudioGest, a portable, plug-and-play system that detects the audio signal generated by finger tapping and sliding on the surface through a set of microphone devices without extensive calibration. First, an audio synthesis-recognition pipeline based on micro-contact dynamics simulation is constructed to generate modal audio synthesis from different materials and physical properties. Then the accuracy and effectiveness of the synthetic audio are verified by mixing the synthetic audio with real audio proportionally as the training sets. Finally, a series of desktop office applications are developed to demonstrate the application potential of AudioGest&#x0027;s scalability and versatility in VR scenarios. IEEE",audio synthesis; Data acquisition; Dynamics; gesture interaction; Human computer interaction; Microphones; Pipelines; Rough surfaces; Surface roughness; Training; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Memory Recall for Data Visualizations in Mixed Reality, Virtual Reality, 3D and 2D",TVCG - Transactions on Visualization and Computer Graphics,A,"This article explores how the ability to recall information in data visualizations depends on the presentation technology. Participants viewed 10 Isotype visualizations on a 2D screen, in 3D, in Virtual Reality (VR) and in Mixed Reality (MR). To provide a fair comparison between the three 3D conditions, we used LIDAR to capture the details of the physical rooms, and used this information to create our textured 3D models. For all environments, we measured the number of visualizations recalled and their order (2D) or spatial location (3D, VR, MR). We also measured the number of syntactic and semantic features recalled. Results of our study show increased recall and greater richness of data understanding in the MR condition. Not only did participants recall more visualizations and ordinal/spatial positions in MR, but they also remembered more details about graph axes and data mappings, and more information about the shape of the data. We discuss how differences in the spatial and kinesthetic cues provided in these different environments could contribute to these results, and reasons why we did not observe comparable performance in the 3D and VR conditions.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Data visualisation; human memory; locomotion; mixed reality; perception; recall; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Stress Assessment for Augmented Reality Applications Based on Head Movement Features,TVCG - Transactions on Visualization and Computer Graphics,A,"Augmented reality is one of the enabling technologies of the upcoming future. Its usage in working and learning scenarios may lead to a better quality of work and training by helping the operators during the most crucial stages of processes. Therefore, the automatic detection of stress during augmented reality experiences can be a valuable support to prevent consequences on people's health and foster the spreading of this technology. In this work, we present the design of a non-invasive stress assessment approach. The proposed system is based on the analysis of the head movements of people wearing a Head Mounted Display while performing stress-inducing tasks. First, we designed a subjective experiment consisting of two stress-related tests for data acquisition. Then, a statistical analysis of head movements has been performed to determine which features are representative of the presence of stress. Finally, a stress classifier based on a combination of Support Vector Machines has been designed and trained. The proposed approach achieved promising performances thus paving the way for further studies in this research direction.  © 1995-2012 IEEE.",Augmented reality; machine learning classifier; stress detection,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Bimanual Ultrasound Mid-Air Haptics for Virtual Reality Manipulation,TVCG - Transactions on Visualization and Computer Graphics,A,"The ability to manipulate and physically feel virtual objects without any real object being present and without equipping the user has been a long-standing goal in virtual reality (VR). Emerging ultrasound mid-air haptics (UMH) technology could potentially address this challenge, as it enables remote tactile stimulation of unequipped users. However, to date, UMH has received limited attention in the field of haptic exploration and manipulation in virtual environments. Existing work has primarily focused on interactions requiring a single hand and thus the delivery of unimanual haptic feedback. Despite being fundamental to a large part of haptic interactions with our environments, bimanual tasks have rarely been studied in the field of UMH interaction in VR. In this paper, we propose the use of non-coplanar mid-air haptic devices for providing simultaneous tactile feedback to both hands during bimanual VR manipulation. We discuss coupling schemes and haptic rendering algorithms for providing bimanual haptic feedback in bimanual interactions with virtual environments. We then present two human participant studies, assessing the benefits of bimanual ultrasound haptic feedback in a two-handed grasping and holding task and in a shape exploration task. Results suggest that the use of multiple non-coplanar UMH devices could be an interesting approach for enriching unencumbered haptic manipulation in virtual environments. IEEE",bimanual; Grasping; Haptic interfaces; Mid-air haptics; Shape; Task analysis; Three-dimensional displays; Ultrasonic imaging; ultrasound; Virtual environments; VR,Title_Abstract,True,
Scopus,journalPaper,2024,F-RDW: Redirected Walking With Forecasting Future Position,TVCG - Transactions on Visualization and Computer Graphics,A,"In order to serve better VR experiences to users, existing predictive methods of Redirected Walking (RDW) exploit future information to reduce the number of reset occurrences. However, such methods often impose a precondition during deployment, either in the virtual environment&#x0027;s layout or the user&#x0027;s walking direction, which constrains its universal applications. To tackle this challenge, we propose a mechanism <italic>F-RDW</italic> that is twofold: (1) forecasts the future information of a user in the virtual space without any assumptions by using the conventional method, and (2) fuse this information while maneuvering existing RDW methods. The backbone of the first step is an LSTM-based model that ingests the user&#x0027;s spatial and eye-tracking data to predict the user&#x0027;s future position in the virtual space, and the following step feeds those predicted values into existing RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respecting their internal mechanism in applicable ways. The results of our simulation test and user study demonstrate the significance of future information when using RDW in small physical spaces or complex environments. We prove that the proposed mechanism significantly reduces the number of resets and increases the traveled distance between resets, hence augmenting the redirection performance of all RDW methods explored in this work. Our project and dataset are available at <uri>https://github.com/YonseiCGnA-VR/F-RDW.</uri> IEEE",Eye tracking; Gaze tracking; Layout; Legged locomotion; path planning; path prediction; Prediction algorithms; Predictive models; redirected walking; Spatial databases; Virtual environments; virtual reality,Keywords,True,
Scopus,journalPaper,2024,Visual Guidance for User Placement in Avatar-Mediated Telepresence between Dissimilar Spaces,TVCG - Transactions on Visualization and Computer Graphics,A,"Rapid advances in technology gradually realize immersive mixed-reality (MR) telepresence between distant spaces. This paper presents a novel visual guidance system for avatar-mediated telepresence, directing users to optimal placements that facilitate the clear transfer of gaze and pointing contexts through remote avatars in dissimilar spaces, where the spatial relationship between the remote avatar and the interaction targets may differ from that of the local user. Representing the spatial relationship between the user/avatar and interaction targets with angle-based interaction features, we assign recommendation scores of sampled local placements as their maximum feature similarity with remote placements. These scores are visualized as color-coded 2D sectors to inform the users of better placements for interaction with selected targets. In addition, virtual objects of the remote space are overlapped with the local space for the user to better understand the recommendations. We examine whether the proposed score measure agrees with the actual user perception of the partner's interaction context and find a score threshold for recommendation through user experiments in virtual reality (VR). A subsequent user study in VR investigates the effectiveness and perceptual overload of different combinations of visualizations. Finally, we conduct a user study in an MR telepresence scenario to evaluate the effectiveness of our method in real-world applications.  © 1995-2012 IEEE.",Mixed reality; telepresence; virtual avatar; visualization,Abstract_Keywords,True,
Scopus,journalPaper,2024,RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in Dynamic Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"It is typically challenging for visual or visual-inertial odometry systems to handle the problems of dynamic scenes and pure rotation. In this work, we design a novel visual-inertial odometry (VIO) system called RD-VIO to handle both of these two problems. First, we propose an IMU-PARSAC algorithm which can robustly detect and match keypoints in a two-stage process. In the first state, landmarks are matched with new keypoints using visual and IMU measurements. We collect statistical information from the matching and then guide the intra-keypoint matching in the second stage. Second, to handle the problem of pure rotation, we detect the motion type and adapt the deferred-triangulation technique during the data-association process. We make the pure-rotational frames into the special subframes. When solving the visual-inertial bundle adjustment, they provide additional constraints to the pure-rotational motion. We evaluate the proposed VIO system on public datasets and online comparison. Experiments show the proposed RD-VIO has obvious advantages over other methods in dynamic environments.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Degenerate motion; dynamic environment; RANSAC; SLAM; VIO,Title,True,
Scopus,journalPaper,2024,Impact of Socio-Demographic Attributes and Mutual Gaze of Virtual Humans on Users' Visual Attention and Collision Avoidance in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"This study investigated the extent that the non-verbal behaviors of virtual humans (VHs) and their socio-demographic attributes altered users' collision avoidance behaviors in Virtual Reality (VR). Users interacted with VHs representing different levels of ethnicities and gender, exhibiting different conditions of physical movement, and gaze behaviors. The VHs were depicted in three major ethnic conditions namely Asian, Caucasian, and Black. The physical movement states of the VHs were either static in the path of the user or walking toward the user in the opposite direction. The non-verbal gaze behavior of the VHs was either direct gaze or averted gaze. We used an HTC Vive tracking system to track users' performing real walking while we collected objective measures (i,e., continuous gaze, fixation gaze, clearance distance, and travel length), and subjective variables (i.e., game experiences and social presence). The results showed that the ethnicity of the VHs significantly impacted the gaze behavior of the users, and the gender of the VHs affected the user avoidance movement and their reciprocal gaze behavior. Our results revealed that users' physical movement, gaze behaviors, and collision avoidance were moderated by the VHs' perceived ethnicity, gender, and gaze behaviors. Understanding the impact of the socio-demographics attributes of VHs and their gaze behavior on users' collision avoidance is critical for applications in which users are navigating through virtual traffic, crowd, and other inter-personal simulations.  © 1995-2012 IEEE.",Collision avoidance; human-computer interaction; user studies; virtual humans and crowds; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Comparative Analysis of Interactive Modalities for Intuitive Endovascular Interventions,TVCG - Transactions on Visualization and Computer Graphics,A,"Endovascular intervention is a minimally invasive method for treating cardiovascular diseases. Although fluoroscopy, known for real-time catheter visualization, is commonly used, it exposes patients and physicians to ionizing radiation and lacks depth perception due to its 2D nature. To address these limitations, a study was conducted using teleoperation and 3D visualization techniques. This <italic>in-vitro</italic> study involved the use of a robotic catheter system and aimed to evaluate user performance through both subjective and objective measures. The focus was on determining the most effective modes of interaction. Three interactive modes for guiding robotic catheters were compared in the study: 1) Mode GM, using a gamepad for control and a standard 2D monitor for visual feedback; 2) Mode GH, with a gamepad for control and HoloLens providing 3D visualization; and 3) Mode HH, where HoloLens serves as both control input and visualization device. Mode GH outperformed other modalities in subjective metrics, except for mental demand. It exhibited a median tracking error of 4.72 mm, a median targeting error of 1.01 mm, a median duration of 82.34 s, and a median natural logarithm of dimensionless squared jerk of 40.38 in the <italic>in-vitro</italic> study. Mode GH showed 8.5&#x0025;, 4.7&#x0025;, 6.5&#x0025;, and 3.9&#x0025; improvements over Mode GM and 1.5&#x0025;, 33.6&#x0025;, 34.9&#x0025;, and 8.1&#x0025; over Mode HH for tracking error, targeting error, duration, and dimensionless squared jerk, respectively. To sum up, the user study emphasizes the potential benefits of employing HoloLens for enhanced 3D visualization in catheterization. The user study also illustrates the advantages of using a gamepad for catheter teleoperation, including user-friendliness and passive haptic feedback, compared to HoloLens. To further gauge the potential of using a more traditional joystick as a control input device, an additional study utilizing the Haption VirtuoseTM robot was conducted. It reveals the potential for achieving smoother trajectories, with a 38.9&#x0025; reduction in total path length compared to a gamepad, potentially due to its larger range of motion and single-handed control. Authors",Augmented Reality; Catheter Navigation; Catheters; Endovascular Intervention; Input devices; Navigation; Resists; Robotic Catheter; Robots; Three-dimensional displays; User study; Visualization,Keywords,True,
Scopus,journalPaper,2024,Simple and Efficient? Evaluation of Transitions for Task-Driven Cross-Reality Experiences,TVCG - Transactions on Visualization and Computer Graphics,A,"The inquiry into the impact of diverse transitions between cross-reality environments on user experience remains a compelling research endeavor. Existing work often offers fragmented perspectives on various techniques or confines itself to a singular segment of the reality-virtuality spectrum, be it virtual reality or augmented reality. This study embarks on bridging this knowledge gap by systematically assessing the effects of six prevalent transitions while users remain immersed in tasks spanning both virtual and physical domains. In particular, we investigate the effect of different transitions while the user is continuously engaged in a demanding task instead of purely focusing on a given transition. As a preliminary step, we evaluate these six transitions within the realm of pure virtual reality to establish a baseline. Our findings reveal a clear preference among participants for brief and efficient transitions in a task-driven experience, instead of transitions that prioritize interactivity and continuity. Subsequently, we extend our investigation into a cross-reality context, encompassing transitions between virtual and physical environments. Once again, our results underscore the prevailing preference for concise and effective transitions. Furthermore, our research offers intriguing insights about the potential mitigation of visual incoherence between virtual and augmented reality environments by utilizing different transitions.  © 1995-2012 IEEE.",Artificial; augmented; graphical user interfaces; user interfaces; virtual realities,Abstract,True,
Scopus,journalPaper,2024,DRCmpVis: Visual Comparison of Physical Targets in Mobile Diminished and Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Numerous physical objects in our daily lives are grouped or ranked according to a stereotyped presentation style. For example, in a library, books are typically grouped and ranked based on classification numbers. However, for better comparison, we often need to re-group or re-rank the books using additional attributes such as ratings, publishers, comments, publication years, keywords, prices, etc., or a combination of these factors. In this article, we propose a novel mobile DR/MR-based application framework named DRCmpVis to achieve in-context multi-attribute comparisons of physical objects with text labels or textual information. The physical objects are scanned in the real world using mobile cameras. All scanned objects are then segmented and labeled by a convolutional neural network and replaced (diminished) by their virtual avatars in a DR environment. We formulate three visual comparison strategies, including filtering, re-grouping, and re-ranking, which can be intuitively, flexibly, and seamlessly performed on their avatars. This approach avoids breaking the original layouts of the physical objects. The computation resources in virtual space can be fully utilized to support efficient object searching and multi-attribute visual comparisons. We demonstrate the usability, expressiveness, and efficiency of DRCmpVis through a user study, NASA TLX assessment, quantitative evaluation, and case studies involving different scenarios.  © 1995-2012 IEEE.",Diminished reality; mixed reality; mobile environment; visual comparison,Title_Keywords,True,
Scopus,journalPaper,2024,MARR : A Multi-Agent Reinforcement Resetter for Redirected Walking,TVCG - Transactions on Visualization and Computer Graphics,A,"The reset technique of Redirected Walking (RDW) forcibly reorients the user&#x0027;s direction overtly to avoid collisions with boundaries, obstacles, or other users in the physical space. However, excessive resetting can decrease the user&#x0027;s sense of immersion and presence. Several RDW studies have been conducted to address this issue. Among them, much research has been done on reset techniques that reduce the number of resets by devising reset direction rules or optimizing them for a given environment. However, existing optimization studies on reset techniques have mainly focused on a single-user environment. In a multi-user environment, the dynamic movement of other users and static obstacles in the physical space increase the possibility of resetting. In this study, we propose Multi-Agent Reinforcement Resetter (MARR), which resets the user taking into account both physical obstacles and multi-user movement to minimize the number of resets. MARR is trained using multi-agent reinforcement learning to determine the optimal reset direction in different environments. This approach allows MARR to effectively account for different environmental contexts, including arbitrary physical obstacles and the dynamic movements of other users in the same physical space. We compared MARR to other reset technologies through simulation tests and user studies, and found that MARR outperformed the existing methods. MARR improved performance by learning the optimal reset direction for each subtle technique used in training. MARR has the potential to be applied to new subtle techniques proposed in the future. Overall, our study confirmed that MARR is an effective reset technique in multi-user environments. Authors",Aerospace electronics; Legged locomotion; Optimization; Redirected walking; Reinforcement learning; Reinforcement learning; Resetting; Resists; Space exploration; Training; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,360° Stereo Image Composition With Depth Adaption,TVCG - Transactions on Visualization and Computer Graphics,A,"360° images and videos have become an economic and popular way to provide VR experiences using real-world content. However, the manipulation of the stereo panoramic content remains less explored. In this article, we focus on the 360° image composition problem, and develop a solution that can take an object from a stereo image pair and insert it at a given 3D position in a target stereo panorama, with well-preserved geometry information. Our method uses recovered 3D point clouds to guide the composited image generation. More specifically, we observe that using only a one-off operation to insert objects into equirectangular images will never produce satisfactory depth perception and generate ghost artifacts when users are watching the result from different view directions. Therefore, we propose a novel per-view projection method that segments the object in 3D spherical space with the stereo camera pair facing in that direction. A deep depth densification network is proposed to generate depth guidance for the stereo image generation of each view segment according to the desired position and pose of the inserted object. We finally combine the synthesized view segments and blend the objects into the target stereo 360° scene. A user study demonstrates that our method can provide good depth perception and removes ghost artifacts. The per-view solution is a potential paradigm for other content manipulation methods for 360° images and videos.  © 1995-2012 IEEE.",Image composition; image synthesis; stereoscopic panoramic image; virtual reality,Keywords,True,
Scopus,journalPaper,2024,iMetaTown: A Metaverse System with Multiple Interactive Functions Based on Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"This work aims to pioneer the development of a real-time interactive and immersive Metaverse Human-Computer Interaction (HCI) system leveraging Virtual Reality (VR). The system incorporates a three-dimensional (3D) face reconstruction method, grounded in weakly supervised learning, to enhance player-player interactions within the Metaverse. The proposed method, two-dimensional (2D) face images, are effectively employed in a 2D Self-Supervised Learning (2DASL) approach, significantly optimizing 3D model learning outcomes and improving the quality of 3D face alignment in HCI systems. The work outlines the functional modules of the system, encompassing user interactions such as hugs and handshakes and communication through voice and text via blockchain. Solutions for managing multiple simultaneous online users are presented. Performance evaluation of the HCI system in a 3D reconstruction scene indicates that the 2DASL face reconstruction method achieves noteworthy results, enhancing the system&#x0027;s interaction capabilities by aiding 3D face modeling through 2D face images. The experimental system achieves a maximum processing speed of 18 frames of image data on a personal computer, meeting real-time processing requirements. User feedback regarding social acceptance, action interaction usability, emotions, and satisfaction with the VR interactive system reveals consistently high scores. The designed VR HCI system exhibits outstanding performance across diverse applications. IEEE",3D Interaction; Animation; Avatar; Blockcha; Blockchains; Face recognition; Faces; Human-Computer Interaction; Image reconstruction; Metaverse; Metaverse; Three-dimensional displays; Vectors; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Multiple Monitors or Single Canvas&#x003F; Evaluating Window Management and Layout Strategies on Virtual Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual displays enabled through head-worn augmented reality have unique characteristics that can yield extensive amounts of screen space. Existing research has shown that increasing the space on a computer screen can enhance usability. Since virtual displays offer the unique ability to present content without rigid physical space constraints, they provide various new design possibilities. Therefore, we must understand the trade-offs of layout choices when structuring that space. We propose a single Canvas approach that eliminates boundaries from traditional multi-monitor approaches and instead places windows in one large, unified space. Our user study compared this approach against a multi-monitor setup, and we considered both purely virtual systems and hybrid systems that included a physical monitor. We looked into usability factors such as performance, accuracy, and overall window management. Results show that Canvas displays can cause users to compact window layouts more than multiple monitors with snapping behavior, even though such optimizations may not lead to longer window management times. We did not find conclusive evidence of either setup providing a better user experience. Multi-Monitor displays offer quick window management with snapping and a structured layout through subdivisions. However, Canvas displays allow for more control in placement and size, lowering the amount of space used and, thus, head rotation. Multi-Monitor benefits were more prominent in the hybrid configuration, while the Canvas display was more beneficial in the purely virtual configuration. IEEE",Augmented Reality; Canvas; Layout; Layout; Monitoring; Multiple Monitors; Operating systems; Task analysis; Two-dimensional displays; User experience; Virtual Displays; Visualization; Window Management,Abstract_Keywords,True,
Scopus,journalPaper,2024,Design and Evaluation of Controller-Based Raycasting Methods for Efficient Alphanumeric and Special Character Entry in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Alphanumeric and special characters are essential during text entry. Text entry in virtual reality (VR) is usually performed on a virtual Qwerty keyboard to minimize the need to learn new layouts. As such, entering capitals, symbols, and numbers in VR is often a direct migration from a physical/touchscreen Qwerty keyboard-that is, using the mode-switching keys to switch between different types of characters and symbols. However, there are inherent differences between a keyboard in VR and a physical/touchscreen keyboard, and as such, a direct adaptation of mode-switching via switch keys may not be suitable for VR. The high flexibility afforded by VR opens up more possibilities for entering alphanumeric and special characters using the Qwerty layout. In this work, we designed two controller-based raycasting text entry methods for alphanumeric and special characters input (Layer-ButtonSwitch and Key-ButtonSwitch) and compared them with two other methods (Standard Qwerty Keyboard and Layer-PointSwitch) that were derived from physical and soft Qwerty keyboards. We explored the performance and user preference of these four methods via two user studies (one short-Term and one prolonged use), where participants were instructed to input text containing alphanumeric and special characters. Our results show that Layer-ButtonSwitch led to the highest statistically significant performance, followed by Key-ButtonSwitch and Standard Qwerty Keyboard, while Layer-PointSwitch had the slowest speed. With continuous practice, participants' performance using Key-ButtonSwitch reached that of Layer-ButtonSwitch. Further, the results show that the key-level layout used in Key-ButtonSwitch led users to parallel mode switching and character input operations because this layout showed all characters on one layer. We distill three recommendations from the results that can help guide the design of text entry techniques for alphanumeric and special characters in VR.  © 1995-2012 IEEE.",alphanumeric and special character entry; keyboard layout; mode-switching; text entry; user study; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"Text-driven 3D scene generation is widely applicable to video gaming, film industry, and metaverse applications that have a large demand for 3D scenes. However, existing text-to-3D generation methods are limited to producing 3D objects with simple geometries and dreamlike styles that lack realism. In this work, we present Text2NeRF, which is able to generate a wide range of 3D scenes with complicated geometric structures and high-fidelity textures purely from a text prompt. To this end, we adopt NeRF as the 3D representation and leverage a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description. Specifically, we employ the diffusion model to infer the text-related image as the content prior and use a monocular depth estimation method to offer the geometric prior. Both content and geometric priors are utilized to update the NeRF model. To guarantee textured and geometric consistency between different views, we introduce a progressive scene inpainting and updating strategy for novel view synthesis of the scene. Our method requires no additional training data but only a natural language description of the scene as the input. Extensive experiments demonstrate that our Text2NeRF outperforms existing methods in producing photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of natural language prompts. Our code and model are available at https://github.com/eckertzhang/Text2NeRF.  © 1995-2012 IEEE.",3D scene generation; depth alignment; NeRF; scene inpainting; Text-to-3D,Abstract,True,
Scopus,journalPaper,2024,VisTellAR: Embedding Data Visualization to Short-form Videos Using Mobile Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"With the rise of short-form video platforms and the increasing availability of data, we see the potential for people to share short-form videos embedded with data in situ (e.g., daily steps when running) to increase the credibility and expressiveness of their stories. However, creating and sharing such videos in situ is challenging since it involves multiple steps and skills (e.g., data visualization creation and video editing), especially for amateurs. By conducting a formative study (N=10) using three design probes, we collected the motivations and design requirements. We then built VisTellAR, a mobile AR authoring tool, to help amateur video creators embed data visualizations in short-form videos in situ. A two-day user study shows that participants (N=12) successfully created various videos with data visualizations in situ and they confirmed the ease of use and learning. AR pre-stage authoring was useful to assist people in setting up data visualizations in reality with more designs in camera movements and interaction with gestures and physical objects to storytelling. IEEE",augmented reality; data visualization; Personal data; short-form video; storytelling,Title_Keywords,True,
Scopus,journalPaper,2024,Point Cloud Completion: A Survey,TVCG - Transactions on Visualization and Computer Graphics,A,"Point cloud completion is the task of producing a complete 3D shape given an input of a partial point cloud. It has become a vital process in 3D computer graphics, vision and applications such as autonomous driving, robotics, and augmented reality. These applications often rely on the presence of a complete 3D representation of the environment. Over the past few years, many completion algorithms have been proposed and a substantial amount of research has been carried out. However, there are not many in-depth surveys that summarise the research progress in such a way that allows users to make an informed choice of what algorithms to employ given the type of data they have, the end result they want, the challenges they may face and the possible strategies they could use. In this study, we present a comprehensive survey and classification of articles on point cloud completion untill August 2023 based on the strategies, techniques, inputs, outputs, and network architectures. We will also cover datasets, evaluation methods, and application areas in point cloud completion. Finally, we discuss challenges faced by the research community and future research directions. © 1995-2012 IEEE.",computer vision; deep learning; Point cloud completion,Abstract,True,
Scopus,journalPaper,2024,ARGUS: Visualization of AI-Assisted Task Guidance in AR,TVCG - Transactions on Visualization and Computer Graphics,A,"The concept of augmented reality (AR) assistants has captured the human imagination for decades, becoming a staple of modern science fiction. To pursue this goal, it is necessary to develop artificial intelligence (AI)-based methods that simultaneously perceive the 3D environment, reason about physical tasks, and model the performer, all in real-time. Within this framework, a wide variety of sensors are needed to generate data across different modalities, such as audio, video, depth, speech, and time-of-flight. The required sensors are typically part of the AR headset, providing performer sensing and interaction through visual, audio, and haptic feedback. AI assistants not only record the performer as they perform activities, but also require machine learning (ML) models to understand and assist the performer as they interact with the physical world. Therefore, developing such assistants is a challenging task. We propose ARGUS, a visual analytics system to support the development of intelligent AR assistants. Our system was designed as part of a multi-year-long collaboration between visualization researchers and ML and AR experts. This co-design process has led to advances in the visualization of ML in AR. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant. © 1995-2012 IEEE.",Application Motivated Visualization; AR/VR/Immersive; Data Models; Image and Video Data; Temporal Data,Abstract,True,
Scopus,journalPaper,2024,Influence of Scenarios and Player Traits on Flow in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Many studies have investigated how interpersonal differences between users influence their experience in Virtual Reality (VR) and it is now well recognized that user's subjective experiences and responses to the same VR environment can vary widely. In this study, we focus on player traits, which correspond to users' preferences for game mechanics, arguing that players react differently when experiencing VR scenarios. We developed three scenarios in the same VR environment that rely on different game mechanics, and evaluate the influence of the scenarios, the player traits and the time of practice of the VR environment on users' perceived flow. Our results show that 1) the type of scenario has an impact on specific dimensions of flow; 2) the scenarios have different effects on flow depending on the order they are performed, the flow preconditions being stronger when performed at last; 3) almost all dimensions of flow are influenced by the player traits, these influences depending on the scenario, 4) the Aesthetic trait has the most influences in the three scenarios. We finally discuss the findings and limitations of the present study that we believe have strong implications for the design of scenarios in VR experiences.  © 1995-2012 IEEE.",flow; game mechanics; player traits; scenario; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,MeTACAST: Target- and Context-Aware Spatial Selection in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose three novel spatial data selection techniques for particle data in VR visualization environments. They are designed to be target- and context-aware and be suitable for a wide range of data features and complex scenarios. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters - with all of them facilitating post-selection threshold adjustment. These techniques allow users to precisely select those regions of space for further exploration - with simple and approximate 3D pointing, brushing, or drawing input - using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes. These new techniques are evaluated in a controlled experiment and compared with the Baseline method, a region-based 3D painting selection. Our results indicate that our techniques are effective in handling a wide range of scenarios and allow users to select data based on their comprehension of crucial features. Furthermore, we analyze the attributes, requirements, and strategies of our spatial selection methods and compare them with existing state-of-the-art selection methods to handle diverse data features and situations. Based on this analysis we provide guidelines for choosing the most suitable 3D spatial selection techniques based on the interaction environment, the given data characteristics, or the need for interactive post-selection threshold adjustment. © 1995-2012 IEEE.",immersive analytics; Spatial selection; target-aware and context-aware interaction for visualization; virtual reality (VR),Keywords,True,
Scopus,journalPaper,2024,"2D, 2.5D, or 3D? An Exploratory Study on Multilayer Network Visualisations in Virtual Reality",TVCG - Transactions on Visualization and Computer Graphics,A,"Relational information between different types of entities is often modelled by a multilayer network (MLN) - a network with subnetworks represented by layers. The layers of an MLN can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. Additionally, layer arrangements with a dimensionality beyond 2D, which are common in this scenario, motivate the use of stereoscopic displays. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. The study employs six analysis tasks that cover the spectrum of an MLN task taxonomy, from path finding and pattern identification to comparisons between and across layers. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs. © 1995-2012 IEEE.",CompSystems; Guidelines; HumanQuant; Network; VisDesign,Title_Abstract,True,
Scopus,journalPaper,2024,Design Patterns for Situated Visualization in Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows. © 1995-2012 IEEE.",Augmented reality; design patterns; design space; immersive analytics; situated visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,HazARdSnap: Gazed-Based Augmentation Delivery for Safe Information Access While Cycling,TVCG - Transactions on Visualization and Computer Graphics,A,"During cycling activities, cyclists often monitor a variety of information such as heart rate, distance, and navigation using a bike-mounted phone or cyclocomputer. In many cases, cyclists also ride on sidewalks or paths that contain pedestrians and other obstructions such as potholes, so monitoring information on a bike-mounted interface can slow the cyclist down or cause accidents and injury. In this article, we present HazARdSnap, an augmented reality-based information delivery approach that improves the ease of access to cycling information and at the same time preserves the user's awareness of hazards. To do so, we implemented real-time outdoor hazard detection using a combination of computer vision and motion and position data from a head mounted display (HMD). We then developed an algorithm that snaps information to detected hazards when they are also viewed so that users can simultaneously view both rendered virtual cycling information and the real-world cues such as depth, position, time to hazard, and speed that are needed to assess and avoid hazards. Results from a study with 24 participants that made use of real-world cycling and virtual hazards showed that both HazARdSnap and forward-fixed augmented reality (AR) user interfaces (UIs) can effectively help cyclists access virtual information without having to look down, which resulted in fewer collisions (51% and 43% reduction compared to baseline, respectively) with virtual hazards.  © 1995-2012 IEEE.",Augmented reality; cycling; eye tracking; object detection and user interfaces; safety,Abstract_Keywords,True,
Scopus,journalPaper,2024,VisionCoach: Design and Effectiveness Study on VR Vision Training for Basketball Passing,TVCG - Transactions on Visualization and Computer Graphics,A,"Vision Training is important for basketball players to effectively search for teammates who has wide-open opportunities to shoot, observe the defenders around the wide-open teammates and quickly choose a proper way to pass the ball to the most suitable one. We develop an immersive virtual reality (VR) system called VisionCoach to simulate the player's viewing perspective and generate three designed systematic vision training tasks to benefit the cultivating procedure. By recording the player's eye gazing and dribbling video sequence, the proposed system can analyze the vision-related behavior to understand the training effectiveness. To demonstrate the proposed VR training system can facilitate the cultivation of vision ability, we recruited 14 experienced players to participate in a 6-week between-subject study, and conducted a study by comparing the most frequently used 2D vision training method called Vision Performance Enhancement (VPE) program with the proposed system. Qualitative experiences and quantitative training results are reported to show that the proposed immersive VR training system can effectively improve player's vision ability in terms of gaze behavior and dribbling stability. Furthermore, training in the VR-VisionCoach Condition can transfer the learned abilities to real scenario more easily than training in the 2D-VPE Condition.  © 1995-2012 IEEE.",basketball VR; computer-aided training; Sports VR; vision training,Abstract,True,
Scopus,journalPaper,2024,Submerse: Visualizing Storm Surge Flooding Simulations in Immersive Display Ecologies,TVCG - Transactions on Visualization and Computer Graphics,A,"We present Submerse, an end-to-end framework for visualizing flooding scenarios on large and immersive display ecologies. Specifically, we reconstruct a surface mesh from input flood simulation data and generate a to-scale 3D virtual scene by incorporating geographical data such as terrain, textures, buildings, and additional scene objects. To optimize computation and memory performance for large simulation datasets, we discretize the data on an adaptive grid using dynamic quadtrees and support level-of-detail based rendering. Moreover, to provide a perception of flooding direction for a time instance, we animate the surface mesh by synthesizing water waves. As interaction is key for effective decision-making and analysis, we introduce two novel techniques for flood visualization in immersive systems: (1) an automatic scene-navigation method using optimal camera viewpoints generated for marked points-of-interest based on the display layout, and (2) an AR-based focus+context technique using an aux display system. Submerse is developed in collaboration between computer scientists and atmospheric scientists. We evaluate the effectiveness of our system and application by conducting workshops with emergency managers, domain experts, and concerned stakeholders in the Stony Brook Reality Deck, an immersive gigapixel facility, to visualize a superstorm flooding scenario in New York City.  © 1995-2012 IEEE.",Camera navigation; flooding simulation; immersive visualization; mixed reality,Keywords,True,
Scopus,journalPaper,2024,Improving Depth Perception in Immersive Media Devices by Addressing Vergence-Accommodation Conflict,TVCG - Transactions on Visualization and Computer Graphics,A,"Recently, immersive media devices have seen a boost in popularity. However, many problems still remain. Depth perception is a crucial part of how humans behave and interact with their environment. Convergence and accommodation are two physiological mechanisms that provide important depth cues. However, when humans are immersed in virtual environments, they experience a mismatch between these cues. This mismatch causes users to feel discomfort while also hindering their ability to fully perceive object distances. To address the conflict, we have developed a technique that encompasses inverse blurring into immersive media devices. For the inverse blurring, we utilize the classical Wiener deconvolution approach by proposing a novel technique that is applied without the need for an eye-tracker and implemented in a commercial immersive media device. The technique's ability to compensate for the vergence-accommodation conflict was verified through two user studies aimed at reaching and spatial awareness, respectively. The two studies yielded a statistically significant 36% and 48% error reduction in user performance to estimate distances, respectively. Overall, the work done demonstrates how visual stimuli can be modified to allow users to achieve a more natural perception and interaction with the virtual environment.  © 1995-2012 IEEE.",Depth perception; depth-of-field; immersive media; inverse blurring; reaching task; space-variant technique; vergence-accommodation conflict; virtual reality; wiener deconvolution,Keywords,True,
Scopus,journalPaper,2024,VisTA-LIVE: A Visualization Tool for Assessment of Laboratories in Virtual Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"A Virtual Reality Laboratory (VR Lab) experiment refers to an experiment session that is being conducted in the virtual environment through Virtual Reality (VR) and aims to deliver procedural knowledge to students similar to that in a physical lab environment. While VR Lab is becoming more popular among education institutes as a learning tool for students, existing designs are mostly considered from a student's perspective. Instructors could only receive limited information on how the students are performing and could not provide useful feedback to aid the students' learning and evaluate their performance. This motivated us to create VisTA-LIVE: a Visualization Tool for Assessment of Laboratories In Virtual Environments. In this article, we present in detail the design thinking approach that was applied to create VisTA-LIVE. The tool is deployed in an Extended Reality (XR) environment, and we report the evaluation results with domain experts and discuss issues related to monitoring and assessing a live VR lab session which lay potential directions for future work. We also describe how the resulting design of the tool could be used as a reference for other education developers who wish to develop similar applications.  © 1995-2012 IEEE.",computer-assisted instruction; Education technology; extended reality; virtual laboratory,Abstract_Keywords,True,
Scopus,journalPaper,2024,VoxAR: Adaptive Visualization of Volume Rendered Objects in Optical See-Through Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"We present VoxAR, a method to facilitate an effective visualization of volume-rendered objects in optical see-through head-mounted displays (OST-HMDs). The potential of augmented reality (AR) to integrate digital information into the physical world provides new opportunities for visualizing and interpreting scientific data. However, a limitation of OST-HMD technology is that rendered pixels of a virtual object can interfere with the colors of the real-world, making it challenging to perceive the augmented virtual information accurately. We address this challenge in a two-step approach. First, VoxAR determines an appropriate placement of the volume-rendered object in the real-world scene by evaluating a set of spatial and environmental objectives, managed as user-selected preferences and pre-defined constraints. We achieve a real-time solution by implementing the objectives using a GPU shader language. Next, VoxAR adjusts the colors of the input transfer function (TF) based on the real-world placement region. Specifically, we introduce a novel optimization method that adjusts the TF colors such that the resulting volume-rendered pixels are discernible against the background and the TF maintains the perceptual mapping between the colors and data intensity values. Finally, we present an assessment of our approach through objective evaluations and subjective user studies. © 1995-2012 IEEE.",Adaptive visualization; augmented reality; situated visualization; volume rendering,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Handling Non-Visible Referents in Situated Visualizations,TVCG - Transactions on Visualization and Computer Graphics,A,"Situated visualizations are a type of visualization where data is presented next to its physical referent (i.e., the physical object, space, or person it refers to), often using augmented-reality displays. While situated visualizations can be beneficial in various contexts and have received research attention, they are typically designed with the assumption that the physical referent is visible. However, in practice, a physical referent may be obscured by another object, such as a wall, or may be outside the user's visual field. In this paper, we propose a conceptual framework and a design space to help researchers and user interface designers handle non-visible referents in situated visualizations. We first provide an overview of techniques proposed in the past for dealing with non-visible objects in the areas of 3D user interfaces, 3D visualization, and mixed reality. From this overview, we derive a design space that applies to situated visualizations and employ it to examine various trade-offs, challenges, and opportunities for future research in this area. © 1995-2012 IEEE.",AR/VR/Immersive; Frameworks; Mobile; Models; Specialized Input/Display Hardware; Taxonomy; Theory,Abstract,True,
Scopus,journalPaper,2024,RL-L: A Deep Reinforcement Learning Approach Intended for AR Label Placement in Dynamic Scenarios,TVCG - Transactions on Visualization and Computer Graphics,A,"Labels are widely used in augmented reality (AR) to display digital information. Ensuring the readability of AR labels requires placing them in an occlusion-free manner while keeping visual links legible, especially when multiple labels exist in the scene. Although existing optimization-based methods, such as force-based methods, are effective in managing AR labels in static scenarios, they often struggle in dynamic scenarios with constantly moving objects. This is due to their focus on generating layouts optimal for the current moment, neglecting future moments and leading to sub-optimal or unstable layouts over time. In this work, we present RL-Label, a deep reinforcement learning-based method intended for managing the placement of AR labels in scenarios involving moving objects. RL-Label considers both the current and predicted future states of objects and labels, such as positions and velocities, as well as the user's viewpoint, to make informed decisions about label placement. It balances the trade-offs between immediate and long-term objectives. We tested RL-Label in simulated AR scenarios on two real-world datasets, showing that it effectively learns the decision-making process for long-term optimization, outperforming two baselines (i.e., no view management and a force-based method) by minimizing label occlusions, line intersections, and label movement distance. Additionally, a user study involving 18 participants indicates that, within our simulated environment, RL-Label excels over the baselines in aiding users to identify, compare, and summarize data on labels in dynamic scenes. © 1995-2012 IEEE.",Augmented Reality; Dynamic Scenarios; Label Placement; Reinforcement Learning,Abstract_Keywords,True,
Scopus,journalPaper,2024,Efficient Binocular Rendering of Volumetric Density Fields With Coupled Adaptive Cube-Map Ray Marching for Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Creating visualizations of multiple volumetric density fields is demanding in virtual reality (VR) applications, which often include divergent volumetric density distributions mixed with geometric models and physics-based simulations. Real-time rendering of such complex environments poses significant challenges for rendering quality and performance. This article presents a novel scheme for efficient real-time rendering of varying translucent volumetric density fields with global illumination (GI) effects on high-resolution binocular VR displays. Our scheme proposes creative solutions to address three challenges involved in the target problem. First, to tackle the doubled heavy workloads of binocular ray marching, we explore the anti-aliasing principles and more advanced potentials of ray marching on interior cube-map faces, and propose a coupled ray-marching technique that converges to multi-resolution cube maps with interleaved adaptive sampling. Second, we devise a fully dynamic ambient GI approximation method that leverages spherical-harmonics (SH) transform information of the phase function to reduce the huge amount of ray sampling required for GI while ensuring fidelity. The method catalyzes spatial ray-marching reuse and adaptive temporal accumulation. Third, we deploy a two-phase ray-tracing algorithm with a tiled k-buffer to achieve fast processing of order-independent transparency (OIT) for multiple volume instances. Consequently, high-quality and high-performance real-time dynamic volume rendering can be achieved under constrained budgets controlled by developers. As our solution supports mixed mesh-volume rendering, the test results prove the practical usefulness of our approach for high-resolution binocular VR rendering on hybrid multi-volumetric and geometric environments. © 1995-2012 IEEE.",Binocular views; density field; global illumination; ray marching; real-time rendering; virtual reality; volume rendering,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Text Entry Performance and Situation Awareness of a Joint Optical See-Through Head-Mounted Display and Smartphone System,TVCG - Transactions on Visualization and Computer Graphics,A,"Optical see-through head-mounted displays (OST HMDs) are a popular output medium for mobile Augmented Reality (AR) applications. To date, they lack efficient text entry techniques. Smartphones are a major text entry medium in mobile contexts but attentional demands can contribute to accidents while typing on the go. Mobile multi-display ecologies, such as combined OST HMD-smartphone systems, promise performance and situation awareness benefits over single-device use. We study the joint performance of text entry on mobile phones with text output on optical see-through head-mounted displays. A series of five experiments with a total of 86 participants indicate that, as of today, the challenges in such a joint interactive system outweigh the potential benefits. 1077-2626  © 2023 IEEE.",Augmented reality; cross-device; head-mounted display; mobile; multi-display; optical see-through; text entry,Abstract_Keywords,True,
Scopus,journalPaper,2024,Leveraging Tendon Vibration to Enhance Pseudo-Haptic Perceptions in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"Pseudo-haptic techniques are used to modify haptic perception by appropriately changing visual feedback to body movements. Based on the knowledge that tendon vibration can affect our somatosensory perception, this article proposes a method for leveraging tendon vibration to enhance pseudo-haptics during free arm motion. Three experiments were performed to examine the impact of tendon vibration on the range and resolution of pseudo-haptics. The first experiment investigated the effect of tendon vibration on the detection threshold of the discrepancy between visual and physical motion. The results indicated that vibrations applied to the inner tendons of the wrist and elbow increased the threshold, suggesting that tendon vibration can augment the applicable visual motion gain by approximately 13% without users detecting the visual/physical discrepancy. Furthermore, the results demonstrate that tendon vibration acts as noise on haptic motion cues. The second experiment assessed the impact of tendon vibration on the resolution of pseudo-haptics by determining the just noticeable difference in pseudo-weight perception. The results suggested that the tendon vibration does not largely compromise the resolution of pseudo-haptics. The third experiment evaluated the equivalence between the weight perception triggered by tendon vibration and that by visual motion gain, that is, the point of subjective equality. The results revealed that vibration amplifies the weight perception and its effect was equivalent to that obtained using a gain of 0.64 without vibration, implying that the tendon vibration also functions as an additional haptic cue. Our results provide design guidelines and future work for enhancing pseudo-haptics with tendon vibration. 1077-2626  © 2023 IEEE.",Cross-modal integration; maximum likelyhood estimation; pseudo-haptics; tendon vibration; virtual reality,Keywords,True,
Scopus,journalPaper,2024,This is the Table I Want! Interactive Data Transformation on Desktop and in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Data transformation is an essential step in data science. While experts primarily use programming to transform their data, there is an increasing need to support non-programmers with user interface-based tools. With the rapid development in interaction techniques and computing environments, we report our empirical findings about the effects of interaction techniques and environments on performing data transformation tasks. Specifically, we studied the potential benefits of direct interaction and virtual reality (VR) for data transformation. We compared gesture interaction versus a standard WIMP user interface, each on the desktop and in VR. With the tested data and tasks, we found time performance was similar between desktop and VR. Meanwhile, VR demonstrates preliminary evidence to better support provenance and sense-making throughout the data transformation process. Our exploration of performing data transformation in VR also provides initial affirmation for enabling an iterative and fully immersive data science workflow. © 2023 IEEE.",Data science; data transformation; empirical study; immersive analytics; interaction; virtual/augmented/mixed reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,MeshWGAN: Mesh-to-Mesh Wasserstein GAN with Multi-Task Gradient Penalty for 3D Facial Geometric Age Transformation,TVCG - Transactions on Visualization and Computer Graphics,A,"As the metaverse develops rapidly, 3D facial age transformation is attracting increasing attention, which may bring many potential benefits to a wide variety of users, e.g., 3D aging figures creation, 3D facial data augmentation and editing. Compared with 2D methods, 3D face aging is an underexplored problem. To fill this gap, we propose a new mesh-to-mesh Wasserstein generative adversarial network (MeshWGAN) with a multi-task gradient penalty to model a continuous bi-directional 3D facial geometric aging process. To the best of our knowledge, this is the first architecture to achieve 3D facial geometric age transformation via real 3D scans. As previous image-to-image translation methods cannot be directly applied to the 3D facial mesh, which is totally different from 2D images, we built a mesh encoder, decoder, and multi-task discriminator to facilitate mesh-to-mesh transformations. To mitigate the lack of 3D datasets containing children's faces, we collected scans from 765 subjects aged 5-17 in combination with existing 3D face databases, which provided a large training dataset. Experiments have shown that our architecture can predict 3D facial aging geometries with better identity preservation and age closeness compared to 3D trivial baselines. We also demonstrated the advantages of our approach via various 3D face-related graphics applications.  © 1995-2012 IEEE.",3D face geometry; Age transformation; mesh generative adversarial networks; MeshWGAN; multi-task gradient penalty,Abstract,True,
Scopus,journalPaper,2024,Unraveling the Design Space of Immersive Analytics: A Systematic Review,TVCG - Transactions on Visualization and Computer Graphics,A,"Immersive analytics has emerged as a promising research area, leveraging advances in immersive display technologies and techniques, such as virtual and augmented reality, to facilitate data exploration and decision-making. This paper presents a systematic literature review of 73 studies published between 2013-2022 on immersive analytics systems and visualizations, aiming to identify and categorize the primary dimensions influencing their design. We identified five key dimensions: Academic Theory and Contribution, Immersive Technology, Data, Spatial Presentation, and Visual Presentation. Academic Theory and Contribution assess the motivations behind the works and their theoretical frameworks. Immersive Technology examines the display and input modalities, while Data dimension focuses on dataset types and generation. Spatial Presentation discusses the environment, space, embodiment, and collaboration aspects in IA, and Visual Presentation explores the visual elements, facet and position, and manipulation of views. By examining each dimension individually and cross-referencing them, this review uncovers trends and relationships that help inform the design of immersive systems visualizations. This analysis provides valuable insights for researchers and practitioners, offering guidance in designing future immersive analytics systems and shaping the trajectory of this rapidly evolving field. A free copy of this paper and all supplemental materials are available at osf.io/5ewaj. © 1995-2012 IEEE.",Augmented Reality; Design Space; Immersive Analytics; Survey; Systematic Review; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Investigating the Correlation Between Presence and Reaction Time in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Measuring presence is critical to improving user involvement and performance in Mixed Reality (MR). Presence, a crucial aspect of MR, is traditionally gauged using subjective questionnaires, leading to a lack of time-varying responses and susceptibility to user bias. Inspired by the existing literature on the relationship between presence and human performance, the proposed methodology systematically measures a user's reaction time to a visual stimulus as they interact within a manipulated MR environment. We explore the user reaction time as a quantity that can be easily measured using the systemic tools available in modern MR devices. We conducted an exploratory study (N = 40) with two experiments designed to alter the users' sense of presence by manipulating place illusion and plausibility illusion. We found a significant correlation between presence scores and reaction times with a correlation coefficient -0.65, suggesting that users with a higher sense of presence responded more swiftly to stimuli. We develop a model that estimates a user's presence level using the reaction time values with high accuracy of up to 80%. While our study suggests that reaction time can be used as a measure of presence, further investigation is needed to improve the accuracy of the model.  © 1995-2012 IEEE.",Mixed reality; presence,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Modeling the Intent to Interact with VR Using Physiological Features,TVCG - Transactions on Visualization and Computer Graphics,A,"Objective: Mixed-Reality (XR) technologies promise a user experience (UX) that rivals the interactive experience with the real-world. The key facilitators in the design of such a natural UX are that the interaction has zero lag and that users experience no excess mental load. This is difficult to achieve due to technical constraints such as motion-to-photon latency as well as false-positives during gesture-based interaction. Methods: In this paper, we explored the use of physiological features to model the user's intent to interact with a virtual reality (VR) environment. Accurate predictions about when users want to express an interaction intent could overcome the limitations of an interactive device that lags behind the intention of a user. We computed time-domain features from electroencephalography (EEG) and electromyography (EMG) recordings during a grab-and-drop task in VR and cross-validated a Linear Discriminant Analysis (LDA) for three different combinations of (1) EEG, (2) EMG and (3) EEG-EMG features. Results & Conclusion: We found the classifiers to detect the presence of a pre-movement state from background idle activity reflecting the users' intent to interact with the virtual objects (EEG: 62 ± 10, EMG: 72 ± 9, EEG-EMG: 69 ± 10) above simulated chance level. The features leveraged in our classification scheme have a low computational cost and are especially useful for fast decoding of users' mental states. Our work is a further step towards a useful classification of users' intent to interact, as a high temporal resolution and speed of detection is crucial. This facilitates natural experiences through zero-lag adaptive interfaces. 1077-2626  © 2023 IEEE.",Brain-computer interfaces; electroencephalography; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,ViewR: Architectural-Scale Multi-User Mixed Reality With Mobile Head-Mounted Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"—The emergence of mobile head-mounted displays with robust “inside-out” markerless tracking and video-passthrough permits the creation of novel mixed reality (MR) experiences in which architectural spaces of arbitrary size can be transformed into immersive multi-user visualisation arenas. Here we outline ViewR, an open-source framework for rapidly constructing and deploying architectural-scale multi-user MR experiences. ViewR includes tools for rapid alignment of real and virtual worlds, tracking loss detection and recovery, user trajectory visualisation and world state synchronisation between users with persistence across sessions. ViewR also provides control over the blending of the real and the virtual, specification of site-specific blending zones, and video-passthrough avatars, allowing users to see and interact with one another directly. Using ViewR, we explore the transformation of large architectural structures into immersive arenas by creating a range of experiences in various locations, with a particular focus on architectural affordances such as mezzanines, stairs, gangways and elevators. Our tests reveal that ViewR allows for experiences that would not be possible with pure virtual reality, and indicate that, with certain strategies for recovering from tracking errors, it is possible to construct large scale multi-user MR experiences using contemporary consumer virtual reality head-mounted displays. © 2023 The Authors.",co-located systems; collaborative systems; mixed / augmented reality; virtual reality; Visualization systems,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"LoCoMoTe - A Framework for Classification of Natural Locomotion in VR by Task, Technique and Modality",TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual reality (VR) research has provided overviews of locomotion techniques, how they work, their strengths and overall user experience. Considerable research has investigated new methodologies, particularly machine learning to develop redirection algorithms. To best support the development of redirection algorithms through machine learning, we must understand how best to replicate human navigation and behaviour in VR, which can be supported by the accumulation of results produced through live-user experiments. However, it can be difficult to identify, select and compare relevant research without a pre-existing framework in an ever-growing research field. Therefore, this work aimed to facilitate the ongoing structuring and comparison of the VR-based natural walking literature by providing a standardised framework for researchers to utilise. We applied thematic analysis to study methodology descriptions from 140 VR-based papers that contained live-user experiments. From this analysis, we developed the LoCoMoTe framework with three themes: navigational decisions, technique implementation, and modalities. The LoCoMoTe framework provides a standardised approach to structuring and comparing experimental conditions. The framework should be continually updated to categorise and systematise knowledge and aid in identifying research gaps and discussions. © 1995-2012 IEEE.",Human-computer interaction; machine learning; navigation; redirected walking; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Wizualization: A 'Hard Magic' Visualization System for Immersive and Ubiquitous Analytics,TVCG - Transactions on Visualization and Computer Graphics,A,"What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (Spellbook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space. © 1995-2012 IEEE.",gestural interaction; Immersive analytics; situated analytics; ubiquitous analytics; voice interaction,Abstract,True,
Scopus,journalPaper,2024,Survey of Annotations in Extended Reality Systems,TVCG - Transactions on Visualization and Computer Graphics,A,"Annotation in 3D user interfaces such as Augmented Reality (AR) and Virtual Reality (VR) is a challenging and promising area; however, there are not currently surveys reviewing these contributions. In order to provide a survey of annotations for Extended Reality (XR) environments, we conducted a structured literature review of papers that used annotation in their AR/VR systems from the period between 2001 and 2021. Our literature review process consists of several filtering steps which resulted in 103 XR publications with a focus on annotation. We classified these papers based on the display technologies, input devices, annotation types, target object under annotation, collaboration type, modalities, and collaborative technologies. A survey of annotation in XR is an invaluable resource for researchers and newcomers. Finally, we provide a database of the collected information for each reviewed paper. This information includes applications, the display technologies and its annotator, input devices, modalities, annotation types, interaction techniques, collaboration types, and tasks for each paper. This database provides a rapid access to collected data and gives users the ability to search or filter the required information. This survey provides a starting point for anyone interested in researching annotation in XR environments.  © 1995-2012 IEEE.",Annotation; augmented reality; extended reality; immersive technologies; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,SceneFusion: Room-Scale Environmental Fusion for Efficient Traveling between Separate Virtual Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Traveling between scenes has become a major requirement for navigation in numerous virtual reality (VR) social platforms and game applications, allowing users to efficiently explore multiple virtual environments (VEs). To facilitate scene transition, prevalent techniques such as instant teleportation and virtual portals have been extensively adopted. However, these techniques exhibit limitations when there is a need for frequent travel between separate VEs, particularly within indoor environments, resulting in low efficiency. In this article, we first analyze the design rationale for a novel navigation method supporting efficient travel between virtual indoor scenes. Based on the analysis, we introduce the SceneFusion technique that fuses separate virtual rooms into an integrated environment. SceneFusion enables users to perceive rich visual information from both rooms simultaneously, achieving high visual continuity and spatial awareness. While existing teleportation techniques passively transport users, SceneFusion allows users to actively access the fused environment using short-range locomotion techniques. User experiments confirmed that SceneFusion outperforms instant teleportation and virtual portal techniques in terms of efficiency, workload, and preference for both single-user exploration and multi-user collaboration tasks in separate VEs. Thus, SceneFusion presents an effective solution for seamless traveling between virtual indoor scenes.  © 1995-2012 IEEE.",collaborative virtual environments; scene transition; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,The Reality of the Situation: A Survey of Situated Analytics,TVCG - Transactions on Visualization and Computer Graphics,A,"The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user's physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.  © 1995-2012 IEEE.",Augmented reality; data visualization; immersive analytics; situated analytics; situated visualization,Abstract_Keywords,True,
Scopus,journalPaper,2024,MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction,TVCG - Transactions on Visualization and Computer Graphics,A,"High-accuracy, high-efficiency physics-based fluid-solid interaction is essential for reality modeling and computer animation in online games or real-time Virtual Reality (VR) systems. However, the large-scale simulation of incompressible fluid and its interaction with the surrounding solid environment is either time-consuming or suffering from the reduced time/space resolution due to the complicated iterative nature pertinent to numerical computations of involved Partial Differential Equations (PDEs). In recent years, we have witnessed significant growth in exploring a different, alternative data-driven approach to addressing some of the existing technical challenges in conventional model-centric graphics and animation methods. This article showcases some of our exploratory efforts in this direction. One technical concern of our research is to address the central key challenge of how to best construct the numerical solver effectively and how to best integrate spatiotemporal/dimensional neural networks with the available MPM's pressure solvers. In particular, we devise the MPMNet, a hybrid data-driven framework supporting the popular and powerful MPM, to combine the comprehensive properties of MPM in numerically handling physical behaviors ranging from fluid to deformable solids and the high efficiency of data-driven models. At the architectural level, our MPMNet comprises three primary components: A data processing module to describe the physical properties by way of the input fields; A deep neural network group to learn the spatiotemporal features; And an iterative refinement process to continue to reduce possible numerical errors. The goal of these special technical developments is to aim at involved numerical acceleration while preserving physical accuracy, realizing efficient and accurate fluid-solid interactions in a data-driven fashion. The extensive experimental results verify that our MPMNet can tremendously speed up the computation compared with the popular numerical methods as the complexity of interaction scenes increases while better retaining the numerical accuracy.  © 1995-2012 IEEE.",Data-driven simulation; fluid-solid interaction; neural networks; physics-based simulation,Abstract,True,
Scopus,journalPaper,2024,An Overview of Enhancing Distance Learning Through Emerging Augmented and Virtual Reality Technologies,TVCG - Transactions on Visualization and Computer Graphics,A,"Although distance learning presents a number of interesting educational advantages as compared to in-person instruction, it is not without its downsides. We first assess the educational challenges presented by distance learning as a whole and identify 4 main challenges that distance learning currently presents as compared to in-person instruction: the lack of social interaction, reduced student engagement and focus, reduced comprehension and information retention, and the lack of flexible and customizable instructor resources. After assessing each of these challenges in-depth, we examine how AR/VR technologies might serve to address each challenge along with their current shortcomings, and finally outline the further research that is required to fully understand the potential of AR/VR technologies as they apply to distance learning.  © 1995-2012 IEEE.",Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality; Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,Leaning-Based Interfaces Improve Simultaneous Locomotion and Object Interaction in VR Compared to the Handheld Controller,TVCG - Transactions on Visualization and Computer Graphics,A,"Physical walking is often considered the gold standard for VR travel whenever feasible. However, limited free-space walking areas in the real-world do not allow exploring larger-scale virtual environments by actual walking. Therefore, users often require handheld controllers for navigation, which can reduce believability, interfere with simultaneous interaction tasks, and exacerbate adverse effects such as motion sickness and disorientation. To investigate alternative locomotion options, we compared handheld Controller (thumbstick-based) and physical walking versus a seated (HeadJoystick) and standing/stepping (NaviBoard) leaning-based locomotion interface, where seated/standing users travel by moving their head toward the target direction. Rotations were always physically performed. To compare these interfaces, we designed a novel simultaneous locomotion and object interaction task, where users needed to keep touching the center of upward moving target balloons with their virtual lightsaber, while simultaneously staying inside a horizontally moving enclosure. Walking resulted in the best locomotion, interaction, and combined performances while the controller performed worst. Leaning-based interfaces improved user experience and performance compared to Controller, especially when standing/stepping using NaviBoard, but did not reach walking performance. That is, leaning-based interfaces HeadJoystick (sitting) and NaviBoard (standing) that provided additional physical self-motion cues compared to controller improved enjoyment, preference, spatial presence, vection intensity, motion sickness, as well as performance for locomotion, object interaction, and combined locomotion and object interaction. Our results also showed that less embodied interfaces (and in particular the controller) caused a more pronounced performance deterioration when increasing locomotion speed. Moreover, observed differences between our interfaces were not affected by repeated interface usage.  © 1995-2012 IEEE.",3D user interface; continuous interaction; cybersickness; dual task; locomotion; motion sickness; travel techniques; virtual reality,Keywords,True,
Scopus,journalPaper,2024,Does Multi-Actuator Vibrotactile Feedback Within Tangible Objects Enrich VR Manipulation?,TVCG - Transactions on Visualization and Computer Graphics,A,"Rich, informative and realistic haptic feedback is key to enhancing Virtual Reality (VR) manipulation. Tangible objects provide convincing grasping and manipulation interactions with haptic feedback of e.g., shape, mass and texture properties. But these properties are static, and cannot respond to interactions in the virtual environment. On the other hand, vibrotactile feedback provides the opportunity for delivering dynamic cues rendering many different contact properties, such as impacts, object vibrations or textures. Handheld objects or controllers in VR are usually restricted to vibrating in a monolithic fashion. In this article, we investigate how spatialiazing vibrotactile cues within handheld tangibles could enable a wider range of sensations and interactions. We conduct a set of perception studies, investigating the extent to which spatialization of vibrotactile feedback within tangible objects is possible as well as the benefits of proposed rendering schemes leveraging multiple actuators in VR. Results show that vibrotactile cues from localized actuators can be discriminated and are beneficial for certain rendering schemes.  © 1995-2012 IEEE.",Haptics; manipulation; tangible; vibrotactile; VR,Abstract,True,
Scopus,journalPaper,2024,3D Gamut Morphing for Non-Rectangular Multi-Projector Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"In a spatially augmented reality system, multiple projectors are tiled on a complex shaped surface to create a seamless display on it. This has several applications in visualization, gaming, education and entertainment. The main challenges in creating seamless and undistorted imagery on such complex shaped surfaces are geometric registration and color correction. Prior methods that provide solutions for the spatial color variation in multi-projector displays assume rectangular overlap regions across the projectors that is possible only on flat surfaces with extremely constrained projector placement. In this article, we present a novel and fully automated method for removing color variations in a multi-projector display on arbitrary shaped smooth surfaces using a general color gamut morphing algorithm that can handle any arbitrarily shaped overlap between the projectors and assures imperceptible color variations across the display surface.  © 1995-2012 IEEE.",Color-correction; gamut morphing; multi-projector displays; photometric correction,Abstract,True,
Scopus,conferencePaper,2013,Towards hand-eye coordination training in virtual knee arthroscopy,VRST - Virtual Reality Software and Technology,A,"Minimally invasive arthroscopic surgery has replaced the common orthopaedic surgery procedures on joints. However it demands from surgeons to acquire very different motor-skills for using special miniature pencil-like instruments and cameras inserted through little incisions on the body while observing the surgical field on a video monitor. Training in virtual reality is becoming an alternative to traditional surgical training based on either real patients or increasingly difficult to procure cadavers. In this paper we propose solutions for simulation in virtual environments a few basic arthroscopic procedures including incision of the arthroscopic camera, positioning of the instrument in front of it, as well as using scissors and graspers. Our approach is based on both full 3D simulation and haptic interaction as well as image-based visualization and haptic interaction.",arthroscopy; haptics; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2013,A two-arm coordination model for phantom limb pain rehabilitation,VRST - Virtual Reality Software and Technology,A,"Following limb loss, patients usually continue having sensations on their missing limbs as if they were still present. Significant amount of such sensations are painful and referred as Phantom Limb Pain (PLP). Previous research has shown that providing the patient with the visual feedback of a limb at the place of the missing one in Virtual Reality (VR) can reduce PLP. In this paper we introduce a model to coordinate the arms allowing the exercising of a much broader range of reach tasks for alleviating the PLP more efficiently. Our Two-Arm Coordination Model (TACM) synthesizes the missing limb pose from the instantaneous variations of the intact opposite limb for a given reach task. Moreover, we propose a setup that makes use of a virtual mirror to enhance the full-body awareness of the patient in the virtual space.",bimanual reach; phantom limb pain; virtual reality based rehabilitation,Abstract_Keywords,True,
Scopus,conferencePaper,2013,A methodology to assess the acceptability of human-robot collaboration using virtual reality,VRST - Virtual Reality Software and Technology,A,"Robots are becoming more and more present in our everyday life: they are already used for domestic tasks, for companionship activities, and soon they will be used to assist humans and collaborate with them in their work. Human-robot collaboration has already been studied in the industry, for ergonomics and efficiency purposes, but more from a safety than from an acceptability point of view. In this work, we focused on how people perceive robots in a collaboration task and we proposed to use virtual reality as a simulation environment to test different parameters, by making users collaborate with virtual robots. A simple use case was implemented to compare different robot appearances and different robot movements. Questionnaires and physiological measures were used to assess the acceptability level of each condition with a user study. The results showed that the perception of robot movements depended on robot appearance and that a more anthropomorphic robot, both in its appearance and movements, was not necessarily better accepted by the users in a collaboration task. Finally, this preliminary use case was also the opportunity to guarantee the relevance of using such a methodology — based on virtual reality, questionnaires and physiological measures — for future studies.",acceptability; human factors; human-robot collaboration; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2013,Can we use a brain-computer interface and manipulate a mouse at the same time?,VRST - Virtual Reality Software and Technology,A,"Brain-Computer Interfaces (BCI) introduce a novel way of interacting with real and virtual environments by directly exploiting cerebral activity. However in most setups using a BCI, the user is explicitly asked to remain as motionless as possible, since muscular activity is commonly admitted to add noise and artifacts in brain electrical signals. Thus, as for today, people have been rarely let using other classical input devices such as mice or joysticks simultaneously to a BCI-based interaction. In this paper, we present an experimental study on the influence of manipulating an input device such as a standard computer mouse on the performance of a BCI system. We have designed a 2-class BCI which relies on Alpha brainwaves to discriminate between focused versus relaxed mental activities. The study uses a simple virtual environment inspired by the well-known Pac-Man videogame and based on BCI and mouse controls. The control of mental activity enables to eat pellets in a simple 2D virtual maze. Different levels of motor activity achieved with the mouse are progressively introduced in the gameplay: 1) no motor activity (control condition), 2) a semi-automatic motor activity, and 3) a highly-demanding motor activity. As expected the BCI performance was found to slightly decrease in presence of motor activity. However, we found that the BCI could still be successfully used in all conditions, and that relaxed versus focused mental activities could still be significantly discriminated even in presence of a highly-demanding mouse manipulation. These promising results pave the way to future experimental studies with more complex mental and motor activities, but also to novel 3D interaction paradigms that could mix BCI and other input devices for virtual reality and videogame applications.",brain-computer interface; EEG; mental activity; motor activity,Abstract,True,
Scopus,conferencePaper,2013,Impact of graphical fidelity on physiological responses in virtual environments,VRST - Virtual Reality Software and Technology,A,"Higher quality computer graphics in interactive applications in the areas of virtual reality and games is generally assumed to create a more immersive experience for the end user. In this study we examined this assumption by testing to what degree graphical fidelity was associated with physiological arousal as measured by a galvanic skin response (GSR) sensor. Thirty-six subjects played two different video games at the highest and lowest graphical quality settings while their GSR activity was measured. No significant difference in GSR was observed that was associated with graphical quality. We conclude that, for applications in which an emotional response is desired, increased graphical quality alone does not predict a physiological arousal response.",electrodermal activity; emotional measurement; fidelity; galvanic skin response; games; immersion; quality; resolution; virtual environments; visual perception,Abstract,True,
Scopus,conferencePaper,2013,Facetons: face primitives with adaptive bounds for building 3D architectural models in virtual environment,VRST - Virtual Reality Software and Technology,A,"We present faceton, a geometric modeling primitive designed for building architectural models, using a six degrees of freedom (DoF) input device in a virtual environment (VE). A faceton is given as an oriented point floating in the air and defines a plane of infinite extent passing through the point. The polygonal mesh model is constructed by taking the intersection of the planes associated with the facetons. With the simple drag-and-drop and group interaction of faceton, users can easily create 3D architecture models in the VE. The faceton primitive and its interaction reduce the overhead associated with standard polygonal mesh modeling in VE, where users have to manually specify vertexes and edges which could be far away. The faceton representation is inspired by the research on boundary representations (B-rep) and constructive solid geometry (CSG), but it is driven by a novel adaptive bounding algorithm and is specifically designed for the 3D modeling activities in an immersive virtual environment.",geometric modeling; polygonal mesh; virtual reality,Keywords,True,
Scopus,conferencePaper,2013,Interacting with danger in an immersive environment: issues on cognitive load and risk perception,VRST - Virtual Reality Software and Technology,A,"Any human-computer interface imposes a certain level of cognitive load to the user task. Analogously, the task itself also imposes different levels of cognitive load. It is common sense in 3D user interfaces research that a higher number of degrees of freedom increases the interface cognitive load. If the cognitive load is significant, it might compromise the user performance and undermine the evaluation of user skills in a virtual environment. In this paper, we propose an assessment of two immersive VR interfaces with varying degrees of freedom in two VR tasks: risk perception and basic object selection. We examine the effectiveness of both interfaces in these two different tasks. Results show that the number of degrees of freedom does not significantly affect a basic selection task, but it affects risk perception task in an unexpected way.",3D interaction; presence; risk perception; virtual reality,Keywords,True,
Scopus,conferencePaper,2013,ShoeSoleSense: proof of concept for a wearable foot interface for virtual and real environments,VRST - Virtual Reality Software and Technology,A,"ShoeSoleSense is a proof of concept, novel body worn interface - an insole that enables location independent hands-free interaction through the feet. Forgoing hand or finger interaction is especially beneficial when the user is engaged in real world tasks. In virtual environments as moving through safety training applications is often conducted via finger input, which is not very suitable. To enable a more intuitive interaction, alternative control concepts utilize gesture control, which is usually tracked by statically installed cameras in CAVE-like-installations. Since tracking coverage is limited, problems may also occur. The introduced prototype provides a novel control concept for virtual reality as well as real life applications. Demonstrated functions include movement control in a virtual reality installation such as moving straight, turning and jumping. Furthermore the prototype provides additional feedback by heating up the feet and vibrating in dedicated areas on the surface of the insole.",eyes-free; foot; hands-free; insole; mobile; physical interface; shoe; tactile feedback; virtual reality; wearable,Abstract_Keywords,True,
Scopus,conferencePaper,2013,"Bubble bee, an alternative to arrow for pointing out directions",VRST - Virtual Reality Software and Technology,A,"We present Bubble Bee - an extension for the 3D bubble cursor in Virtual Environments (VEs). This technique provides an alternative to arrows for pointing out a direction in a 3D scene.Bubble Bee is based on a ring concept. A circular ring in 3D appears like an ellipse, according to its orientation. This orientation is easy to infer by comparing the minor radius which varies with the view angle, to the reference major radius which is constant and equal to the radius of the ring. Bubble Bee is a sphere with several rings oriented towards the same direction. The rings give a natural axis to the sphere. A color gradient sets the direction of this axis.We compared the performance of Bubble Bee and a 3D arrow through an experiment. The participants were asked to indicate which object was pointed by the two competing techniques. No significant differences on decision time were found, while Bubble Bee was shown to be nearly as accurate as a 3D arrow.",3D interaction; pointing aid; virtual reality,Keywords,True,
Scopus,conferencePaper,2013,Robust prediction of auditory step feedback for forward walking,VRST - Virtual Reality Software and Technology,A,"Virtual reality systems supporting real walking as a navigation interface usually lack auditory step feedback, although this could give additional information to the user e.g. about the ground he is walking on. In order to add matching auditory step feedback to virtual environments, we propose a calibration-free and easy to use system that can predict the occurrence time of stepping sounds based on human gait data.Our system is based on the timing of reliably occurring characteristic events in the gait cycle which are detected using foot mounted accelerometers and gyroscopes. This approach not only allows us to detect but to predict the time of an upcoming step sound in realtime. Based on data gathered in an experiment, we compare different suitable events that allow a tradeoff between the maximum precision of the prediction and the maximum time by which the sound can be predicted.",auditory feedback; gait; human walking; step prediction; step sound; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2013,Persuading people in a remote destination to sing by beaming there,VRST - Virtual Reality Software and Technology,A,"We built a Collaborative Virtual Environment (CVE) allowing one person, the 'visitor' to be digitally transported to a remote destination to interact with local people there. This included full body tracking, vibrotactile feedback and voice. This allowed interactions in the same CVE between multiple people situated in different physical remote locations. This system was used for an experiment to study whether the conveyance of touch has an impact on the willingness of participants embodied in the CVE to sing in public.In a first experimental condition, the experimenter virtually touched the avatar of the participants on the shoulder, producing vibrotactile feedback. In another condition using the identical physical setup, the vibrotactile displays were not activated, so that they would not feel the touch. Our hypothesis was that the tactile touch condition would produce a greater likelihood of compliance with the request to sing. In a second part we examined the hypothesis that people might be more willing to sing (execute an embarrassing task) in a CVE, because of the anonymity provided by virtual reality. Hence we carried out a similar study in physical reality.The results suggest that the tactile intervention had no effect on the sensations of body ownership, presence or the behaviours of the participants, in spite of the finding that the sensation of touch itself was effectively realised. Moreover we found an overall similarity in responses between the VR and real conditions.",collaborative virtual environments; embodiment; haptic interaction; presence; social touch,Abstract,True,
Scopus,conferencePaper,2013,Supporting interoperability and presence awareness in collaborative mixed reality environments,VRST - Virtual Reality Software and Technology,A,"In the BEAMING project we have been extending the scope of collaborative mixed reality to include the representation of users in multiple modalities, including augmented reality, situated displays and robots. A single user (a visitor) uses a high-end virtual reality system (the transporter) to be virtually teleported to a real remote location (the destination). The visitor may be tracked in several ways including emotion and motion capture. We reconstruct the destination and the people within it (the locals). In achieving this scenario, BEAMING has integrated many heterogeneous systems. In this paper, we describe the design and key implementation choices in the Beaming Scene Service (BSS), which allows the various processes to coordinate their behaviour. The core of the system is a light-weight shared object repository that allows loose coupling between processes with very different requirements (e.g. embedded control systems through to mobile apps). The system was also extended to support the notion of presence awareness. We demonstrate two complex applications built with the BSS.",interoperability; mixed reality; presence; telepresence; telerobotics; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2013,Real time whole body motion mapping for avatars and robots,VRST - Virtual Reality Software and Technology,A,"We describe a system that allows for controlling different robots and avatars from a real time motion stream. The underlying problem is that motion data from tracking systems is usually represented differently to the motion data required to drive an avatar or a robot: there may be different joints, motion may be represented by absolute joint positions and rotations or by a root position, bone lengths and relative rotations in the skeletal hierarchy. Our system resolves these issues by remapping in real time the tracked motion so that the avatar or robot performs motions that are visually close to those of the tracked person. The mapping can also be reconfigured interactively at run-time. We demonstrate the effectiveness of our system by case studies in which a tracked person is embodied as an avatar in immersive virtual reality or as a robot in a remote location. We show this with a variety of tracking systems, humanoid avatars and robots.",avatars; motion capture; robots; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2013,Color correction for optical see-through displays using display color profiles,VRST - Virtual Reality Software and Technology,A,"In optical see-through displays, light coming from background objects mixes with the light originating from the display, causing what is known as the color blending problem. Color blending negatively affects the usability of such displays as it impacts the legibility and color encodings of digital content. Color correction aims at reducing the impact of color blending by finding an alternative display color which, once mixed with the background, results in the color originally intended.In this paper we model color blending based on two distortions induced by the optical see-through display. The render distortion explains how the display renders colors. The material distortion explains how background colors are changed by the display material. We show the render distortion has a higher impact on color blending and propose binned-profiles (BP) - descriptors of how a display renders colors - to address it. Results show that color blending predictions using BP have a low error rate - within nine just noticeable differences (JND) in the worst case. We introduce a color correction algorithm based on predictions using BP and measure its correction capacity. Results show light display colors can be better corrected for all backgrounds. For high intensity backgrounds light colors in the neutral and CyanBlue regions perform better. Finally, we elaborate on the applicability, design and hardware implications of our approach.",augmented reality; color blending; color correction; display binned-profile; interface design; optical see-through displays,Keywords,True,
Scopus,conferencePaper,2013,Intercept tags: enhancing intercept-based systems,VRST - Virtual Reality Software and Technology,A,"In some virtual reality (VR) systems, OpenGL intercept methods are used to capture and render a desktop application's OpenGL calls within an immersive display. These systems often suffer from lower frame rates due to network bandwidth limitations, implementation of the intercept routine, and in some cases, the intercepted application's frame rate. To mitigate these issues and to enhance intercept-based systems in other ways, we present intercept tags, which are OpenGL geometries that are interpreted instead of rendered. We have identified and developed several uses for intercept tags, including hand-off interactions, display techniques, and visual enhancements. To demonstrate the value of intercept tags, we conducted a user study to compare a simple virtual hand technique implemented with and without intercept tags. Our results show that intercept tags significantly improve user performance and experience.",intercept tags; intercept-based systems; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2013,Are virtual patients effective to train diagnostic skills? a study with bulimia nervosa virtual patients,VRST - Virtual Reality Software and Technology,A,"Differential diagnosis is carried out early during the diagnostic interview, and this process requires a series of abilities that must be developed through sound training. The use of virtual reality in interactive simulations with Virtual Patients (VPs) enables students to learn by doing, through first-person experience, without interaction with real patients. VPs are interactive computer simulations of patient encounters used in health care education for learning and assessment. They typically include interactive features for illness history taking, explorations, tests, and features for suggesting diagnosis and treatment plans [Fors, Muntean, Botezatu and Zary, 2009]. VPs have been shown to have a great educational value especially for training clinical reasoning [Cook and Triola, 2009] and differential diagnosis [Peñaloza-Salazar et al. 2011]. The present study tested an application for teaching healthcare professionals the skills required to perform the differential diagnosis of bulimia nervosa.",,Abstract,True,
Scopus,conferencePaper,2013,Cue-elicited craving for food in virtual reality,VRST - Virtual Reality Software and Technology,A,"This study explores the use of virtual reality technology as an alternative to in vivo exposure in cue-exposure therapy for bingeing behavior, and assesses the ability of different virtual environments to elicit craving for food in a non-clinical sample. Previous research has indicated that craving for food can be elicited by exposure to food cues [Ferriday and Brunstrom 2011; Sobik, Hutchinson and Craighead 2005]. Given that craving for food is considered a trigger of bingeing, cue-exposure therapy with response prevention of bingeing may be effective in extinguishing the craving response in patients with eating disorders and obesity. However, the application of the in vivo cue exposure technique in the therapist's office faces logistical difficulties and is hampered by a lack of ecological validity [Koskina, Campbell and Schmidt 2013]. The use of Virtual reality (VR) technology may overcome the difficulties described. Nevertheless, before VR-based cue-exposure can be used for therapeutic purposes, the ability of VR scenarios to elicit craving responses in participants must be assessed. This is the objective of the present study.",,Title_Abstract,True,
Scopus,conferencePaper,2014,DigiTap: an eyes-free VR/AR symbolic input device,VRST - Virtual Reality Software and Technology,A,"In this paper we present DigiTap—a wrist-worn device specially designed for symbolic input in virtual and augmented reality (VR/AR) environments. DigiTap is able to robustly sense thumb-to-finger taps on the four fingertips and the eight minor knuckles. These taps are detected by an accelerometer, which triggers capturing of an image sequence with a small wrist-mounted camera. The tap position is then extracted with low computational effort from the images by an image processing pipeline. Thus, the device is very energy efficient and may potentially be integrated in a smartwatch-like device, allowing an unobtrusive, always available, eyes-free input. To demonstrate the feasibility of our approach an initial user study with our prototype device was conducted. In this study the suitability of the twelve tapping locations was evaluated, and the most prominent sources of error were identified. Our prototype system was able to correctly classify 92% of the input locations.",character input; optical glove; system control; virtual/augmented reality; wrist camera,Abstract_Keywords,True,
Scopus,conferencePaper,2014,Third person view and guidance for more natural motor behaviour in immersive basketball playing,VRST - Virtual Reality Software and Technology,A,"The use of Virtual Reality (VR) in sports training is now widely studied with the perspective to transfer motor skills learned in virtual environments (VEs) to real practice. However precision motor tasks that require high accuracy have been rarely studied in the context of VE, especially in Large Screen Image Display (LSID) platforms. An example of such a motor task is the basketball free throw, where the player has to throw a ball in a 46cm wide basket placed at 4.2m away from her. In order to determine the best VE training conditions for this type of skill, we proposed and compared three training paradigms. These training conditions were used to compare the combinations of different user perspectives: first (1PP) and third-person (3PP) perspectives, and the effectiveness of visual guidance. We analysed the performance of eleven amateur subjects who performed series of free throws in a real and immersive 1:1 scale environment under the proposed conditions. The results show that ball speed at the moment of the release in 1PP was significantly lower compared to real world, supporting the hypothesis that distance is underestimated in large screen VEs. However ball speed in 3PP condition was more similar to the real condition, especially if combined with guidance feedback. Moreover, when guidance information was proposed, the subjects released the ball at higher - and closer to optimal - position (5-7% higher compared to no-guidance conditions). This type of information contributes to better understand the impact of visual feedback on the motor performance of users who wish to train motor skills using immersive environments. Moreover, this information can be used by exergames designers who wish to develop coaching systems to transfer motor skills learned in VEs to real practice.",basketball training; immersive room; perception of distance in VR; performance; visual feedback,Abstract,True,
Scopus,conferencePaper,2014,Illumination independent marker tracking using cross-ratio invariance,VRST - Virtual Reality Software and Technology,A,"Marker tracking is used in numerous applications. Depending on the context and its constraints, tracking accuracy can be a crucial component of the application. In this paper, we firstly highlight that the tracking accuracy depends on the illumination, which is usually not controlled in most applications. Particularly, we show how corner detection can shift of several pixels when light power or background context change, even if the camera and the marker are static in the scene. Then, we propose a method, based on the cross ratio invariance, that allows to re-estimate the corner extraction so that the cross ratio of the marker model corresponds to the one computed from the extracted corners in the image. Finally, we show on real data that our approach improves the tracking accuracy, particularly along the camera depth axis, up to several millimeters, depending on the marker depth.",augmented reality; cross-ratio; illumination conditions; marker tracking,Keywords,True,
Scopus,conferencePaper,2014,I'm in VR! using your own hands in a fully immersive MR system,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel fully immersive Mixed Reality system that we have recently developed where the user freely walks in a life-size virtual scenario wearing an HMD and can see and use her/his own body when interacting with objects. This form of natural interaction is made possible in our system because the user's hands are real-time captured by means of a RGBD camera on the HMD. This allow the system to have in real-time a texturized geometric mesh of the hands and body (as seen from her/his own perspective) that can be rendered like any other polygonal model in the scene. Our hypothesis is that by presenting to the users an egocentric view of the virtual environment ""populated"" by their own bodies, a very strong feeling of presence is developed as well.",hand gestures; mixed reality; natural interaction,Abstract_Keywords,True,
Scopus,conferencePaper,2014,User-perspective augmented reality magic lens from gradients,VRST - Virtual Reality Software and Technology,A,"In this paper we present a new approach to creating a geometrically-correct user-perspective magic lens and a prototype device implementing the approach. Our prototype uses just standard color cameras, with no active depth sensing. We achieve this by pairing a recent gradient domain image-based rendering method with a novel semi-dense stereo matching algorithm inspired by PatchMatch. Our stereo algorithm is simple but fast and accurate within its search area. The resulting system is a real-time magic lens that displays the correct user perspective with a high-quality rendering, despite the lack of a dense disparity map.",augmented reality; gradient domain; image based rendering; magic lens; semi-dense stereo; user-perspective,Title_Keywords,True,
Scopus,conferencePaper,2014,Simulator sickness and presence using HMDs: comparing use of a game controller and a position estimation system,VRST - Virtual Reality Software and Technology,A,"Consumer-grade head-mounted displays (HMD) such as the Oculus Rift have become increasingly available for Virtual Reality recently. Their high degree of immersion and presence provokes usually amazement when first used. Nevertheless, HMDs also have been reported to cause adverse reactions such as simulator sickness. As their impact is growing, it is important to understand such side effects. This paper presents the results of a relatively large scale user experiment which compares using a conventional game controller versus positioning in the virtual world based upon the signal of the internal Inertial Measurement Unit (IMU) using Oculus Rift DK1. We show that simulator sickness is significantly reduced when using a position estimation system rather than using the more traditional game controller for navigation. However the sense of presence was not enhanced by the possibility of 'real walking'. We also show the impact of other factors, such as prior experience or motion history, and discuss the results.",locomotion; oculus rift; presence; simulator sickness; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2014,Desktop virtual reality for emergency preparedness: user evaluation of an aircraft ditching experience under different fear arousal conditions,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR), in the form of 3D interactive simulations of emergency scenarios, is increasingly used for emergency preparedness training. This paper advances knowledge about different aspects of such virtual emergency experiences, showing that: (i) the designs we propose in the paper are effective in improving emergency preparedness of common citizens, considering aviation safety as a relevant case study, (ii) changing specific visual and auditory features is effective to create emotionally different versions of the same experience, increasing the level of fear aroused in users, and (iii) the protection motivation role of fear highlighted by psychological studies of traditional media applies to desktop VR too.",aviation safety; emergency preparedness; fear arousal; training systems; user studies; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2014,Performance improvement using data tags for handheld spatial augmented reality,VRST - Virtual Reality Software and Technology,A,"Mobile devices such as some recent phones are now fitted with projection capabilities that support Spatial Augmented Reality (SAR) and require investigation to uncover new interaction possibilities. This paper presents a study measuring user performance in a search and select task using a tracked handheld projector and data tags, a 3D physical cue. This physical cue is used to mark the location of hidden SAR information. The experiment required participants to search for virtual symbols presented on two 5ft, multi-sided control panels. Two methods of presenting AR information were employed, SAR alone and SAR with the inclusion of physical cues to indicate the location of the information. The results showed that attaching data tags, compared to virtual content alone lowered the overall task completion time and reduced handheld projector movement. Subjectively, participants also preferred the combination of virtual data with data tags across both task variations",asynchronous collaboration; handheld projector; physical cues; spatial augmented reality; tangible user interface,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2014,A usability scale for handheld augmented reality,VRST - Virtual Reality Software and Technology,A,"Handheld augmented reality (HAR) applications must be carefully designed and improved based on user feedback to sustain commercial use. However, no standard questionnaire considers perceptual and ergonomic issues found in HAR. We address this issue by creating a HAR Usability Scale (HARUS).To create HARUS, we performed a systematic literature review to enumerate user-reported issues in HAR applications. Based on these issues, we created a questionnaire measuring manipulability – the ease of handling the HAR system, and comprehensibility – the ease of understanding the information presented by HAR. We then provide evidences of validity and reliability of the HARUS questionnaire by applying it to three experiments. The results show that HARUS consistently correlates with other subjective and objective measures of usability, thereby supporting its concurrent validity. Moreover, HARUS obtained a good Cronbach's alpha in all three experiments, thereby demonstrating internally consistency.HARUS, as well as its decomposition into individual manipulability and comprehensibility scores, are evaluation tools that researchers and professionals can use to analyze their HAR applications. By providing such a tool, they can gain quality feedback from users to improve their HAR applications towards commercial success.",augmented reality; evaluation method; handheld devices; usability; user studies,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2014,The influence of step frequency on the range of perceptually natural visual walking speeds during walking-in-place and treadmill locomotion,VRST - Virtual Reality Software and Technology,A,"Walking-In-Place (WIP) techniques make relatively natural walking experiences within immersive virtual environments possible when the physical interaction space is limited in size. In order to facilitate such experiences it is necessary to establish a natural connection between steps in place and virtual walking speeds. This paper details a study investigating the effects of movement type (treadmill walking and WIP) and step frequency (1.4, 1.8 and 2.2 steps per second) on the range of perceptually natural visual walking speeds. The results suggests statistically significant main effects of both movement type and step frequency but no significant interaction between the two variables.",locomotion; perceived naturalness; speed perception; virtual reality; walking-in-place,Keywords,True,
Scopus,conferencePaper,2014,Displaying shapes with various types of surfaces using visuo-haptic interaction,VRST - Virtual Reality Software and Technology,A,"In this paper, we proposed a visuo-haptic system for displaying various shapes which have curve, edge, and inclined surfaces, using a simple transmutative physical device and the effect of visuo-haptic interaction. We aim to construct a perception-based shape display system to provide users with the sensation of touching virtual objects of varying shapes using only a simple mechanism. We have confirmed that the perception of each primitive shape such as curvature and angle could be modified by displacing a user's hand image on the monitor as if s/he were touching the visual shape while actually touching another shape. In this study, we constructed the method to merge these findings for displaying more various shapes, including angular ones. We built a transmutative device, which the user touches. The device does not undergo significant transformation, but its surface can be slightly bumped in and out, and displayed various shapes with various angles, length and curvature. The results of experimental trials confirmed that our method for displaying each primitive shape can also worked as designed when we combine these findings to display more complex objects using this device which transforms slightly.",perception-based shape display; virtual reality; visuo-haptics,Keywords,True,
Scopus,conferencePaper,2014,In touch with the remote world: remote collaboration with augmented reality drawings and virtual navigation,VRST - Virtual Reality Software and Technology,A,"Augmented reality annotations and virtual scene navigation add new dimensions to remote collaboration. In this paper, we present a touchscreen interface for creating freehand drawings as world-stabilized annotations and for virtually navigating a scene reconstructed live in 3D, all in the context of live remote collaboration. Two main focuses of this work are (1) automatically inferring depth for 2D drawings in 3D space, for which we evaluate four possible alternatives, and (2) gesture-based virtual navigation designed specifically to incorporate constraints arising from partially modeled remote scenes. We evaluate these elements via qualitative user studies, which in addition provide insights regarding the design of individual visual feedback elements and the need to visualize the direction of drawings.",augmented reality; CSCW; depth interpretation; gesture recognition; telepresence; touch; video-mediated communication,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2014,A perspective geometry approach to user-perspective rendering in hand-held video see-through augmented reality,VRST - Virtual Reality Software and Technology,A,"Video see-through Augmented Reality (V-AR) displays a video feed overlaid with information, co-registered with the displayed objects. In this paper we consider the type of V-AR that is based on a hand-held device with a fixed camera. In most of the VA-R applications the view displayed on the screen is completely determined by the orientation of the camera, i.e., the device-perspective rendering; the screen displays what the camera sees. The alternative method is to use the relative pose of the user's view and the camera, i.e., the user-perspective rendering. In this paper we present an approach to the user perspective V-AR using 3D projective geometry. The view is adjusted to the user's perspective and rendered on the screen, making it an augmented window. We created and tested a running prototype based on our method.",augmented reality; dynamic frustum; user-perspective; video see-through,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2014,A projection-based mixed-reality display for exterior and interior of a building diorama,VRST - Virtual Reality Software and Technology,A,"This paper proposes an interactive display system that displays both of the exterior and interior construction of a building diorama by using a projection-based Mixed-Reality (MR) technique, which is useful for understanding the complex construction and the spatial relationships between outside and inside. The users can hold and move the diorama model using their hands/body motion, so that they can observe the model from their favorite viewpoint. Our system obtains both of the user's information (the viewpoint and the gesture) and the diorama model's information (the pose) in 3D space by using two RGB-D cameras. The CG image corresponding to the user's viewpoint, gesture and the pose of the diorama is rendered by Dual Rendering algorithm in real time. As the result, the generated CG image is projected onto the diorama to realize MR display. We confirm the effectiveness of our proposed method by developing a pilot system.",diorama interface; interactive display; projection-based mixed reality; RGB-D camera; visualization,Keywords,True,
Scopus,conferencePaper,2014,A view from the hill: where cross reality meets virtual worlds,VRST - Virtual Reality Software and Technology,A,"We present the cross reality [Lifton 2007] system 'Mirrorshades', which enables a user to be present and aware of both a virtual reality environment and the real world at the same time. In so doing the challenge of the vacancy problem is addressed by lightening the cognitive load needed to switch between realities and to navigate the virtual environment. We present a case study in the context of a cultural heritage application wherein users are able to compare a reconstruction of an important 15th century chapel with its present day instantiation, whilst walking through them.",cross reality; head mounted display; indoor positioning,Abstract,True,
Scopus,conferencePaper,2014,Dual sensor filtering for robust tracking of head-mounted displays,VRST - Virtual Reality Software and Technology,A,"We present a low-cost solution for yaw drift in head-mounted display systems that performs better than current commercial solutions and provides a wide capture area for pose tracking. Our method applies an extended Kalman filter to combine marker tracking data from an overhead camera with onboard head-mounted display accelerometer readings. To achieve low latency, we accelerate marker tracking with color blob localisation and perform this computation on the camera server, which only transmits essential pose data over WiFi for an unencumbered virtual reality system.",fast feature tracking; head-mounted display,Abstract,True,
Scopus,conferencePaper,2014,On the benefits of stereo graphics in virtual obstacle avoidance tasks,VRST - Virtual Reality Software and Technology,A,"In virtual reality, stereo graphics is a very common way of increasing the level of perceptual realism in the visual part of the experience. However, stereo graphics comes at cost, both in technical terms and from a user perspective. In this paper, we present the preliminary results of an experiment to see if stereo makes any quantifiable, statistically significant difference in the ability to avoid collisions with virtual obstacles while navigating a 3-D space under constant acceleration. Our results indicate that for this particular application scenario, stereo does provide a significant benefit in terms of the amount of time that participants were able to avoid obstacles.",head-mounted displays; navigation; perception; stereo graphics,Abstract,True,
Scopus,conferencePaper,2014,The collaborative design platform protocol: a protocol for a mixed reality installation for improved incorporation of laypeople in architecture,VRST - Virtual Reality Software and Technology,A,"We present the conceptual design and implementation of the Collaborative Design Platform Protocol (CDPP), a communication protocol that offers the synchronisation of virtual worlds between two mixed reality peers. The CDPP is applied to connect the Collaborative Design Platform (CDP), a design tool which supports the architectural design process in an early stage, with the immersive Cave Automatic Virtual Environment (CAVE) display, where the design is visualised in life-size. This creates a prototype which enables a cost-efficient and easily comprehensible presentation of the early-staged design, and thus significantly simplifies the incorporation of laypeople in the early design process. By this means, the creative capabilities of laymen are exploited to a greater extent.",architecture; collaboration; protocol,Title_Abstract,True,
Scopus,conferencePaper,2014,Virtualized welding: a new paradigm for tele-operated welding,VRST - Virtual Reality Software and Technology,A,"We present a new mixed reality system that supports tele-operation of a welding robot. We create a 3D mockup of the welding pieces and use projector-based displays to visualize the welding process directly on the 3D display. Multi-cameras are used to capture both the welding environment and the operator's motion. The welder can therefore monitor and control the welding process as if the welding is on the mock-up, which provides proper spatial and 3D cues. We evaluated our system with a number of control tasks and the results shows the effectiveness of our system as compared to traditional alternatives.",engineering; human factors; human machine interaction; immersion; mixed reality; tele-operation; welding,Abstract_Keywords,True,
Scopus,conferencePaper,2015,Indoor skydiving in immersive virtual reality with embedded storytelling,VRST - Virtual Reality Software and Technology,A,"We describe the Virtual Jump Simulator, which allows subjects to perform an indoor parachute jump in a virtual environment. The necessity to jump physically off a platform combined with immersive virtual reality and tactile feedback creates an experience with a high amount of presence, as the evaluation of the prototype confirms. The system consists of a steel cube, a mechanical absorber system with stacked eccentric wheels and counterweights that allows subjects in the weight range from 35 to 150kg to jump without the need for individual calibration, a virtual reality setup with high-quality 3D content and tactile stimuli. In the immersive virtual jump experience, we embed a story using rich multimedia content, such as images and sound. We iteratively tested the entire system with users of different backgrounds. Thereby, we gathered user feedback from the very beginning to create a novel virtual reality system that allows for actual physical jumping and flying with free body movement.",entertainment; hybrid physical digital installation; immersive virtual reality; model of interactivity,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2015,Head-mounted display with mid-air tactile feedback,VRST - Virtual Reality Software and Technology,A,"Virtual and physical worlds are merging. Currently users of head-mounted displays cannot have unobtrusive tactile feedback while touching virtual objects. We present a mid-air tactile feedback system for head-mounted displays. Our prototype uses the focus of a modulated ultrasonic phased array for unobtrusive mid-air tactile feedback generation. The array and the hand position sensor are mounted on the front surface of a head-mounted virtual reality display. The presented system can enhance 3D user interfaces and virtual reality in a new way.To evaluate the tactile feedback together with visuals on an Oculus Rift VR headset, we had 13 participants do a simple virtual keypad tapping task with and without tactile feedback. The results indicate that while the measured speed and accuracy differed only a little, the subjects were nearly unanimous in that they preferred to use the tactile feedback. The ""raw"" NASA TLX questionnaires conducted after use revealed that the participants felt slightly less mental, physical and temporal demand with the tactile feedback. The participants' self-assessment of their performance was also higher with the tactile feedback.",3D interaction; 3D user interfaces; augmented reality; gestures; head-mounted display; mid-air tactile feedback; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2015,Virtual reality based laparoscopic surgery simulation,VRST - Virtual Reality Software and Technology,A,"With the development of computer graphic and haptic devices, training surgeons with virtual reality technology has proven to be very effective in surgery simulation. Many successful simulators have been deployed for training medical students. However, due to the various unsolved technical issues, the laparoscopic surgery simulation has not been widely used. Such issues include modeling of complex anatomy structure, large soft tissue deformation, frequent surgical tools interactions, and the rendering of complex material under the illumination of headlight. A successful laparoscopic surgery simulator should integrate all these required components in a balanced and efficient manner to achieve both visual/haptic quality and a satisfactory refreshing rate. In this paper, we propose an efficient framework integrating a set of specially tailored and designed techniques, ranging from deformation simulation, collision detection, soft tissue dissection and rendering. We optimize all the components based on the actual requirement of laparoscopic surgery in order to achieve an improved overall performance of fidelity and responding speed.",collision detection; deformation; dissection; laparoscopic surgery; rendering,Title_Abstract,True,
Scopus,conferencePaper,2015,Embodied interaction using non-planar projections in immersive virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper we evaluate the use of non-planar projections as a means to increase the Field of View (FoV) in embodied Virtual Reality (VR). Our main goal is to bring the virtual body into the user's FoV and to understand how this affects the virtual body/environment relation and quality of interaction. Subjects wore a Head Mounted Display (HMD) and were instructed to perform a selection and docking task while using either Perspective (≈ 106 ° vertical FoV), Hammer or Equirectangular (≈ 180 ° vertical FoV for both) projection. The increased FoV allowed for a shorter search time as well as less head movements. However, quality of interaction was generally inferior, requiring more time to dock, increasing docking error and producing more body/environment collisions. We also assessed cybersickness and the sense of embodiment toward the virtual body through questionnaires, for which the difference between projections seemed to be less pronounced.",immersive interaction; non-planar projections; sense of embodiment,Title_Abstract,True,
Scopus,conferencePaper,2015,Optimal camera placement for motion capture systems in the presence of dynamic occlusion,VRST - Virtual Reality Software and Technology,A,"Optical motion capture is based on estimating the three-dimensional positions of markers by triangulation from multiple cameras. Successful performance depends on points being visible from at least two cameras and on the accuracy of the triangulation. Triangulation accuracy is strongly related to the positions and orientations of the cameras. Thus, the configuration of the camera network has a critical impact on performance. A poor camera configuration may result in a low quality three-dimensional (3D) estimation and consequently low quality of tracking. This paper proposes a camera configuration metric that is based on a probabilistic model of target point visibility from cameras with ""good"" views. An efficient algorithm, based on simulated annealing, is introduced for estimating the optimal configuration of an arbitrary set of cameras for a given distribution of target points. The accuracy and robustness of the algorithm are evaluated through both simulation and empirical measurement. An implementation of the method is available for download as a tool for the community [Rahimian and Kearney 2015] (Figure 1).",augmented reality; camera placement; CAVE; dynamic occlusion; motion capture; virtual reality,Keywords,True,
Scopus,conferencePaper,2015,Realizing a low-latency virtual reality environment for motor learning,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) has the potential to support motor learning in ways exceeding beyond the possibilities provided by real world environments. New feedback mechanisms can be implemented that support motor learning during the performance of the trainee and afterwards as a performance review. As a consequence, VR environments excel in controlled evaluations, which has been proven in many other application scenarios.However, in the context of motor learning of complex tasks, including full-body movements, questions regarding the main technical parameters of such a system, in particular that of the required maximum latency, have not been addressed in depth. To fill this gap, we propose a set of requirements towards VR systems for motor learning, with a special focus on motion capturing and rendering. We then assess and evaluate state-of-the-art techniques and technologies for motion capturing and rendering, in order to provide data on latencies for different setups. We focus on the end-to-end latency of the overall system, and present an evaluation of an exemplary system that has been developed to meet these requirements.",low-latency; motor learning; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2015,Dynamic projection mapping onto a deformable object with occlusion based on high-speed tracking of dot marker array,VRST - Virtual Reality Software and Technology,A,"In recent years, projection mapping has attracted much attention in a variety of fields. Generally, however, the objects in projection mapping are limited to rigid and static or quasistatic objects. Dynamic projection mapping onto a deformable object could remarkably expand the possibilities. In order to achieve such a projection mapping, it is necessary to recognize the deformation of the object even when it is occluded. However, it is still a challenging problem to achieve this task in real-time with low latency. In this paper, we propose an efficient, high-speed tracking method utilizing high-frame-rate imaging. Our method is able to track an array of dot markers arranged on a deformable object even when there is external occlusion caused by the user interaction and self-occlusion caused by the deformation of the object itself. Additionally, our method can be applied to a stretchable object. Dynamic projection mapping with our method showed robust and consistent display onto a sheet of paper and cloth with a tracking performance of about 0.2 ms per frame, with the result that the projected pattern appeared to be printed on the deformable object.",augmented reality; deformable object; high-speed vision; projection mapping; tracking,Keywords,True,
Scopus,conferencePaper,2015,Modeling spatial relations of human body parts for indexing and retrieving close character interactions,VRST - Virtual Reality Software and Technology,A,"Retrieving pre-captured human motion for analyzing and synthesizing virtual character movement have been widely used in Virtual Reality (VR) and interactive computer graphics applications. In this paper, we propose a new human pose representation, called Spatial Relations of Human Body Parts (SRBP), to represent spatial relations between body parts of the subject(s), which intuitively describes how much the body parts are interacting with each other. Since SRBP is computed from the local structure (i.e. multiple body parts in proximity) of the pose instead of the information from individual or pairwise joints as in previous approaches, the new representation is robust to minor variations of individual joint location. Experimental results show that SRBP outperforms the existing skeleton-based motion retrieval and classification approaches on benchmark databases.",close interaction; human motion; motion classification; motion retrieval; spatial relations,Abstract,True,
Scopus,conferencePaper,2015,Augmented reality for collision warning and path guide in a vehicle,VRST - Virtual Reality Software and Technology,A,"Today, the automotive industry is focused on human-vehicle interaction related with safety and convenience while driving. One of those is head-up display (HUD) to implement augmented reality (AR) in a vehicle for information offering [Park et al. 2013]. In this study, we discuss a vehicle Augmented Reality Information System (vARIS) to offer information such as collision warning and path guide through augmented reality (AR) in a vehicle. This system realizes AR on windshield using projection type HUD and provides AR information on 50 inch HUD display for safety and convenience of a driver. A currently commercialized HUD has 3 5 inch display.",,Title_Abstract,True,
Scopus,conferencePaper,2015,Local context based recognition + internet of things: complementary infrastructures for future generic mixed reality space,VRST - Virtual Reality Software and Technology,A,"The ""Internet of Things (IoT)"" is one of the most promising emerging technologies today. IoT refers to the concept of, if not all, many everyday objects possessing information processing power and network connectivity from which personal and context based services can be derived collectively [Perera et al, 2014]. Any useful and successful service will require on an effective and usable means for interaction, and Augmented Reality (AR) has been touted as one such ideal method. One difficulty with the proliferation of the AR technology (and associated services) is with the provision of large scale and usable contents. Useful contents and services often require recognition and tracking of a very high number of objects, and as the target objects and respective services increase, so will data redundancy and error.",,Title_Abstract,True,
Scopus,conferencePaper,2015,Resolving view difference between eye and camera for proprioceptive pointing and selection in augmented reality applications,VRST - Virtual Reality Software and Technology,A,"In this poster, we propose ""proprioceptive"" pointing (and selection) in which we use the finger and the sense of proprioception, without focusing on the finger, to point and select an object in the real world for the purpose of further interaction. The assumption is that this will reduce user fatigue since the user will switch one's focus less frequently, while still being able to point effectively through the proprioceptive sense. Figure 1 illustrates the concept and how such a technique could be effectively used e.g. for object query system using a see-through glass. In this typical scenario, the user selects an object in the real world (by proprioceptive pointing/designation), which in turn is captured by the on-glass camera and then identified and recognized for final augmentation. Note the ""blurry"" fingertip used for aiming to the target object.",,Title,True,
Scopus,conferencePaper,2015,Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality,VRST - Virtual Reality Software and Technology,A,"In this work, we present a technique based on image saliency analysis to improve the conspicuity of the foreground augmentation to the background real world medium by adjusting the local brightness contrast. The proposed technique is implemented on a mobile platform considering the usage nature of AR. The saliency computation is carried out for the augmented object's representative color rather than all the pixels, and searching and adjusting over only a discrete number of brightness levels to produce the highest contrast saliency, thereby making near-real time computation possible. Thus, while the resulting imagery may not be optimal due to such a simplification, our tests showed that the visibility was still significantly improved without much difference to the ""optimal"" ground truth in terms of correctly perceiving and recognizing the augmented information",,Title,True,
Scopus,conferencePaper,2015,Evaluating warped projection cameras,VRST - Virtual Reality Software and Technology,A,"Our primary motivation in this work is creating head-worn virtual reality (VR) and augmented reality (AR) systems that substantially improve human task performance, particularly in searching, counting, and navigation tasks. We focus on both widening the user's field of view, and also providing the user with options for seeing around occluding objects.",,Abstract,True,
Scopus,conferencePaper,2015,Evaluation of factors affecting distance perception in architectural project review in immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"Distances are perceived as being more compressed in immersive virtual environments (IVEs) than in real environments. The goal of this study is to identify the most important factors that influence decision making and accuracy of distance perception in the context of architectural project reviews. Technical factors such as field of view, display devices and motion parallax were widely studied. In this paper, we have investigated other individual and contextual factors. We conducted a between-subject experiment using an immersive large screen display to examine the influence of the three factors: 1) the cognitive profile of the user (visual, auditory and kinesthetic - VAK), 2) the furnishing of the house, and 3) the locomotion speed, on distance perception. Results reveal that participants with visual profile were more accurate in distance estimation. Further, furnished houses were more suitable for virtual visits. The locomotion speed also seems to influence virtual visits which were better with a slow locomotion speed. Based on the results of our study, we finally present guidelines for setting up architectural project review tools which employ similar setup.",architectural project review; cognitive profile; distance estimation; distance perception; immersive virtual environment; virtual reality; virtual visits,Keywords,True,
Scopus,conferencePaper,2015,Augmented reality-aided tele-presence system for robot manipulation in industrial manufacturing,VRST - Virtual Reality Software and Technology,A,"This work investigates the use of a highly immersive telepresence system for industrial robotics. A Robot Operating System integrated framework is presented where a remote robot is controlled through operator's movements and muscle contractions captured with a wearable device. An augmented 3D visual feedback is sent to the user providing the remote environment scenario from the robot's point of view and additional information pertaining to the task execution. The system proposed, using robot mounted RGB-D camera, identifies known objects and relates their pose to robot arm pose and to targets relevant to the task execution. The system is preliminary validated during a pick-and-place task using a Baxter robot. The experiment shows the practicability and the effectiveness of the proposed approach.",augmented and mixed reality; tele-operation and tele-presence; tracking and sensing,Title_Keywords,True,
Scopus,conferencePaper,2016,Localized color correction for optical see-through displays via weighted linear regression,VRST - Virtual Reality Software and Technology,A,"Visual consistency in augmented reality displays requires truthful color reproduction of virtual images. However, the color distortion of Optical See-Through Displays hinders truthful color reproduction. We propose a color correction method for Optical See-Through Displays with three contributions. First, we handle non-linearity of color distortion by localized regression. Second, we model the color distortion in CIE XYZ domain, a device-independent representation of color, based on color measurements. This supports the locally linear modeling of color distortion. Finally, we introduce Hue-constrained gamut mapping for color correction. Experimental results validate the three contributions by showing critically meaningful performance gain.",CIE XYZ color space; color distortion correction; gamut mapping; local linear regression; OST-HMD,Abstract,True,
Scopus,conferencePaper,2016,Perceptual enhancement for stereoscopic videos based on horopter consistency,VRST - Virtual Reality Software and Technology,A,"Audience discomfort, such as eye strain and dizziness, is one of the urgent issues that virtual reality and 3D movie technologies should tackle. Except for inappropriate horizontal and vertical disparity, one major problem is that people's binocular vergence and focal length in the cinema remain inconsistent from normal visual habits. Psychologists discovered the horopter and Panum's fusional area to describe zero-disparity points projected on the retinas based on accommodation-convergence consistency. In this paper, inspired by these concepts, we propose a stereoscopic effect correction system for perceptual enhancement according to fixated region and scene information. As a preprocessing step, tracking and stereo matching algorithms are implemented to prepare cues for further transformation in 3D space. Then in order to accomplish certain visual effects, we describe a geometric framework for disparity refinement and image warping based on parameter adjustment of the virtual stereoscopic rig. For evaluation, subjective experiments have been conducted to prove the effectiveness of our method. Therefore, our work provides a possibility to improve the audience experience from a formerly underexplored perspective.",horopter consistency; image warping; perceptual enhancement; stereoscopic videos; virtual rig modification,Abstract,True,
Scopus,conferencePaper,2016,Eye gaze tracking with google cardboard using purkinje images,VRST - Virtual Reality Software and Technology,A,"Mobile phone-based Virtual Reality (VR) is rapidly growing as a platform for stereoscopic 3D and non-3D digital content and applications. The ability to track eye gaze in these devices would be a tremendous opportunity on two fronts: firstly, as an interaction technique, where interaction is currently awkward and limited, and secondly, for studying human visual behavior. We propose a method to add eye gaze tracking to these existing devices using their on-board display and camera hardware, with a minor modification to the headset enclosure. We present a proof-of-concept implementation of the technique and show results demonstrating its feasibility. The software we have developed will be made available as open source to benefit the research community.",eye tracking; low cost; purkinje images; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Acoustic selfies for extraction of external ear features in mobile audio augmented reality,VRST - Virtual Reality Software and Technology,A,"Virtual and augmented realities are expected to become more and more important in everyday life in the next future; the role of spatial audio technologies over headphones will be pivotal for application scenarios which involve mobility. This paper introduces the SelfEar project, aimed at low-cost acquisition and personalization of Head-Related Transfer Functions (HRTFs) on mobile devices. This first version focuses on capturing individual spectral features which characterize external ear acoustics, through a self-adjustable procedure which guides users in collecting such information: their mobile device must be held with the stretched arm and positioned at several specific elevation points; acoustic data are acquired by an audio augmented reality headset which embeds a pair of microphones at listener ear-canals. A preliminary measurement session assesses the ability of the system to capture spectral features which are crucial for elevation perception. Moreover, a virtual experiment using a computational auditory model predicts clear vertical localization cues in the measured features.",binaural audio; computational auditory model; head-related transfer function; headphones; mobile augmented reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,The impact of latency on perceptual judgments and motor performance in closed-loop interaction in virtual reality,VRST - Virtual Reality Software and Technology,A,"Latency between a user's movement and visual feedback is inevitable in every Virtual Reality application, as signal transmission and processing take time. Unfortunately, a high end-to-end latency impairs perception and motor performance. While it is possible to reduce feedback delay to tens of milliseconds, these delays will never completely vanish. Currently, there is a gap in literature regarding the impact of feedback delays on perception and motor performance as well as on their interplay in virtual environments employing full-body avatars. With the present study at hand, we address this gap by performing a systematic investigation of different levels of delay across a variety of perceptual and motor tasks during full-body action inside a Cave Automatic Virtual Environment. We presented participants with their virtual mirror image, which responded to their actions with feedback delays ranging from 45 to 350 ms. We measured the impact of these delays on motor performance, sense of agency, sense of body ownership and simultaneity perception by means of psychophysical procedures. Furthermore, we looked at interaction effects between these aspects to identify possible dependencies. The results show that motor performance and simultaneity perception are affected by latencies above 75 ms. Although sense of agency and body ownership only decline at a latency higher than 125 ms, and deteriorate for a latency greater than 300 ms, they do not break down completely even at the highest tested delay. Interestingly, participants perceptually infer the presence of delays more from their motor error in the task than from the actual level of delay. Whether or not participants notice a delay in a virtual environment might therefore depend on the motor task and their performance rather than on the actual delay.",body ownership; full-body motion capture; latency; sense of agency; simultaneity perception; virtual mirror,Title_Abstract,True,
Scopus,conferencePaper,2016,The asynchronous time warp for virtual reality on consumer hardware,VRST - Virtual Reality Software and Technology,A,"To help create a true sense of presence in a virtual reality experience, a so called ""time warp"" may be used. This time warp does not only correct for the optical aberration of the lenses used in a virtual reality headset, it also transforms the stereoscopic images based on the very latest head tracking information to significantly reduce the motion-to-photon delay (or end-to-end latency). The time warp operates as close as possible to the display refresh, retrieves updated head tracking information and transforms a stereoscopic pair of images from representing a view at the time it was rendered, to representing the correct view at the time it is displayed. When run asynchronously to the stereoscopic rendering, the time warp can be used to increase the perceived frame rate and to smooth out inconsistent frame rates. Asynchronous operation can also improve the overall graphics hardware utilization by not requiring the stereoscopic rendering to be synchronized with the display refresh cycle. However, on today's consumer hardware it is challenging to implement a high quality time warp that is fast, has predictable latency and throughput, and runs asynchronously. This paper discusses the various challenges and the different trade-offs that need to be considered when implementing an asynchronous time warp on consumer hardware.",image warping; latency; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Towards comparable evaluation methods and measures for timing behavior of virtual reality systems,VRST - Virtual Reality Software and Technology,A,"A low latency is a fundamental timeliness requirement to reduce the potential risks of cyber sickness and to increase effectiveness, efficiency, and user experience of Virtual Reality Systems. The effects of uniform latency degradation based on mean or worst-case values are well researched. In contrast, the effects of latency jitter, the distribution pattern of latency changes over time has largely been ignored so far although today's consumer VR systems are extremely vulnerable in this respect. We investigate the applicability of the Walsh, generalized ESD, and the modified z-score test for the detection of outliers as one central latency distribution aspect. The tests are applied to well defined test cases mimicking typical timing behavior expected from concurrent architectures of today. We introduce accompanying graphical visualization methods to inspect, analyze and communicate the latency behavior of VR systems beyond simple mean or worst-case values. As a result, we propose a stacked modified z-score test for more detailed analysis.",cyber sickness; latency; outlier; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,The effects of cybersickness on persons with multiple sclerosis,VRST - Virtual Reality Software and Technology,A,"Cybersickness is commonly experienced by the users in immersive Virtual Environments (VE). It has symptoms similar to Motion Sickness, such as dizziness, nausea etc. Although there have been many cybersickness experiments conducted with persons without disabilities, persons with disabilities, such as Multiple Sclerosis (MS), have been minimally studied. This is an important area of research because cybersickness could have negative effects on virtual rehabilitation effectiveness and the accessibility of VEs. For this experiment, we recruited 16 participants - 8 persons with MS and 8 persons without MS from similar demographics (e.g. age, race). Two participants from population without MS could not complete the experiment due to severe cybersickness. We asked each participant to experience a VE. We collected Galvanic Skin response (GSR) data before and during VR exposure; GSR is commonly used as an objective measure of cybersickness. Also, Simulator Sickness Questionnaire (SSQ) feedback was recorded before and after the experiment. SSQ results show that the VE induced cybersickness in the participants. The GSR data suggests that the cybersickness may have induced similar physiological changes in participants with MS as participants without MS, albeit with greater variability in participants without MS. However, participants with MS had significantly lower GSR during VR exposure. In this paper, we compare the effects of cybersickness between the people with MS and the people without MS with respect to SSQ score and GSR data.",accessibility; cybersickness; multiple sclerosis; user studies; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,Investigating the process of emotion recognition in immersive and non-immersive virtual technological setups,VRST - Virtual Reality Software and Technology,A,"This paper investigates the use of Immersive Virtual Environment (IVE) to evaluate the process of emotion recognition from faces (ERF). ERF has been mostly probed by using still photographs resembling universal expressions. However, this approach does not reflect the vividness of faces. Virtual Reality (VR) makes use of animated agents, trying to overcome this issue by reproducing the inherent dynamic of facial expressions, but outside a natural environment. We suggest that a setup using IVE technology simulating a real scene in combination with virtual agents (VAs) displaying dynamic facial expressions should improve the study of ERF. To support our claim we carried out an experiment in which two groups of subjects had to recognize VAs facial expression of universal and basic emotions in IVE and No-IVE condition. The goal was to evaluate the impact of the immersion in VE for ERF investigation. Results showed that the level of immersion in IVE does not interfere with the recognition task and a high level of accuracy in facial recognition suggests that IVE can be used to investigate the process of ERF.",ekman basic emotion; emotion recognition; emotional virtual agents; facial expression; immersive virtual environment; virtual environments; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Provision of automated step-by-step procedural guidance in virtual reality surgery simulation,VRST - Virtual Reality Software and Technology,A,"One of the roadblocks to the wide-spread use of virtual reality simulation as a surgical training platform is the need for expert supervision during training to ensure proper skill acquisition. To fully utilize the capacity of virtual reality in surgical training, it is imperative that the guidance process is automated. In this paper, we discuss a method of providing one aspect of performance guidance: advice on the steps of a surgery or procedural guidance. We manually segment the surgical trajectory of an expert surgeon into steps and present them one at a time to guide trainees through a surgical procedure. We show, using a randomized controlled trial, that this form of guidance is effective in moving trainee behavior towards an expert ideal.To support practice variation and different surgical styles adopted by experts, separate guidance templates have to be generated. To enable this, we introduce a method of automatically segmenting a surgical trajectory into steps. We propose a pre-processing step that uses domain knowledge specific to our application to reduce the solution space. We show how this can be incorporated into existing trajectory segmentation methods, as well as a greedy approach that we propose. We compare this segmentation method to existing techniques and show that it is accurate and efficient.",automated guidance; surgery simulation; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Head turn scaling below the threshold of perception in immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"Immersive virtual environments allow to experience presence, the feeling of being present in a virtual environment. When accessing virtual reality with virtual reality goggles, head tracking is used to update the virtual viewpoint according to the user's head movement. While typically used unmodified, the extent to which the virtual viewpoint follows the real head motion can be scaled. In this paper, the effect of scaling below the threshold of perception on presence during a target acquisition task was studied. It was assumed, that presence is reduced when head motion is scaled. No effect on presence, simulator sickness and performance was found. A significant effect on physical task load was found. The results yield information for further work and for the required verification of the used concept of presence. It can be assumed, that load can be modified by the scaling without significantly influencing the quality of presence.",empirical study; head tracking manipulation; immersive virtual environments; perception; presence; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,"Noise-cancelling, steps and soundscapes: the effect of auditory stimulation on presence in virtual realities while walking",VRST - Virtual Reality Software and Technology,A,"This study investigates the influence of different auditory stimuli on perceived presence, the feeling of ""being there"", on a walk through a park-like virtual environment. A single-factorial design with five levels varying the surrounding sound impressions was employed, including the conditions ""No Headphones"", ""Noise-Cancelling"", ""Steps"", ""Soundscape"" and ""Steps &amp; Soundscape"", in order to find out which of the conditions would enhance the feeling of presence most. 36 participants rated their impression of presence using a questionnaire after walking on a treadmill while wearing a head-mounted display and Noise-Cancelling headphones. Statistical analysis of the data showed that the conditions including soundscapes resulted in significantly higher ratings of presence and realism, compared to all other auditory conditions. The results are placed into the context of findings in other studies and point to further research needs regarding the auditory enhancement of virtual environments.",acoustic; head-mounted display; multimodality; noise-cancelling; presence; self-motion; soundscape; treadmill; virtual reality; visual,Keywords,True,
Scopus,conferencePaper,2016,PedVR: simulating gaze-based interactions between a real user and virtual crowds,VRST - Virtual Reality Software and Technology,A,"We present a novel interactive approach, PedVR, to generate plausible behaviors for a large number of virtual humans, and to enable natural interaction between the real user and virtual agents. Our formulation is based on a coupled approach that combines a 2D multi-agent navigation algorithm with 3D human motion synthesis. The coupling can result in plausible movement of virtual agents and can generate gazing behaviors, which can considerably increase the believability. We have integrated our formulation with the DK-2 HMD and demonstrate the benefits of our crowd simulation algorithm over prior decoupled approaches. Our user evaluation suggests that the combination of coupled methods and gazing behavior can considerably increase the behavioral plausibility.",crowds; human agents; multi-agent simulation; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,A real-time x-ray mobile application using augmented reality and google street view,VRST - Virtual Reality Software and Technology,A,"X-ray view can be defined as the ability one has to see through real surfaces. Although this skill is often associated with superheroes and medical examination, there are several researches conducted to employ X-ray view in numerous applications. However, the generation of X-ray visualization includes numerous challenges regarding occlusion, realistic appearance, and depth perception.In this paper, we present a mobile application that uses Augmented Reality and Google Street View to allow users experience real-time X-ray vision. The proposed application was designed to enhance previous Augmented Reality X-ray systems, by introducing a silhouette computation method to provide visual context from the occluder and a perspective estimation system that improves the projection of occluded images into the real scene.Additionally, we implemented two usability studies to assess qualitative aspects of both silhouettes and perspective estimation to generate better X-ray effects. Results indicate good acceptance of the novel X-ray visualization method and a great usability score on the SUS scale for the mobile application.",augmented reality; image and video processing in UI; mobile and embedded devices,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,A study on improving close and distant device movement pose manipulation for hand-held augmented reality,VRST - Virtual Reality Software and Technology,A,"Hand-held smart devices are equipped with powerful processing units, high resolution screens and cameras, that in combination makes them suitable for video see-through Augmented Reality. Many Augmented Reality applications require interaction, such as selection and 3D pose manipulation. One way to perform intuitive, high precision 3D pose manipulation is by direct or indirect mapping of device movement.There are two approaches to device movement interaction; one fixes the virtual object to the device, which therefore becomes the pivot point for the object, thus makes it difficult to rotate without translate. The second approach avoids latter issue by considering rotation and translation separately, relative to the object's center point. The result of this is that the object instead moves out of view for yaw and pitch rotations.In this paper we study these two techniques and compare them with a modification where user perspective rendering is used to solve the rotation issues. The study showed that the modification improves speed as well as both perceived control and intuitiveness among the subjects.",augmented reality; device interaction; device perspective; user study; user-perspective; video see-through,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Multi-view gesture annotations in image-based 3D reconstructed scenes,VRST - Virtual Reality Software and Technology,A,"We present a novel 2D gesture annotation method for use in image-based 3D reconstructed scenes with applications in collaborative virtual and augmented reality. Image-based reconstructions allow users to virtually explore a remote environment using image-based rendering techniques. To collaborate with other users, either synchronously or asynchronously, simple 2D gesture annotations can be used to convey spatial information to another user. Unfortunately, prior methods are either unable to disambiguate such 2D annotations in 3D from novel viewpoints or require relatively dense reconstructions of the environment.In this paper, we propose a simple multi-view annotation method that is useful in a variety of scenarios and applicable to both very sparse and dense 3D reconstructions. Specifically, we employ interactive disambiguation of the 2D gestures via a second annotation drawn from another viewpoint, triangulating two drawings to achieve a 3D result. Our method automatically chooses an appropriate second viewpoint and uses image-based rendering transitions to keep the user oriented while moving to the second viewpoint. User experiments in an asynchronous collaboration scenario demonstrate the usability of the method and its superiority over a baseline method. In addition, we showcase our method running on a variety of image-based reconstruction datasets and highlight its use in a synchronous local-remote user collaboration system.",3D reconstruction; annotations; augmented reality; collaboration; image-based reconstruction; image-based rendering; interactive disambiguation; virtual navigation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Robot gardens: an augmented reality prototype for plant-robot biohybrid systems,VRST - Virtual Reality Software and Technology,A,"Robot Gardens are an augmented reality concept allowing a human user to design a biohybrid, plant-robot system. Plants growing from deliberately placed seeds are directed by robotic units that the user can position, configure and activate. For example, the robotic units may serve as physical shields or frames but they may also guide the plants' growth through emission of light. The biohybrid system evolves over time to redefine architectural spaces. This gives rise to the particular challenge of designing a biohybrid system before its actual implementation and potentially long before its developmental processes unfold. Here, an augmented reality interface featuring according simulation models of plants and robotic units allows one to explore the design space a priori. In this work, we present our first functional augmented reality prototype to design biohybrid systems. We provide details about its workings and elaborate on first empirical studies on its usability.",augmented reality; biohybrids; interactive simulation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,A platform for bimanual virtual assembly training with haptic feedback in large multi-object environments,VRST - Virtual Reality Software and Technology,A,"We present a virtual reality platform which addresses and integrates some of the currently challenging research topics in the field of virtual assembly: realistic and practical scenarios with several complex geometries, bimanual six-DoF haptic interaction for hands and arms, and intuitive navigation in large workspaces. We put an especial focus on our collision computation framework, which is able to display stiff and stable forces in 1 kHz using a combination of penalty- and constraint-based haptic rendering methods. Interaction with multiple arbitrary geometries is supported in realtime simulations, as well as several interfaces, allowing for collaborative training experiences. Performance results for an exemplary car assembly sequence which show the readiness of the system are provided.",haptic devices; haptic rendering; interaction techniques; virtual assembly,Abstract,True,
Scopus,conferencePaper,2016,A fast and robust Six-DoF god object heuristic for haptic rendering of complex models with friction,VRST - Virtual Reality Software and Technology,A,"Collision detection and force computation between complex geometries are essential technologies for virtual reality and robotic applications. Penalty-based haptic rendering algorithms provide a fast collision computation solution, but they cannot avoid the undesired interpenetration between virtual objects, and have difficulties with thin non-watertight geometries. God object methods or constraint-based haptic rendering approaches have shown to solve this problem, but are typically complex to implement and computationally expensive. This paper presents an easy-to-implement god object approach applied to six-DoF penalty-based haptic rendering algorithms. Contact regions are synthesized to penalty force and torque values and these are used to compute the position of the god object on the surface. Then, the pose of this surface proxy is used to render stiff and stable six-DoF contacts with friction. Independently of the complexity of the used geometries, our implementation runs in only around 5 μs and the results show a maximal penetration error of the resolution used in the penalty-based haptic rendering algorithm.",haptic devices; haptic rendering; interaction techniques; virtual assembly,Abstract,True,
Scopus,conferencePaper,2016,VR360HD: a VR360° player with enhanced haptic feedback,VRST - Virtual Reality Software and Technology,A,"We present a VR360° video player with haptic feedback playback. The VR360HD application enhances VR viewing experience by triggering customized haptic effects associated with user's activities, biofeedback, network messages and customizable timeline triggers incorporated in the VR media. The app is developed in the Unity3D game engine and tested using a GearVR headset, therefore allowing users to add animations to VR gameplay and to the VR360° streams. A custom haptic plugin allows users to author and associate animated haptic effects to the triggers, and playback these effects on a custom haptic hardware, the Haptic Chair. We show that the VR360HD app creates rich tactile effects and can be easily adapted to other media types.",haptic feedback; virtual reality; VR viewing,Keywords,True,
Scopus,conferencePaper,2016,Procedurally generated virtual reality from 3D reconstructed physical space,VRST - Virtual Reality Software and Technology,A,"We present a novel system for automatically generating immersive and interactive virtual reality (VR) environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas (WA) to enable real-walking in the generated virtual environment (VE). Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our approach is new, in that it allows a casual user to easily create virtual reality worlds in any indoor space of arbitrary size and shape without requiring specialized equipment or training. We demonstrate our approach through a fully working system implemented on the Google Project Tango tablet device.",3D reconstruction; computer vision; depth cameras; locomotion; mobile computing; obstacle avoidance; procedural generation; tracking; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Large scale cut plane: an occlusion management technique for immersive dense 3D reconstructions,VRST - Virtual Reality Software and Technology,A,"Dense 3D reconstructions of real-world environments become wide spread and are foreseen to act as data base to solve real world problems, such as remote inspections. Therefore not only scene viewing is required but also the ability to interact with the environment, such as selection of a user-defined part of the reconstruction for later usage. However, inter-object occlusion is inherent to large dense 3D reconstructions, due to scene geometry or reconstruction artifacts that might result in object containment. Since prior art lacks approaches for occlusion management in environments that consist of one or multiple (large) continuous surfaces, we propose the novel technique Large Scale Cut Plane that enables segmentation and subsequent selection of visible, partly or fully occluded patches within a large 3D reconstruction, even at far distance. We combine Large Scale Cut Plane with an immersive virtual reality setup to foster 3D scene understanding and natural user interactions. We furthermore present results from a user study where we investigate performance and usability of our proposed technique compared to a baseline technique. Our results indicate Large Scale Cut Plane to be superior in terms of speed and precision, while we found need of improvement of the user interface. The presented investigations has to the authors' best knowledge not been subject to previous research.",3D selection; dense 3D surface reconstruction; immersive virtual reality; occlusion management,Abstract_Keywords,True,
Scopus,conferencePaper,2016,6-DOF computation and marker design for magnetic 3D dexterous motion-tracking system,VRST - Virtual Reality Software and Technology,A,"We describe our approach that derives reliable 6-DOF information including the translation and the rotation of a rigid marker in a 3D space from a set of insufficient 5-DOF measurements. As a practical example, we carefully constructed a prototype and its design and evaluated it in our 3D dexterous motion-tracking system, IM6D, which is our novel real-time magnetic 3D motion-tracking system that uses multiple identifiable, tiny, lightweight, wireless, and occlusion-free markers. The system contains two key technologies; a 6-DOF computation algorithm and a marker design for 6D marker. The 6-DOF computation algorithm computes the result of complete 6-DOF information including translation and rotation in 3D space for a single rigid marker that consists of three LC coils. We propose several possible approaches for implementation, including geometric, matrix-based kinematics, and computational approaches. In addition, we introduce workflow to find an optimal marker design for the system to achieve the best compromise between its smallness and accuracy based on the tracking principle. We experimentally compare the performances of some typical marker prototypes with different layouts of LC coils. Finally, we also show another experimental result to prove the effectiveness of the results from the solutions in these two problems.",3D interaction; 3D user interface; augmented reality; input devices; motion capture; sensor; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,Missing the point: an exploration of how to guide users' attention during cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"Recent technological advances have brought Virtual Reality (VR) into the homes of consumers, and there is a growing interest in bringing cinematic experiences from the screen and into VR. However, cinematic VR limits filmmakers' ability to effectively guide the audience's attention. In this paper we present a taxonomy of approaches to guiding users' attention, and present a study comparing two such approaches with a control condition devoid of guidance. One approach guides users by controlling their body's orientation, and the other implicitly directs their attention by encouraging them to follow a firefly with their gaze. The results revealed interesting, albeit statistically insignificant, indications that assuming control of the user's action may negatively influence presence, whereas the firefly was perceived as significantly more helpful.",attention; cinematic VR; film; presence; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Depth information from binocular disparity and familiar size is combined when reaching towards virtual objects,VRST - Virtual Reality Software and Technology,A,"Reaching movements towards stereoscopically presented virtual objects have been reported to be imprecise. This might be a problem for touch interaction with virtual environments. Estimating the distance to an object in personal space relies on binocular disparity and other depth cues but previous work on the influence of familiar size for reaching and grasping has produced conflicting results. We presented a virtual tennis ball and manipulated binocular disparity as well as the size of the tennis ball. The results suggest that depth information from binocular disparity and from familiar size is combined for reaching movements towards virtual objects. However, subjects differed in the weight they assigned to each depth cue.",depth perception; distance estimates; reaching; stereoscopy; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,Perceiving depth: optical versus video see-through,VRST - Virtual Reality Software and Technology,A,"Head-Mounted Displays (HMDs) and similar 3D visualization devices are becoming ubiquitous. Going a step forward, HMD see-through systems bring virtual objects to real world settings, allowing augmented reality to be used in complex engineering scenarios. Of these, optical and video see-through systems differ on how the real world is captured by the device. To provide a seamless integration of real and virtual imagery, the absolute depth and size of both virtual and real objects should match appropriately. However, these technologies are still in their early stages, each featuring different strengths and weaknesses which affect the user experience. In this work we compare optical to video see-through systems, focusing on depth perception via exocentric and egocentric methods. Our study pairs Meta Glasses, an off-the-shelf optical see-through, to a modified Oculus Rift setup with attached video-cameras, for video see-through. Results show that, with the current hardware available, the video see-through configuration provides better overall results. These experiments and our results can help interaction designers for both virtual and augmented reality conditions.",augmented reality; depth perception; see-through system; user evaluation,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Vista widgets: a framework for designing 3D user interfaces from reusable interaction building blocks,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) has been an active field of research for several decades, with 3D interaction and 3D User Interfaces (UIs) as important sub-disciplines. However, the development of 3D interaction techniques and in particular combining several of them to construct complex and usable 3D UIs remains challenging, especially in a VR context. In addition, there is currently only limited reusable software for implementing such techniques in comparison to traditional 2D UIs. To overcome this issue, we present ViSTA Widgets, a software framework for creating 3D UIs for immersive virtual environments. It extends the ViSTA VR framework by providing functionality to create multi-device, multi-focus-strategy interaction building blocks and means to easily combine them into complex 3D UIs. This is realized by introducing a device abstraction layer along sophisticated focus management and functionality to create novel 3D interaction techniques and 3D widgets. We present the framework and illustrate its effectiveness with code and application examples accompanied by performance evaluations.",3D interaction; 3D user interfaces; framework; multi-device; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Improving freehand placement for grasping virtual objects via dual view visual feedback in mixed reality,VRST - Virtual Reality Software and Technology,A,"This paper presents a first study into the use of dual view visual feedback in an exocentric MR environment for assisting freehand grasping of virtual objects. Recent work has highlighted problems associated with user errors in freehand grasping, via an analysis of virtual object type, location and size. Our work presents an extension to this, where 30 participants are recruited for two experiments (one assessing object size and the second object position), giving 1710 grasps in total. We report on results following the same protocol of the aforementioned study using a dual view visual feedback method. Results show that dual view visual feedback significantly increases user z placement accuracy and improves grasp placement in the x and y axes, however completion time is significantly higher. No improvement was found in user grasp aperture using dual view visual feedback for changes in object size and position.",dual view feedback; freehand interaction; grasping; human performance measurement; mixed reality; natural hand interaction; visual feedback,Title_Keywords,True,
Scopus,conferencePaper,2016,Combining bimanual interaction and teleportation for 3D manipulation on multi-touch wall-sized displays,VRST - Virtual Reality Software and Technology,A,"While multi-touch devices are well established in our everyday life, they are currently becoming larger and larger. Large screens such as wall-sized displays are now equipped with multi-touch capabilities. Multi-touch wall-sized displays will become widespread in a near future in various places such as public places or meeting rooms. These new devices are an interesting opportunity to interact with 3D virtual environments: the large display surface offers a good immersion, while the multi-touch capabilities could make interaction with 3D content accessible to the general public.In this paper, we aim to explore touch-based 3D interaction in the situation where users are immersed in a 3D virtual environment and move in front of a vertical wall-sized display. We design In(SITE), a bimanual touch-based technique combined with object teleportation features which enables users to interact on a large wall-sized display. This technique is compared with a standard 3D interaction technique for performing 6 degrees of freedom manipulation tasks on a wall-sized display. The results of two controlled experiments show that participants can reach the same level of performance for completion time and a better precision for fine adjustments of object position with the In(SITE) technique. They also suggest that combining object teleportation with both techniques improves translations in terms of ease of use, fatigue, and user preference.",3D manipulation; multi-touch interaction; virtual reality; wall-sized display,Keywords,True,
Scopus,conferencePaper,2016,"A compact, wide-FOV optical design for head-mounted displays",VRST - Virtual Reality Software and Technology,A,"We present a new optical design for head-mounted displays (HMD) which has an exceptionally wide field of view (FOV). It can cover even the full human FOV. It is based on seamless lenses and screens curved around the eyes. The proof-of-concept prototypes are promising, and one of them far exceeds the human FOV, although the effective FOV is limited by the anatomy of the human head. The presented optical design has advantages such as compactness, light weight, low cost and super-wide FOV with high resolution. Even though this is still work-in-progress and display functionality is not yet implemented, it suggests a feasible way to significantly expand the FOV of HMDs.",field-of-view; head-mounted display; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,"A low-cost, variable, interactive surface for mixed-reality tabletop games",VRST - Virtual Reality Software and Technology,A,"This paper introduces an interactive surface concept for Mixed Reality (MR) tabletop games that combines a variable (LCD and/or projection) screen configuration with the detection of finger touches, in-air gestures, and tangibles. It is low-cost and minimally requires an ordinary table, a TV screen, and a Kinect v2 sensor. Existing applications can easily be connected by being compliant to standards. The concept is intended to foster further research on collaborative tabletop situations, not limited to games, but also including learning, meetings, and social interaction.",interactive surface; mixed reality; tabletop game,Abstract_Keywords,True,
Scopus,conferencePaper,2016,AR interaction paradigm for closed reduction of long-bone fractures via external fixation,VRST - Virtual Reality Software and Technology,A,"We present an intuitive and ergonomic AR strategy to be coupled with a standard external fixation system aimed at aiding the accurate closed reduction of long-bone shaft fractures. The correct six DOF alignment between the bone fragments can be retrieved by manually repositioning a pair of reference frames constrained to the two extremities of the fixator so as to minimize the geometric distance, on the image plane, between planned/virtual landmarks and their observed/real counterparts. The reduction accuracy was positively validated in vitro in a pilot study that involved an orthopedic surgeon.",augmented reality; content strategy; novel interaction techniques; surgical navigation,Keywords,True,
Scopus,conferencePaper,2016,Are age differences missing in relative and absolute distance perception of stereoscopically presented virtual objects?,VRST - Virtual Reality Software and Technology,A,"Nowadays there is a wide variety of Virtual Reality (VR) applications for users of all age groups. An essential part of most VR systems is stereoscopy. From perceptual research, it is known that stereoscopic perception deteriorates with age [Garnham and Sloper 2006]. As indicator for stereoscopic perception, the authors of this and further studies used stereo acuity, i.e. the smallest disparity difference detected by the visual system. Norman et al. [2000] showed that the declined stereo acuity in older humans can cause these users to perceive less depth in random-dot stereograms. Since VR applications for rehabilitation are advancing, one can expect increasing numbers of elderly users. However, up to now it is unclear whether the age-specific changes in stereoscopic perception might impair the use of VR applications for older people. In our study, we presented stereoscopically rendered virtual objects. The primary aim of this study was to examine possible age differences in fusion range, relative and absolute distance perception, and visual fatigue. Here, we present results of the two tasks concerning distance perception.",binocular eye movements; depth perception; stereoscopy,Abstract,True,
Scopus,conferencePaper,2016,Audio feedback and illusion of virtual body ownership in mixed reality,VRST - Virtual Reality Software and Technology,A,"This paper presents an exploratory experiment measuring the role of audio feedback on the illusion of virtual body ownership (IVBO) under non-immersive mixed reality (MR) settings with Human and Non-Human avatars. Our preliminary results revealed that all avatars elicited a similar level of IVBO, despite the addition of audio feedback.",audio; avatar embodiment; mixed reality; realism,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Augmented reality based remote coaching system,VRST - Virtual Reality Software and Technology,A,"In this paper, we present an AR-based tele-coaching system for fast-paced tasks, applied to the game of tennis, and explore for interface design guidelines through a user study. We have evaluated the player's performance for instruction understanding in three different modalities of augmentation: (1) AR — visual only, (2) Sound — aural only and (3) Multimodal — both visual and aural. The augmented instructions were useful even in the stringent temporal conditions, and most effective with the visual only augmentation due to its modal capability of encoding and presenting several information compactly at once.",augmented reality; multimodal feedback; pre-attentive recognition; tele-coaching,Title_Keywords,True,
Scopus,conferencePaper,2016,Augmented reasoning in the mirror world,VRST - Virtual Reality Software and Technology,A,"In order to enable a social agent to behave in a believable and realistic way, it needs a wide range of information in the form of both low-level value-based data as well as high-level semantic knowledge. In this work we propose a system that puts a virtual reality layer between the real world and an agent's knowledge representation. This mirror world allows the agent to use its abstract representation of the environment and inferred events as an additional source of knowledge when reasoning about the real world. Additionally, users and developers can use the mirror world, with its visualized data and highlighting of the agent's reasoning, for further understanding of the agent's behavior, debugging and testing, or the simulation of additional sensor input.",knowledge representation; mirror world; MR; reasoning; social robots; visualization; VR,Abstract,True,
Scopus,conferencePaper,2016,Avatar anthropomorphism and acrophobia,VRST - Virtual Reality Software and Technology,A,"In this paper, we investigate the impact of avatar anthropomorphism on the fear of heights, when using full body avatar embodiment under an immersive virtual reality (VR) setting. Clear differences could be found in perceived anthropomorphism, but preliminary results do not show differences in stress level between Human and Non-Human avatars, although a high level of perceived secureness was reported with Non-Human avatars.",acrophobia; avatar embodiment; virtual therapy,Abstract,True,
Scopus,conferencePaper,2016,Breaking bad behavior: immersive training of class room management,VRST - Virtual Reality Software and Technology,A,"This article presents a fully immersive portable low-cost Virtual Reality system to train classroom management skills. An instructor controls the simulation of a virtual classroom populated with 24 semi-autonomous virtual agents via a desktop-based graphical user interface (GUI). The GUI provides behavior control and trainee evaluation widgets alongside a non-immersive view of the class and the trainee. The trainee's interface uses an Head-Mounted Display (HMD) and earphones for output. A depth camera and the HMD's built-in motion sensors are used for tracking the trainee and for avatar animation. An initial evaluation of both interfaces confirms the system's usefulness, specifically its capability to successfully simulate critical aspects of classroom management.",class room management; student simulation; virtual agent interaction; virtual reality training,Abstract_Keywords,True,
Scopus,conferencePaper,2016,"Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds",VRST - Virtual Reality Software and Technology,A,"In this paper we present a VR locomotion technique based on the Leap Motion device and compare it to other often-used locomotion techniques — gaze-directed locomotion and gamepad-based locomotion. We performed a user experiment to evaluate the three techniques based on their performance (time to complete the task), comfort (through the ISO 9241–9 ssessment of comfort questionnaire), and simulation sickness (through the Simulation Sickness Questionnaire). Results indicate that the gamepad technique is both faster and more comfortable than either the Leap Motion-based or the gaze-directed techniques.",HCI; interaction device; leap motion; locomotion; performance measurement; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,"Concept for content-aware, automatic shifting for spherical panoramas",VRST - Virtual Reality Software and Technology,A,"With the adaption of virtual reality in the consumer space, spherical panorama photos are gaining popularity. Through wide-angle head-mounted displays, they can be experienced in a natural way and offer the user an immersive view of the captured scene. While being used in virtual reality, the alignment of the saved image does not matter much. However, when displaying the panorama on a 2D screen, the alignment can make a difference on how pleasant the image looks. We propose an automatic method to do lossless shifting of the image to make it look better on 2D screens.",360 degree panoramas; image processing; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Concept for using eye tracking in a head-mounted display to adapt rendering to the user's current visual field,VRST - Virtual Reality Software and Technology,A,"With increasing spatial and temporal resolution in head-mounted displays (HMDs), using eye trackers to adapt rendering to the user is getting important to handle the rendering workload. Besides using methods like foveated rendering, we propose to use the current visual field for rendering, depending on the eye gaze. We use two effects for performance optimizations. First, we noticed a lens defect in HMDs, where depending on the distance of the eye gaze to the center, certain parts of the screen towards the edges are not visible anymore. Second, if the user looks up, he cannot see the lower parts of the screen anymore. For the invisible areas, we propose to skip rendering and to reuse the pixels colors from the previous frame. We provide a calibration routine to measure these two effects. We apply the current visual field to a renderer and get up to 2x speed-ups.",eye tracking; head-mounted display; rendering; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,Cyber sick but still having fun,VRST - Virtual Reality Software and Technology,A,"In this paper, we present our efforts towards creating a deliberately sickening virtual reality (VR) game. Based on a considerable number of tests, we can show that games can be enjoyable despite experienced adverse effects that arise from frequent acceleration in VR and additional post-processing distortions of the rendered scene. We briefly explain the rationale that drove us to take these measures, details about their realisation and results from a questionary-based user evaluation.",driving; locomotion; motion sickness; simulator sickness; VR; VR sickness,Abstract,True,
Scopus,conferencePaper,2016,Effects of speed and transitions on target-based travel techniques,VRST - Virtual Reality Software and Technology,A,"Travel on Virtual Environments is the simple action where a user moves from a starting point A to a target point B. Choosing an incorrect type of technique could compromise the Virtual Reality experience and cause side effects such as spatial disorientation, fatigue and cybersickness. The design of effective travelling techniques demands to be as natural as possible, thus real walking techniques presents better results, despite their physical limitations. Approaches to surpass these limitations employ techniques that provide an indirect travel metaphor such as point-steering and target-based. In fact, target-based techniques evince a reduction in fatigue and cybersickness against the point-steering techniques, even though providing less control. In this paper we investigate further effects of speed and transition on target-based techniques on factors such as comfort and cybersickness using a Head-Mounted Display setup.",cyber-sickness; navigation; travel techniques; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Exploring floating stereoscopic driver-car interfaces with wide field-of-view in a mixed reality simulation,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose a floating, multi-layered, wide field-of-view user interface for car drivers. It utilizes stereoscopic depth and focus blurring to highlight items with high priority or urgency. Individual layers are additionally used to separate groups of UI elements according to importance or context. Our work is motivated by two main prospects: a fundamentally changing driver-car interaction and ongoing technology advancements for mixed reality devices. A working prototype has been implemented as part of a custom driving simulation and will be further extended. We plan evaluations in contexts ranging from manual to fully automated driving, providing context-specific suggestions. We want to determine user preferences for layout and prioritization of the UI elements, perceived quality of the interface and effects on driving performance.",3DUI; automotive; MR simulation; user-centered,Title_Abstract,True,
Scopus,conferencePaper,2016,Hybrid team interaction in the mixed reality continuum,VRST - Virtual Reality Software and Technology,A,"This paper describes a system, which enables collaboration in a hybrid team consisting of a robot, physically present humans and remote humans, where the latter are connected via Virtual Reality. This setup spans the whole continuum between Physical and Virtual Reality, including Augmented Reality. The work presented herein, describes how such a scattered, hybrid team can interact and cooperate in a virtual representation of a factory, using eye-, head-, hand- and gesture-tracking as multimodal control and communication input.",augmented reality; human-robot interaction; interactive collaboration; multimodal input; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Immersive remote grasping: realtime gripper control by a heterogenous robot control system,VRST - Virtual Reality Software and Technology,A,"Current developments in the field of user interface (UI) technologies as well as robotic systems provide enormous potential to reshape the future of human-robot interaction (HRI) and collaboration. However, the design of reliable, intuitive and comfortable user interfaces is a challenging task. In this paper, we focus on one important aspect of such interfaces, i.e., teleoperation. We explain how to setup a heterogeneous, extendible and immersive system for controlling a distant robotic system via the network. Therefore, we exploit current technologies from the area of virtual reality (VR) and the Unity3D game engine in order to provide natural user interfaces for teleoperation. Regarding robot control, we use the well-known robot operating system (ROS) and apply its freely available modular components. The contribution of this work lies in the implementation of a flexible immersive grasping control system using a network layer (ROSbridge) between Unity3D and ROS for arbitary robotic hardware.",human-robot interaction; tele-operation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,"Inception: a creative coding environment for virtual reality, in virtual reality",VRST - Virtual Reality Software and Technology,A,"In this paper we build and evaluate a live-programming system for digital artists to create artwork within a virtual reality environment. Using the large display space that virtual reality provides, we develop an interaction for artists to see parallel evaluations of their art. A study of the system with ten participants demonstrated that parallel editing and execution is accessible to designers and that designers can leverage these techniques to survey more options faster.",programming; smartwatch interfaces; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Industrial maintenance with augmented reality: two case studies,VRST - Virtual Reality Software and Technology,A,"Remote maintenance of industrial manipulators often is performed via telephone support. Recent approaches in the context of the 'Industry 4.0' consider internet technologies and Augmented Reality (AR) to enhance situation awareness between external experts and local service technicians. We present two AR-based case studies: First, a mobile AR architecture based on optical see through glasses is used for an on-site local repair task. Second, a remote architecture based on a portable tablet PC and a high precision tracking system is used to realize an off-site expert access. The to-be-serviced machine is visualized inside of a large area similar to a machinery hall and can be inspected by the experts walking around this virtual plant using the tablet and perspectively correct rendering to understand the production process and the operation context. Both methods have been evaluated in first user studies.",augmented reality; industrial internet; industry 4.0; maintenance; situation awareness,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Interactive gamified 3D-training of affine transformations,VRST - Virtual Reality Software and Technology,A,"This article presents the Gamified Training Environment for Affine Transformations (GEtiT). GEtiT uses a 3D environment to visualize the effects of object rotation, translation, scaling, reflection, and shearing in 3D space. It encodes the abstract knowledge about homogeneous transformations and their order of application using specific game mechanics encoding 3D movements on different levels of abstraction. Progress in the game requires mastering of the game mechanics of a certain level of abstraction to modify objects in 3D space to a desired goal position and/or shape. Each level increases the abstraction of the representation towards a final 4 × 4 homogeneous matrix representation. Executing the game mechanics during the gameplay results in an effective training of knowledge due to a constant repetition. Evaluation showed a learning effect that is equal to a traditional training method while it achieved a higher enjoyment of use indicating that the learning quality was superior to the traditional training method.",education; gamification; serious games; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,Interference measurement of kinect for xbox one,VRST - Virtual Reality Software and Technology,A,"Microsoft Kinect is widely used for tracking human body in a range of applications. Although Kinect for Xbox One allows for multi-user tracking, it is not possible to use it in large spaces due to its limited range. Hence, using multiple Kinect sensors for large environments seems to be an appropriate solution. Thus, it is important to know if multiple sensors can be used simultaneously for such applications without interfering with each other. In this paper, we investigate the effect of using multiple Kinects on each other by performing multiple measurements in different settings. Our results show that some occasional interference might happen in some specific constellations, when the sensors are facing the same target. Our recommendation is to avoid such constellations, or to perform a simple interference measurement before using multiple sensors in specific settings.",structured light; time of flight; tracking; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,Maintainable management and access of lexical knowledge for multimodal virtual reality interfaces,VRST - Virtual Reality Software and Technology,A,"This poster presents a maintainable method to manage lexical information required for multimodal interfaces. It is tailored for the application in real-time interactive systems, specifically for Virtual Reality, and solves three problems commonly encountered in this context: (1) The lexical information is defined on and grounded in a common knowledge representation layer (KRL) based on OWL. The KRL describes application objects and possible system functions in one place and avoids error-prone redundant data management. (2) The KRL is tightly integrated into the simulator platform using a semantically enriched object model that is auto-generated from the KRL and thus fosters high performance access. (3) A well-defined interface provides application wide access to semantic application state information in general and the lexical information in specific, which greatly contributes to decoupling, maintainability, and reusability.",multimodal platforms; real-time interactive systems; software architecture; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,Medical imaging VR: can immersive 3D aid in diagnosis?,VRST - Virtual Reality Software and Technology,A,"In the radiology diagnosis process, medical images are most often visualized slice by slice on 2D screens or printed. At the same time, the visualization based on 3D volumetric rendering of the data is considered useful and has increased its field of application. In this work we present a user study with medical specialists to assess the diagnostic effectiveness of VR usage in fracture identification over 3D volumetric reconstructions. We then performed user experiments to validate the approach in the medical practice. In addition, we assessed the subjects perception of the 3D reconstruction quality and ease of interaction. Among other results, we have found a very high level of effectiveness of the VR interface in identifying superficial fractures on head CTs.",3D images; diagnostic imaging; healthcare; oculus rift; radiology; virtual reality,Keywords,True,
Scopus,conferencePaper,2016,Postural stability analysis in virtual reality using the HTC vive,VRST - Virtual Reality Software and Technology,A,"Postural stability is an important measure for many medical diseases such as Parkinson. In the last years, research focused on using inexpensive and portable devices to measure postural stability, while the visual targets were physical objects in the environment. Sensing balancing boards were used to measure stance forces, while movements of the upper body were not taken into account. Within this paper, postural stability was measured using the HTC Vive. A variation of a virtual fixation point's distance was analyzed and compared to a reference condition with closed eyes. It is shown that body sway in the VR conditions is increased in the anterior-posterior and decreased in the medial-lateral direction.",body sway; postural stability; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2016,Shop 'til you hear it drop: influence of interactive auditory feedback in a virtual reality supermarket,VRST - Virtual Reality Software and Technology,A,"In this paper we describe an experiment aiming to investigate the impact of auditory feedback in a virtual reality supermarket scenario. The participants were asked to read a shopping list and collect items one by one and place them into a shopping cart. Three conditions were presented randomly, where audio feedback was (1) absent, (2) had impact sounds for collisions including when grasping, (3) had impact sounds as well as continuous sounds when moving the products. The subjects experience of the simulation during the three experimental conditions were studied using a questionnaire where ratings on presence, body ownership, awareness of own movements, usability and enjoyment were collected. The results are presented and discussed.",auditory feedback; gesture control; interaction; supermarkets; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2016,SIAMC: a socially immersive avatar mediated communication platform,VRST - Virtual Reality Software and Technology,A,"In this paper, we present a avatar-mediated communication platform for socially immersive interaction in virtual reality (VR). Our approach is based on the combination of body tracking, facial expression tracking and ""fishtank"" VR. Our prototype enables two remote users to communicate via avatars.",avatars; computer-mediated communication; social virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2016,Temporal antialiasing for head mounted displays in virtual reality,VRST - Virtual Reality Software and Technology,A,"This paper identifies a new temporal aliasing problem caused by unintended head movement by users with VR HMDs. The images that users see slightly change even in the case that the users intend to hold and concentrate on a certain part of VR content. The slight change is more perceivable, because the images are magnified by lenses of VR HMDs. We propose the head movement based temporal antialiasing approach which blends colors that users see in the middle of head movement. In our approach, the way to determine locations and weights of colors to be blended is based on head movement and time stamp. Speed of head movement also determines proportions of colors in the past and at present in blending. Our approach is effective to reduce the temporal aliasing caused by unintended head movement.",head mounted display; temporal antialiasing; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2016,Texture analysis and repacking for improved storage efficiency,VRST - Virtual Reality Software and Technology,A,"Textures are widely used in modern computer graphics. Their size, however, is often a limiting factor. Considering the widespread adaptation of mobile virtual and augmented reality applications, efficient storage of textures has become an important factor.We present an approach to analyse textures of a given mesh and compute a new set of textures with the goal of improving storage efficiency and reducing memory requirements. During this process the texture coordinates of the mesh are updated as required. Textures are analysed based on the UV-coordinates of one or more meshes and deconstructed into per-triangle textures. These are further analysed to detect single coloured as well as identical per-triangle textures. Our approach aims to remove these redundancies in order to reduce the amount of memory required to store the texture data. After this analysis, the per-triangle textures are compiled into a new set of texture images of user defined size. Our algorithm aims to pack texture data as tightly as possible in order to reduce the memory requirements.",texture; texture optimization; texture packing,Abstract,True,
Scopus,conferencePaper,2016,The Effects of Indirectly Implied Real Body Cues to Virtual Body Ownership and Presence in a Virtual Reality Environment,VRST - Virtual Reality Software and Technology,A,"While direct associations, such as through visual, audio and tactile senses, play an obvious role in giving a person a perception of body presence in an immersive virtual environment, indirect implied cues can also be effective factors in providing the illusion of reality. Thus, as the direct use of implied association can arouse desired illusions of reality, we believed the indirect associations also has effects that can similarly arouse such illusions in virtual settings. In this paper, we report on an experiment we conducted to explore the effects of indirect implication of a participant's body parts to virtual body ownership and presence.",Presence; Virtual Body Ownership; Virtual Reality,Title_Keywords,True,
Scopus,conferencePaper,2016,TickTockRay: smartwatch-based 3D pointing for smartphone-based virtual reality,VRST - Virtual Reality Software and Technology,A,"TickTockRay is a smartwatch-based raycasting technique designed for smartphone-based head mounted displays. It demonstrates that smartwatch-based raycasting can be reliably implemented on an off-the-shelf smartphone and may provide a feasible alternative for specialized input devices. We release TickTockRay to the research community as an open-source plugin for Unity along with an example application, a Minecraft VR game clone, that shows the utility of the technique for placement and destruction of Minecraft blocks.",3D pointing; freehand pointing; game input; immersive systems; myo; smartphone; smartwatch; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2016,Webizing human interface devices for virtual reality,VRST - Virtual Reality Software and Technology,A,"Recently virtual reality (VR) technology has been widely distributed, but VR interaction devices supported in web environments are limited compared with in the traditional VR environment. In the traditional VR environment, the Virtual-Reality Peripheral Network (VRPN) provides a device-independent and network-transparent interface. To promote the development of WebVR applications with various interaction devices, a method like VRPN is required in the web environment as well. In this paper, we propose a webizing method for human interface devices and related events that serves as either VRPN messages or HTML DOM events to deal with interaction events.",human interface devices; user interaction; VRPN; webizing,Title_Abstract,True,
Scopus,conferencePaper,2017,Beyond cute: exploring user types and design opportunities of virtual reality pet games,VRST - Virtual Reality Software and Technology,A,"Virtual pet games, such as handheld games like Tamagotchi or video games like Petz, provide players with artificial pet companions or entertaining pet-raising simulations. Prior research has found that virtual pets have the potential to promote learning, collaboration, and empathy among users. While virtual reality (VR) has become an increasingly popular game medium, litle is known about users' expectations regarding game avatars, gameplay, and environments for VR-enabled pet games. We surveyed 780 respondents in an online survey and interviewed 30 participants to understand users' motivation, preferences, and game behavior in pet games played on various medium, and their expectations for VR pet games. Based on our findings, we generated three user types that reflect users' preferences and gameplay styles in VR pet games. We use these types to highlight key design opportunities and recommendations for VR pet games.",pet game; user types; virtual pet; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,The matrix has you: realizing slow motion in full-body virtual reality,VRST - Virtual Reality Software and Technology,A,"While we perceive time as a constant factor in the real world, it can be manipulated in media. Being quite easy for linear media, this is used for various aspects of storytelling e.g., by applying slow motion in movies or TV. Interactive media like VR however poses additional challenges, because user interaction speed is independent from media speed. While it is still possible to change the speed of the environment, for interaction it is also necessary to deal with the emerging speed mismatch, e.g., by slowing down visual feedback of user movements. In this paper, we explore the possibility of such manipulations of visual cues, with the goal of enabling the use of slow motion also in immersive interactive media like VR. We conducted a user study to investigate the impact of limiting angular velocity of a virtual character in first person view in VR. Our findings show that it is possible to use slow motion in VR while maintaining the same levels of presence, enjoyment and susceptibility to motion sickness, while users adjust to the maximum speed quickly. Moreover, our results also show an impact of slowing down user movements on their time estimations.",evaluation; slow motion; time perception; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2017,Can you cut it? an exploration of the effects of editing in cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"The advent of affordable virtual reality (VR) displays and 360° video cameras has sparked an interest in bringing cinematic experiences from the screen and into VR. However, it remains uncertain whether traditional approaches to filmmaking can be directly applied to cinematic VR. Historically editing has provided filmmakers with a powerful tool for shaping stories and guiding the attention of audiences. However, will an immersed viewer, experiencing the story from inside the fictional world, find cuts disorienting? This paper details two studies exploring how cut frequency influences viewers' sense of disorientation and their ability to follow the story, during exposure to fictional 360° films experienced using a head-mounted display. The results revealed no effects of increased cut frequency which leads us to conclude that editing need not pose a problem in relation to cinematic VR, as long as the participants' attention is appropriately guided at the point of the cut.",360 degree film; cinematic virtual reality; editing,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,A comparison of head-mounted displays vs. large-screen displays for an interactive pedestrian simulator,VRST - Virtual Reality Software and Technology,A,"This investigation compared how people performed a complex perception-action task - crossing traffic-filled roadways - in a CAVE vs. an HMD virtual environment. Participants physically crossed a virtual roadway with continuous cross traffic in either a CAVE-like or an HTC Vive pedestrian simulator. The 3D model and traffic scenario were identical in both simulators, allowing for a direct comparison between the two display systems. We found that participants in the Vive group accepted smaller gaps for crossing than participants in the CAVE group. They also timed their entry into the gap more precisely and tended to cross somewhat more quickly. As a result, participants in the Vive group had a somewhat larger margin of safety when they exited the roadway than those in the CAVE group. The results provide a foundation for future studies of pedestrian behavior and other tasks involving full-body motion using HMD-based VR.",CAVE; HTC vive; pedestrian road crossing; virtual reality,Keywords,True,
Scopus,conferencePaper,2017,Outside-in monocular IR camera based HMD pose estimation via geometric optimization,VRST - Virtual Reality Software and Technology,A,"Accurately tracking a Head Mounted Display (HMD) with a 6 degree of freedom is essential to achieve a comfortable and a nausea free experience in Virtual Reality. Existing commercial HMD systems using synchronized Infrared (IR) camera and blinking IR-LEDs can achieve highly accurate tracking. However, most of the off-the-shelf cameras do not support frame synchronization. In this paper, we propose a novel method for real time HMD pose estimation without using any camera synchronization or LED blinking. We extended over the state of the art pose estimation algorithm by introducing geometrically constrained optimization. In addition, we propose a novel system to increase robustness to the blurred IR-LEDs patterns appearing at high-velocity movements. The quantitative evaluations showed significant improvements in pose stability and accuracy over wide rotational movements as well as a decrease in runtime.",monocular IR camera; perspective-n-point problem; position tracking; vision-based pose estimation,Abstract,True,
Scopus,conferencePaper,2017,A tile based colour picture with hidden QR code for augmented reality and beyond,VRST - Virtual Reality Software and Technology,A,"Most existing Augmented Reality (AR) applications use either template (picture) markers or bar-code markers to overlay computer-generated graphics on the real world surfaces. The use of template markers is computationally expensive and unreliable. On the other hand, bar-code markers display only black and white blocks; thus, they look uninteresting and uninformative. In this short paper, we describe a new way to optically hide a QR code inside a tile based colour picture. Each AR marker is built from hundreds of small tiles (just like tiling a bathroom), and the unique gaps between the tiles are used to determine the elements of the hidden QR Code. This novel type of AR marker presents not only a realistic-looking colour picture but also contains self-Correcting information (stored in QR code). In this article, we demonstrate that this tile based colour picture with hidden QR code is relatively robust under various conditions and scaling. We believe many nowadays' AR challenges could be solved with this type of marker. AR-enabled medias could then be easily generated. For instance, it would be capable of storing and displaying virtual figures of an entire book or magazine. Thus, it provides a promising AR approach to be used in many different AR applications; and beyond, it may even replace the barcodes and QR Codes in some cases.",augmented reality; computer vision; QR code,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Virtual reality studies outside the laboratory,VRST - Virtual Reality Software and Technology,A,"Many user studies are now conducted outside laboratories to increase the number and heterogeneity of participants. These studies are conducted in diverse settings, with the potential to give research greater external validity and statistical power at a lower cost. The feasibility of conducting virtual reality (VR) studies outside laboratories remains unclear because these studies often use expensive equipment, depend critically on the physical context, and sometimes study delicate phenomena concerning body awareness and immersion. To investigate, we explore pointing, 3D tracing, and body-illusions both in-lab and out-of-lab. The in-lab study was carried out as a traditional experiment with state-of-the-art VR equipment; 31 completed the study in our laboratory. The out-of-lab study was conducted by distributing commodity cardboard VR glasses to participants; 57 completed the study anywhere they saw fit. The effects found in-lab were comparable to those found out-of-lab, with much larger variations in the settings in the out-of-lab condition. A follow-up study showed that performance metrics are mostly governed by the technology used, where more complex VR phenomena depend more critically on the internal control of the study. We argue that conducting VR studies outside the laboratory is feasible, and that certain types of VR studies may advantageously be run this way. From the results, we discuss the implications and limitations of running VR studies outside the laboratory.",consumer VR; crowdsourcing; google cardboard; user studies,Title_Abstract,True,
Scopus,conferencePaper,2017,Agent: automatic generation of experimental protocol runtime,VRST - Virtual Reality Software and Technology,A,"Due to the nature of Virtual Reality (VR) research, conducting experiments in order to validate the researcher's hypotheses is a must. However, the development of such experiments is a tedious and time-consuming task. In this work, we propose to make this task easier, more intuitive and faster with a method able to describe and generate the most tedious components of VR experiments. The main objective is to let experiment designers focus on their core tasks: designing, conducting, and reporting experiments. To that end, we propose the use of Domain-Specific Languages (DSLs) to ease the description and generation of VR experiments. An analysis of published VR experiments is used to identify the main properties that characterize VR experiments. This allowed us to design AGENT (Automatic Generation of ExperimeNtal proTocol runtime), a DSL for specifying and generating experimental protocol runtimes. We demonstrated the feasibility of our approach by using AGENT on two experiments published in the VRST'16 proceedings.",automatic generation of experiments; domain-specific language; reusability,Abstract,True,
Scopus,conferencePaper,2017,Accurate real-time occlusion for mixed reality,VRST - Virtual Reality Software and Technology,A,"Properly handling occlusion between real and virtual objects is an important property for any mixed reality (MR) system. Existing methods have typically required known geometry of the real objects in the scene, either specified manually, or reconstructed using a dense mapping algorithm. This limits the situations in which they can be applied. Modern RGBD cameras are cheap and widely available, but the depth information they provide is typically too noisy and incomplete to use directly to provide quality results.In this paper, a method is proposed which makes use of both the colour and depth information provided by an RGBD camera to provide improved occlusion. This method, Cost Volume Filtering Occlusion, is capable of running in real time, and can also handle occlusion of virtual objects by dynamic, moving objects - such as the user's hands. The method operates on individual RGBD frames as they arrive, meaning it can function immediately in unknown environments, and respond appropriately to sudden changes. The accuracy of the presented method is quantified using a novel approach capable of comparing the results of algorithms such as this to dense SLAM-based approaches. The proposed approach is shown to be capable of producing superior results to both previous image-based approaches and dense RGBD reconstruction, at lower computational cost.",image processing; mixed and augmented reality; user interfaces,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Design and exploration of braiding swarms in VR,VRST - Virtual Reality Software and Technology,A,"Swarm-based braiding of structures represents a novel research direction in the domain of building architecture. The idea is that autonomous agents, for instance robots that unroll threads or plants that grow, are programmed or influenced to braid. It is an aspect of biohybrid systems where organisms and robots join forces. In order to harness this idea, we have developed a swarm-based model that allows architects to explore the resulting design spaces in virtual reality. In this paper, we present (1) the model of our swarm-based simulation that aims at growing braided structures, (2) the design elements to guide the otherwise self-organising virtual agents, and (3) the user interface that allows the user to configure, place and grow the swarms of braiding agents. We also present results of a first user study with students and faculty from architecture, in which we tried to capture the usability of our first prototype based on a survey and an analysis of the built results.",agent-based modeling; architecture; braiding; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,A hybrid CRF framework for semantic 3D reconstruction,VRST - Virtual Reality Software and Technology,A,"Nowadays, in order to achieve an immersive experience, virtual reality systems usually require vivid 3D models and a good understanding of particular scenes. The limitations of separately optimizing image segmentation and 3D modeling from images have gradually been seen by more and more researchers, so plenty of novel methods on how to combine them for a better result begin to be put forward widely. In this paper, we propose a new hybrid framework to generate semantic 3D dense models from monocular images. Based on the available hierarchical CRFs model, we make full use of the correlation between voxels and their corresponding pixels from different images. Naturally, valuable information from 3D space can be added as one of the important energy items in the model. Either pixels, segments or voxles are all regarded as a node in the huge graph we build. Our ultimate goal is to realize a joint optimization for both 3D dense reconstruction and image segmentation. Experiments have been done on four real challenging datasets and all of the results prove the efficiency of our proposed hybrid framework.",dense 3D modeling; graphics/3D; image segmentation; semantic reconstruction,Abstract,True,
Scopus,conferencePaper,2017,Collaborators awareness for user cohabitation in co-located collaborative virtual environments,VRST - Virtual Reality Software and Technology,A,"In a co-located collaborative virtual environment, multiple users share the same physical tracked space and the same virtual workspace. When the virtual workspace is larger than the real workspace, navigation interaction techniques must be deployed to let the users explore the entire virtual environment. When a user navigates in the virtual space while remaining static in the real space, his/her position in the physical workspace and in the virtual workspace are no longer the same. Thus, in the context where each user is immersed in the virtual environment with a Head-Mounted-Display, a user can still perceive where his/her collaborators are in the virtual environment but not where they are in real world. In this paper, we propose and compare three methods to warn users about the position of collaborators in the shared physical workspace to ensure a proper cohabitation and safety of the collaborators. The frst one is based on a virtual grid shaped as a cylinder, the second one is based on a ghost representation of the user and the last one displays the physical safe-navigation space on the foor of the virtual environment. We conducted a user-study with two users wearing a Head-Mounted-Display in the context of a collaborative First-Person-Shooter game. Our three methods were compared with a condition where the physical tracked space was separated into two zones, one per user, to evaluate the impact of each condition on safety, displacement freedom and global satisfaction of users. Results suggest that the ghost avatar and the cylinder grid can be good alternatives to the separation of the tracked space.",collaborative virtual environment; virtual reality,Keywords,True,
Scopus,conferencePaper,2017,Towards seamless interaction between physical and virtual locations for asymmetric collaboration,VRST - Virtual Reality Software and Technology,A,"Virtual Reality allows rapid prototyping and simulation of physical artefacts, which would be difficult and expensive to perform otherwise. On the other hand, when the design process is complex and involves multiple stakeholders, decisions are taken in meetings hosted in the physical world. In the case of aerospace industrial designs, the process is accelerated by having asymmetric collaboration between the two locations: experts discuss the possibilities in a meeting room while a technician immersed in VR tests the selected alternatives. According to experts, the current approach is not without limitations, and in this work, we present prototypes designed to tackle them. The described artefacts were created to address the main issues: awareness of the remote location, remote interaction and manipulation, and navigation between locations. First feedback from experts regarding the prototypes is also presented. The resulting design considerations can be used in other asymmetric collaborative scenarios.",asymmetric collaboration; head mounted display; mixed reality; spatial augmented reality; tangible user interfaces; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Legomotion: scalable walking-based virtual locomotion,VRST - Virtual Reality Software and Technology,A,"Using real walking for virtual navigation generally delivers the most natural and immersive virtual reality experience, but its usage is generally bounded by available tracking space. To navigate beyond the confines of available tracking space, users need to switch to an artificial locomotion technique, such as controller input. However, having to switch from leg-based input to hand-based input is considered to break presence. We present a hybrid handsfree locomotion technique called legomotion that lets users seamlessly switch between real walking input and walking-in-place input to enable navigation at scale. A user study with 18 participants compared legomotion to full locomotion using a controller. Legomotion led to higher presence as switching to controller input was found to be more tedious. Because controller input is also faster than walking, we observed most users to abandon positional tracking input altogether and primary use a controller for navigation - which then led to a lower presence. This finding could have major implications for the design of VR locomotion.",locomotion; presence; virtual reality; VR sickness; walking-in-place,Abstract_Keywords,True,
Scopus,conferencePaper,2017,A unified model for interaction in 3D environment,VRST - Virtual Reality Software and Technology,A,"The Virtual (VR), Augmented (AR) and Mixed Reality (MR) devices are currently evolving at a very fast pace. This rapid evolution affects significantly the maintainability and portability of the applications. In this paper, we present a model for designing VR, AR and MR applications independently of any device. To do this, we use degrees of freedom to define an abstraction layer between the tasks to be performed and the interaction device.",input techniques; model-based interactive system development; virtual worlds,Abstract,True,
Scopus,conferencePaper,2017,A study on improving performance in gesture training through visual guidance based on learners' errors,VRST - Virtual Reality Software and Technology,A,"Gesture training, especially for technical gestures, requires supervisors to point out errors made by trainees. Virtual reality (VR) makes it possible to reduce reliance on supervisors (fewer interventions and of shorter duration) and to reduce the length of training, using extrinsic feedback that provides training or learning assistance using different modalities (visual, auditory, and haptic). Visual feedback has received much attention in recent decades. Users can be guided by a metaphor in a virtual environment. This metaphor may be a 3D trace of canonical movements, a visual cue pointing in the right direction, or gestures by an avatar that the trainee must mimic. However, with many kinds of feedback, trainees are not aware of their errors while performing gestures. Our hypothesis is that guiding users with a dynamic metaphor based on the visualization of errors will reduce these errors and improve performance. To this end, in a previous work we designed and implemented a new 3D metaphor called EBAGG to guide users in real time.In the present paper we evaluate EBAGG in relation to two other visual cues: first, a feedforward technique that displays the trace of a reference movement, and, second, a concurrent orientation feedback. The results of the user study show that EBAGG outperformed the others in improving users' performances over a training session. Moreover, the information assimilated during training with this dynamic feedback had a persistent effect when the metaphor was no longer displayed.",gestures; guidance; performance; user study; virtual reality; visual feedback,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Auris: creating affective virtual spaces from music,VRST - Virtual Reality Software and Technology,A,"Affective virtual spaces are of interest in many virtual reality applications such as education, wellbeing, rehabilitation, and entertainment. In this paper we present Auris, a system that attempts to generate affective virtual environments from music. We use music as input because it inherently encodes emotions that listeners readily recognize and respond to. Creating virtual environments is a time consuming and labor-intensive task involving various skills like design, 3D modeling, texturing, animation, and coding. Auris helps make this easier by automating the virtual world generation task using mood and content extracted from song audio and lyrics data respectively. Our user study results indicate virtual spaces created by Auris successfully convey the mood of the songs used to create them and achieve high presence scores with the potential to provide novel experiences of listening to music.",deep neural networks; generative models; music; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Using custom transformation axes for mid-air manipulation of 3D virtual objects,VRST - Virtual Reality Software and Technology,A,"Virtual Reality environments are able to offer natural interaction metaphors. However, it is difficult to accurately place virtual objects in the desired position and orientation using gestures in mid-air. Previous research concluded that the separation of degrees-of-freedom (DOF) can lead to better results, but these benefits come with an increase in time when performing complex tasks, due to the additional number of transformations required. In this work, we assess whether custom transformation axes can be used to achieve the accuracy of DOF separation without sacrificing completion time. For this, we developed a new manipulation technique, MAiOR, which offers translation and rotation separation, supporting both 3-DOF and 1-DOF manipulations, using personalized axes for the latter. Additionally, it also has direct 6-DOF manipulation for coarse transformations, and scaled object translation for increased placement. We compared MAiOR against an exclusively 6-DOF approach and a widget-based approach with explicit DOF separation. Results show that, contrary to previous research suggestions, single DOF manipulations are not appealing to users. Instead, users favored 3-DOF manipulations above all, while keeping translation and rotation independent.",3D user interfaces; custom manipulation axis; DOF separation; mid-air object manipulation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Computational design of hand-held VR controllers using haptic shape illusion,VRST - Virtual Reality Software and Technology,A,"Humans are capable of haptically perceiving the shape of an object by simply wielding it, even without seeing it. On the other hand, typical hand-held controllers for virtual reality (VR) applications are pre-designed for general applications, and thus not capable of providing appropriate haptic shape perception when wielding specific virtual objects. Contradiction between haptic and visual shape perception causes a lack of immersion and leads to inappropriate object handling in VR. To solve this problem, we propose a novel method for designing hand-held VR controllers which illusorily represent haptic equivalent of visual shape in VR. In ecological psychology, it has been suggested that the perceived shape can be modeled using the limited mass properties of wielded objects. Based on this suggestion, we built a shape perception model using a data-driven approach; we aggregated data of perceived shapes against various hand-held VR controllers with different mass properties, and derived the model using regression techniques. We implemented a design system which enables automatic design of hand-held VR controllers whose actual shapes are smaller than target shapes while maintaining their haptic shape perception. We verified that controllers designed with our system can present aimed shape perception irrespective of their actual shapes.",computational design; data-driven; perception; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Occlusion in outdoor augmented reality using geospatial building data,VRST - Virtual Reality Software and Technology,A,"Aligning virtual and real objects in Augmented Reality (AR) is essential for the user experience. Without alignment, the user loses suspension of disbelief and the sense of depth, distance, and size. Occlusion is a key feature to be aligned. Virtual content should be partially or fully occluded if real world objects are in its line-of-sight. The challenge for simulating occlusion is to construct the geometric model of the environment. Earlier studies have aimed to create realistic occlusions, yet most have either required depth-sensing hardware or a static predefined environment. This paper proposes and evaluates an alternative model-based method for dynamic outdoor AR of virtual buildings rendered on non depth-sensing smartphones. It uses geospatial data to construct the geometric model of real buildings surrounding the virtual building. The method removes the target regions from the virtual building using masks constructed from real buildings. While the method is not pixel-perfect, meaning that the simulated occlusion is not fully realistic, results from the user study indicate that it fulfilled its goal. A majority of the participants expressed that their experience and depth perception improved with the method activated. The result from this study has applications to mobile AR since the majority of smartphones are not equipped with depth sensors. Using geospatial data for simulating occlusions is a sufficiently effective solution until depth-sensing AR devices are more widely available.",AR; augmented reality; geospatial data; occlusion; open street maps; physical simulation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Exploring the effects of observed physicality conflicts on real-virtual human interaction in augmented reality,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) enables the illusion of computer-generated virtual objects and humans co-existing with us in the real world. Virtual humans (VHs) in AR can further induce an illusion of physicality in the real world due to their form of presentation and their behavior, such as showing awareness of their surroundings. However, certain behaviors can cause a conflict that breaks this illusion, for example, when we see a VH passing through a physical object.In this paper we describe a human-subject study that we performed to test the hypothesis that participants experience higher copresence in conflict-free circumstances, and we investigate the magnitude of this effect and behavioral manifestations. Participants perceived a social situation in a room that they shared with a VH as seen through a HoloLens head-mounted display. The behavior of the VH either caused conflicts with (occupied the same space as) physical entities, or avoided them. Our results show that the conflicts in physicality significantly reduced subjective reports of copresence. Moreover, we observed that participants were more likely to cause a conflict (occupy the same space as) virtual entities in case the VH had avoided the conflict. We discuss implications for future research and shared AR setups with real-virtual human interactions.",augmented reality; copresence; physicality; virtual humans,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,PanoTrace: interactive 3D modeling of surround-view panoramic images in virtual reality,VRST - Virtual Reality Software and Technology,A,"Full-surround panoramic imagery can provide a viewer with a high-resolution visual impression of a pictured real or realistically rendered environment, but it does not provide as high a level of immersion as modeled 3D geometry can, when viewed with virtual reality (VR) headsets or projection-based setups. In this paper, we demonstrate that augmenting panorama images with geometrical models can be done simply in VR itself and can significantly increase the feeling of immersion a viewer experiences. We propose a novel interactive modeling tool that allows users to model geometry depicted in a surround-panoramic scene directly in VR, utilizing projection mapping of the panorama on top of the evolving geometry. The user interface is intuitive and allows novice users to produce geometry that approximates ground truth models sufficiently to enhance a user's VR viewing experience. We designed a user study that compares users' self-reported levels of immersion, scene realism, and discomfort on a set of created models and comparison cases. Our results indicate that our modeled scenes produce a significantly higher sense of immersion than a basic dome geometry for the panorama when viewed in VR with head orientation and position tracking.",geometric modeling; panorama imaging; virtual reality; VR modeling tools,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Case-based planning for large virtual agent societies,VRST - Virtual Reality Software and Technology,A,"In this paper we discuss building large scale virtual reality reconstructions of historical heritage sites and populating it with crowds of virtual agents. Such agents are capable of performing complex actions, while respecting the cultural and historical accuracy of agent behaviour. In many commercial video games such agents either have very limited range of actions (resulting primitive behaviour) or are manually designed (resulting high development costs). In contrast, we follow the principles of automatic goal generation and automatic planning. Automatic goal generation in our approach is achieved through simulating agent needs and then producing a goal in response to those needs that require satisfaction. Automatic planning refers to techniques that are concerned with producing sequences of actions that can successfully change the state of an agent to the state where its goals are satisfied. Classical planning algorithms are computationally costly and it is difficult to achieve real-time performance for our problem domain with those. We explain how real-time performance can be achieved with Case-Based Planning, where agents build plan libraries and learn how to reuse and combine existing plans to archive their dynamically changing goals. We illustrate the novelty of our approach, its complexity and associated performance gains through a case-study focused on developing a virtual reality reconstruction of an ancient Mesopotamian settlement in 5000 B.C.",case-based planning; social simulations; virtual agents,Abstract,True,
Scopus,conferencePaper,2017,Pulse and vital sign measurement in mixed reality using a HoloLens,VRST - Virtual Reality Software and Technology,A,"Cardiography, quantitative measurement of the functioning of the heart, traditionally requires customized obtrusive contact sensors. Using new methods photoplethysmography and ballistocardiography signals can be captured using ubiquitous sensors, such as webcams and accelerometers. However, these signals are not visible to the unaided eye. We present Cardiolens - a mixed reality system that enables real-time, hands-free measurement and visualization of blood flow and vital signs from multiple people. The system combines a front-facing webcam, imaging ballistocardiography, and remote imaging photoplethysmography methods for recovering pulse signals. A heads up display allows users to view their own heart rate whenever they are wearing the device and the heart rate and heart rate variability of another person simply by looking at them. Cardiolens provides the wearer with a new way to understand physiological signals and has applications in human-computer interaction and in the study of social psychology.",health; interoception; mixed reality; physiology; remote sensing,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Virtual reality navigation system for prostate biopsy,VRST - Virtual Reality Software and Technology,A,"Prostate cancer is the most common non-cutaneous cancer in America. Tumor detection involves non-invasive screening tests, but positive results must be confirmed by a prostate biopsy. About twelve random samples are obtained during the biopsy, which is a systematic procedure traditionally performed with trans-rectal ultrasound (TRUS) guidance to determine prostate location. Recently, methods of fusion between TRUS and preoperative MRI have been introduced in order to perform targeted biopsies aimed to reduce the number of samples to few suspicious areas. Since the TRUS displaces the prostate during the procedure, the preoperative MRI does not match patient anatomy. Therefore, complex MRI deformation algorithms are needed. However, despite the substantial increase in complexity and cost, there is no strong evidence that the TRUS-MRI fusion actually improves accuracy and surgical outcomes.This paper presents an innovative virtual reality surgical navigation system for performing targeted prostate biopsies, without the need of the uncomfortable TRUS. Both biopsy needle and patient anatomy are constantly tracked by an electromagnetic tracking system that provides their 3D position and orientation with respect to the surgical bed. Multiple fiducial markers are placed on the patient skin (at the iliac crest and pubic bone) during MRI scanning. Once in the operative room, the surgeon is presented a stereoscopic 3D volumetric rendering and multiple orthogonal views of the patient anatomy, as well as a virtual representation of the tracked needle. After a simple registration process between the MRI and the tracker coordinate system, the navigation system guides the needle insertion in the patient perineum through several anatomical layers towards the biopsy targets.",graphics/3D; medical and health support; prototyping/implementation,Title_Abstract,True,
Scopus,conferencePaper,2017,Measurement of exceptional motion in VR video contents for VR sickness assessment using deep convolutional autoencoder,VRST - Virtual Reality Software and Technology,A,"This paper proposes a new objective metric of exceptional motion in VR video contents for VR sickness assessment. In VR environment, VR sickness can be caused by several factors which are mismatched motion, field of view, motion parallax, viewing angle, etc. Similar to motion sickness, VR sickness can induce a lot of physical symptoms such as general discomfort, headache, stomach awareness, nausea, vomiting, fatigue, and disorientation. To address the viewing safety issues in virtual environment, it is of great importance to develop an objective VR sickness assessment method that predicts and analyses the degree of VR sickness induced by the VR content. The proposed method takes into account motion information that is one of the most important factors in determining the overall degree of VR sickness. In this paper, we detect the exceptional motion that is likely to induce VR sickness. Spatio-temporal features of the exceptional motion in the VR video content are encoded using a convolutional autoencoder. For objectively assessing the VR sickness, the level of exceptional motion in VR video content is measured by using the convolutional autoencoder as well. The effectiveness of the proposed method has been successfully evaluated by subjective assessment experiment using simulator sickness questionnaires (SSQ) in VR environment.",cybersickness; machine learning; virtual reality,Keywords,True,
Scopus,conferencePaper,2017,Information recall in a virtual reality disability simulation,VRST - Virtual Reality Software and Technology,A,"The purpose of this paper is to investigate the effect of the sense of presence on one aspect of learning, information recall, in an immersive virtual reality (VR) disability simulation. Previous research has shown that the use of VR technology in education may facilitate improved learning outcomes, however, it is still an active research topic as the learning outcomes can vary widely. We hypothesized that a higher level of immersion and involvement in a VR disability simulation that leads to a high sense of presence will help the user improve information recall. To investigate this hypothesis, we conducted a between subjects experiment in which participants were presented information about multiple sclerosis in different immersive conditions and afterwards they attempted to recall the information. We also looked into whether there is any adverse effect of cybersickness on the information recall task in our disability simulation. The results from our study suggest that participants who were in immersive conditions were able to recall the information more effectively than the participants who experienced a non-immersive condition.",games for health; information recall; learning; or change (primary keyword); persuasion; user studies; virtual/augmented reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,"Presence and immersion of ""easy"" mobile VR with open flip-on lenses",VRST - Virtual Reality Software and Technology,A,"Mobile virtual reality (M-VR) uses an inexpensive and light headset into which the smartphone is inserted to conveniently experience immersive contents. Even so the headset is bulky and difficult to carry around, and makes the smartphone inaccessible. Recently, an alternative form of M-VR has appeared in the market in which the magnifying lenses are simply clipped on the smartphone (dubbed ""EasyVR""). Despite being open and the user'speripheral view not shut from the outside world, it still gives a good level of immersion with a wide magnified field of view. EasyVR has the added advantages of quick switch between with the regular smartphone usage mode and access to the touch screen for the seamless interaction. In this paper, we examine and compare the level of presence and immersion as provided by three different display configurations of M-VR: (1) EasyVR, (2) the usual headset which completely isolates the user from the outer-world with the vignetted view (ClosedVR), and (3) completely open hand-held smartphone view (OpenVR). We also control the environment condition, static or dynamic, as seen and perceived through the peripheral view and possibly having an effect on the level of presence and immersion in the respective display configuration. Our findings first show the easily expected, namely, both ClosedVR and EasyVR clearly exhibiting a much higher level of presence and immersion than OpenVR. The results also show that even though there is a substantial extent within the peripheral view showing the outer environment, the level of presence and immersion of EasyVR is nearly comparable to that of ClosedVR. Only when the environment was dynamic (as visible in the peripheral view ends), EasyVR showed a lower level of presence and immersion than ClosedVR, but still significantly higher than OpenVR. Therefore, EasyVR is a very attractive alternative to the usual ClosedVR, especially as a ""use-anywhere"" VR considering its clearly improved convenience and sufficient level of immersion beyond just for casual purposes.",distraction; immersion; mobile virtual reality; open/closed VR display; presence,Abstract_Keywords,True,
Scopus,conferencePaper,2017,The effect of avatar realism in immersive social virtual realities,VRST - Virtual Reality Software and Technology,A,"This paper investigates the effect of avatar realism on embodiment and social interactions in Virtual Reality (VR). We compared abstract avatar representations based on a wooden mannequin with high fidelity avatars generated from photogrammetry 3D scan methods. Both avatar representations were alternately applied to participating users and to the virtual counterpart in dyadic social encounters to examine the impact of avatar realism on self-embodiment and social interaction quality. Users were immersed in a virtual room via a head mounted display (HMD). Their full-body movements were tracked and mapped to respective movements of their avatars. Embodiment was induced by presenting the users' avatars to themselves in a virtual mirror. Afterwards they had to react to a non-verbal behavior of a virtual interaction partner they encountered in the virtual space. Several measures were taken to analyze the effect of the appearance of the users' avatars as well as the effect of the appearance of the others' avatars on the users. The realistic avatars were rated significantly more human-like when used as avatars for the others and evoked a stronger acceptance in terms of virtual body ownership (VBO). There also was some indication of a potential uncanny valley. Additionally, there was an indication that the appearance of the others' avatars impacts the self-perception of the users.",avatars; lifelike; social interaction; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Effects of virtual arm representations on interaction in virtual environments,VRST - Virtual Reality Software and Technology,A,"Many techniques for visualization and interaction that potentially increase user performance have been studied in the growing field of virtual reality. However, the effects of virtual-arm representations on users' performance and perception in selection tasks have not been studied before. This paper presents the results of a user study of three different representations of the virtual arm: ""hand only,"" ""hand+forearm,"" and ""whole arm"" which includes the upper arm. In addition to the representations' effects on performance and perception in selection tasks, we investigate how the users' performance changes depending on whether collisions with objects are allowed or not. The relationship between the virtual-arm representations and the senses of agency and ownership are also explored. Overall, we found that the ""whole arm"" condition performed worst.",3D interaction; natural hand interaction; selection performance; virtual arm; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Dual task based cognitive stress induction and its influence on path integration,VRST - Virtual Reality Software and Technology,A,"Current stress induction methods are often too theoretical and do not reflect to real life scenarios. In this study, we used a dual task paradigm in virtual reality, combining a navigation task with a reaction task. With this setup, we aimed at creating a novel benchmark stress induction approach utilizing modern virtual reality technology. Results show that our paradigm induced small scale physiological and subjective state changes. Lastly, we discuss our paradigm and experimental results from the perspective of ecological validity and present suggestions for improving our stress induction methodology as well as potential areas of use.",cognitive stress; ecological validity; navigation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,A comparative study of 2D and 3D mobile keypad user interaction preferences in virtual reality graphic user interfaces,VRST - Virtual Reality Software and Technology,A,"Graphical User Interfaces (GUI) on mobiles involves user interaction of touch input on a 2D surface. With advances in Augmented/Virtual Reality, possibilities of 3D GUIs will emerge. However, 3D GUIs do not have many design heuristics. This paper reports an experiment by collating quantitative and qualitative responses from 15 users, to explore usability problems that are likely to be encountered when a 2D interface element such as number keypad is replaced with a 3D element interface in Virtual reality. Would an interface with 3D elements perform better than the existing 2D GUIs is a moot research question? The results indicate user motivation towards using the interface inspired from 3D elements. The paper discusses issues of interaction in 2D and 3D virtual spaces with their possible implications for upcoming 3D VR environments.",3D GUI; mobile keypad; user study; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,A virtual reality based internet-of-things (IoT) framework for micro devices assembly,VRST - Virtual Reality Software and Technology,A,"The emergencelof Virtual Reality (VR) based technologies holds the potential to facilitate global collaboration in various fields of engineering. Micro Devices Assembly (MDA) is an emerging domain involving the assembly of micron sized objects and devices. In this paper, the focus of the discussion is the design of a VR based Internet-of-Things (IoT) based framework to support collaborative assembly of micro devices using both cyber and physical resources.At the center of this IoT framework is a Virtual Reality (VR) based simulation environment which serves as the link between cyber resources (which can support design analysis and planning) and physical manufacturing resources (which can accomplish the targeted assembly of micron sized products and parts). The feasibility analysis of proposed assembly plans is supported by stand-alone as well as networked based environments which enable proposing, comparing and modifying assembly sequences and plans. Several algorithms are available to generate near optimal assembly plans; these include Genetic Algorithm and Insertion Algorithm based approaches, which focus on determining near optimal assembly sequences based on target part destinations and part feeder positions. The benefits of such an integrated VR based cyber physical approach is to enable collaborative manufacturing frameworks to be more agile and respond to changing customer designs and requirements. Multiple partner organizations can potentially work together as Virtual Enterprises (VEs) sharing both their cyber and physical resources to accomplish the manufacturing of target designs [Cecil et al. 2017a].",assembly planning; collaborative manufacturing; internet-of-things; virtual reality based simulation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Acoustical manipulation for redirected walking,VRST - Virtual Reality Software and Technology,A,"Redirected Walking (RDW) manipulates a scene that is displayed to VR users so that they unknowingly compensate for scene motion and can thus explore a large virtual world on a limited space. So far, mostly visual manipulation techniques have been studied.This paper shows that users can also be manipulated by means of acoustical signals. In an experiment with a dynamically moving audio source we see deviations of up to 30% from a 20 m long straight-line walk for male participants and of up to 25% for females. Static audio has about two thirds of this impact.",motion perception; redirected walking; spatial audio; virtual locomotion; virtual reality,Keywords,True,
Scopus,conferencePaper,2017,Analysis of the user experience in a 3D gesture-based supported mobile VR game,VRST - Virtual Reality Software and Technology,A,"The work presented in this paper, explored the enhancement of User Experience (UX) by introducing a novel gesture-based controller in a mobile multiplayer Virtual Reality (VR) game. Using only the smartphone's RGB camera, the image input was used for both gesture analysis, capable of understanding user actions, as well as segmenting the real hand that was illustrated in the Virtual Environment (VE). Users were also able to share the VR space by cooperating in a survival-strategy scenario. The results from the user studies indicated that both the bare hand controller and the addition of another player in the VR scene, affected the experience for the participants. Users had a stronger feeling of presence in the VE when participated with an other user, and the visual representation of their hand in the VR world made the interactions seem more natural. Even though, there is still a number of limitations, this project nodes this approach capable of offering a natural and engaging solution of VR interaction, capable of rich UX while maintaining a low entry level for the end users.",bare-hand interaction; hand gestures; HCI; multiplayer VR; usability analysis; VR gaming,Abstract,True,
Scopus,conferencePaper,2017,Augmented invaders: a mixed reality multiplayer outdoor game,VRST - Virtual Reality Software and Technology,A,"Many virtual and mixed reality games focus on single player experiences. In this paper, we describe the concept and prototype implementation of a mixed reality multiplayer game that can be played with a smartphone and an HMD in outdoor environments. Players can team up to fight against attacking alien drones. The relative positions between the players are tracked using GPS, and the rear camera of the smartphone is used to augment the environment and teammates with virtual objects. The combination of multiplayer, mixed reality, the use of geographical location and outdoor action together with affordable, mobile equipment enables a novel strategic and social game experience.",AR; augmented reality; games; GPS; mixed reality; multiplayer,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Augmented reality system with collision response simulation using measured coefficient of restitution of real objects,VRST - Virtual Reality Software and Technology,A,"In this paper, we present an Augmented Reality (AR) system for video see-through Head Mounted Displays (HMD) which implements natural physical interaction between augmented virtual balls (which represent real balls) and real world objects. We measure the coefficient of restitution (COR) between real balls and real objects in the environment and use the corresponding COR when the virtual ball collides with a real world object. Experiment result show that subjects prefer the physical behavior of augmented balls using our method over the physical behavior using fixed COR.",augmented reality; coefficient of restitution; collision response simulation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Building a hybrid virtual agent for testing user empathy and arousal in response to avatar (micro-)expressions,VRST - Virtual Reality Software and Technology,A,"This poster paper describes a hybrid (i.e., film and CG) method for capturing and implementing facial expressions for/in VR. A video camera was used to capture an actor's performance. The actor's eyes and mouth were isolated, and footage was processed as movie textures to overlay a static 3D model of a head. Micro-expressions (subtle, rapid movements of muscles in and around the eyes and mouth in particular) are thus captured in a fine-grained, yet low- cost and low-tech alternative to established techniques. A future experiment will compare the emotive efficacy of the hybrid virtual agent with that of a conventional (fully CG) rigged avatar head in a 6DoF scenario that transitions from sympathetic (gauging empathy by self-report) to confrontational (gauging physiological arousal by heart-rate or GSR). The experiment's prospective design is discussed, as well as its significance for the study of the crucial intersection of social plausibility and perceptual realism in VR.",avatar capture; social plausibility; social presence; virtual reality,Keywords,True,
Scopus,conferencePaper,2017,Capturing aboriginal heritage in virtual reality,VRST - Virtual Reality Software and Technology,A,"The culture of Aboriginal Australians is unique and diverse, but, unfortunately, it is under threat of extinction. This is the culture that does not rely on written records, as knowledge is predominantly being shared in the form of oral tales and demonstrations. There are also many sensitivities associated with sharing knowledge with outsiders and with viewing pictures or videos of deceased people. To address these limitations we show how the combination of Virtual Reality, Motion Capture and Artificial Intelligence can help with providing interactive facilities for Aboriginal People that allow for preserving their cultural heritage and sharing it with others.",social simulations; virtual agents; virtual heritage,Title_Abstract,True,
Scopus,conferencePaper,2017,Color consistency of specular highlights in consumer cameras,VRST - Virtual Reality Software and Technology,A,"The latest advancements in Augmented Reality (AR) and Diminished Reality (DR) have allowed the development of many consumer-oriented applications (such as sales and driving aid, or education). To increase the realism in rendering, estimating the illumination in the scene is a key element. A lot of works tackle this problem but rarely discuss the color of the reconstructed illumination. The Dichromatic Model indicates that the specular component is not affected in color by the texture underneath and holds the light source's color. Though theoretically sound, in practice consumer cameras are subject to nonlinear behaviors which change RGB ratios and create inconsistencies when estimating the illumination. In this paper, we study the conditioning and limits of inverting local illumination models while relying on the Dichromatic Model. We show that the reconstructed specular component has an inconsistent color because it changes depending on the surface's colors.",augmented reality; color consistency; dichromatic model; local illumination; reflectance; saturation; specular highlight,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Diegetic cues for guiding the viewer in cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"Cinematic Virtual Reality has been increasing in popularity in the last years. Watching 360° movies with a Head Mounted Display, the viewer can freely choose the direction of view, and thus the visible section of the movie. We explored three cinematic methods of guiding the viewers' attention: lights, sounds, and movements. For that, we developed a measurement technique to obtain heat maps of viewing directions and applied statistical analysis methods for spatial data. The results of our work show that the attention of the viewer can be directed by sound and movements. New sound induces the viewer to search for the source of the sound, not all participants paid attention to the direction of the sound. In our experiments, lights without movements did not draw more attention than other objects. However, a moving light cone changed the viewing direction considerably.",cinematic virtual reality; directing gaze; guiding attention; spatial sound,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Direct retinal signals for virtual environments,VRST - Virtual Reality Software and Technology,A,"We present a novel signaling method for head-mounted displays, which surpasses eye (pupil) and delivers guiding light signals directly to retina through tissue near the eyes. This method preserves full visual acuity on the display and does not block view to the scene, while also delivering additional visual signals.",360° video; 3D interaction; HMD; virtual reality; VR viewers,Keywords,True,
Scopus,conferencePaper,2017,Do you feel what you see? Multimodal perception in virtual reality,VRST - Virtual Reality Software and Technology,A,"This paper discusses how different physically existing materials can be mapped on virtual textures in mixed reality environments by carrying out an explorative user study (n=101). For physical materials-in form of 3d trackable and moveable cubes-acrylic, wood and aluminum have been used. The virtual textures convey the impression of ceramic, fabric, glass, leather, paper, wood, acrylic, quartz, granite and aluminum. The study reveals which virtual textures match well with the different virtual textures and which do not match at all.",haptics; mixed reality; perception; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,"""Drop the beat"": virtual reality based mindfulness and cognitive behavioral therapy for panic disorder — a pilot study",VRST - Virtual Reality Software and Technology,A,"In this paper, we present a virtual reality based content/system called the ""Drop the beat"" designed to help the mindfulness and train one to overcome panic disorder. The two main elements of the proposed system are the (1) use of 360-degree video for presenting the panic inducing situation and, (2) facilitation of the mindfulness through a compelling scenario and immersive experience with multimodal feedback. In particular, we hypothesized that the direct observance and tangibly feeling for the beating heart in one's hand would help the user train to rationalize and overcome the situation (and e.g. effectively bring down one's heart rate back to a normal level). We conducted a small pilot study, administering the proposed VR content to five panic disorder patients and report the interim results.",cognitive behavioral therapy; mindfulness; multimodal feedback; panic disorder; psychotherapy; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Emotionally mediated spatial experience with AR,VRST - Virtual Reality Software and Technology,A,"This paper speculates and explores how emotional awareness and communication can be enhanced with the mediation of spatial experience. Based on two exploratory user studies, we designed and prototyped a conceptual system that mediates the spatial attributes of the surroundings according to user's choices and their emotional state. We then conducted user studies with the prototype. We contribute to existing literature by sharing our insights into potential use cases and implications of an emotionally responsive space.",affective computing; augmented reality; emotionscape; mediated reality; prototyping,Keywords,True,
Scopus,conferencePaper,2017,Enjoyable carving with ChiselDevice in mixed reality space,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose a system that can carve virtual objects in Mixed Reality (MR) space. The procedures of real-world carving include sculpting a rough outline, shaping sections, and carving patterns onto an object's surface. Of these, we focus on the procedure for carving patterns. Users of our system stroke a real object directly using ChiselDevice, and the surfaces of 3D virtual objects superimposed on the real object are ""carved."" This paper describes the design and development of the ChiselDevice and the MR carving system and the findings of users' experiences of our system.",carving system; mixed reality; tooldevice,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,EyeExpression: exploring the use of eye expressions as hands-free input for virtual and augmented reality devices,VRST - Virtual Reality Software and Technology,A,"Current virtual reality (VR) and augmented reality (AR) devices rely on handheld devices, hand gestures, head tracking, and voice input. This paper presents an initial exploration of using eye expressions as hands-free input modality for head-mounted AR/VR devices (HMDs). We consulted interaction designers and ophthalmologists and enumerated 12 eye expressions, and conducted a 12-person user study to better understand users' ability to perform them as well as preferences.",eye expression; eye muscle movement; head-mounted display,Title_Abstract,True,
Scopus,conferencePaper,2017,GalVR: a novel collaboration interface using GVS,VRST - Virtual Reality Software and Technology,A,"GalVR is a navigation interface that uses galvanic vestibular stimulation (GVS) during walking to cause users to turn from their planned trajectory. We explore GalVR for collaborative navigation in a two-player virtual reality (VR) game. The interface affords a novel game design that exploits the differences in first and third person perspectives, allowing VR and non-VR users to share a play experience. By introducing interdependence arising from dissimilar points of view, players can uniquely contribute to the shared experience based on their roles. We detail the design of our asymmetrical game, Dark Room and present some insights from a pilot study. Trust emerged as the defining factor for successful play.",collaboration; galvanic vestibular stimulation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Hand-free natural user interface for VR HMD with IR based facial gesture tracking sensor,VRST - Virtual Reality Software and Technology,A,We proposed a hand-free Natural User Interface (NUI) for VR Head-Mounted-Display (HMD) with infra-red (IR) based sensor tracking facial gestures. We have realized NUI based on the real-time recognition of user intuitions for VR HMD without any additional control devices except for a built-in Gyroscope and IR couplers with readout circuitry integrated in the foam interface of an HMD. We implemented seven control commands affordable for 3D interactions with virtual objects. The experimental data show that the proposed system provides a convenient and efficient 2D/3D user interface for both manipulating objects and controlling commands while wearing a VR HMD.,facial gesture recognition; head-mounted-display; natural user interface; virtual reality; wearable sensors,Keywords,True,
Scopus,conferencePaper,2017,Immercity: communicating about virtual and augmented realities,VRST - Virtual Reality Software and Technology,A,"Augmented and Mixed Reality technologies bring often definitions understanding issues for novice users. In this paper, we introduce our work in progress, Immercity, regarding the development of a content curation application which manage the idea of communicating on these technologies by their use.",augmented reality; content curation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Integrating performer into a real-time augmented reality performance spatially by using a multi-sensory prop,VRST - Virtual Reality Software and Technology,A,"The authors developed a system for augmenting live performances where the performer can drive the rendered background image via an infrared LED transmitter prop. A multi-sensory prop was designed to aid the position tracking of performer on large scale stage. An infrared LED and a wireless circuit were integrated into the prop, it could be captured by an infrared camera and host computer. It could also trigger the interaction between performer and virtual objects or visual effects, then the virtual objects or special effects will be matched with the position of real performer and displayed on the background large screen to make a real-time augmented live performance.",augmented reality; circuit; infrared LED; multi-modal interfaces; multi-sensory; performance; wireless,Title_Keywords,True,
Scopus,conferencePaper,2017,Multi-device mixed reality TV: a collaborative experience with joint use of a tablet and a headset,VRST - Virtual Reality Software and Technology,A,"A multi-user experience extending a standard TV content with AR elements is presented. It runs with both a standard tablet and a premium MR headset, the Microsoft HoloLens. A virtual TV mosaic is displayed around the TV screen and used as a GUI to control both TV and MR content. This paper focuses on the collaborative and personalized dimension offered by the experience. Unlike most AR applications, it can be simultaneously run by several users using different devices. The users can share content with others while keeping a personalized display. The added-value of such an extended TV experience has been demonstrated through complementary types of content, and user feedback confirms a real interest in this new kind of home entertainment, at the same time immersive, interactive, collaborative and personalized.",extended TV; GUI; mixed reality; multi-device; multi-user,Title_Keywords,True,
Scopus,conferencePaper,2017,PeriText+: utilizing peripheral vision for reading text on augmented reality smart glasses,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) provides real-time information by super-imposing virtual information onto users' view of the real world. Our work is the first to explore how peripheral vision, instead of central vision, can be used to read text on AR and smart glasses. We present PeriText+, a multiword reading interface using rapid serial visual presentation (RSVP). This enables users to observe the real world using central vision, while using peripheral vision to read text. We conducted a lab study to compare reading efficiency among 40 different conditions of text transformation. We also conducted a field study to evaluate the information transfer while using PeriText+ in a real-world walking scenario.",augmented reality; peripheral vision; reading interface; text transformation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,ProjectDR: augmented reality system for displaying medical images directly onto a patient,VRST - Virtual Reality Software and Technology,A,"The internal anatomy of a patient can be difficult for clinicians and other patients to visualize and analyzed in context. The ability to project medical images (CT and MRI scans) directly onto a patient's body helps those viewing these images to recover missing anatomical context for more accurate interpretation. This paper proposes such a system. Various types of images can be displayed using volume rendering techniques for realistic visualization of the internal anatomy and 3D models from segmented images. Calibration is performed on multiple systems to obtain an accurate common coordinate system, as well as correcting visual distortions from the cameras and the projector. This projected AR system provides a common perspective that is not tied to an individual point-of-view which can be used by others such as a surgical team. The system is easily extendable to other display technology and has many potential applications including education, surgical planning, laparoscopic surgery, and entertainment.",3D tracking; medical display; projected augmented reality,Title_Keywords,True,
Scopus,conferencePaper,2017,Real-time wall outline extraction for redirected walking,VRST - Virtual Reality Software and Technology,A,"Existing redirected walking applications use accurate tracking systems to determine the position and orientation of the user within a designated tracking space. In order to plan redirection and to ensure the user's safety, it is necessary to define the walking area in advance. However, when using ad hoc redirected walking, this is not possible, because the user's surroundings are not known beforehand.This paper introduces an approach to reconstruct the geometry of the available walking area as an outline representing the walls and similar structures. The outline is generated in real-time using a commercial SLAM tracking device and will be used for a wall warner safety mechanism and a redirection planner.",area tracking; redirected walking; virtual reality,Keywords,True,
Scopus,conferencePaper,2017,ScatAR: a mobile augmented reality application that uses scattering delay networks for room acoustic synthesis,VRST - Virtual Reality Software and Technology,A,"We present an augmented reality (AR) audio application where scattering delay networks efficiently generate and organize a reverberator, based on room geometry scanned by an AR device. The application allows for real-time processing and updating of reflection path geometry. It provides a proof-of-concept for plausible audio-spatial registration of a virtual object in a real environment, but further tests are needed in perceptual evaluation.",audio; augmented reality; real-time physics-based modeling; real-time rendering,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Searching and exploring software repositories in virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose a new approach to visualization of software repositories that allow users to search and explore software projects in virtual reality. In provided environment, respective information structures are mapped to interactive 3D objects. We assume that such transformation of information space to its visible representation will enable the users to gain problem domain knowledge subliminally during the explorations, and that the acquired knowledge will help them to fulfill future tasks more effectively.",feature location; search results visualization; software repositories; software visualization; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,The impact of avatar-owner visual similarity on body ownership in immersive virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper we report on an investigation of the effects of a self-avatar's visual similarity to a user's actual appearance, on their perceptions of the avatar in an immersive virtual reality (IVR) experience. We conducted a user study to examine the participant's sense of body ownership, presence and visual realism under three levels of avatar-owner visual similarity: (L1) an avatar reconstructed from real imagery of the participant's appearance, (L2) a cartoon-like virtual avatar created by a 3D artist for each participant, where the avatar shoes and clothing mimic that of the participant, but using a low-fidelity model, and (L3) a cartoon-like virtual avatar with a pre-defined appearance for the shoes and clothing. Surprisingly, the results indicate that the participants generally exhibited the highest sense of body ownership and presence when inhabiting the cartoon-like virtual avatar mimicking the outft of the participant (L2), despite the relatively low participant similarity. We present our experiment and main findings, also, discuss the potential impact of a self-avatar's visual differences on human perceptions in IVR.",body ownership; HMD; presence; self-avatar; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Towards multimodal interactions: robot jogging in mixed reality,VRST - Virtual Reality Software and Technology,A,"The recent progress made in the field of Augmented Reality/Mixed Reality (AR/MR) has opened new possibilities and approaches to research areas that can benefit from 3D visualization of digital content in the real world. In fact, human-robot interaction design and the design of user interfaces have very much to gain from MR technologies. Nonetheless, designing the user-robot interaction and processing multimodal feedbacks are very challenging tasks. In this paper we focus in particular on interactions in mixed reality.The main contribution of this paper is the implementation of a control system for an industrial manipulator through the user's interactions with MR content displayed with the Microsoft HoloLens. The system is based on the communication between Unity3D (used to design the user experience) and ROS, therefore extendible to any ROS-compatible robotic hardware.",mixed reality; multimodal interactions; robotics; ROS; unity3D,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Using virtual reality for scaffolding computer programming learning,VRST - Virtual Reality Software and Technology,A,"Learning how to analyze computational problems, to think critically, and to transfer algorithmic logic into language-specific code is central to computer programming. A critical step towards acquiring these skills is analyzing and debugging existing code usually starting with the famous ""Hello World"" program.Even after learning basic structure and syntax of a computer language, new learners struggle to understand algorithmic process, to mentally visualize effects of algorithms on data, and to remain engaged while learning it. We explore the use of virtual reality that teaches introductory concepts of computer programming to students in a 3D interactive space, while scaffolding their progression.",computer programming; interactive learning environments; technology-enhanced learning; virtual reality in education,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,VR wildfire prevention: teaching campfire safety in a gamified immersive environment,VRST - Virtual Reality Software and Technology,A,"Due to an increase in the prevalence and intensity of wildfires worldwide [Liu et al. 2010], it is becoming more important to understand campfire safety in order to prevent human-caused wildfires. In the United States, the most common fire safety advice comes in the form of commercials and posters as a part of the Smokey the Bear campaign [Smo 2017]. Presenting this information through a virtual reality game provides a controlled and engaging environment to practice and learn how to safely control a campfire. This immersive experience guides the user through every step of creating and extinguishing a campfire based on information from the Smokey the Bear campaign. VR Wildfire Prevention aims to engage and educate people in campfire safety by providing a controlled environment to practice the relevant techniques while incentivizing proper behavior through gamification. Players of the game report that the game is an enjoyable experience.",immersion; safety training; serious game; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2017,Cubely: virtual reality block-based programming environment,VRST - Virtual Reality Software and Technology,A,"Block-based programming languages are successfully being used as an alternative way of teaching introductory programming concepts. The success is in part due to the low barrier of entry and the visual game-like appeal fostering experimentation and creativity. Virtual reality (VR) presents a step further to an even more immersive and engaging experience. In this demo, we showcase our project Cubely, an immersive VR programming environment in which novice programmers solve programming puzzles within a virtual world. The puzzles are similar to Code.org exercises and solutions to the exercises are assembled by the programmer within the same virtual world using the cubes representing program instructions. The whole environment is templated to a theme of the popular Minecraft video game.",virtual learning environment; virtual reality; visual programming,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,LabDesignAR: configuring multi-camera motion capture systems in augmented reality,VRST - Virtual Reality Software and Technology,A,"We present LabDesignAR, an augmented reality application to support the planning, setup, and reconfiguration of marker-based motion capture systems with multiple cameras. LabDesignAR runs on the Microsoft HoloLens and allows the user to place an arbitrary number of virtual ""holographic"" motion capture cameras into an arbitrary space, in situ. The holographic cameras can be arbitrarily positioned, and different lens configurations can be selected to visualize the resulting fields of view and their intersections. Lab-DesignAR also demonstrates a hybrid natural gestural interaction technique, implemented through a fusion of the vision-based hand tracking capabilities of an augmented reality headset and instrumented gesture recognition with an electromyography armband. The source code for LabDesignAR and its supporting components can be found online.",augmented reality; gestural interaction; hololens; LabDesignAR; motion capture; natural interaction,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Solving Poisson's equation on the Microsoft HoloLens,VRST - Virtual Reality Software and Technology,A,We present a mixed reality application (HoloFEM) for the Microsoft HoloLens. The application lets a user define and solve a physical problem governed by Poisson's equation with the surrounding real world geometry as input data. Holograms are used to visualise both the problem and the solution. The finite element method is used to solve Poisson's equation. Solving and visualising partial differential equations in mixed reality could have potential usage in areas such as building planning and safety engineering.,FEniCS; finite element method; HoloLens; Poisson's equation,Abstract,True,
Scopus,conferencePaper,2017,The smart pin: a novel object manipulation technique for immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"In this paper we describe a demo setup showing the potential usefulness of a novel single-handed manipulation technique, designed to be used with immersive Virtual Environments. The technique allows manipulation control over objects in the scene through the use of a single 3D widget, allowing easy and separated control of translation, rotation and scaling actions. The goal is to provide an intuitive, easy-to-use and accurate way to perform simple manipulation tasks using only one hand. User tests demonstrated that the widget is intuitive and effective.",3D widgets; mid air manipulation; user evaluation; virtual reality,Keywords,True,
Scopus,conferencePaper,2017,Walkable self-overlapping virtual reality maze and map visualization demo: public virtual reality setup for asymmetric collaboration,VRST - Virtual Reality Software and Technology,A,This paper describes our demonstration of a walkable self-overlapping maze and its corresponding map to facilitate asymmetric collaboration for room-scale virtual reality setups in public places.,asymmetric collaboration; computer graphics; demo; public spaces; room-scale virtual reality; self-overlapping maze; virtual reality; visualization,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Sublime: a hands-free virtual reality menu navigation system using a high-frequency SSVEP-based brain-computer interface,VRST - Virtual Reality Software and Technology,A,"In this work we present Sublime, a new concept of Steady-State Visually Evoked Potential (SSVEP) based Brain-Computer Interface (BCI) where brain-computer communication occurs by capturing imperceptible visual stimuli integrated in the virtual scene and effortlessly conveying subliminal information to a computer. The technology was tested in a Virtual Reality (VR) environment, where the subject could navigate between the different menus by just gazing at them. The ratio between the stimuli frequencies and the refresh rate of the VR display creates an undesired perception of beats for which different solutions are proposed. To inform the user of target activation, real-time feedback in the form of loading bars is incorporated under each selectable object. We conducted experiments with several subjects and though the system is slower than a conventional joystick, users reported a satisfactory overall experience, in part due to the unexpected responsiveness of the system, as well as due to the fact that virtual objects flickered at a rate that did not cause annoyance. Since the imperceptible visual stimuli can be integrated unobtrusively to any element of the virtual world, we conclude that the potential applications of Sublime are extensive, especially in situations where knowing user's visual focus can be relevant.",brain-computer interface; electroencephalography; steady-state visually evoked potentials; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Standards-compliant HTTP adaptive streaming of static light fields,VRST - Virtual Reality Software and Technology,A,"Static light fields are an effective technology to precisely visualize complex inanimate objects or scenes, synthetic and real-world alike, in Augmented, Mixed and Virtual Reality contexts. Such light fields are commonly sampled as a collection of 2D images. This sampling methodology inevitably gives rise to large data volumes, which in turn hampers real-time light field streaming over best effort networks, particularly the Internet. This paper advocates the packaging of the source images of a static light field as a segmented video sequence so that the light field can then be interactively network streamed in a quality-variant fashion using MPEG-DASH, the standardized HTTP Adaptive Streaming scheme adopted by leading video streaming services like YouTube and Netflix. We explain how we appropriate MPEG-DASH for the purpose of adaptive static light field streaming and present experimental results that prove the feasibility of our approach, not only from a networking but also a rendering perspective. In particular, real-time rendering performance is achieved by leveraging video decoding hardware included in contemporary consumer-grade GPUs. Important trade-offs are investigated and reported on that impact performance, both network-wise (e.g., applied sequencing order and segmentation scheme for the source images of the static light field) and rendering-wise (e.g., disk-versus-GPU caching of source images). By adopting a standardized transmission scheme and by exclusively relying on commodity graphics hardware, the net result of our work is an interoperable and broadly deployable network streaming solution for static light fields.",experimental evaluation; H.264; HTTP adaptive streaming; IBR; JPEG; MPEG-DASH; static light fields; video compression,Abstract,True,
Scopus,conferencePaper,2018,Design and implementation of a multi-person fish-tank virtual reality display,VRST - Virtual Reality Software and Technology,A,"A mixed reality experience with a physical display, that situates 3D virtual content within the real world, has the potential to help people work and play with 3D information. However, almost all of such ""fish tank virtual reality"" (FTVR) systems have been isolated to a single-person experience, making them unsuitable for collaborative tasks. In this paper, we present a display system that allows two people to have unobstructed 3D perspective views into a spherical display while still being able to see and talk to one another. We evaluated the system through qualitative observation at a four-day exhibition and found it was effective for providing a convincing, shared 3D experience.",3D displays; co-location; collaboration; fish tank virtual reality; spherical displays; stereo,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Comparison of the usability of a car infotainment system in a mixed reality environment and in a real car,VRST - Virtual Reality Software and Technology,A,"Instead of installing new control modes for infotainments systems in a real vehicle for testing, it is an attractive idea (saving time and cost) to evaluate and develop these systems in a mixed reality (MR) environment. The central question of the study is whether the usability evaluation of a car entertainment system within a MR environment provides the same results as the evaluation of the car entertainment system within a real car. For this purpose a prototypical car infotainment system was built and integrated into a real car and into a MR environment. The MR environment represents the interior of the car and uses finger tracking and real haptic control elements of the center console of a car. Two test groups were assigned to the two different test environments. The study shows, that the usability is rated similar in both environments although readability and representation within the infotainment system is problematic.",car infotainment system; mixed reality; study; usability; user experience,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Camera time warp: compensating latency in video see-through head-mounted-displays for reduced cybersickness effects,VRST - Virtual Reality Software and Technology,A,"We introduce Camera Time Warp (CamWarp), a novel reprojection technique for video-see-through augmented reality, which reduces the registration error between captured real-world videos and rendered virtual images. Instead of rendering the image plane locked to the virtual camera, CamWarp renders the image plane at the real-world position it was captured at, and compensates for potential artifacts. We conducted two experiments to evaluate the effectiveness of CamWarp. In the first experiment participants were asked to report subjective discomfort while moving their head in a pattern inspired by the ISO 9241-9 Fitts' Law task at different speeds while the video feed was rendered at varying frame rates. The results show that the technique can significantly reduce subjective levels of discomfort and cybersickness symptoms for all tested configurations. In the second experiment participants were asked to move physical objects on a projected path as quickly and precisely as possible. Results show a positive effect of CamWarp on speed and accuracy.",augmented reality; cybersickness; latency compensation,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Performer vs. observer: whose comfort level should we consider when examining the social acceptability of input modalities for head-worn display?,VRST - Virtual Reality Software and Technology,A,"The popularity of head-worn displays (HWD) technologies such as Virtual Reality (VR) and Augmented Reality (AR) headsets is growing rapidly. To predict their commercial success, it is essential to understand the acceptability of these new technologies, along with new methods to interact with them. In this vein, the evaluation of social acceptability of interactions with these technologies has received significant attention, particularly from the performer's (i.e., user's) viewpoint. However, little work has considered social acceptability concerns from observers' (i.e., spectators') perspective. Although HWDs are designed to be personal devices, interacting with their interfaces are often quite noticeable, making them an ideal platform to contrast performer and observer perspectives on social acceptability. Through two studies, this paper contrasts performers' and observers' perspectives of social acceptability interactions with HWDs under different social contexts. Results indicate similarities as well as differences, in acceptability, and advocate for the importance of including both perspectives when exploring social acceptability of emerging technologies. We provide guidelines for understanding social acceptability specifically from the observers' perspective, thus complementing our current practices used for understanding the acceptability of interacting with these devices.",augmented reality; HWDs; input modalities; social acceptance,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Merging environments for shared spaces in mixed reality,VRST - Virtual Reality Software and Technology,A,"In virtual reality a real walking interface limits the extent of a virtual environment to our local walkable space. As local spaces are specific to each user, sharing a virtual environment with others for collaborative work or games becomes complicated. It is not clear which user's walkable space to prefer, or whether that space will be navigable for both users.This paper presents a technique which allows users to interact in virtual reality while each has a different walkable space. With this method mappings are created between pairs of environments. Remote users are then placed in the local environment as determined by the corresponding mapping.A user study was conducted with 38 participants. Pairs of participants were invited to collaborate on a virtual reality puzzle-solving task while in two different virtual rooms. An avatar representing the remote user was mapped into the local user's space. The results suggest that collaborative systems can be based on local representations that are actually quite different.",augmented reality; computer graphics; head-mounted display; mixed reality; planar map; remote collaboration; virtual co-location; virtual environments; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Perceived weight of a rod under augmented and diminished reality visual effects,VRST - Virtual Reality Software and Technology,A,"We can use augmented reality (AR) and diminished reality (DR) in combination, in practice. However, to the best of our knowledge, there is no research on the validation of the cross-modal effects in AR and DR. Our research interest here is to investigate how this continuous visual changes between AR and DR would change our weight sensation of an object. In this paper, we built a system that can continuously extend and reduce the amount of visual entity of real objects using AR and DR renderings to confirm that users can perceive things heavier and lighter than they actually are in the same manner as SWI. Different from the existing research where either AR or DR visual effects were used, we validated one of cross-modal effects in the context of both continuous AR and DR visuo-haptic. Regarding the weight sensation, we found that such cross-modal effect can be approximated with a continuous linear relationship between the weight and length of real objects. Our experimental results suggested that the weight sensation is closely related to the positions of the center of gravity (CoG) and perceived CoG positions lie within the object's entity under the examined conditions.",augmented reality; diminished reality; sense of ownership; visuo-haptic system; weight sensation,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Tracking projection mosaicing by synchronized high-speed optical axis control,VRST - Virtual Reality Software and Technology,A,"Projectors, as information display devices, have improved substantially and to achieve both the wide range and high resolution is desired for the dynamic human gaze. However, a fixed projector has a trade-off between the angle of projection and a resolution with limited pixels. Conventional methods with dynamic optical axis control lack the potential speed of the devices. We propose a tracking projection mosaicing with a high-speed projector and a high-speed optical axis controller for a randomly moving position, such as the gaze. We also propose a synchronization strategy by queuing and alternating operations to reduce motion-based artifacts, which realize a high-quality static image projection during the dynamic optical axis control. We have experimentally validated the geometric and temporal consistency of the proposed synchronization method and have attempted a demonstration of the tracking projection mosaicing for the dynamically moving bright spot of a laser pointer.",high-speed mirror; high-speed projector; high-speed visual feedback; projection-based augmented reality; synchronization,Keywords,True,
Scopus,conferencePaper,2018,Gaze navigation in the real world by changing visual appearance of objects using projector-camera system,VRST - Virtual Reality Software and Technology,A,"This paper proposes a method for gaze navigation in the real world by projecting an image onto a real object and changing its appearance. In the proposed method, a camera captures an image of objects in the real world. Next all the pixels in the image but those in a specified region are slightly shifted to left and right. Then the obtained image is projected onto the original objects. As a result, the objects not in the specified region looks blurred. We conducted user experiments and showed that the users' gaze were navigated to the specified region.",augmented reality; gaze navigation; procam; shift filter,Keywords,True,
Scopus,conferencePaper,2018,Eyestrain impacts on learning job interview with a serious game in virtual reality: a randomized double-blinded study,VRST - Virtual Reality Software and Technology,A,"Purpose: This study explores eyestrain and its possible impacts on learning performances and quality of experience using different apparatuses and imaging. Materials and Methods: 69 participants played a serious game simulating a job interview with a Samsung Gear VR Head Mounted Display (HMD) or a computer screen. The study was conducted according to a double-blinded protocol. Participants were randomly assigned to 3 groups: PC, HMD biocular and HMD stereoscopy (S3D). Participants played the game twice, allowing between group analyses. Eyestrain was assessed pre- and post-exposure on a chin-head rest with optometric measures. Learning traces were obtained in-game by registering response time and scores. Quality of experience was measured with questionnaires assessing Presence, Flow and Visual Comfort. Results: eyestrain was significantly higher with HMDs than PC based on Punctum Proximum of accommodation and visual acuity variables and tends to be higher with S3D. Learning was more efficient in HMDs conditions based on time for answering but the group with stereoscopy performed lower than the binocular imaging one. Quality of Experience was better based on visual discomfort with the PC condition than with HMDs. Conclusion: learning expected answers from a job interview is more efficient while using HMDs than a computer screen. However, eyestrain tends to be higher while using HMDs and S3D. The quality of experience was also negatively impacted with HMDs compared to computer screen. Not using S3D or lowering its impact should be explored to provide comfortable learning experience.1",eyestrain; head mounted display; learning; serious game; stereoscopy; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2018,Keep my head on my shoulders! why third-person is bad for navigation in VR,VRST - Virtual Reality Software and Technology,A,"Head-Mounted Displays are useful to place users in virtual reality (VR). They do this by totally occluding the physical world, including users' bodies. This can make self-awareness problematic. Indeed, researchers have shown that users' feeling of presence and spatial awareness are highly influenced by their virtual representations, and that self-embodied representations (avatars) of their anatomy can make the experience more engaging. On the other hand, recent user studies show a penchant towards a third-person view of one's own body to seemingly improve spatial awareness. However, due to its unnaturality, we argue that a third-person perspective is not as effective or convenient as a first-person view for task execution in VR. In this paper, we investigate, through a user evaluation, how these perspectives affect task performance and embodiment, focusing on navigation tasks, namely walking while avoiding obstacles. For each perspective, we also compare three different levels of realism for users' representation, specifically a stylized abstract avatar, a mesh-based generic human, and a real-time point-cloud rendering of the users' own body. Our results show that only when a third-person perspective is coupled with a realistic representation, a similar sense of embodiment and spatial awareness is felt. In all other cases, a first-person perspective is still better suited for navigation tasks, regardless of representation.",augmented reality; avatar; embodiment; full-body tracking; travel; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Dynamic HDR environment capture for mixed reality,VRST - Virtual Reality Software and Technology,A,"Rendering accurate and convincing virtual content into mixed reality (MR) scenes requires detailed illumination information about the real environment. In existing MR systems, this information is often captured using light probes [1, 8, 9, 17, 19–21], or by reconstructing the real environment as a preprocess [31, 38, 54]. We present a method for capturing and updating a HDR radiance map of the real environment and tracking camera motion in real time using a self-contained camera system, without prior knowledge about the real scene. The method is capable of producing plausible results immediately and improving in quality as more of the scene is reconstructed. We demonstrate how this can be used to render convincing virtual objects whose illumination changes dynamically to reflect the changing real environment around them.",3D reconstruction; HDR; mixed reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,An evaluation of pupillary light response models for 2D screens and VR HMDs,VRST - Virtual Reality Software and Technology,A,"Pupil diameter changes have been shown to be indicative of user engagement and cognitive load for various tasks and environments. However, it is still not the preferred physiological measure for applied settings. This reluctance to leverage the pupil as an index of user engagement stems from the problem that in scenarios where scene brightness cannot be controlled, the pupil light response confounds the cognitive-emotional response. What if we could predict the light response of an individual's pupil, thus creating the opportunity to factor it out of the measurement? In this work, we lay the groundwork for this research by evaluating three models of pupillary light response in 2D, and in a virtual reality (VR) environment. Our results show that either a linear or an exponential model can be fit to an individual participant with an easy-to-use calibration procedure. This work opens several new research directions in VR relating to performance analysis and inspires the use of eye tracking beyond gaze as a pointer and foveated rendering.",eyetracking; light response; pupil dilation; videos; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Occlusion handling using semantic segmentation and visibility-based rendering for mixed reality,VRST - Virtual Reality Software and Technology,A,"Real-time occlusion handling is a major problem in outdoor mixed reality system because it requires great computational cost mainly due to the complexity of the scene. Using only segmentation, it is difficult to accurately render a virtual object occluded by complex objects such as vegetation. In this paper, we propose a novel occlusion handling method for real-time mixed reality given a monocular image and an inaccurate depth map. We modify the intensity of the overlayed CG object based on the texture of the underlying real scene using visibility-based rendering. To determine the appropriate level of visibility, we use CNN-based semantic segmentation and assign labels to the real scene based on the complexity of object boundary and texture. Then we combine the segmentation results and the foreground probability map from the depth image to solve the appropriate blending parameter for visibility-based rendering. Our results show improvement in handling occlusions for inaccurate foreground segmentation compared to existing blending-based methods.",mixed reality; occlusion handling; semantic segmentation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,A longitudinal study of small group interaction in social virtual reality,VRST - Virtual Reality Software and Technology,A,"Now that high-end consumer phones can support immersive virtual reality, we ask whether social virtual reality is a promising medium for supporting distributed groups of users. We undertook an exploratory in-the-wild study using Samsung Gear VR headsets to see how existing social groups that had become geographically dispersed could use VR for collaborative activities. The study showed a strong propensity for users to feel present and engaged with group members. Users were able to bring group behaviors into the virtual world. To overcome some technical limitations, they had to create novel forms of interaction. Overall, the study found that users experience a range of emotional states in VR that are broadly similar to those that they would experience face-to-face in the same groups. The study highlights the transferability of existing social group dynamics in VR interactions but suggests that more work would need to be done on avatar representations to support some intimate conversations.",affective states; avatar representation; in-the-wild study; social VR; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Human upper-body inverse kinematics for increased embodiment in consumer-grade virtual reality,VRST - Virtual Reality Software and Technology,A,"Having a virtual body can increase embodiment in virtual reality (VR) applications. However, comsumer-grade VR falls short of delivering sufficient sensory information for full-body motion capture. Consequently, most current VR applications do not even show arms, although they are often in the field of view. We address this shortcoming with a novel human upper-body inverse kinematics algorithm specifically targeted at tracking from head and hand sensors only. We present heuristics for elbow positioning depending on the shoulder-to-hand distance and for avoiding reaching unnatural joint limits. Our results show that our method increases the accuracy compared to general inverse kinematics applied to human arms with the same tracking input. In a user study, participants preferred our method over displaying disembodied hands without arms, but also over a more expensive motion capture system. In particular, our study shows that virtual arms animated with our inverse kinematics system can be used for applications involving heavy arm movement. We demonstrate that our method can not only be used to increase embodiment, but can also support interaction involving arms or shoulders, such as holding up a shield.",animation; embodiment; inverse kinematics; motion capture; presence; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Immersion and coherence in a stressful virtual environment,VRST - Virtual Reality Software and Technology,A,"We report on the design and results of two experiments investigating Slater's Place Illusion (PI) and Plausibility Illusion (Psi) in a virtual visual cliff environment. PI (the illusion of being in a place) and Psi (the illusion that the depicted events are actually happening) were proposed by Slater as orthogonal components of virtual experience which contribute to realistic response in a VE. To that end, we identified characteristics of a virtual reality experience that we expected to influence one or the other of PI and Psi. We designed two experiments in which each participant experienced a given VE in one of four conditions chosen from a 2×2 design: high or low levels of PI-eliciting characteristics (that is, immersion) and high or low levels of Psi-eliciting characteristics. Following Skarbez, we use the term ""coherence"" for those characteristics which contribute to Psi, parallel to the use of ""immersion"" for characteristics that contribute to PI. We collected both questionnaire-based and physiological metrics. Several existing presence questionnaires could not reliably distinguish the effects of PI from those of Psi. They did, however, indicate that high levels of PI-eliciting characteristics and Psi-eliciting characteristics together result in higher presence, compared any of the other three conditions. This suggests that ""breaks in PI"" and ""breaks in Psi"" belong to a broader category of ""breaks in experience,"" any of which result in a degraded user experience. Participants' heart rates, however, responded markedly differently in the two Psi conditions; no such difference was observed across the PI conditions. This indicates that a VE that exhibits unusual or confusing behavior can cause stress in a user that affects physiological responses, and that one must take care to eliminate such confusing behaviors if one is using physiological measurement as a proxy for subjective experience in a VE.",coherence; immersion; physiological metrics; place illusion(PI); plausibility illusion (Psi); presence; user studies; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,The physical-virtual table: exploring the effects of a virtual human's physical influence on social interaction,VRST - Virtual Reality Software and Technology,A,"In this paper, we investigate the effects of the physical influence of a virtual human (VH) in the context of face-to-face interaction in augmented reality (AR). In our study, participants played a tabletop game with a VH, in which each player takes a turn and moves their own token along the designated spots on the shared table. We compared two conditions as follows: the VH in the virtual condition moves a virtual token that can only be seen through AR glasses, while the VH in the physical condition moves a physical token as the participants do; therefore the VH's token can be seen even in the periphery of the AR glasses. For the physical condition, we designed an actuator system underneath the table. The actuator moves a magnet under the table which then moves the VH's physical token over the surface of the table. Our results indicate that participants felt higher co-presence with the VH in the physical condition, and participants assessed the VH as a more physical entity compared to the VH in the virtual condition. We further observed transference effects when participants attributed the VH's ability to move physical objects to other elements in the real world. Also, the VH's physical influence improved participants' overall experience with the VH. We discuss potential explanations for the findings and implications for future shared AR tabletop setups.",augmented reality; mediated physicality; virtual humans,Abstract_Keywords,True,
Scopus,conferencePaper,2018,With a little help from a holographic friend: the OpenIMPRESS mixed reality telepresence toolkit for remote collaboration systems,VRST - Virtual Reality Software and Technology,A,"Remote mixed reality (MR) collaboration systems allow for multimodal, real-time support from remote experts. We present our open toolkit that provides a flexible end-to-end solution for building such systems using off-the-shelf hardware. From related work, three core design aspects have been identified: 1) the independence of the viewpoint that the visitor (the remote expert) can take in relation to position and viewpoint of the visitee, 2) the immersiveness of the presentation technology for visitor and visitee, and 3) the extent to which the visitor's body is represented in the visitee's environment. This paper describes the implementation of our system, which includes these aspects. In a study aimed at validating whether we implemented these core aspects to good effect, conducted with a collaborative puzzle application built with our toolkit, we examine how variations of these aspects contribute to usability, performance and social presence related metrics.",collaboration; embodiment; mixed reality; telepresence,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Training in IVR: investigating the effect of instructor design on social presence and performance of the VR user,VRST - Virtual Reality Software and Technology,A,"We investigate instructor representations (IRs) in the context of virtual trainings with head mounted displays (HMD). Despite the recently increased industry and research focus on virtual training in immersive virtual reality (IVR), the effect of IRs on the performer (VR user) has received little attention. We present the results of a study (N=33), evaluating the effect of three IRs - webcam, avatar and sound-only - on social presence (SP) and performance (PE) of the VR user during task completion. Our results show that instructor representation has an effect on SP and that, contrary to our assumption based on prior work, it affects performance negatively.",immersive virtual reality; instructor design; social presence,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Am I in the theater? usability study of live performance based virtual reality,VRST - Virtual Reality Software and Technology,A,"Duplicating the audience experience of an art performance with VR technology is a promising VR application, which is considered to provide better viewer experience than the conventional video. As various forms of art performances are recorded by the panoramic camera and broadcasted on the Internet, the impact of this new VR-based media to the viewers needs to be systematically studied. In this work, a two-level usability framework is proposed, which combines the traditional concepts of presence and the quality evaluation of art performances, aiming to systematically study the usability of such VR application. Both the conventional video and the panoramic video of a theatre performance were captured simultaneously, and were replayed to two groups of viewers in a cinematic setup and through an HMD respectively. The psychological measurement methods, including the questionnaire and the interview, as well as the psychophysical measurement methods, including the EEG and the motion capture techniques were both used in the study. The results show that the such VR application duplicates the live performance better by providing a higher sense of presence, higher engagement levels, and stronger desire to see live performance. For visual intensive performance contents, the new VR-based media can provide a better user experience. The future development of the new media forms based on the panoramic video technique could benefit from this work.",EEG; live performance; usability evaluation; viewer experience; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2018,Discrete scene rotation during blinks and its effect on redirected walking algorithms,VRST - Virtual Reality Software and Technology,A,"Moving through a virtual environment (VE) by real walking is beneficial to user immersion, feeling of presence and way finding. However, the available physical spaces are of limited size and usually much smaller than the VE. One solution to this problem is using redirection techniques (RDTs). While the focus of existing research has been mostly on continuous RDTs, work on discrete RDTs is still limited.In this paper, we present our research results on the discrete rotation of a virtual scene during walking. A study with 14 subjects was conducted to identify the detection threshold of the scene rotation in two conditions: during blinking and when eyes are open. Results showed that on average, users failed to detect a scene rotation of 9.1 degrees during blinking, as compared to 2.4 degrees when eyes are open. Simulations were then performed to investigate the effects of incorporating discrete scene orientation during blinks into existing algorithms such as steer-to-center and steer-to-orbit when different predefined paths are followed. Results showed that on average the number of resets is reduced by 13%, and the minimum space required for encountering no reset is reduced by 20%. A reset technique was also proposed and shown to give better performance than the existing two-one turn reset technique.",blink; discrete rotation; redirected walking; virtual reality; visual suppression,Keywords,True,
Scopus,conferencePaper,2018,The effect of chair type on users' viewing experience for 360-degree video,VRST - Virtual Reality Software and Technology,A,"The consumption of 360-degree videos with head-mounted displays (HMDs) is increasing rapidly. A large number of HMD users watch 360-degree videos at home, often on non-swivel seats; however videos are frequently designed to require the user to turn around. This work explores how the difference in users' chair type might influence their viewing experience. A between-subject experiment was conducted with 41 participants. Three chair conditions were used: fixed, half-swivel and full-swivel. A variety of measures were explored using eye-tracking, questionnaires, tasks and semi-structured interviews. Results suggest that the fixed and half-swivel chairs discouraged exploration for certain videos compared with the full-swivel chair. Additionally, participants in the fixed chair had worse spatial awareness and greater concern about missing something for certain video than those in the full-swivel chair. No significant differences were found in terms of incidental memory, general engagement and simulator sickness among the three chair conditions. Furthermore, thematic analysis of post-experiment interviews revealed four themes regarding the restrictive chairs: physical discomfort, difficulty following moving objects, reduced orientation and guided attention. Based on the findings, practical implications, limitations and future work are discussed.",cinematic virtual reality; panoramic video; user study,Keywords,True,
Scopus,conferencePaper,2018,Data-driven modeling of group entitativity in virtual environments,VRST - Virtual Reality Software and Technology,A,We present a data-driven algorithm to model and predict the socio-emotional impact of groups on observers. Psychological research finds that highly entitative i.e. cohesive and uniform groups induce threat and unease in observers. Our algorithm models realistic trajectory-level behaviors to classify and map the motion-based entitativity of crowds. This mapping is based on a statistical scheme that dynamically learns pedestrian behavior and computes the resultant entitativity induced emotion through group motion characteristics. We also present a novel interactive multi-agent simulation algorithm to model entitative groups and conduct a VR user study to validate the socio-emotional predictive power of our algorithm. We further show that model-generated high-entitativity groups do induce more negative emotions than low-entitative groups.,crowd simulation; data driven simulation; group dynamics; motion model; pedestrian behavior; virtual reality,Keywords,True,
Scopus,conferencePaper,2018,Automatic transfer of musical mood into virtual environments,VRST - Virtual Reality Software and Technology,A,"This paper presents a method that automatically transforms a virtual environment (VE) according to the mood of input music. We use machine learning to extract a mood from the music. We then select images exhibiting the mood and transfer their styles to the textures of objects in the VE photorealistically or artistically. Our user study results indicate that our method is effective in transferring valence-related aspects, but not arousal-related ones. Our method can still provide novel experiences in virtual reality and speed up the production of VEs by automating its procedure.",affect; mood; music; transfer; virtual environment,Abstract,True,
Scopus,conferencePaper,2018,Step aside: an initial exploration of gestural input for lateral movement during walking-in-place locomotion,VRST - Virtual Reality Software and Technology,A,"Walking-in-place (WIP) techniques provide users with a relatively natural way of walking in virtual reality. However, previous research has primarily focused on WIP during forward movement and tasks involving turning. Thus, little is known about what gestures to use in combination with WIP in order to enable sidestepping. This paper presents two user studies comparing three different types of gestures based on movement of the hip, leaning of the torso, and actual sidesteps. The first study focuses on purely lateral movement while the second involves both forward and lateral movement. The results of both studies suggest that leaning yielded significantly more natural walking experiences and this gesture also produced significantly less positional drift.",locomotion; travel; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Haptic around: multiple tactile sensations for immersive environment and interaction in virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we present Haptic Around, a hybrid-haptic feedback system, which utilizes fan, hot air blower, mist creator and heat light to recreate multiple tactile sensations in virtual reality for enhancing the immersive environment and interaction. This system consists of a steerable haptic device rigged on the top of the user head and a handheld device also with haptics feedbacks to simultaneously provide tactile sensations to the users in a 2m x 2m space. The steerable haptic device can enhance the immersive environment for providing full body experience, such as heat in the desert or cold in the snow mountain. Additionally, the handheld device can enhance the immersive interaction for providing partial body experience, such as heating the iron or quenching the hot iron. With our system, the users can perceive visual, auditory and haptic when they are moving around in virtual space and interacting with virtual object. In our study, the result has shown the potential of the hybrid-haptic feedback system, which the participants rated the enjoyment, realism, quality, immersion higher than the other.",haptics; immersive environment; immersive experience; multiple tactile sensation; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Can we perceive changes in our moving speed: a comparison between directly and indirectly powering the locomotion in virtual environments,VRST - Virtual Reality Software and Technology,A,"Many categories of the illusion of self-motion have been widely studied with the potential support of virtual reality. However, the effects of directly and indirectly powering the movement on the possibility of perceiving changes in moving speed and their relationship with sensory feedback on users' speed change perception have not been investigated before. In this paper, we present the results of our user study on the difference in perceiving changes in moving speed between two different movement techniques: ""pedaling"" and ""throttling"". We also explore the effects of different velocity gains, accelerations and speeds of airflow, and their interactions with the movement techniques on users' perception of speed changes in addition to user performance and perception. We built a bike simulator that supports both of the movement techniques and provides sensory feedback. In general, ""pedaling"" gave users more possibility to perceive changes in moving velocity than ""throttling"".",bike simulator; locomotion; speed perception; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,HFX studio: haptic editor for full-body immersive experiences,VRST - Virtual Reality Software and Technology,A,"Current virtual reality systems enable users to explore virtual worlds, fully embodied in avatars. This new type of immersive experience requires specific authoring tools. The traditional ones used in the movie and the video games industries were modified to support immersive visual and audio content. However, few solutions exist to edit haptic content, especially when the whole user's body is involved. To tackle this issue we propose HFX Studio, a haptic editor based on haptic perceptual models. Three models of pressure, vibration and temperature were defined to allow the spatialization of haptic effects on the user's body. These effects can be designed directly on the body (egocentric approach), or specified as objects of the scene (allocentric approach). The perceptual models are also used to describe capabilities of haptic devices. This way the created content is generic, and haptic feedback is rendered on the available devices. The concept has been implemented with the Unity®game engine, a tool already used in VR production. A qualitative pilot user study was conducted to analyze the usability of our tool with expert users. Results shows that the edition of haptic feedback is intuitive for these users.",edition; full body; haptics; immersive experience,Abstract,True,
Scopus,conferencePaper,2018,The impact of fear of the sea on working memory performance: a research based on virtual reality,VRST - Virtual Reality Software and Technology,A,"The sea has been manifested to cause the emotion of fear to people when it comes to a very depth, especially to those who have thalassophobia. Many people have to work in the sea while nearly no research on influence of fear of the sea to cognition has been carried out. This study explores the impact of fear of the sea induced by immersive virtual reality on working memory which is a cognitive system with a limited capacity. Participants were required to complete n-back working memory task of three difficulty levels in the non-emotional environment and the undersea environment respectively by means of virtual reality. Pupil diameter changes were recorded along with the task performance. In addition to reaction times and accuracy (correctly press a button in response to targets) as two task performance indices used in most researches, the commission errors (incorrectly press a button in response to non-targets) and omission errors (incorrectly do not press a button in response to targets) were also differentiated herein. The results of the study indicated that the virtual undersea environment did induce the emotion of fear. As for the task performance, except that the performance of low-level task did not differ much between the two environments, the fear of the sea increased the accuracy of the medium level n-back task but decreased it of high-level n-back task. Result of omission errors was just the opposite and commission errors were increased in both levels of task. The findings, including the positive role of a moderate level of fear of the sea in the performance of working memory task, make a lot of sense for future cognitive work in the sea.",fear of the sea; task performance; virtual reality; working memory,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Investigating the reason for increased postural instability in virtual reality for persons with balance impairments,VRST - Virtual Reality Software and Technology,A,"The objective of this study is to investigate how different visual components of Virtual Reality (VR), such as field of view, frame rate, and display resolution affect postural stability in VR. Although previous studies identified these visual components as some of the primary factors that differ significantly in VR from reality, the effect of each component on postural stability is yet unknown. While most people experience postural instability in VR, it is worse for people with balance impairments (BIs). This may be because they depend more on their visual cues to maintain postural stability. We conducted a study with ten people with balance impairments due to Multiple Sclerosis (MS) and seven people without balance impairments to investigate the effect of different visual components on postural stability. In each condition, we varied one of the visual components and kept all other components fixed. Each participant explored the virtual environment (VE) in a controlled fashion to make sure that the effect of the visual components was consistent for all participants. Results from our study suggest that for people with BIs, decreased field of view and decreased frame rate have significant negative effects on postural stability, but the effect of display resolution is inconclusive. However, for people without BIs, there were no significant differences for any of the visual components. Therefore, VR systems targeting people with balance impairments should focus on improving field of view and frame rate before improving display resolution.",accessibility; balance; head-mounted display; postural stability; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,In-pulse: inducing fear and pain in virtual experiences,VRST - Virtual Reality Software and Technology,A,"Researchers have attempted to increase the realism of virtual reality (VR) applications in many ways. Combinations of the visual, auditory and haptic feedback have successfully simulated experiences in VR, however, multimedia contents may also stimulate emotions. In this paper, we especially paid attention to negative emotions that may be perceived in such experiences (e.g., fear). We hypothesized that volunteering, visual, mechanical, and electrical feedback may induce negative emotional feedback to users. In-Pulse is a novel system and approach to explore the potential of bringing this emotional feedback to users. We designed a head-mounted display (HMD) combined with mechanical and electrical muscle stimulation (EMS) actuators. A user study was performed to explore the effect of our approaches with combinations with VR contents. The results suggest that mechanical actuators and EMS can improve the experience of virtual experiences.",electrical muscle stimulation; emotion; fear; head-mounted display; pain; virtual reality; wearables,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Investigating different modalities of directional cues for multi-task visual-searching scenario in virtual reality,VRST - Virtual Reality Software and Technology,A,"In this study, we investigated and compared the effectiveness of visual, auditory, and vibrotactile directional cues on multiple simultaneous visual-searching tasks in an immersive virtual environment. Effectiveness was determined by the task-completion time, the range of head movement, the accuracy of the identification task, and the perceived workload. Our experiment showed that the on-head vibrotactile display can effectively guide users towards virtual visual targets, without affecting their performance on the other simultaneous tasks, in the immersive VR environment. These results can be applied to numerous applications (e.g. gaming, driving, and piloting) in which there are usually multiple simultaneous tasks, and the user experience and performance could be vulnerable.",auditory; directional cue; multi-task; vibration; virtual reality; visual,Title_Keywords,True,
Scopus,conferencePaper,2018,A lightweight and efficient system for tracking handheld objects in virtual reality,VRST - Virtual Reality Software and Technology,A,"While the content of virtual reality (VR) has grown explosively in recent years, the advance of designing user-friendly control interfaces in VR still remains a slow pace. The most commonly used device, such as gamepad or controller, has fixed shape and weight, and thus can not provide realistic haptic feedback when interacting with virtual objects in VR. In this work, we present a novel and lightweight tracking system in the context of manipulating handheld objects in VR. Specifically, our system can effortlessly synchronize the 3D pose of arbitrary handheld objects between the real world and VR in realtime performance. The tracking algorithm is simple, which delicately leverages the power of Leap Motion and IMU sensor to respectively track object's location and orientation. We demonstrate the effectiveness of our system with three VR applications use pencil, ping-pong paddle, and smartphone as control interfaces to provide users more immersive VR experience.",haptic feedback; object tracking; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,AR DeepCalorieCam V2: food calorie estimation with CNN and AR-based actual size estimation,VRST - Virtual Reality Software and Technology,A,"In most of the cases, the estimated calories are just associated with the estimated food categories, or the relative size compared to the standard size of each food category which are usually provided by a user manually. In addition, in the case of calorie estimation based on the amount of meal, a user conventionally needs to register a size-known reference object in advance and to take a food photo with the registered reference object. In this demo, we propose a new approach for food calorie estimation with CNN and Augmented Reality (AR)-based actual size estimation. By using Apple ARKit framework, we can measure the actual size of the meal area by acquiring the coordinates on the real world as a three-dimensional vector, we implemented this demo app. As a result, it is possible to calculate the size more accurately than in the previous method by measuring the meal area directly, the calorie estimation accuracy has improved.",application; augmented reality; deep learning; food calorie estimation; iOS,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Automatic 3D modeling of artwork and visualizing audio in an augmented reality environment,VRST - Virtual Reality Software and Technology,A,"In recent years, traditional art museums have begun to use AR/VR technology to make visits more engaging and interactive. This paper details an application which provides features designed to be immediately engaging and educational to museum visitors within an AR view. The application superimposes an automatically generated 3D representation over a scanned artwork, along with the work's authorship, title, and date of creation. A GUI allows the user to exaggerate or decrease the depth scale of the 3D representation, as well as to search for related works of music. Given this music as audio input, the generated 3D model will act as an audio visualizer by changing depth scale based on input frequency.",3D model generation; art museum; audio visualization; augmented reality; education; music,Title_Keywords,True,
Scopus,conferencePaper,2018,Design-led 3D visualization of nanomedicines in virtual reality,VRST - Virtual Reality Software and Technology,A,"Nanomedicines are a promising addition to the arsenal of new cancer therapies. During development, scientists must precisely track their distribution in the body, a task that can be severely limited by traditional 2D displays. With its stereoscopic capacity and real-time interactivity, virtual reality (VR) provides an encouraging platform to accurately visualize dynamic 3D volumetric data. In this research, we develop a prototype application to track nanomedicines in VR. This platform has the potential to enhance data assessment, comprehension and communication in preclinical research which may ultimately influence the paradigm of future clinical protocols.",data visualization; education; interface design; medical imaging; nanotechnology; PET-CT; science; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,EXG wearable human-machine interface for natural multimodal interaction in VR environment,VRST - Virtual Reality Software and Technology,A,"Current assistive technologies are complicated, cumbersome, not portable, and users still need to apply extensive fine motor control to operate the device. Brain-Computer Interfaces (BCIs) could provide an alternative approach to solve these problems. However, the current BCIs have low classification accuracy and require tedious human-learning procedures. The use of complicated Electroencephalogram (EEG) caps, where many electrodes must be attached on the user's head to identify imaginary motor commands, brings a lot of inconvenience. In this demonstration, we will showcase EXGbuds, a compact, non-obtrusive, and comfortable wearable device with non-invasive biosensing technology. People can comfortably wear it for long hours without tiring. Under our developed machine learning algorithms, we can identify various eye movements and facial expressions with over 95% accuracy, such that people with motor disabilities could have a fun time to play VR games totally ""Hands-free"".",human-machine interface; machine learning; physiological signal processing; virtual reality; wearable device,Keywords,True,
Scopus,conferencePaper,2018,Future-mine VR as narrative decision making tool,VRST - Virtual Reality Software and Technology,A,"This work presents a narrative story of a Future Mine scenario that uses Virtual Reality as a medium to replace traditional spreadsheet-based policy making framework currently widely used in government agencies for decision making process. The scenario presented envisions user exploring underground mine, where extraction processes had been almost fully automated, and environment is constantly monitored by a variety of modern and futuristic sensors. The use of story-telling using VR is explored to present novel application scenarios for sensing technologies and to facilitate better understanding of the context in which they will be used. Further the experience is translated into informed decision making.",informed decision making; interactive design; storytelling; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,GravityCup: a liquid-based haptics for simulating dynamic weight in virtual reality,VRST - Virtual Reality Software and Technology,A,"During interaction in a virtual environment, haptic displays provide users with sensations such as vibration, texture simulation, and electrical muscle stimulation. However, as humans perceive object weights naturally in daily life, objects picked up in virtual reality feel unrealistically light. To create an immersive experience in virtual reality that includes weight sensation, we propose GravityCup, a liquid-based haptic feedback device that simulates realistic object weights and inertia when moving virtual handheld objects. In different scenarios, GravityCup uses liquid to provide users with a dynamic weight sensation experience that enhances interaction with handheld objects in virtual reality.",haptics; liquid-based; virtual reality; weight simulation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Hand motion prediction for just-in-time thermo-haptic feedback,VRST - Virtual Reality Software and Technology,A,This paper presents two innovative design solutions for thermal feedback displays in virtual environments. First solution is aiming to eliminate or decrease the time delay between the user action and onset of the thermal feedback using Machine Learning for user motion prediction. Second is the design of compact but efficient water cooling system necessary to provide cold sensations using peltier elements. Presented thermal display is wearable and battery powered.,haptic feedback; machine learning; motion prediction; neural networks; thermal feedback; virtual reality,Keywords,True,
Scopus,conferencePaper,2018,Indoor AR navigation using tilesets,VRST - Virtual Reality Software and Technology,A,"This paper demonstrates the methodology and findings of creating an augmented reality navigation app that uses tilesets to create the navigation. It illustrates the method in which the app was created - using vector data and uploading it to MapBox, then accessing that data in Unity through the MapBox API and map editor and then overlaying the camera input with the navigation path layer. The application was tested by creating multiple arbitrary navigation scenarios and checking them for various factors. The main finding of this research is that this navigation solution works better than GPS indoor navigation.",augmented reality; indoor navigation; tilesets,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Interactive virtual exhibition: creating custom virtual art galleries using web technologies,VRST - Virtual Reality Software and Technology,A,"This paper presents an immersive 3D virtual reality application accessed through the web that allows users to create their own custom virtual art galleries. The application allows users to select paintings based on a time range or country and then it dynamically generates the 3D virtual exhibit. Various features about the exhibit can be customized, such as the floor texture and wall color. Users can also save their exhibit, so it can be shared with others.",art gallery; immersive world; user interface; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Tap-tap menu: body touching for virtual interactive menus,VRST - Virtual Reality Software and Technology,A,"Virtual and mixed realities make it possible to view and interact with virtual objects in 3D space. However, where to position menus in 3D space and how to interact with them are often problems. Existing studies developed methods of displaying a menu on the hand or arm. In this study, we proposed a menu system that appears at various body parts. By placing the menu on the body, it enables the user to operate the menus comfortably through kinesthesia, and perceive tactile feedback. Furthermore, displaying the menu not only in the hands and arms but also in the upper legs and the abdomen, the menu display area can be expanded. In this study, we developed a modeling application and introduced a proposed menu design for that application.",gestural input; head-mounted display; menu; mixed reality; virtual reality,Keywords,True,
Scopus,conferencePaper,2018,TransFork: using olfactory device for augmented tasting experience with video see-through head-mounted display,VRST - Virtual Reality Software and Technology,A,"When people eat, the taste is very complex and be influenced easily by other senses. Such as visual, olfactory, and haptic, even past experiences, can affect the human perception, which in turn creates more taste possibilities. We present TransFork, an eating tool with olfactory feedback, which augments the tasting experience with video see-through head-mounted display. Additionally, we design a recipe via preliminary experiments to find out the taste conversion formula, which could enhance the flavor of foods and change the user perception to recognize the food. In this demonstration, we prepare a mini feast with bite-sized fruit, the participants use the TransFork to eat food A and smell the scent of food B stored at the aromatic box via airflow guiding. Before they deliver the food to their mouth, the head-mounted display augmented the color of food B on food A by the QR code on the aromatic box. With this augmented reality techniques and the recipe, the tasting experience could be augmented or enhanced, which is a potential approach and could be a playful used for eating.",augmented reality; augmented tasting experience; olfactory; video see-through head-mounted display,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Using mixed reality for promoting brand perception,VRST - Virtual Reality Software and Technology,A,"Mixed reality offers an immersive and interactive experience through the use of head mounted displays and in-air gestures. Visitors can discover additional content virtually, on top of existing physical items. For a small-scale exhibition at a cafe, we developed a Microsoft HoloLens application to create an interactive experience on top of a collection of historic physical items. Through public experiences of this exhibition, we received positive feedback of our system, and found that it also helped to promote brand perception. In this demo, visitors can experience a similar mixed reality experience that was shown at the exhibition.",brand perception; exhibition; mixed reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Virtual reality environment to support activity in the real world: a case of working environment using microscope,VRST - Virtual Reality Software and Technology,A,"This manuscript introduces a virtual reality (VR) environment to support research activity in the real world. We constructed a prototype to support intellectual activity in the field of life sciences using VR. In the prototype, the users can operate a real microscope from a virtual space, along with other useful equipment such as huge displays, and analyze images carefully and intuitively using a immersive visualizer seamlessly integrated in the environment. We belive that our prototype is promising for expanding the potential of VR applications.",life science; microscope; virtual laboratory; virtual reality; working environment,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,VirtualHaus: a collaborative mixed reality application with tangible interface,VRST - Virtual Reality Software and Technology,A,"We present VirtualHaus, a collaborative mixed reality application allowing two participants to recreate Mozart's apartment as it used to be by interactively placing furniture. Each participant has a different role and therefore uses a different application: the visitor uses an immersive virtual reality application, while the supervisor uses an augmented reality application. The two applications are wirelessly synchronised and display the same information with distinct viewpoints and tools.",collaborative applications; cultural heritage; mixed reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Visualizing and exploring OSGi-based software architectures in augmented reality,VRST - Virtual Reality Software and Technology,A,"This demo presents an immersive augmented reality solution for visualizing OSGi-based software architectures. By employing an island metaphor, we map abstract software entities to tangible real-world objects. Using advanced input modalities, such as voice and gesture control, our approach allows for interactive exploration and examination of complex software systems.",augmented reality; real-world metaphor; software visualization; user interface design,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,VRTe do: the way of the virtual hand,VRST - Virtual Reality Software and Technology,A,"We are presenting a Virtual Reality training system for Karate kata based on motion capture and Virtual Reality technologies. The system is built as a game, in which the player needs to learn and repeat different kata to progress and reach the next level. Different levels are represented by obi (belts) of different color, corresponding real Karate obi. We capture players' motion with a Kinect camera and enable interaction with game objects. A database is integrated in the game so that different players can use, save and track their training progress.",interactivity; karate; kinect; learning; sport; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,3D model augmentation using depth information in an AR environment,VRST - Virtual Reality Software and Technology,A,"This paper proposes a method for augmenting a 3D CAD model onto an image using a mobile device. An image and its depth map of a target are obtained using a Phab 2 mobile device. The image is processed to extract line segments of the target. Next a rectangular planar region of the target is selected, which is then refined using the depth data. The chosen region is then compared with the CAD model, and a planar face in the CAD model that matches the selected region is obtained using various geometric properties. Using the matching planar faces, a pose of the camera is computed, which is then used for augmenting the CAD model onto the image correctly. The test results demonstrate that the method can be used for real fabrication in a complex environment.",augmented reality(AR); CAD model augmentation; camera pose estimation,Keywords,True,
Scopus,conferencePaper,2018,An AR system for artistic creativity education,VRST - Virtual Reality Software and Technology,A,"Creativity and innovation training is the core of the art education. Modern technology provides more effective tools to help students obtain artistic creativity. In this paper, we propose to employ augmented reality technology to assist artistic creativity education. We first analyze the inefficiency of traditional artistic creation training. We then introduce our AR-based smartphone app with technical detail and explain how it can improve accelerate artistic creativity training. We finally show 3 examples created by our AR app to demonstrate the effectiveness of our proposed method.",artistic creativity education; augmented reality; interaction,Abstract_Keywords,True,
Scopus,conferencePaper,2018,An evaluation of smartphone-based interaction in AR for constrained object manipulation,VRST - Virtual Reality Software and Technology,A,"In Augmented Reality, interaction with the environment can be achieved with a number of different approaches. In current systems, the most common are hand and gesture inputs. However experimental applications also integrated smartphones as intuitive interaction devices and demonstrated great potential for different tasks. One particular task is constrained object manipulation, for which we conducted a user study. In it we compared standard gesture-based approaches with a touch-based interaction via smartphone. We found that a touch-based interface is significantly more efficient, although gestures are being subjectively more accepted. From these results we draw conclusions on how smartphones can be used to realize modern interfaces in AR.",augmented reality; smartphone; study; user interface design,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Analysis of the R-V dynamics illusion behavior in terms of auditory stimulation,VRST - Virtual Reality Software and Technology,A,"The R-V Dynamics illusion is a phenomenon where weight perception is changed by superimposing a CG case with a movable portion (CG) onto a real object using mixed reality technology. In previous studies, it has been confirmed that weight perception is affected by the size/volume of the CG, and a virtual collision sound between the case and the movable portion could also be a cause of this illusionary phenomenon. However, in previous studies, only one virtual collision sound is applied. Therefore, in this study, we consider the influence of the physical characteristics of virtual collision sound such as the size and weight of the movable object in the phenomenon. As a result, it was confirmed that the weight perception changes according to the virtual collision sound, and participants lightly perceived the real object when a virtual collision sound is played with a smaller and lighter object.",mixed reality; sense of weight; visual stimulation,Abstract_Keywords,True,
Scopus,conferencePaper,2018,AR navigation solution using vector tiles,VRST - Virtual Reality Software and Technology,A,"This study discusses the results and findings of an augmented reality navigation app that was created using vector data uploaded to an online mapping software for indoor navigation. The main objective of this research is to determine the current issues with a solution of indoor navigation that relies on the use of GPS signals, as these signals are sparse in buildings. The data was uploaded in the form of GeoJSON files to MapBox which relayed the data to the app using an API in the form of Tilesets. The application converted the tilesets to a miniaturized map and calculated the navigation path, and then overlaid that navigation line onto the floor via the camera.Once the project setup was completed, multiple navigation paths have been tested numerous times between the different sync points and destination rooms. At the end, their accuracy, ease of access and several other factors, along with their issues, were recorded. The testing revealed that the navigation system was not only accurate despite the lack of GPS signal, but it also detected the device motion precisely. Furthermore, the navigation system did not take much time to generate the navigation path, as the app processed the data tile by tile. The application was also able to accurately measure the ground plane along with the walls, perfectly overlaying the navigation line. However, a few observations indicated various factors affected the accuracy of the navigation, and testing revealed areas where major improvements can be made to improve both accuracy and ease of access.",augmented reality; indoor navigation; tilesets,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Automatic 3D modeling of artwork and visualizing audio in an augmented reality environment,VRST - Virtual Reality Software and Technology,A,"In recent years, traditional art museums have begun to use AR/VR technology to make visits more engaging and interactive. This paper details an application which provides features designed to be immediately engaging and educational to museum visitors within an AR view. The application superimposes an automatically generated 3D representation over a scanned artwork, along with the work's authorship, title, and date of creation. A GUI allows the user to exaggerate or decrease the depth scale of the 3D representation, as well as to search for related works of music. Given this music as audio input, the generated 3D model will act as an audio visualizer by changing depth scale based on input frequency.",3D model generation; art museum; audio visualization; augmented reality; education; music,Title_Keywords,True,Duplicate
Scopus,conferencePaper,2018,Being them: presence of using non-human avatars in immersive virtual environment,VRST - Virtual Reality Software and Technology,A,"This work examines the differences of the effects between using humanoid and non-humanoid avatars on the user's Illusion of Virtual Body Ownership (IVBO) and experience. We used three kinds of avatars: bipedalism group (human), quadrupedalism group (wolf), and serpentine motion group (snake). The result shows that using non-humanoid avatars feel more sense of change of their body. Users feel more proficient when using the humanoid avatar, but are more pleased with the non-humanoid avatars.",non-humanoid avatar; virtual body ownership; virtual reality,Keywords,True,
Scopus,conferencePaper,2018,BoatAR: a multi-user augmented-reality platform for boat,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) allows virtual object projection with an unblocked view of the physical world which provides reference and other people. The mixed scene provides an agile platform for communication and collaboration, especially on a product that would be difficult or expensive to present otherwise. In the boating industry, high customization leaves dealers with a high cost on inventory, financially and spatially. In this work, we present BoatAR, a multi-user AR boat configuration system designed for addressing these issues. A prototype system was implemented using HoloLens with shared experience, and demonstrated to a group of boat dealers and received positive feedback. BoatAR provided an example of how a multi-user AR system could help in the conventional industry.",augmented reality; collaboration,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Designing dynamic aware interiors,VRST - Virtual Reality Software and Technology,A,"We are pursuing a vision of reactive interior spaces that are aware of people's actions and transform according to changing needs. We envision furniture and walls that act as interactive displays and that shapeshift to the correct physical form, and the appropriate interactive visual content and modality. This paper briefly describes our proposal based on our recent efforts on realizing this vision.",3D user interface; augmented reality; human-computer interactions; interactive tabletop and surfaces; robot; virtual reality,Keywords,True,
Scopus,conferencePaper,2018,Does automatic game difficulty level adjustment improve acrophobia therapy? differences from baseline.,VRST - Virtual Reality Software and Technology,A,"This paper presents the design and development of a Virtual Reality game for treating acrophobia, as well as a comparative study between the players' performance in the game, under two different conditions - one in which the difficulty levels are adjusted according to the subjects' biophysical data and one in which they are not. The results showed an improvement of the parameters correlated with fear level in the first experiment.",acrophobia; deep learning; fear estimation; game level prediction; gamification; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Dual-MR: interaction with mixed reality using smartphones,VRST - Virtual Reality Software and Technology,A,"Mixed reality (MR) has changed the perspective we see and interact with our world. While the current-generation of MR head-mounted devices (HMDs) are capable of generating high quality visual contents, interation in most MR applications typically relies on in-air hand gestures, gaze, or voice. These interfaces although are intuitive to learn, may easily lead to inaccurate operations due to fatigue or constrained by the environment. In this work, we present Dual-MR, a novel MR interation system that i) synchronizes the MR viewpoints of HMD and handheld smartphone, and ii) enables precise, tactile, immersive and user-friendly object-level manipulations throught the multi-touch input of smartphone. In addition, Dual-MR allows multiple users to join the same MR coordinate system to facilite the collaborate in the same physical space, which further broadens its usability. A preliminary user study shows that our system easily overwhelms the conventional interface, which combines in-air hand gesture and gaze, in the completion time for a series of 3D object manipulation tasks in MR.",3D manipulation interface; human computer interaction; mixed reality; multi-touch input; smartphone,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Effect of accompanying onomatopoeia to interaction sound for altering user perception in virtual reality,VRST - Virtual Reality Software and Technology,A,"Onomatopoeia refers to a word that phonetically imitates, resembles the sound, or depict an event at hand. In languages like Korean and Japanese, it is used in everyday conversation to emphasize certain situation and enrich the prose. In this poster, we explore if the use of onomatopoeia, visualized and added to the usual sound feedback, could be taken advantage to increase or alter the perceived realism of the sound feedback itself, and furthermore of the situation at hand in virtual reality. A pilot experiment was run to compare the user's subjective perceived realism and experience under four test conditions of presenting a simple physical interaction, accompanying it with: (1) just the ""as-is"" sound (baseline), (2) ""as-is"" sound and onomatopoeia, (3) a representative sound sample (e.g. one for all different collision conditions), and (4) a representative sound sample and onomatopoeia. Our pilot study has found that the use of onomatopoeia can alter and add on to the perceived realism/naturalness of the virtual situation such that the experiences of the single representative sound added with the onomatopoeia and ""as-is"" sound were deemed similar.",onomatopoeia; realism; sound feedback; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Effect of accompanying onomatopoeia with sound feedback toward presence and user experience in virtual reality,VRST - Virtual Reality Software and Technology,A,"Onomatopoeia refers to a word that phonetically imitates the sound. It is often used, in comics or video, in caption as a way to dramatize, emphasize, exaggerate and draw attention the situation. In this paper we explore if the use of onomatopoeia could also bring about similar effects and improve the user experience in virtual reality. We present an experiment comparing the user's subjective experiences and attentive performance in two virtual worlds, each configured in two test conditions: (1) sound feedback with no onomatopoeia and (2) sound feedback with it. Our experiment has found that the moderate and strategic use of onomatopoeia can indeed help direct user attention, offer object affordance and thereby enhance user experience and even the sense of presence and immersion.",onomatopoeia; sounds visualization; user experience; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Effects of head-display lag on presence in the oculus rift,VRST - Virtual Reality Software and Technology,A,"We measured presence and perceived scene stability in a virtual environment viewed with different head-to-display lag (i.e., system lag) on the Oculus Rift (CV1). System lag was added on top of the measured benchmark system latency (22.3 ms) for our visual scene rendered in OpenGL Shading Language (GLSL). Participants made active head oscillations in pitch at 1.0Hz while viewing displays. We found that perceived scene instability increased and presence decreased when increasing system lag, which we attribute to the effect of multisensory visual-vestibular interactions on the interpretation of the visual information presented.",oculus rift; perception; presence; virtual reality; VR,Keywords,True,
Scopus,conferencePaper,2018,Evaluating ray casting and two gaze-based pointing techniques for object selection in virtual reality,VRST - Virtual Reality Software and Technology,A,"Selecting an object is a basic interaction task in virtual reality (VR) environments. Interaction techniques with gaze pointing have potential for this elementary task. There appears to be little empirical evidence concerning the benefits and drawbacks of these methods in VR. We ran an experiment studying three interaction techniques: ray casting, dwell time and gaze trigger, where gaze trigger was a combination of gaze pointing and controller selection. We studied user experience and interaction speed in a simple object selection task. The results indicated that ray casting outperforms both gaze-based methods while gaze trigger performs better than dwell time.",controller pointing; gaze pointing; object selection in virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,EXController: enhancing interaction capability for VR handheld controllers using real-time vision sensing,VRST - Virtual Reality Software and Technology,A,"This paper presents EXController, a new controller-mounted finger posture recognition device specially designed for VR handheld controllers. We seek to provide additional input through real-time vision sensing by attaching a near infrared (NIR) camera onto the controller. We designed and implemented an exploratory prototype with a HTC Vive controller. The NIR camera is modified from a traditional webcam and applied with a data-driven Convolutional Neural Network (CNN) classifier. We designed 12 different finger gestures and trained the CNN classifier with a dataset from 20 subjects, achieving an average accuracy of 86.17% across - subjects, and, approximately more than 92% on three of the finger postures, and more than 89% on the top-4 accuracy postures. We also developed a Unity demo that shows matched finger animations, running at approximately 27 fps in real-time.",gesture recognition; handheld controller; virtual reality,Keywords,True,
Scopus,conferencePaper,2018,Experience the dougong construction in virtual reality,VRST - Virtual Reality Software and Technology,A,"Dougong is a unique culture in Chinese traditional architecture. In University, the Architectural students usually use video, pictures, and even handmade craft to learn the knowledge and culture about Dougong. However, making these complicated Dougong components by hands requires a lot of facilities. To solve these problems, this paper builds a learning application using Virtual Reality (VR) technology, where students can master how to construct Dougong by interacting with the virtual models. In addition to learning module, the application creates a simulated scene showing students the great charm and design ideas of ancient Chinese buildings. The comparison experiments indicate that the students learning via VR-based application identify more Dougong components and their placement than those learning via conventional teaching.",architecture education; experiential teaching; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Extending recreational environments with a landscape-superimposed display using mixed reality,VRST - Virtual Reality Software and Technology,A,"Herein, we describe a system that extends recreational experiences by overlaying a virtual landscape of a remote place over the currently experienced real landscape using mixed reality (MR) technology and displaying avatars of other users. There are many recreational activities that can be performed outdoors. However, such activities usually involve some traveling costs, preparation time, and require schedule adjustments. To reduce the impact of these factors, we implemented a system that extends recreational environments, thereby allowing free movement through the manipulation of the visual information using MR.",mixed reality; panorama; recreation; virtual human,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Gigapixel virtual reality employing live superzoom cameras,VRST - Virtual Reality Software and Technology,A,"We present a live gigapixel virtual reality system employing a 360° camera, a superzoom camera with a pan-tilt robotic head, and a head-mounted display (HMD). The system is capable of showing on-demand gigapixel-level subregions of 360° videos. Similar systems could be used to have live feed for foveated rendering HMDs.",360° video; foveated rendering; gigapixel; head-mounted display; superzoom; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Hamlet: directing virtual actors in computational live theater,VRST - Virtual Reality Software and Technology,A,"We present ""Hamlet"", a prototype implementation of a virtual reality experience in which a player takes on a role of the theater director. The objective of the experience is to direct Adam, a virtual actor, to deliver the best possible performance of Hamlet's famous ""To be, or not to be"" soliloquy. The player interacts with Adam using voice commands, gestures, and body motion. Adam responds to acting directions, offers his own interpretations of the soliloquy, acquires the choreography from the player's body motion, and learns the scene blocking by following the player's pointing gestures.",immersion; virtual performance; virtual reality; virtual theater,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Hands-free vibrotactile feedback for object selection tasks in virtual reality,VRST - Virtual Reality Software and Technology,A,"Interactions between humans and virtual environments rely on timely and consistent sensory feedback, including haptic feedback. However, many questions remain open concerning the spatial location of haptics on the user's body in VR. We studied how simple vibrotactile collision feedback on two less studied locations, the temples, and the wrist, affects an object picking task in a VR environment. We compared visual feedback to three visual-haptic conditions, providing haptic feedback on the participants' (N=16) wrists, temples or simultaneously on both locations. The results indicate that for continuous, hand-based object selection, the wrist is a more promising feedback location than the temples. Further, even a suboptimal feedback location may be better than no haptic collision feedback at all.",collision detection; haptic feedback; object selection in virtual reality; visual feedback,Title_Keywords,True,
Scopus,conferencePaper,2018,Illumination for 360 degree cameras,VRST - Virtual Reality Software and Technology,A,"Additional illumination improves the capture of omnidirectional 360° video and images, especially for dark or high-contrast environments. There is no ""behind"" for 360° cameras, so the placement of lights is a problem. We explore ways to position lights on some 360° cameras, and propose two good locations.",360° cameras; cinematic virtual reality; head-mounted display; illumination; surround video; visualization,Keywords,True,
Scopus,conferencePaper,2018,Image compensation and stabilization for immersive 360-degree videos from capsule endoscopy,VRST - Virtual Reality Software and Technology,A,"This paper describes image processing that can be used to develop immersive 360-degree videos using capsule endoscopy procedures. When viewed through a head-mounted display (HMD), doctors are able to inspect the human gastrointestinal tract as if they were inside the patient's body. Although the endoscopy capsule has two tiny fisheye cameras, the images captured by these cameras cannot be converted to equirectangular images which is the basic format used to produce 360-degree videos. This study proposes a method to generate a pseudo-omnidirectional video from the original images and stabilizes the video to prevent virtual reality (VR) sickness.",capsule endoscopy; omnidirectional video; video stabilization,Abstract,True,
Scopus,conferencePaper,2018,Interactive virtual exhibition: creating custom virtual art galleries using web technologies,VRST - Virtual Reality Software and Technology,A,"This paper presents an immersive 3D virtual reality application accessed through the web that allows users to create their own custom virtual art galleries. The application allows users to select paintings based on a time range or country and then it dynamically generates the 3D virtual exhibit. Various features about the exhibit can be customized, such as the floor texture and wall color. Users can also save their exhibit, so it can be shared with others.",art gallery; immersive world; user interface; virtual reality,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2018,Low-cost VR collaborative system equipped with haptic feedback,VRST - Virtual Reality Software and Technology,A,"In this paper, we present a low-cost virtual reality (VR) collaborative system equipped with a haptic feedback sensation system. This system is composed of a Kinect sensor for bodies and gestures detection, a microcontroller and vibrators to simulate outside interactions, and smartphone powered cardboard, all of this are put into a network implemented with Unity 3D game engine.",collaborative virtual reality; haptic feedback system,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Measuring physical exertion in virtual reality exercise games,VRST - Virtual Reality Software and Technology,A,"We demonstrate a novel method of applying the capabilities of mobile virtual reality technology to the health sciences by measuring physical exertion in a VR exercise game. By measuring changes in heart rate in a thirteen person user study, we find evidence to suggest that virtual reality exercise is able to induce a moderate to high level of physical exertion and produce an immersive an intriguing experience.",exercise games; human-computer interaction; virtual reality; walking-in-place,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Multi-view augmented reality with a drone,VRST - Virtual Reality Software and Technology,A,"This paper presents some early results from an exploration into Augmented Reality (AR) applications where users have access to controllable alternative viewing positions based on a camera mounted unmanned aerial vehicle (UAV). These results include a system specification that defines and identifies the requirements of multi-view AR; and a demo application where the user can switch between the traditional first person and third person view. While being an initial step in the investigation, the results do illustrate practical applications for multi-view AR functionality. The paper concludes with a discussion on the next steps for the investigation.",augmented reality; camera views; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Perceptual model optimized efficient foveated rendering,VRST - Virtual Reality Software and Technology,A,"Higher resolution, wider FOV and increasing frame rate of HMD are demanding more VR computing resources. Foveated rendering is a key solution to these challenges. This paper introduces a perceptual model optimized foveated rendering. Tessellation levels and culling areas are adaptively adjusted based on visual sensitivity. We improve rendering performance while satisfying visual perception.",computer graphics; perceptual model; rendering; virtual reality,Keywords,True,
Scopus,conferencePaper,2018,PeriTextAR: utilizing peripheral vision for reading text on augmented reality smart glasses,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) provides real-time information by superimposing virtual information onto users' view of the real world. Our work is the first to explore how peripheral vision, instead of central vision, can be used to read text on AR and smart glasses. We present PeriTextAR, a multiword reading interface using rapid serial visual presentation (RSVP)[5]. This enables users to observe the real world using central vision, while using peripheral vision to read virtual information. We first conducted a lab-based study to determine the effect of different text transformation by comparing reading efficiency among 3 capitalization schemes, 2 font faces, 2 text animation methods, and 3 different numbers of words for RSVP paradigm. Another lab-based study followed, investigating the performance of the PeriTextAR against control text, and the results showed significant better performance.",augmented reality; mobile; multiword; peripheral vision; rapid serial visual presentation; reading interface,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,plARy: sound augmented reality system using video game background music,VRST - Virtual Reality Software and Technology,A,"The authors of this paper explored the possibility of enhancing reality interpretation by synchronizing real-life situation with videogame soundtrack. ""plARy"" is a music based augmented reality application that immerses users in a world of video games with playing soundtracks, enhancing user's interpretation of the real world. By playing known game music according to the locations of individual users, they will recall the scenes and emotions experienced while playing the game based on users' previous learning. The authors of this paper implemented a system that uses Apple iBeacon for proximity detection and evaluated it through experiment. From participants reviews, many people answered that they felt they had imagined a world of the game, and felt that the background music became associated with locations.",augmented reality; location-based; music; video game,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Realistic simulation of progressive vision diseases in virtual reality,VRST - Virtual Reality Software and Technology,A,"People with a visual impairment perceive their surroundings differently than those with healthy vision. It can be difficult to understand how affected perceive their surroundings, even for themselves. We introduce a virtual reality (VR) platform capable of simulating the effects of common visual impairments. With this system we are able to create a realistic VR representation of actual visual fields obtained from a medical perimeter.",glaucoma; medical devices; ophthalmology; perimetry; virtual reality; vision diseases; vision simulation; visual field,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Real-time human motion forecasting using a RGB camera,VRST - Virtual Reality Software and Technology,A,"We propose a real-time human motion forecasting system which visualize the future pose in virtual reality using a RGB camera. Our system consists of three parts: 2D pose estimation from RGB frames using a residual neural network, 2D pose forecasting using a recurrent neural network, and 3D recovery from the predicted 2D pose using a residual linear network. To improve the prediction learning quantity of temporal feature, we propose a special method using lattice optical flow for the joints movement estimation. After fitting the skeleton, a predicted 3d model of target human will be built 0.5s in advance in a 30-fps video.",deep neural network; motion forecasting; real-time pose prediction,Abstract,True,
Scopus,conferencePaper,2018,Resolving occlusion for 3D object manipulation with hands in mixed reality,VRST - Virtual Reality Software and Technology,A,"Due to the need to interact with virtual objects, the hand-object interaction has become an important element in mixed reality (MR) applications. In this paper, we propose a novel approach to handle the occlusion of augmented 3D object manipulation with hands by exploiting the nature of hand poses combined with tracking-based and model-based methods, to achieve a complete mixed reality experience without necessities of heavy computations, complex manual segmentation processes or wearing special gloves. The experimental results show a frame rate faster than real-time and a great accuracy of rendered virtual appearances, and a user study verifies a more immersive experience compared to past approaches. We believe that the proposed method can improve a wide range of mixed reality applications that involve hand-object interactions.",hand tracking; mixed reality; occlusion,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,System of delivering virtual object to user in remote place by handing gestures,VRST - Virtual Reality Software and Technology,A,"In order to communicate with a person in a remote place, there are many means such as sending sentences, making a phone call, chatting by video. A contact system with a distant person becomes a communication tool through an avatar by a virtual reality system, and we feel that there is a barrier to reality. So, we build a system to deliver virtual objects to a user in remote place by behaving as if handing the objects. Remote and present space views are projected on a wall using video chat, and each virtual object is handed over by using an Augmented Reality (AR) marker. The system promotes communication by feeling the connection of the space in a remote place.",augmented reality; gesture; video chat,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Texture synthesis for stable planar tracking,VRST - Virtual Reality Software and Technology,A,"We propose a texture synthesis method to enhance the trackability of a target planar object by embedding natural features into the object in the object design process. To transform an input object into an easy-to-track object in the design process, we extend an inpainting method for naturally embedding the features into the texture. First, a feature-less region in an input object is extracted based on feature distribution based segmentation. Then, the region is filled by using an inpainting method with a feature-rich region searched in an object database. By using context based region search, the inpainted region can be consistent in terms of the object context while improving the feature distribution.",augmented reality; keypoint matching; planar tracking,Keywords,True,
Scopus,conferencePaper,2018,The impact of camera height in cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"Watching a 360° movie with Head Mounted Displays (HMDs) the viewer feels to be inside the movie and can experience it in an immersive way. The head of the viewer is exactly in the same place as the camera was when the scene was recorded. Viewing a movie by HMDs from the perspective of the camera can raise some challenges, e.g. heights of well-known objects can irritate the viewer in the case the camera height does not correspond to the physical eye height. The aim of this work is to study how the position of the camera influences presence, sickness and the user experience of the viewer. For that we considered several watching postures as well as various camera heights. The results of our experiments suggest that differences between camera and eye heights are more accepted, if the camera position is lower than the viewer's own eye height. Additionally, sitting postures are preferred and can be adapted easier than standing postures. These results can be applied to improve guidelines for 360° filmmakers.",360° movie; camera height; cinematic virtual reality; eye height,Title_Keywords,True,
Scopus,conferencePaper,2018,Towards unobtrusive obstacle detection and notification for VR,VRST - Virtual Reality Software and Technology,A,We present results of a preliminary study on our planned system for the detection of obstacles in the physical environment by means of an RGB-D sensor and their unobtrusive signalling using metaphors within the virtual environment (VE).,3D interaction; collision avoidance; interaction metaphor; notifications; range imaging; RGB-D; virtual reality; walking workspace,Keywords,True,
Scopus,conferencePaper,2018,User-centric classification of virtual reality locomotion,VRST - Virtual Reality Software and Technology,A,"Traveling in a virtual world, while confined in the real world requires a virtual reality locomotion (VRL) method. VRL remains an issue because of three fundamental challenges, sickness, presence, and fatigue. We propose a User-Centric Classification (UCC) of VRL methods based on a method's ability to address these challenges. UCC provides a framework to discuss and compare different VRL methods and to examine performance trade-offs. We designed and implemented a testbed to study several VRL methods, and initial results demonstrated the effectiveness of the UCC framework [1].",fatigue; locomotion; presence; sickness; virtual reality; VR,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,"""Virtual ability simulation"" to boost rehabilitation exercise performance and confidence for people with disability",VRST - Virtual Reality Software and Technology,A,"The purpose of this paper is to investigate a concept called virtual ability simulation (VAS) for people with disability in a virtual reality (VR) environment. In a VAS people with disabilities perform tasks that are made easier in the virtual environment (VE) compared to the real world. We hypothesized that putting people with disabilities in a VAS will increase confidence and enable more efficient task completion than without a VAS. To investigate this hypothesis, we conducted a within-subjects experiment in which participants performed a virtual task called ""kick the ball"" in two different conditions: a no gain condition (i.e., same difficulty as in the real world) and a rotational gain condition (i.e., physically easier than the real world but visually the same). The results from our study suggest that VAS increased participants' confidence which in turn enables them to perceive the difficulty of the same task easier.",ability simulation; cybersickness; head-mounted display; HMD; immersion; presence; virtual ability simulation; virtual reality; VR,Abstract_Keywords,True,
Scopus,conferencePaper,2018,Virtual gaze: exploring use of gaze as rich interaction method with virtual agent in interactive virtual reality content,VRST - Virtual Reality Software and Technology,A,"Nonverbal cues, especially eye gaze, plays an important role in our daily communication, not just as an indicator of interest, but also as a method to convey information to another party. In this work, we propose a simulation of human eye gaze in Virtual Reality content to improve immersion of interaction between user and virtual agent. We developed an eye-tracking integrated interactive narrative content with a focus on player's interaction with gaze aware virtual agent, which is capable of reacting towards the player's gaze to simulate real human-to-human communication in VR environment and conducted an initial study to measure user's reaction.",eye tracking; game; human computer interaction; virtual agent; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Virtual reality interactivity in a museum environment,VRST - Virtual Reality Software and Technology,A,"We present research based off of the needs of museum interaction between users and artwork. Through 360 degree footage, we were able to explore the possibilities of an augmented and virtual environment. Adding user interaction features to enhance the surroundings helped us to achieve the type of immersion that a museum could elicit. Museums are struggling to connect with the younger generation these days, therefore, incorporating virtual reality technology not only adds an exciting element for regular visitors, but entices new visitors as well. Our goal was to find ways to use virtual reality to do exactly this, enhancing and expanding the impact of these provoking spaces.",360 video; art museum; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,Visualization of neural networks in virtual reality using Unreal Engine,VRST - Virtual Reality Software and Technology,A,Many applications today use deep learning to provide intelligent behavior. To understand and explain how deep learning models come to certain decisions can be hard or completely in-transparent. We propose a visualization of convolutional neural networks in Virtual Reality (VR). The interactive application shows the internal processes and allows to inspect the results. Large networks can be visualized in real-time with special rendering techniques.,deep learning; explainable ai; neural networks; visualization,Title_Abstract,True,
Scopus,conferencePaper,2018,Visualization of software components and dependency graphs in virtual reality,VRST - Virtual Reality Software and Technology,A,We present the visualization of component-based software architectures in Virtual Reality (VR) to understand complex software systems. We describe how to get all relevant data for the visualization by data mining on the whole source tree and on source code level. The data is stored in a graph database for further analysis and visualization. The software visualization uses an island metaphor. Storing the data in a graph database allows to easily query for different aspects of the software architecture.,3D visualization; graph database; OSGi; real-world metaphor; software architecture; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2018,VR sickness measurement with EEG using DNN algorithm,VRST - Virtual Reality Software and Technology,A,"Recently, VR technology is rapidly developing and attracting public attention. However, VR Sickness is a problem that is still not solved in the VR experience. The VR sickness is presumed to be caused by crosstalk between sensory and cognitive systems [1]. However, since there is no objective way to measure sensory and cognitive systems, it is difficult to measure VR sickness. In this paper, we collect EEG data while participants experience VR videos. We propose a Deep Neural Network (DNN) deep learning algorithm by measuring VR sickness through electroencephalogram (EEG) data. Experiments have been conducted to search for an appropriate EEG data preprocessing method and DNN structure suitable for the deep learning, and the accuracy of 99.12% is obtained in our study.",deep learning; EEG; virtual reality; VR sickness,Keywords,True,
Scopus,conferencePaper,2019,Out-of-body Locomotion: Vectionless Navigation with a Continuous Avatar Representation,VRST - Virtual Reality Software and Technology,A,"Teleportation is a popular and low risk means of navigating in VR. Because teleportation discontinuously translates the user’s viewpoint, no optical flow is generated that could lead to vection-induced VR sickness. However, instant viewpoint translations and resulting discontinuous avatar representation is not only detrimental to presence and spatial awareness but also presents a challenge for gameplay design–particularly for multiplayer games. We compare out-of-body locomotion, a hybrid viewpoint technique that lets users seamlessly switch between a first-person and third-person avatar view, to traditional pointer-based teleportation. While in third-person, if the user doesn’t move, the camera remains stationary to avoid any optical flow generation. Third-person also lets users precisely and continuously navigate their avatar without risk of getting VR sick. The viewpoint automatically switches back to first-person as soon the users breaks line of sight with their avatar or the user requests to rejoin the avatar with a button press. A user study compares out-of-body locomotion to teleportation with participants (n=22) traversing an obstacle course. Results show that out-of-body locomotion requires significantly fewer (67%) viewpoint transitions than teleportation while there was no significant difference in performance. In addition to being able to offer a continuous avatar representation, participants also deemed out-of-body locomotion to be faster.",Locomotion; Teleportation.; Virtual Reality; VR sickness,Keywords,True,
Scopus,conferencePaper,2019,Obstacle Detection and Alert System for Smartphone AR Users,VRST - Virtual Reality Software and Technology,A,"This paper presents an obstacle detection and alert system for the pedestrians who use smartphone AR applications. The system analyzes the input camera image to extract feature points and determines whether the feature points come from obstacles ahead in the path. With the obstacle detector, two experiments were made. The first investigated the obstacle alert interfaces, and the second investigated the orientation guide interfaces that instruct users to hold their smartphones with some angles/orientations appropriate to capture the environment. Then, the best interfaces identified from the experiments were integrated and tested to examine their usability and user experiences.",alert interface; augmented reality; pedestrian safety,Keywords,True,
Scopus,conferencePaper,2019,Sensitivity to Rate of Change in Gains Applied by Redirected Walking,VRST - Virtual Reality Software and Technology,A,"Redirected walking allows for natural locomotion in virtual environments that are larger than a user’s physical environment. The mapping between real and virtual motion is modified by scaling some aspect of motion. As a user traverses the virtual environment these modifications (or gains) must be dynamically adjusted to prevent collision with physical obstacles. A significant body of work has established perceptual thresholds on rates of absolute gain, but the effect of changing gain is little understood. We present the results of a user study on the effects of rate of gain change. A psychophysical experiment was conducted with 21 participants. Each participant completed a series of two-alternative forced choice tasks in which they determined whether their virtual motion differed from their physical motion while experiencing one of three different methods of gain change: sudden gain change, slow gain change and constant gain. Gain thresholds were determined by 3 interleaved 2-up 1-down staircases, one per condition. Our results indicate that slow gain change is significantly harder to detect than sudden gain change.",head-mounted display; redirected walking; virtual reality,Keywords,True,
Scopus,conferencePaper,2019,Lower body control of a semi-autonomous avatar in Virtual Reality: Balance and Locomotion of a 3D Bipedal Model,VRST - Virtual Reality Software and Technology,A,"Animated virtual humans may rely on full-body tracking system to reproduce user motions. In this paper, we reduce tracking to the upper-body and reconstruct the lower body to follow autonomously its upper counterpart. Doing so reduces the number of sensors required, making the application of virtual humans simpler and cheaper. It also enable deployment in cluttered scenes where the lower body is often hidden. The contribution here is the inversion of the well-known capture problem for bipedal walking. It determines footsteps rather than center-of-mass motions and yet can be solved with an off-the-shelf capture problem solver. The quality of our method is assessed in real-time tracking experiments on a wide variety of movements.",Humanoid Locomotion; Motion Capture; Virtual Reality,Title_Keywords,True,
Scopus,conferencePaper,2019,Technologies for Social Augmentations in User-Embodied Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Technologies for Virtual, Mixed, and Augmented Reality (VR, MR, and AR) allow to artificially augment social interactions and thus to go beyond what is possible in real life. Motivations for the use of social augmentations are manifold, for example, to synthesize behavior when sensory input is missing, to provide additional affordances in shared environments, or to support inclusion and training of individuals with social communication disorders. We review and categorize augmentation approaches and propose a software architecture based on four data layers. Three components further handle the status analysis, the modification, and the blending of behaviors. We present a prototype (injectX) that supports behavior tracking (body motion, eye gaze, and facial expressions from the lower face), status analysis, decision-making, augmentation, and behavior blending in immersive interactions. Along with a critical reflection, we consider further technical and ethical aspects.",artificial intelligence; augmented social interaction; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Avatar Type Affects Performance of Cognitive Tasks in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Current consumer virtual reality applications typically represent the user by an avatar comprising a simple head/torso and decoupled hands. In the prior work of Steed et al. it was shown that the presence or absence of an avatar could have a significant impact on the cognitive load of the user. We extend that work in two ways. First they only used a full-body avatar with articulated arms, so we add a condition with hands-only representation similar to the majority of current consumer applications. Second we provide a real-world benchmark so as to start to get at the impact of using any immersive system. We validate the prior results: real and full body avatar performance on a memory task is significantly better than no avatar. However the hands only condition is not significantly different than either these two extremes. We discuss why this might be, in particular we discuss the potential for a individual variation in response to the embodiment level.",Avatar; Cognitive Tasks; Embodiment; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Being More Focused and Engaged in Firefighting Training: Applying User-Centered Design to VR System Development,VRST - Virtual Reality Software and Technology,A,"Although virtual reality (VR) programs to provide firefighting training continue to be developed and adopted, our investigation with 15 firefighters indicates that a current VR training system tends to convey behavioral tips and does not sufficiently reflect actual firefighters’ needs and realities in the field. It often provides somewhat simplified fire simulations and actually lowers the effectiveness of the training. In this paper, we employ Human-Computer Interaction (HCI) methods to examine and identify core elements in firefighting scenarios and develop a VR system that incorporates such elements. We evaluate our system with respect to presence and three design components of the VR simulation (i.e., reality, meaning, play) through a user study with 22 participants. Our study results demonstrate greater user experience and perception toward the four elements in firefighting training with our VR system compared to the existing one. We discuss design implications (e.g., move control, degree of freedom, sight hindrance by smoke, unexpected events) of our study that are expected to help implement and provide an effective VR training system for firefighters.",Firefighting training system; Presence; Triadic game design; User study; User-centered design; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Fearing Disengagement from the Real World,VRST - Virtual Reality Software and Technology,A,"With the adoption of mobile head mounted displays (HMDs) amongst non-experts outside of lab settings, it becomes increasingly important to understand what factors influence a holistic mobile virtual reality (MVR) user experience. We present the results of a field study (N=34), in which we used three methods - a drawing task, a storytelling exercise, and the technology acceptance questionnaire (TAM) - to explore factors, beyond technical capability, that influence the user experience of HMDs. Our analysis (1) highlights factors that designers and researchers can adopt to create and evaluate socially acceptable MVR systems for non-expert users outside a lab context, and (2) puts these factors in context with existing research from industry and academia.",Field study; Mobile Virtual Reality; Qualitative study,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Immersive Insights: A Hybrid Analytics System forCollaborative Exploratory Data Analysis,VRST - Virtual Reality Software and Technology,A,"In the past few years, augmented reality (AR) and virtual reality (VR) technologies have experienced terrific improvements in both accessibility and hardware capabilities, encouraging the application of these devices across various domains. While researchers have demonstrated the possible advantages of AR and VR for certain data science tasks, it is still unclear how these technologies would perform in the context of exploratory data analysis (EDA) at large. In particular, we believe it is important to better understand which level of immersion EDA would concretely benefit from, and to quantify the contribution of AR and VR with respect to standard analysis workflows. In this work, we leverage a Dataspace reconfigurable hybrid reality environment to study how data scientists might perform EDA in a co-located, collaborative context. Specifically, we propose the design and implementation of Immersive Insights, a hybrid analytics system combining high-resolution displays, table projections, and augmented reality (AR) visualizations of the data. We conducted a two-part user study with twelve data scientists, in which we evaluated how different levels of data immersion affect the EDA process and compared the performance of Immersive Insights with a state-of-the-art, non-immersive data analysis system.",Augmented Reality; Clustering; Data Visualization; Dataspace; Exploratory Data Analysis; Hybrid Reality; Virtuality Continuum,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Adventures in Hologram Space: Exploring the Design Space of Eye-to-eye Volumetric Telepresence,VRST - Virtual Reality Software and Technology,A,"Modern volumetric projection-based telepresence approaches are capable of providing realistic full-size virtual representations of remote people. Interacting with full-size people may not be desirable due to the spatial constraints of the physical environment, application context, or display technology. However, the miniaturization of remote people is known to create an eye gaze matching problem. Eye-contact is essential to communication as it allows for people to use natural nonverbal cues and improves the sense of “being there”. In this paper we discuss the design space for interacting with volumetric representations of people and present an approach for dynamically manipulating scale, orientation and the position of holograms which guarantees eye-contact. We created a working augmented reality-based prototype and validated it with 14 participants.",Augmented Reality; Eye-to-eye; Holograms; Volumetric Projection,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Exploring the Use of a Robust Depth-sensor-based Avatar Control System and its Effects on Communication Behaviors,VRST - Virtual Reality Software and Technology,A,"To interact as fully-tracked avatars with rich hand gestures in Virtual Reality (VR), we often need to wear a tracking suit or attach extra sensors on our bodies. User experience and performance may be impacted by the cumbersome devices and low fidelity behavior representations, especially in social scenarios where good communication is required. In this paper, we use multiple depth sensors and focus on increasing the behavioral fidelity of a participant’s virtual body representation. To investigate the impact of the depth-sensor-based avatar system (full-body tracking with hand gestures), we compared it against a controller-based avatar system (partial-body tracking with limited hand gestures). We designed a VR interview simulation for a single user to measure the effects on presence, virtual body ownership, workload, usability, and perceived self-performance. Specifically, the interview process was recorded in VR, together with all the verbal and non-verbal cues. Subjects then took a third-person view to evaluate their previous performance. Our results show that the depth-sensor-based avatar control system increased virtual body ownership and also improved the user experience. In addition, users rated their non-verbal behavior performance higher in the full-body depth-sensor-based avatar system.",avatar control; communication behavior; depth sensor; motion capture; tracking; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Investigating the Detection of Bimanual Haptic Retargeting in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Haptic retargeting is a virtual reality (VR) interaction technique enabling virtual objects to be ”remapped” to different haptic proxies by offsetting the user’s virtual hand from their physical hand. While researchers have investigated single-hand retargeting, the effects of bimanual interaction in the context of haptic retargeting have been less explored. In this study, we present an evaluation of perceptual detection rates for bimanual haptic retargeting in VR. We tested 64 combinations of simultaneous left- and right-hand retargeting ranging from − 24° to + 24° offsets and found that bimanual retargeting can be more noticeable to users when the hands are redirected in different directions as opposed to the same direction.",bimanual; Haptic retargeting; virtual reality.; visuo-haptic illusion,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,ElectroCutscenes: Realistic Haptic Feedback in Cutscenes of Virtual Reality Games Using Electric Muscle Stimulation,VRST - Virtual Reality Software and Technology,A,"Cutscenes in Virtual Reality (VR) games enhance story telling by delivering output in the form of visual, auditory, or haptic feedback (e.g., using vibrating handheld controllers). Since they lack interaction in the form of user input, cutscenes would significantly benefit from improved feedback. We introduce the concept and implementation of ElectroCutscenes, where Electric Muscle Stimulation (EMS) is leveraged to elicit physical user movements to different body parts to correspond to those of personal avatars in cutscenes of VR games while the user stays passive. Through a user study (N=22) in which users passively received kinesthetic feedback resulting in involuntarily movements, we show that ElectroCutscenes significantly increases perceived presence and realism compared to controller-based vibrotactile and no haptic feedback. Furthermore, we found preliminary evidence that combining visual and EMS feedback can evoke movements that are not actuated by either of them alone. We discuss how to enhance realism and presence of cutscenes in VR games even when EMS can partially rather than completely actuate the desired body movements.",EMS; Haptic Feedback; Haptics; Head-mounted Displays,Title_Abstract,True,
Scopus,conferencePaper,2019,Virtual Reality Forge: Pattern-Oriented Authoring of Virtual Reality Nuggets,VRST - Virtual Reality Software and Technology,A,"A current educational trend is to divide learning content in relatively small and independent learning units, referred to as learning nuggets. These “bite-sized” nuggets often rely on patterns in order to reuse these patterns within highly diverse curricular structures like lessons, presentations or demos. In this paper, we explore how virtual reality (VR) can be utilized as a medium for learning purposes similar to learning nuggets. We present a nugget-inspired VR system design and dovetail the pattern-oriented nugget concept in relatively small VR systems. We call this authoring approach with VR nuggets forging. Furthermore, we propose a VR authoring system for these VR nuggets – the VR forge. The system design for realizing VR nuggets and the authoring system are presented and implemented in Unity. For an example we utilize a set of basic patterns from the educational domain. In an expert user study, we use the resulting bite-sized VR applications to evaluate four critical aspects concerning VR and nugget-like usage and show that the educational experts accepted the VR nuggets. Within an additional study, we indicate that our authoring system which reflects the simplistic pattern-oriented content creation paradigm of learning nuggets has potential for general laymen authoring of VR application.",Laymen Authoring; Microlearning; Pattern-Based Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,POL360: A Universal Mobile VR Motion Controller using Polarized Light,VRST - Virtual Reality Software and Technology,A,"We introduce POL360: the first universal VR motion controller that leverages the principle of light polarization. POL360 enables a user who holds it and wears a VR headset to see their hand motion in a virtual world via its accurate 6-DOF position tracking. Compared to other techniques for VR positioning, POL360 has several advantages as follows. (1) Mobile compatibility: Neither additional computing resource like a PC/console nor any complicated pre-installation is required in the environment. Only necessary device is a VR headset with an IR LED module as a light source to which a thin-film linear polarizer is attached. (2) On-device computing: Our POL360’s computation for positioning is completed on the microprocessor in the device. Thus, it does not require additional computing resource of a VR headset. (3) Competitive accuracy and update rate: In spite of POL360’s superior mobile compatibility and affordability, POL360 attains competitive performance of accuracy and fast update rates. That is, it achieves the subcentimeter accuracy of positioning and the tracking rate higher than 60 Hz. In this paper, we derive the mathematical formulation of 6-DOF positioning using light polarization for the first time and implement a POL360 prototype that can directly operate with any commercial VR headset systems. In order to demonstrate POL360’s performance and usability, we carry out thorough quantitative evaluation and a user study and develop three game demos as use cases.",light polarization; spatial interaction; Virtual reality,Keywords,True,
Scopus,conferencePaper,2019,HawKEY: Efficient and Versatile Text Entry for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Text entry is still a challenging task in modern Virtual Reality (VR) systems. The lack of efficient text entry methods limits the applications that can be used productively in VR. Previous work has addressed this issue through virtual keyboards or showing the physical keyboard in VR. While physical keyboards afford faster text entry, they usually require a seated user and an instrumented environment. We introduce a new keyboard, worn on a hawker’s tray in front of the user, which affords a compact, simple, flexible, and efficient text entry solution for VR, without restricting physical movement. In our new video condition, we also show the keyboard only when the user is looking down at it. To evaluate our novel solution and to identify good keyboard visualizations, we ran a user study where we asked participants to enter both lowercase sentences as well as complex text while standing. The results show that text entry rates are affected negatively by simplistic keyboard visualization conditions and that our solution affords desktop text entry rates, even when standing.",Text Entry; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,DexController : Designing a VR Controller with Grasp-Recognition for Enriching Natural Game Experience,VRST - Virtual Reality Software and Technology,A,"We present DexController, which is a hand-held controller leveraging grasp as an additional modality for virtual reality (VR) game. The pressure-sensitive surface of DexController was designed to recognize two different grasp-poses (i.e. precision grip and power grip) and detect grasp-force. Based on the results of two feasibility tests, a VR defense game was designed in which players could attack each enemy using the proper weapon with a proper level of force. A within-subject comparative study is conducted with a button-based controller which has the same physical form of DexController. The results indicated that DexController enhanced the perceived naturalness of the controller and game enjoyment, with having acceptable physical demand. This study clarifies the empirical effect of utilizing grasp-recognition on VR game controller to enhance interactivity. Also, we provide insight for the integration of VR game elements with the grasping modality of a controller.",controller; game experience; gaming; natural interaction; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,A Technique for Mixed Reality Remote Collaboration using 360 Panoramas in 3D Reconstructed Scenes,VRST - Virtual Reality Software and Technology,A,"Mixed Reality (MR) remote collaboration provides an enhanced immersive experience where a remote user can provide verbal and nonverbal assistance to a local user to increase the efficiency and performance of the collaboration. This is usually achieved by sharing the local user's environment through live 360 video or a 3D scene, and using visual cues to gesture or point at real objects allowing for better understanding and collaborative task performance. While most of prior work used one of the methods to capture the surrounding environment, there may be situations where users have to choose between using 360 panoramas or 3D scene reconstruction to collaborate, as each have unique benefits and limitations. In this paper we designed a prototype system that combines 360 panoramas into a 3D scene to introduce a novel way for users to interact and collaborate with each other. We evaluated the prototype through a user study which compared the usability and performance of our proposed approach to live 360 video collaborative system, and we found that participants enjoyed using different ways to access the local user's environment although it took them longer time to learn to use our system. We also collected subjective feedback for future improvements and provide directions for future research.",360 Panorama; 3D Scene Reconstruction; Interaction Methods; Mixed Reality; Remote Collaboration; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Depth Perception in Projective Augmented Reality: An Evaluation of Advanced Visualization Techniques,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) is a promising tool to convey useful information at the place where it is needed. However, perceptual issues with augmented reality visualizations affect the estimation of distances and depth and thus can lead to critically wrong assumptions. These issues have been successfully investigated for video see-through modalities. Moreover, advanced visualization methods encoding depth information by displaying additional depth cues were developed. In this work, state-of-the-art visualization concepts were adopted for a projective AR setup. We conducted a user study to assess the concepts’ suitability to convey depth information. Participants were asked to sort virtual cubes by using the provided depth cues. The investigated visualization concepts consisted of conventional Phong shading, a virtual mirror, depth-encoding silhouettes, pseudo-chromadepth rendering and an illustrative visualization using supporting line depth cues. Besides different concepts, we altered between a monoscopic and a stereoscopic display mode to examine the effects of stereopsis. Consistent results across variables show a clear ranking of examined concepts. The supporting lines approach and the pseudo-chromadepth rendering performed best. Stereopsis was shown to provide significant advantages for depth perception, while the current visualization technique had only little effect on investigated measures in this condition. However, similar results were achieved using the supporting lines and the pseudo-chromadepth concepts in a monoscopic setup. Our study showed the suitability of advanced visualization concepts for the rendering of virtual content in projective AR. Specific depth estimation results contribute to the future design and development of applications for these systems.",Depth Perception; Distance Estimation; Projective Augmented Reality; Visualization,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Portable Mid-air Imaging Optical System on Glossy Surface,VRST - Virtual Reality Software and Technology,A,"We propose a portable optical system, PortOn, that displays an upright mid-air image when simply placed on a flat and glossy surface such as a desk or floor. Mid-air imaging is promising for glasses-free mixed reality because the user can see images without wearing a special device. However, there is a limitation in terms of where the conventional mid-air imaging optical systems can be installed. Therefore, we propose a mid-air optical system that solves this limitation. Our contribution is a practical optical design that enables the system to be easily installed. The advantage of our method is that it erases unnecessary light that is produced when mid-air images are displayed and shows beautiful mid-air images clearly when view-angle control and polarization are added to the system. We evaluate whether undesired light is erased by measuring luminance. As a result, the luminance of the undesired light is much lower than that of mid-air images.",glass-free mixed reality; Muller matrix; polarizer,Abstract_Keywords,True,
Scopus,conferencePaper,2019,The Stroop Room: A Virtual Reality-Enhanced Stroop Test,VRST - Virtual Reality Software and Technology,A,"The Stroop Test is a well known and regularly employed stressor in laboratory research. In contrast to other methods, it is not based on fear of physical harm or social shame. Consequently, it is more likely accepted by a wide population. In our always-on, technology-driven, social-media centered world, large-scale in-field stress research will need adequate experimental tools to explore the increasing prevalence of stress-related diseases without bringing subjects into laboratories. This is why we designed the Stroop Room: A virtual reality-based adaptation of the Stroop Test using elements of the virtual world to extend the demands of the original test and at the same time make it easily accessible. It is open source and can be used and improved by anyone as an in-the-wild, repeatable, laboratory-quality stressor. In this work, the method is presented and an evaluation study described, to demonstrate its effectiveness in provoking cognitive stress. 16 male and 16 female subjects were tested in the Stroop Room while recording the electrocardiogram, electrodermal activity, saliva based cortisol and alpha-amylase, performance metrics and an array of questionnaire-based assessments regarding psychological confounders, stress state and likability of the simulation. Our results show that the Stroop Room increases heart rate on average by 19%, other heart rate variability time-domain parameters (RMSSD, pNN50) decrease by 24%-47%, and its most stress-correlated frequency-parameter (LF/HF) increases by 107%. Skin conductance (SC) level increases by 63% and non-specific SC responses by 135% on average. Salivary cortisol and alpha-amylase concentrations increase significantly in some specific conditions. Compared to related work using the Stroop Test, this is an improvement for some metrics by around 30%-40%. Questionnaire evaluation show a strong engagement of users with the simulation and some aspects of a flow-induction. These findings support the effectiveness of a Stroop Test involving 3-dimensional interactivity and thus the Stroop Room demonstrates how this can be applied in a playful interaction that could be used pervasively.",amylase; cortisol; EDA; heart rate; HRV; psychological stress; stroop test; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Underwater Virtual Reality System for Neutral Buoyancy Training: Development and Evaluation,VRST - Virtual Reality Software and Technology,A,"During terrestrial activities, sensation of pressure on the skin and tension in muscles and joints provides information about how the body is oriented relative to gravity and how the body is moving relative to the surrounding environment. In contrast, in aquatic environments when suspended in a state of neutral buoyancy, the weight of the body and limbs is offloaded, rendering these cues uninformative. It is not yet known how this altered sensory environment impacts virtual reality experiences. To investigate this question, we converted a full-face SCUBA mask into an underwater head-mounted display and developed software to simulate jetpack locomotion outside the International Space Station. Our goal was to emulate conditions experienced by astronauts during training at NASA's Neutral Buoyancy Lab. A user study was conducted to evaluate both sickness and presence when using virtual reality in this altered sensory environment. We observed an increase in nausea related symptoms underwater, but we cannot conclude that this is due to VR use. Other measures of sickness and presence underwater were comparable to measures taken above water. We conclude with suggestions for improved underwater VR systems and improved methods for evaluation of these systems based on our experience.",Head-mounted Display; Presence; Sickness; Simulation; Space; Spacewalk; Training; Underwater; Virtual Reality; Waterproof,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Hitting the Wall: Mid-Air Interaction for Eye-Hand Coordination,VRST - Virtual Reality Software and Technology,A,"Reaction time training systems are used to improve user performance. Until now, such setups use physical 2D flat surfaces, e.g., a 2D touch screen or buttons mounted on a wall. We designed and investigated a mid-air reaction time training system with an immersive virtual reality (VR) headset. 12 participants performed an eye-hand coordination reaction test in three conditions: both in mid-air with or without VR controller as well as with passive haptic feedback through hitting a soft-surface wall. We also altered target and cursor sizes and used a Fitts’ law task to analyze user performance. According to the results, subjects were slower and their throughput was lower when they hit a solid surface to interact with virtual targets. Our results show that Fitts’s model can be applied to these systems to measure and assess participant training.",Fitts’ task; mid-air interaction; performance assessment; reaction test; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Juggling in VR: Advantages of Immersive Virtual Reality in Juggling Learning,VRST - Virtual Reality Software and Technology,A,"In this paper, we follow up on research dealing with motion learning in Virtual Reality (VR). We investigate the impact of VR motion learning on motion performance, motivation for motion learning and willingness to continue with the motion learning. In our research, we used three ball juggling as a subject of learning. We performed a user study with 30 participants. A VR application was used in our study which allows setting up lower gravity and thus slowing down the motion for learning purposes. The results were statistically evaluated and we comment on the positive influence of virtual reality on motivation and possibilities of using VR in the motion learning process.",juggling; motion learning; motor learning; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Investigating a Physical Dial as a Measurement Tool for Cybersickness in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This study explores ways to increase comfort in Virtual Reality by minimizing cybersickness. Cybersickness is related to classical motion sickness and causes unwanted symptoms when using immersive technologies. We developed a dial interface to accurately capture momentary user cybersickness and feed this information back to the user. Using a seated VR roller coaster environment, we found that the dial is significantly positively correlated with post-immersion questionnaires and is a valid tool compared to verbal rating approaches.",cybersickness; human-computer interaction; physical dial; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,"In AI We Trust: Investigating the Relationship between Biosignals, Trust and Cognitive Load in VR",VRST - Virtual Reality Software and Technology,A,"Human trust is a psycho-physiological state that is difficult to measure, yet is becoming increasingly important for the design of human-computer interactions. This paper explores if human trust can be measured using physiological measures when interacting with a computer interface, and how it correlates with cognitive load. In this work, we present a pilot study in Virtual Reality (VR) that uses a multi-sensory approach of Electroencephalography (EEG), galvanic skin response (GSR), and Heart Rate Variability (HRV) to measure trust with a virtual agent and explore the correlation between trust and cognitive load. The goal of this study is twofold; 1) to determine the relationship between biosignals, or physiological signals with trust and cognitive load, and 2) to introduce a pilot study in VR based on cognitive load level to evaluate trust. Even though we could not report any significant main effect or interaction of cognitive load and trust from the physiological signal, we found that in low cognitive load tasks, EEG alpha band power reflects trustworthiness on the agent. Moreover, cognitive load of the user decreases when the agent is accurate regardless of task’s cognitive load. This could be possible because of small sample size, tasks not stressful enough to induce high cognitive load due to lab study and comfortable environment or timestamp synchronisation error due to fusing data from various physiological sensors with different sample rate.",Cognitive Load; Physiological signals; Trust; Virtual Assistant; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Is the Pen Mightier than the Controller? A Comparison of Input Devices for Selection in Virtual and Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Controllers are currently the typical input device for commercial Virtual Reality (VR) systems. Yet, such controllers are not as efficient as other devices, including the mouse. This motivates us to investigate devices that substantially exceed the controller’s performance, for both VR and Augmented Reality (AR) systems. We performed a user study to compare several input devices, including a mouse, controller, and a 3D pen-like device on a VR and AR pointing task. Our results show that the 3D pen significantly outperforms modern VR controllers in all evaluated measures and that it is comparable to the mouse. Participants also liked the 3D pen more than the controller. Finally, we show how 3D pen devices could be integrated into today’s VR and AR systems.",3D pointing; input devices; Virtual and Augmented Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,"Augmented Reality for Children in a Confirmation Task: Time, Fatigue, and Usability",VRST - Virtual Reality Software and Technology,A,"The objective of this paper is to explore three different interaction methods in a confirmation task on a head-mounted Augmented Reality (AR) device with a population of children aged 9-11 years. The three interaction methods we look at are voice recognition, gesture recognition, and controller. We conducted a within-subjects study using a Fitts’ Law confirmation task performed by children with a Microsoft HoloLens. We measured elapsed time during the completion of the tasks. Also, we collected usability and fatigue measures using the System Usability Scale and the OMNI RPE (Ratings of Perceived Exertion) scale. We found significant differences between voice and controller for time, fatigue and usability. We also found significant differences between gesture and controller for time, fatigue and usability. We hope to apply the results of this study to improve augmented reality educational tools for children in the future.",Augmented Reality; Children; Fitts’ Law; Usability Studies,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Smart3DGuides: Making Unconstrained Immersive 3D Drawing More Accurate,VRST - Virtual Reality Software and Technology,A,"Most current commercial Virtual Reality (VR) drawing applications for creativity rely on freehand 3D drawing as their main interaction paradigm. However, the presence of the additional third dimension makes accurate freehand drawing challenging. Some systems address this problem by constraining or beautifying user strokes, which can be intrusive and can limit the expressivity of freehand drawing. In this paper, we evaluate the effectiveness of relying solely on visual guidance to increase overall drawing shape-likeness. We identified a set of common mistakes that users make while creating freehand strokes in VR and then designed a set of visual guides, the Smart3DGuides, which help users avoid these mistakes. We evaluated Smart3DGuides in two user studies, and our results show that non-constraining visual guides help users draw more accurately.",3D User Interfaces; Drawing; Virtual Reality Drawing,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Extended Sliding in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Although precise 3D positioning is not always necessary in virtual environments, it is still an important task for current and future applications of Virtual Reality (VR), including 3D modelling, engineering, and scientific applications. We focus on 3D positioning techniques in immersive environments that use a 6DOF controller as input device and present a new technique that improves 3D positioning performance in VR, in both speed and accuracy. Towards this goal, we adapted an extended sliding technique to VR systems with a controller as input device and compared it with previously presented 3DOF positioning techniques. The results showed that our new Extended VR Sliding technique significantly improved the accuracy for 3D positioning tasks, especially for targets in contact with the scene.",3D positioning; object sliding,Title_Abstract,True,
Scopus,conferencePaper,2019,SlingDrone: Mixed Reality System for Pointing and Interaction Using a Single Drone,VRST - Virtual Reality Software and Technology,A,"We propose SlingDrone, a novel Mixed Reality interaction paradigm that utilizes a micro-quadrotor as both pointing controller and interactive robot with a slingshot motion type. The drone attempts to hover at a given position while the human pulls it in desired direction using a hand grip and a leash. Based on the displacement, a virtual trajectory is defined. To allow for intuitive and simple control, we use virtual reality (VR) technology to trace the path of the drone based on the displacement input. The user receives force feedback propagated through the leash. Force feedback from SlingDrone coupled with visualized trajectory in VR creates an intuitive and user friendly pointing device. When the drone is released, it follows the trajectory that was shown in VR. Onboard payload (e.g. magnetic gripper) can perform various scenarios for real interaction with the surroundings, e.g. manipulation or sensing. Unlike HTC Vive controller, SlingDrone does not require handheld devices, thus it can be used as a standalone pointing technology in VR.",haptics; human-robot interaction; mixed reality; quadrotor,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,2D/3D Mixed Interface for Furniture Placement in Smartphone-based Mobile Augmented Reality,VRST - Virtual Reality Software and Technology,A,"In this work, we propose to use an approximate 2D map of the environment generated from the latest environment modeling technology and enhance the object manipulation performance for the touch based mobile augmented reality. We validated the advantage of the proposed interface through a pilot experiment and confirmed that the use of the 2D map helps reduce the task completion time almost 2 times and improve the usability as well.",3D interaction technique; Augmented reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,A Comparative Study of Planar Surface and Spherical Surface for 3D Pointing Using Direct Touch,VRST - Virtual Reality Software and Technology,A,"We investigated the performance of 3D pointing using direct touch in a planar surface condition (PC) and a spherical surface condition (SC). In addition, we examined the performance in terms of Fitts’ law. Although the results showed that the performance in SC was slightly worse than PC, SC was higher conformed to Fitts’ law than PC without the conditions involving head rotation (PC’s and SC’s R2 is 0.945 and 0.971, respectively).",Curved surface; Fitts’ law; selection performance; virtual reality,Keywords,True,
Scopus,conferencePaper,2019,A Content-Aware Approach for Analysing Eye Movement Patterns in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Observing eye movement is a direct way to analyse human’s attention. Eye movement patterns in normal environment have been widely investigated. In virtual reality (VR) environment, previous studies of eye movement patterns are mainly based on content-unrelated influential factors. Considering this issue, in this paper, a novel content-related factor is studied. One crucial kind of region of interest (ROI), namely vision-penetrable entrance, is chosen to analyse eye movement pattern differences. The results suggest that users show more interest in vision-penetrable entrances than in other regions. Furthermore, this kind of difference is identified as higher average density of fixation. As far as we know, this paper is the first attempt to study specific types of ROI in virtual reality environments. The method utilised in this paper can be applied in other ROI analysis.",Omnidirectional panoramas; Virtual reality; Visual saliency,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,A Framework for Virtual Reality Training to Improve Public Speaking,VRST - Virtual Reality Software and Technology,A,"This paper presents the logic and construction of a prototype virtual reality (VR) tool for public speech training. It reflects upon previous endeavours in this area, using them to make informed design decisions. A dictation recognizer is implemented to perform speech to text conversions. With this training simulator, users are be able to step into a virtual environment resembling a podium in an auditorium, with their speech appearing on a virtual cue card. Also, users are presented with a performance metric at the end of their speech to grade their overall performance. We suggest that the VR immersive prototype using speech-to-text recognition has a potential to be engaging and to serve as a tool for public speaking training.",public speaking; usability test; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,A Mobile Augmented Reality Interface for Teaching Folk Dances,VRST - Virtual Reality Software and Technology,A,"This paper presents a prototype mobile augmented reality interface for assisting the process of learning folk dances. As a case study, a folk dance was digitized based on recordings from professional dancers. To assess the effectiveness of the technology, it was comparatively evaluated with a large back-projection system in laboratory conditions. Sixteen participants took part in the study, and their movements were captured using motion capture system and then compared with the recordings from the professional dancers. Experimental results indicate that augmented reality has the potential to be used for learning folk dances.",augmented reality; motion capturing; motion tracking; user studies,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,A Scalability Benchmark for a Virtual Audience Perception Model in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we describe the implementation and performance of a Virtual Audience perception model for Virtual Reality (VR). The model is a VR adaptation of an existing desktop model. The system allows a user in VR to easily build and experience a wide variety of atmospheres with small or large groups of virtual agents.The paper describes results of early evaluations for this model in VR. Our first scalability benchmark results demonstrated the ability to simultaneously handle one hundred virtual agents without significantly affecting there commended frame rate for VR applications.This research is conducted in the context of a classroom simulation software for teachers’ training.",Education; Perception model; Virtual Agent; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,A Virtual Reality Simulator for Training Gaze Control of Wheeled Tele-Robots,VRST - Virtual Reality Software and Technology,A,"People who cannot use their hands may use eye-gaze to interact with robots. Emerging virtual reality head-mounted displays (HMD) have built-in eye-tracking sensors. Previous studies suggest that users need substantial practice for gaze steering of wheeled robots with an HMD. In this paper, we propose to apply a VR-based simulator for training of gaze-controlled robot steering. The simulator and preliminary test results are presented.",eye tracking; gaze interaction; head-mounted display; human-robot interaction; simulator; tele-robots; training; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,A Web-based Augmented Reality Plat-form using Pictorial QR Code for Educational Purposes and Beyond,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) provides the capability to overlay virtual 3D information onto a 2D printed flat surface; for example, displaying a 3D model on a single flat card that accompanies with the diagram shown in a learning text-book. The student can zoom in and out, rotate, and perceive the animation of the figure in real-time. This will make the educational theory more attractive; hence, motivates students to learn. AR is a great tool; however, the setup and display are not straight-forward (there are many different AR markers with different encryption, decryption methods, and displaying flat-forms). In this paper, we proposed a portable browser-based platform which uses the advantages of AR along with scan-able QR Code on mobile phones to enhance instant 3D visualisation. The user only needs a smart-phone (Apple iPhone or Android) with Internet-enabled; no specific Apps are needed to install. The user scans the QR Code embedded in a colour image, the code will link to a public website, and the website will produce AR Experience right on top of the browser. As a result, it provides a stress-free, low-cost, portable, and promising solution for not only educational purposes but also many other fields such as gaming, property selling, e-commerce, reporting. The set up is convenient: the user uploads a picture (e.g. a racing car), and what actions to be related to it (a 3D model to display, or a movie to play). The system will add on the picture one small colour QR code (to redirect to an online URL) and a thin black border. The user also uploads the 3D model (GLTF files) that he wants to display on top of the card to finish the set-up. At the display, the user can print the AR card, point their smart-phone towards the card, and pre-setup AR models or actions will appear on it. To students, these 3D graphics or animations will allow them to learn and understand the lessons in a much more intuitive way.",Augmented Reality; Education; QR Code,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,An Evaluation of Head-Mounted Virtual Reality for Special Education from the Teachers’ Perspective,VRST - Virtual Reality Software and Technology,A,"In this research, we explore the use of head-mounted virtual reality for special education from the teachers’ perspective. We asked a group of special educators to assess the use of VR headset while students with mental disabilities played a VR game. The teachers concluded that head-mounted VR can be used for teaching students to follow instruction and training for work.",head-mounted display; mental disabilities; special education; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Action Units: Directing User Attention in 360-degree Video based VR,VRST - Virtual Reality Software and Technology,A,"A key challenge to effective storytelling using Virtual Reality (VR), such as with 360-degree videos, is how to direct user attention to important content without taking away user agency for free exploration. In this paper, we introduce the notion of an Action Unit system, composed of social cues such as head and arm movements, as a way of directing users to focus on content important for the given narrative. We applied this idea to a 360-degree VR tour, and evaluated its effects on memory, engagement, enjoyment, and cyber-sickness. The results indicate that the levels of engagement and enjoyment increased when these Action Units were applied. Users also preferred the Action Units for their diegetic aspects.",360-degree video; social cues; storytelling; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,AHMED: Toolset for Ad-Hoc Mixed-reality Exhibition Design,VRST - Virtual Reality Software and Technology,A,"We present “AHMED”, a mixed-reality toolset that allows visitors to experience mixed-reality museum or art exhibitions created ad-hoc at locations such as event venues, private parties,or a living room. The system democratizes access to exhibitions for populations that cannot visit these exhibitions in person for reasons of disability, time-constraints, travel restrictions, or socio-economic status.",augmented reality; heritage; mixed reality; museum; photogrammetry; volumetric capture,Keywords,True,
Scopus,conferencePaper,2019,Analysis of VR Sickness and Gait Parameters During Non-Isometric Virtual Walking with Large Translational Gain,VRST - Virtual Reality Software and Technology,A,,Cybersickness; Locomotion; Navigation; Redirected Walking; Virtual Reality; Walking,Keywords,True,
Scopus,conferencePaper,2019,Augmented Reality Approach For Position-based Service using Handheld Smartphone,VRST - Virtual Reality Software and Technology,A,"In this work, we present an augmented reality (AR) approach for position based service using a smartphone in an indoor environment. The AR method, combined with position estimation, provides a user with a smartphone with a service that is specific to a particular position without using a marker or any other hardware device. The position in an indoor environment is estimated using an IMU sensor only in the smartphone. The accuracy of the position and heading direction of the user is improved by integrating the values from the accelerometer and the gyro using Principal Component Analysis(PCA) and Extended Kalman Filter(EKF). Then, a drift noise of the estimated position is reduced by a registration step performed at a specific position. The estimated position is given to the position based service, which is provided to the user on the smartphone screen through AR. The concept of the proposed method is demonstrated with some examples.",Augmented reality; indoor position estimation; position-based service,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Augmented Reality Visualisation Concepts to Support Intraoperative Distance Estimation,VRST - Virtual Reality Software and Technology,A,"The estimation of distances and spatial relations between surgical instruments and surrounding anatomical structures is a challenging task for clinicians in image-guided surgery. Using augmented reality (AR), navigation aids can be displayed directly at the intervention site to support the assessment of distances and reduce the risk of damage to healthy tissue. To this end, four distance-encoding visualisation concepts were developed using a head-mounted optical see-through AR setup and evaluated by conducting a comparison study. Results suggest the general advantage of the proposed methods compared to a blank visualisation providing no additional information. Using a Distance Sensor concept signalising the proximity of nearby structures resulted in the least time the instrument was located below 5mm to surrounding risk structures and yielded the least amount of collisions with them.",Distance Estimation; Medical Augmented Reality; Visualisation,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Can We Predict Susceptibility to Cybersickness?,VRST - Virtual Reality Software and Technology,A,This study investigated whether individual differences in postural stability/activity can be used to predict who will become sick when exposed to head-mounted display (HMD) based virtual reality (VR). We found that participants who reported feeling sick after at least one exposure to VR displayed different postural activity than those who remained well. Importantly these differences were present in their sway data before they even donned the HMD. These results are inline with the postural instability theory of motion sickness and suggest that we can identify individuals who are more susceptible HMD-based cybersickness based on their spontaneous postural sway.,Cybersickness; Head-Mounted Display; Motion sickness; Spontaneous Postural Sway; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Cinévoqué: Development of a Passively Responsive Framework for Seamless Evolution of Experiences in Immersive Live-Action Movies,VRST - Virtual Reality Software and Technology,A,"Cinematic Virtual Reality’s (CVR) inherent feature of allowing the user to choose their Point of View (POV) within a 360° space brings forth new challenges to storytelling. The approaches used in traditional films do not translate directly to this medium, as it is uncertain if the user would follow all the Points of Interest (POIs) consistently. Our framework, Cinévoqué, aims to address this issue by using the real-time data generated during a VR film to passively alter the narrative and parts of the experience to suit the user’s viewing behavior. In this poster, we discuss the technical approaches used to implement this framework and create responsive live-action CVR.",Presence; Responsive Narrative; Storytelling; Virtual Reality; VR Cinema,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Comfortable Locomotion in VR: Teleportation is Not a Complete Solution,VRST - Virtual Reality Software and Technology,A,"We compared two common techniques of controller-based locomotion (teleportation and steering locomotion) in virtual reality (VR) in terms of the cybersickness they produce. Participants had to continuously navigate a commercial VR application for 16 minutes using each technique, while standing and seated. While teleportation produced less cybersickness than steering locomotion on average, a number of participants reported teleportation to be more sickening. These ‘telesick’ participants were found to have greater medio/lateral positional variability in their spontaneous postural sway than ‘steersick’ participants prior to VR exposure. We conclude that different individuals may require unique techniques to comfortably locomote in VR.",Cybersickness; Head-Mounted Display; Locomotion; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Design and Realization of Sustainable Rural Housing Using Immersive Virtual Reality Platform,VRST - Virtual Reality Software and Technology,A,"Rapid urbanization in developing countries has paved way to spontaneous settlements, which are overcrowded. The aim of this work is to assess the impact of Virtual Reality (VR) on different types of sustainable construction techniques that are proposed for rural slum communities. The work mainly focuses on a walkthrough and interactions on a prototype of a sustainable housing unit in a rural slum community built with eco-friendly building materials, natural light source and ventilation.",Controller based interaction; Prototype; Rural slums; Sustainable design; Virtual reality; Walkthrough,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Design of Portable Thermal Sensor Device for VR,VRST - Virtual Reality Software and Technology,A,"This study proposes a thermal sensibility haptic system that can be used in the VR environment to stimulate multiple sensory receptors. In addition, the object can be distinguished through the touch if it is reproduced by adjusting the intensity of the stimulus based on the intrinsic thermal energy and surface curvature of the object.",Contact Temperature; Haptic Feedback; Thermal Cues; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2019,Drone-Steering: A Novel VR Traveling Technique,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel technique of navigation in Virtual Reality (VR) called Drone-Steering. This technique has been designed to facilitate path learning and traveling in VR by reducing both cybersickness and disorientation. We compared this technique to traditional Hand-Steering in a landmark-free environment. Our first experiment confirmed a significantly lower level of cybersickness during traveling and significantly better path learning. We believe that our technique constitutes a promising alternative to current VR navigation techniques, and will especially interest researchers and developers targeting large VR environments.",Path Learning; Sickness; Travel; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Effects of Age and Motivation for Visiting on AR Museum Experiences,VRST - Virtual Reality Software and Technology,A,"Augmented reality(AR) provides a unique viewing experience at museums where people understand abstract history through physical artifacts. Although AR usage in museum settings has been increasing, it is not well understood how AR viewing experience differs in different groups of visitors, which can be problematic considering that museums are places visited by diverse groups of people. In this study, we evaluate the differences in AR experiences according to the characteristics of the visitors. The results show the effect of AR usage in museum settings with visitors’ different age groups and motivations for visiting.",Augmented reality; Museums; User characteristics,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Effects of Tactile Perception on Emotion and Immersion to Film Viewing in a Virtual Environment,VRST - Virtual Reality Software and Technology,A,"In this research, we compared three tactile conditions (No vs. Soft vs. Spiky) in both positive and negative scenes to explore whether tactile perception could influence emotional responses and immersive experience in a virtual environment (VE). The results showed that, when viewing positive scenes with soft stimuli, participants experienced an increase in both positive emotions and their level of immersion compared to those in the No and Spiky tactile conditions. We also found that participants in the No and Spiky tactile conditions reported no significant differences in either emotion or immersion when viewing positive scenes. During the viewing of negative scenes, spiky stimuli did not intensify negative feelings, while soft stimuli decreased negative emotions. In terms of immersion, there was no meaningful difference between the three tactile conditions for negative scenes. Overall, this study has demonstrated the important association between tactile perception, emotion, and immersion in a VE.",Emotion; Immersion; Tactile Perception; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2019,Enhancement of Pointing Towards Non-Haptic Augmented Reality Interfaces by Increasing the Arm Position Sense,VRST - Virtual Reality Software and Technology,A,"Interactive user interfaces in head-mounted Augmented Reality environments are not always projected onto a physical surface. However, operating such free-floating interfaces by touch gestures is challenging because they do not provide haptic feedback. Considering a pointing gesture, in this work we present a user study evaluating the benefits of increasing the arm position sense for operating non-haptic interface. Our findings confirm that haptic feedback is required and show that an increased arm sense compensates for the lack of haptic feedback. The results suggest that applying 0.3 times of the pointing arm’s weight significantly speeds up direct object selection for free-floating interfaces. We also show that the correction phase of the underlying pointing movement is affected by boosting the arm sense.",Augmented Reality; haptic feedback; interaction; selection,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Evaluation of Navigation Operations in Immersive Microscopic Visualization,VRST - Virtual Reality Software and Technology,A,"In this study, we evaluated the quantitative effectiveness of navigation operation in a virtual reality (VR) volumetric viewer, in order to confirm the effectiveness of VR in life sciences. The analytical work for biological data is a promising application of VR because users can manipulate 3D data intuitively in VR. However, few studies have focused on the quantitative evaluations of such applications. Therefore, we conducted an experiment to evaluate the speedup of navigation operation (sequences of translation, rotation, and scaling) in VR applications for 3D microscopy. We compared the task completion time between a non-VR visualization tool and a VR visualization tool. The speedup by the VR immersive visualizer was found to be 203% in the most effective case. The result showed that the VR immersive visualizer enables more efficient navigation than the conventional volumetric viewer.",immersive visualization; life science; microscope; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Experiencing Waiting Time in Virtual Reality,VRST - Virtual Reality Software and Technology,A,This article investigates the impact of waiting in Virtual Reality (VR) on the perception of time. We manipulated the visual quality of a virtual room replicating a real one (360-picture vs. 3D-model) with and without avatar embodiment (no-avatar vs. avatar). We only observed a significant difference in the estimated time duration between the real and the virtual worlds when using no avatar within a 3D model of the room. Our early results suggest that a VR environment with an avatar and a simple 3D model or 360 picture room is not significantly perturbing time perception and thus could be used for diagnosis and therapy of psychiatric conditions related to altered time perception.,Avatar Embodiment; Time Perception; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Exploring Experiences of Virtual Reality among Young and Older Adults in a Subway Fire Scenario: a Pilot Study,VRST - Virtual Reality Software and Technology,A,"We report a pilot study investigating experiences of virtual reality (VR) among young and older adults in a subway fire scenario. We created VR environments in subway fire scenarios and ran an experiment by asking 5 young and 5 older adults to explore VR environments. After the experiment, participants were asked to fill out a survey questionnaire to report their feelings. Additionally, we conducted semi-structured interviews with participants to understand challenges they faced while exploring VR environments. We found that compared with young adults, older adults tended to be different during the process of evacuating a subway station in virtual reality. We suggest design opportunities for creating VR environments for more effective training of older adults.",fire evacuation; older adults; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Exploring Immersive Technologies to Simulate Fear of Crime,VRST - Virtual Reality Software and Technology,A,"The implementation of Virtual Reality (VR) tools in criminological research is very scarce, and almost non-existent in the fear of crime (FoC) field. Our objective is to assess the feasibility of Immersive Technologies for research on FoC. To do so, a simulation (360° video) grounded on the manipulation of environmental variables (street lighting) was conducted. Our preliminary results suggest that: (a) virtual simulation of absence of urban lighting elicits experiences of FoC, and (b) that simulation of experiences of FoC in virtual reality is an adequate strategy for analysis of this phenomenon.",360° video; Criminology; environmental variables.; fear of crime; immersive technologies; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Impact of Gamified Interaction with Virtual Nature on Sustained Attention and Self-Reported Restoration &nbsp;—&nbsp; A Pilot Study,VRST - Virtual Reality Software and Technology,A,"Interaction with nature in virtual reality has been shown to induce similar restorative benefits as interaction with real-life nature. Drawing from Attention restoration theory, restorative benefits from being in virtual nature are likely to be improved through greater active engagement techniques with specific virtual natural features. Gamification is the process of adding game design elements in non-game scenarios in order to improve engagement and motivation. In the present pilot study, six participants completed either a gamified interaction with virtual nature, one where game design elements had been added in order to improve engagement with specific virtual nature features and thus possibly further facilitating sustained attention and self-reported restoration, having them pick plants and gain rewards such as a higher level in return, or a non-gamified task, one where they explored the virtual nature environment and looked at plants at their own pace without any game design elements. Gamified interaction improved sustained attention restoration more than non-gamified interaction. Additionally, gamified interaction was also shown to have reduced negative effect in self-reported restoration more than non-gamified interaction. While there are still several limitations, gamified interaction with virtual nature seems to offer vast potential as an engagement technique in improving sustained attention and self-reported restoration.",engagement; gamification; sustained attention; virtual nature; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Indian Virtual reality affective database with self-report measures and EDA,VRST - Virtual Reality Software and Technology,A,"The current work assesses the physiological and psychological responses to the 360° emotional videos selected from Stanford virtual reality (VR) affective database [Li et al., 2017], presented using VR head-mounted display (HMD). Participants were asked to report valence and arousal level after watching each video. The electro-dermal activity (EDA) was recorded while watching the videos. The current pilot study shows no significant difference in skin-conductance response (SCR) between the high and low arousal experience. Similar trends were observed during high and low valence. The self-report pilot data on valence and arousal shows no statistically significant difference between Stanford VR affective responses and the corresponding Indian population psychological responses. Despite positive result of no-significant difference in self-report across cultures, we are limited to generalize the result because of small sample size.",360° videos; Arousal; EDA; Valence; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Interactive Virtual-Reality Fire Extinguisher with Haptic Feedback,VRST - Virtual Reality Software and Technology,A,"We present an interactive virtual-reality (VR) fire extinguisher that provides both realistic viewing using a head-mounted display (HMD) and kinesthetic experiences using a pneumatic muscle and vibrotactile transducer. The VR fire extinguisher is designed to train people to use a fire extinguisher skillfully in real fire situations. We seamlessly integrate three technologies: VR, object motion tracking, and haptic feedback. A fire scene is immersed in the HMD, and a motion tracker is used to replicate a real designed object into the virtual environment to realize augmented reality. In addition, when the handle of the fire extinguisher is squeezed to release the extinguishing agent, the haptic device generates both vibrotactile and air flow tactile feedback signals, providing the same experience as that obtained while using a real fire extinguisher.",Firefighting; Haptic feedback; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Interactive Visualization of Painting Data with Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Exploration of Augmented Reality technologies has increased substantially and the increase in both popularity and technological maturity has also led to several applications being developed for educational and museum environments. Specifically, a greater focus has been placed upon creating memorable experiences that both attract and educate museum patrons. Attempts to do this involve creating both Virtual Reality and Augmented Reality experiences, such as having users enter into immersive worlds that demonstrate the history of a certain time period, or applications that overlay life-like models of those animals in the very room the user is standing in. Many of these experiences are quite exceptional but begin to lack in variety when moving towards the art gallery, and mainly focus on making painting information more accessible. In an attempt to address this, this project outlines the design and evaluation of a proof-of-concept meant to study if adding interaction through Augmented Reality to paintings themselves would be both technologically feasible and desirable.",art; augmented reality; interactive; painting; visualization,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,IRIS: Inter-Reality Interactive Surface,VRST - Virtual Reality Software and Technology,A,"While many metaphors were developed for interactions from a specific point at the reality-virtuality continuum, much less attention has been paid to designing metaphors that allow the users to cross the boundaries between the virtual, the augmented, and the real. We propose a use of an Inter-Reality Interactive Surface (IRIS) that enables users to collaborate across the reality-virtuality continuum within the same application. While we examine IRIS in the context of an immersive educational platform, UniVResity, the metaphor can be generalized to many other application domains.",augmented reality; collaboration; immersive learning; interaction metaphor; reality-virtuality continuum; virtual reality,Keywords,True,
Scopus,conferencePaper,2019,Learning-based Estimation of 6-DoF Camera Poses from Partial Observation of Large Objects for Mobile AR*,VRST - Virtual Reality Software and Technology,A,"We propose a method that estimates 6-DoF camera pose from a partially visible large object, by exploiting information of its subparts that are detected using a state-of-the-art convolutional neural network (CNN). The trained CNN outputs two-dimensional bounding boxes around subparts and associated classes. Information from detection is then fed to a deep neural network that regresses to camera's 6-DoF poses. Experimental results show that the proposed method is more robust to occlusions than conventional learning-based methods.",deep learning; large object; Mobile augmented reality; partial observation; pose estimation,Keywords,True,
Scopus,conferencePaper,2019,Mixed Reality Speaker Identification as an Accessibility Tool for Deaf and Hard of Hearing Users,VRST - Virtual Reality Software and Technology,A,"People who are Deaf or Hard of Hearing (DHH) benefit from text captioning to understand audio, yet captions alone are often insufficient for the complex environment of a panel presentation, with rapid and unpredictable turn-taking among multiple speakers. It is challenging and tiring for DHH individuals to view captioned panel presentations, leading to feelings of misunderstanding and exclusion. In this work, we investigate the potential of Mixed Reality (MR) head-mounted displays for providing captioning with visual cues to indicate which person on the panel is speaking. For consistency in our experimental study, we simulate a panel presentation in virtual reality (VR) with various types of MR visual cues; in a study with 18 DHH participants, visual cues made it easier to identify speakers.",Deaf and Hard of Hearing; Mixed Reality; Speaker Identification,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Monocular Viewing Protects Against Cybersickness Produced by Head Movements in the Oculus Rift,VRST - Virtual Reality Software and Technology,A,We compared the cybersickness produced when a virtual environment (VE) was viewed binocularly and monocularly through an Oculus Rift CV1 head-mounted display (HMD). During each exposure to the VE participants made continuous yaw head movements in time with a computer-generated metronome. Across trials we also varied their head movement frequency (0.5 or 1.0 Hz) and motion-to-photon delays (from ∼5 - ∼212 ms). We found that: 1) cybersickness severity increased with added display lag; and 2) monocular viewing appeared to protect against these increases in cybersickness. We conclude that active binocular viewing with this HMD introduced artifacts that increased the likelihood of more severe sickness.,Cybersickness; Oculus Rift; Perception; Virtual Reality; VR,Keywords,True,
Scopus,conferencePaper,2019,Optical-Reflection Type 3D Augmented Reality Mirrors,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) mirrors can show virtual objects overlaid onto the physical world reflected in the mirror. Optical-reflection type AR mirror displays use half-silvered mirrors attached in front of a digital display. However, prior work suffered from visual depth mismatch between the optical reflection of the 3D physical space and 2D images displayed on the surface of the mirror. In this research, we use 3D visualisation to overcome this problem and improve the user experience by providing better depth perception for watching and interacting with the content displayed on an AR mirror. As a proof of concept, we developed two prototype optical-reflection type 3D AR mirror displays, one using glasses-free multi-view 3D display and another using a head tracked 3D stereoscopic display that supports hand gesture interaction.",3D visualization; Augmented mirror; depth mismatch,Title_Abstract,True,
Scopus,conferencePaper,2019,Predicting the Torso Direction from HMD Movements for Walk-in-Place Navigation through Deep Learning,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose to use the deep learning technique to estimate and predict the torso direction from the head movements alone. The prediction allows to implement the walk-in-place navigation interface without additional sensing of the torso direction, and thereby improves the convenience and usability. We created a small dataset and tested our idea by training an LSTM model and obtained a 3-class prediction rate of about 90%, a figure higher than using other conventional machine learning techniques. While preliminary, the results show the possible inter-dependence between the viewing and torso directions, and with richer dataset and more parameters, a more accurate level of prediction seems possible.",deep learning; locomotion; Virtual reality; walking in place,Keywords,True,
Scopus,conferencePaper,2019,Preliminary Evaluation of the Usability of a Virtual Reality Game for Mudslide Education for Children,VRST - Virtual Reality Software and Technology,A,"Mudslide education is important for children. In this study, a design-based research approach was used to develop an educational VR mudslide game for children. Eleven children participated in the usability evaluations. The results indicated the importance of intuitive, easy-to-learn controls. Six major refinements of the VR mudslide game were made to increase usabilities. Feedback from the participants will guide future game refinements to increase users’ engagement and interaction.",,Title,True,
Scopus,conferencePaper,2019,Proposing a Hand-Tracking Device using a Tangential Force Mechanical Sensor,VRST - Virtual Reality Software and Technology,A,"Conventional hand-tracking devices are constructed with inertial measurement units, bending sensors, and optical technologies. However, these are limited by their high-cost and environmental factors. In this research, a hand-tracking device using a tangential force mechanical sensor for use in Immersive Virtual Environments is proposed.",cyberglove; hand-tracking; sensor; virtual reality; wearable,Keywords,True,
Scopus,conferencePaper,2019,Real-time Monitoring Method for Cybersickness using Physiological Signals,VRST - Virtual Reality Software and Technology,A,"The potential for cybersickness remains a critical problem when engaged in Virtual Reality experiences. Cybersickness is difficult to resolve because, although there are commonly accepted symptoms and theories, there is still no consensus on how to overcome the problem. In this study, a method of real-time monitoring of physiological signals is proposed as an approach to measure the potential onset of cybersickness. An application called Cybatica which displays physiological data and a unique metric termed Onset of Cybersickness (OCS) has been developed.",Cybersickness; Physiological Data; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Scalebridge VR: Immersive Proportional Reasoning Game for Children with Brain-Computer Interface for Difficulty Scaling,VRST - Virtual Reality Software and Technology,A,"We present the design and evaluation of Scalebridge VR, an immersive educational game that teaches children the mathematical skill of proportional reasoning. The game uses brain-computer-interface-based adaptive level difficulty to modulate difficulty of the game based on the player’s attention and meditation state. The game is an adaptation of previously introduced Scalebridge game that did not use virtual reality, but was shown to be an effective tool for learning proportional reasoning.",freehand interaction; immersive learning; mathematical skills; proportional reasoning; STEM; virtual reality; VR game,Abstract_Keywords,True,
Scopus,conferencePaper,2019,SolarVR for inter-cognitive and intra-cognitive communication,VRST - Virtual Reality Software and Technology,A,"The aim of this research is to design and implement a Solar Virtual Reality environment (SolarVR) for inter-cognitive and intra-cognitive communication by connecting users and sensors to a real-world solar panel plant for remote monitoring, maintenance and collaboration. The paper outlines the development of a VR solution which can be utilized for remote monitoring and communication, skills training and science education.",communication; design; solar; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,The Impact of Stereo Rendering on the Perception of Normal Mapped Geometry in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This paper investigates the effects of normal mapping on the perception of geometric depth between stereoscopic and non-stereoscopic views. Results show, that in a head-tracked environment, the addition of binocular disparity has no impact on the error rate in the detection of normal-mapped geometry. It does however significantly shorten the detection time.",Binocular Disparity; Motion Parallax; Normal Maps; Virtual Reality,Title_Keywords,True,
Scopus,conferencePaper,2019,Toward Effective Virtual Reality Intervention Development Planning for People with Persistent Postural-Perceptual Dizziness,VRST - Virtual Reality Software and Technology,A,"Persistent Postural-Perceptual Dizziness (PPPD) is defined by World Health organization as ”Persistent non-vertiginous dizziness, unsteadiness, or both lasting three months or more”. With the most common provocations are situations like up-right position, self-motions, looking at fast moving objects or disruptions in a crowded environment. Besides conventional treatments, scientists are looking at the possibility of using creative technology including virtual reality (VR) to assist improving symptoms. Here, we have proposed a strategy that would strengthen the initial phase of discussion between VR technologists and PPPD experts on developing an effective VR based intervention tool.",intervention; persistent postural-perceptual dizziness; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Towards Dynamic Positioning of Text Content on a Windshield Display for Automated Driving,VRST - Virtual Reality Software and Technology,A,"Windshield displays (WSDs) are a promising new technology to augment the entire windscreen with additional information about vehicle state, highlight critical objects in the surrounding, or serve as replacement for conventional displays. Typically, augmentation is provided in a screen-fixed manner as overlay on the windscreen. However, it is unclear to date if this is optimal in terms of usability/UX. In this work, we propose ”StickyWSD” – a world-fixed positioning strategy – and evaluate its impact on quantitative measures compared to screen-fixed positioning. Results from a user study conducted in a virtual reality driving simulator (N = 23) suggest that the dynamic world-fixed positioning technique shows increased performance and lowered error rates as well as take-over times. We conclude that the ”StickyWSD” approach offers lot of potential for WSDs that should be researched further.",augmented reality; automated driving; focus distance; user study; virtual reality; windshield display,Abstract_Keywords,True,
Scopus,conferencePaper,2019,TTT: Time Synchronization Method by Time Distortion for VR Training including Rapidly Moving Objects,VRST - Virtual Reality Software and Technology,A,"Providing an experience that includes high-speed objects, such as tennis balls, with a virtual reality (VR) training environment might provide efficient training for trainers but is challenging to achieve. Because of the drawing performance of the display, high-speed objects are perceived as poor visual information more than in reality, such as images in a stroboscope. The faster the object, the more noticeable it becomes, and the harder it is to perceive it correctly. Therefore, if the training is performed at the actual speed, the perception becomes more difficult than real space training due to the low reproduction accuracy. To solve this problem, we propose the computational time-space that controls high-speed objects in VR space, based on the user’s body movement. The method facilitates the perception of fast-moving objects by synchronizing the time of the ball with the movement of the body.",Assisting Training; Immersive Virtual Environments; Time Perception,Abstract,True,
Scopus,conferencePaper,2019,Understanding Enjoyment in VR Games with GameFlow,VRST - Virtual Reality Software and Technology,A,"In this paper, we report on a work in progress project that aims to understand affordances and inhibiters of enjoyment in virtual reality (VR) video games. We apply the GameFlow model to review and analyse VR and non-VR versions of the same games to identify differences in enjoyment. Our approach includes conducting expert reviews using the GameFlow model, as well as conducting qualitative analysis on video game reviews, using GameFlow as a conceptual foundation. In this paper, we report our initial findings for the game Superhot. Our ongoing work evaluates a selection of games to map opportunities and pitfalls when designing games for VR.",enjoyment; player experience; videogames; Virtual reality; VR,Abstract_Keywords,True,
Scopus,conferencePaper,2019,UniVResity: Face-to-Face Class Participation for Remote Students using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We describe a prototype of the virtual reality remote classroom participation system called UniVResity. UniVResity mirrors in virtual reality the ongoing face-to-face classroom activities, taking into account potentially low bandwidth data connection and lack of VR equipment in class. Our system attempts to combine the benefits of online education and face-to-face education, and makes face-to-face learning more accessible.",distance learning; education; remote learning; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Vertical Locomotion in VR Using Full Body Gestures,VRST - Virtual Reality Software and Technology,A,"Virtual Reality experiences today are majorly based on horizontal locomotion. In these experiences, movement in the virtual space is accomplished using teleportation, gaze input or tracking in physical space which is limited to a certain extent. Our work focuses on intuitive interactions for vertical locomotion involving both hands and feet. Such an instance of vertical locomotion is - ladder climbing. In this paper, we present an interaction technique for climbing a ladder in Virtual Reality (VR). This technique is derived from the natural motions of the limbs while climbing a ladder in reality, adhering to safe climbing practices. The developed interaction can be used in training experiences as well as gaming experiences. Preliminary evaluation of our interaction technique showed positive results across dimensions like - learnability, natural mapping, and intuitiveness.",Gestures; Interactions; Ladder Climbing; Locomotion; Teleportation; Tracking; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Virtual Immersive Educational Systems:The case of 360° video and co-learning design.,VRST - Virtual Reality Software and Technology,A,"The focus of this research is to depict the design process of a cost-effective, robust but user-friendly Virtual Immersive Educational (VIE) system. Thus, assist researchers, instructors and designers in identifying an effective method to design VIE systems. In this report, we describe our initial steps to design such a system in order to educate engineering students on the basic health and safety guidelines of safe interaction with a robotic arm. To do so, a set of 360° videos have been designed, developed and tested.",360° videos; Immersive Education Systems; Immersive technologies; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2019,Visualizing Convolutional Neural Networks with Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Software systems and components are increasingly based on machine learning methods, such as Convolutional Neural Networks (CNNs). Thus, there is a growing need for common programmers and machine learning newcomers to understand the general functioning of these algorithms. However, as neural networks are complex in nature, novel presentation means are required to enable rapid access to the functionality. For that purpose, this paper examines how CNNs can be visualized in Virtual Reality. A first exploratory study has confirmed that our visualization approach is both intuitive to use and conductive to learning.",knowledge learning; neural networks; virtual reality; visualization,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Where to Place the Camera,VRST - Virtual Reality Software and Technology,A,"This paper describes aspects which are important for camera positioning in cinematic virtual reality. For our findings, we took a closer look at proxemics, the study on how humans behave in regard to space and distances. We explored well-known shot sizes used in traditional filmmaking and put them in relation to proxemics distances. The results were adapted to camera distances in cinematic virtual reality.",360° movies; camera distances; camera positions; character distances; Cinematic Virtual Reality; proxemics; shot sizes,Abstract_Keywords,True,
Scopus,conferencePaper,2019,WiredSwarm: High Resolution Haptic Feedback Provided by a Swarm of Drones to the User’s Fingers for VR interaction,VRST - Virtual Reality Software and Technology,A,"We propose a concept of a novel interaction strategy for providing rich haptic feedback in Virtual Reality (VR), when each user’s finger is connected to micro-quadrotor with a wire. Described technology represents the first flying wearable haptic interface. The solution potentially is able to deliver high resolution force feedback to each finger during fine motor interaction in VR. The tips of tethers are connected to the centers of quadcopters under their bottom. Therefore, flight stability is increasing and the interaction forces are becoming stronger which allows to use smaller drones.",drone; force feedback; haptics; human-swarm interaction; multi-agent system; quadrotor; swarm; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,ALS-SimVR: Advanced Life Support Virtual Reality Training Application,VRST - Virtual Reality Software and Technology,A,The delivery of ongoing training and support to Advanced Life Support (ALS) teams poses significant resourcing and logistical challenges. A reduced exposure to cardiac arrests and mandated re-accreditation pose further challenges for educators to overcome. This work presents the ALS-SimVR (Advanced Life Support Simulation in VR) application. The application is intended for use as a supplementary training and refresher asset for ALS team leaders. The purpose of the application is to allow critical care clinicians to rehearse the role of ALS Team leader in their own time and location of choice. The application was developed for the Oculus-Go and ported to the Oculus-Quest. The application is also supported for a desktop and server based streaming release.,Clinical; Simulation; Virtual Reality,Title_Keywords,True,
Scopus,conferencePaper,2019,AssessAR: An Augmented Reality Based Environmental Impact Assessment Framework,VRST - Virtual Reality Software and Technology,A,"Human activities can have a lasting impact on the environment and society. Environmental impact assessment (EIA) which focusses on evaluating the impact of proposed developmental projects on the environment, helps in transparent decision-making and involves multiple stakeholders. However, EIA is data and effort-intensive and often becomes complex and long-drawn. Moreover, EIA is currently performed using primarily two-dimensional traditional mediums which could be vastly restrictive and difficult to navigate and comprehend. Here, we present an immersive approach which can create 3D interactive elements, modelling the real-world using augmented/mixed reality. Because of the inherent benefits of using three-dimensional representations and associated real-world interactions, we posit that our approach will facilitate better and faster, collaboration-enabled analysis of a developmental project proposal, thereon reducing processing time and promoting high fidelity.",Augmented Reality; Environmental Impact Assessment,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Development of MirrorShape: High Fidelity Large-Scale Shape Rendering Framework for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Today there is a high variety of haptic devices capable of providing tactile feedback. Although most of existing designs are aimed at realistic simulation of the surface properties, their capabilities are limited in attempts of displaying shape and position of virtual objects. This paper suggests a new concept of distributed haptic display for realistic interaction with virtual object of complex shape by a collaborative robot with shape display end-effector. MirrorShape renders the 3D object in virtual reality (VR) system by contacting the user hands with the robot end-effector at the calculated point in real-time. Our proposed system makes it possible to synchronously merge the position of contact point in VR and end-effector in real world. This feature provides presentation of different shapes, and at the same time expands the working area comparing to desktop solutions. The preliminary user study revealed that MirrorShape was effective at reducing positional error in VR interactions. Potentially this approach can be used in the virtual systems for rendering versatile VR objects with wide range of sizes with high fidelity large-scale shape experience.",3D interaction; collaborative technologies; haptics; interaction technologies; robotics; shape-changing interfaces; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,DexController : Hand-Held Controller Recognizing Grasp-Pose and Grasp-Force in Virtual Reality Defense Game,VRST - Virtual Reality Software and Technology,A,"We developed a hand-held controller named DexController, leveraging grasp as an additional input modality for virtual reality(VR) game. The pressure-sensitive surface of DexController could recognize two different grasp-poses (i.e. precision grip and power grip) and detect grasp-force. For demonstration, we designed a VR defense game in which players should attack different virtual enemies using the proper weapon with a proper level of force. User study confirmed that utilizing meaningful information of grasping facilitates natural mapping with game contents, which led VR game users to experience enhanced presence and enjoyment.",controller; game experience; natural interaction; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,"Dynamic Virtual Proteins: Visualization, Interaction and Collaboration in Virtual Reality",VRST - Virtual Reality Software and Technology,A,,Amino-acid; Animation; Augmented Reality; Bio-chemistry; Biology; Education; Interaction Design; Mixed Reality; Physics; Protein Structure; Science; Usability; User Experience; User Interaction; Virtual Protein; Virtual Reality; Visualization,Title_Keywords,True,
Scopus,conferencePaper,2019,Emotion Evoking Art Exhibition in VR,VRST - Virtual Reality Software and Technology,A,"Many museums today lack an aspect of technology that will attract younger visitors to visit the art. By implementing Virtual Reality into art museum solves this problem. Virtual Reality is a popular phenomenon that attracts many viewers and is growing every day. Art museums want to express emotion through their art and Virtual Reality can evoke that emotion more. By creating a virtual museum that not only has all the art on display but also is set an outdoor environment such as a garden or a dark forest will further enhance the emotion. If a piece of art is supposed to show warmth or positive feelings, why not place it in a garden? If the art is supposed to show darkness or cold why not place it in a dead forest? Using Virtual Reality allows us to place art in these environments so further museum goal of expressing emotion.",art; interactive; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Extended Reality for Chronic Pain Relief,VRST - Virtual Reality Software and Technology,A,"Chronic pain is ongoing pain lasting for long periods of time after the initial injury or disease has healed. Chronic pain is difficult to treat and can affect the daily lives of patients. Distraction therapy is a proven way of relieving pain by redirecting the focus of patients’ attention. Virtual reality is an effective platform for distraction therapy as it immerses the user visually, aurally, and even somewhat physically in a virtual world detached from reality. There is little research done on the effects that physical interactions have on pain management. This project aims to evaluate different types of extended reality (XR) interactions, including full body movement, for chronic pain patients to determine which is the best for pain relief. We are building a prototype for participants to interact both mentally and physically and measuring the reduction in subjective pain ratings at various points of the XR experience.",augmented reality; chronic pain; mixed reality; pain management; pain relief; user study; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Interactive Visualization of Painting Data with Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Exploration of Augmented Reality technologies has increased substantially and the increase in both popularity and technological maturity has also led to several applications being developed for educational and museum environments. Specifically, a greater focus has been placed upon creating memorable experiences that both attract and educate museum patrons. Attempts to do this involve creating both Virtual Reality and Augmented Reality experiences, such as having users enter into immersive worlds that demonstrate the history of a certain time period, or applications that overlay life-like models of those animals in the very room the user is standing in. Many of these experiences are quite exceptional but begin to lack in variety when moving towards the art gallery, and mainly focus on making painting information more accessible. In an attempt to address this, this project outlines the design and evaluation of a proof-of-concept meant to study if adding interaction through Augmented Reality to paintings themselves would be both technologically feasible and desirable.",art; augmented reality; interactive; painting; visualization,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2019,Layered Horizons: a Geospatial Humanities Research Platform,VRST - Virtual Reality Software and Technology,A,"In this demo we showcase Layered Horizons, a Virtual Reality (VR) experience we have developed for use in an ARC-funded research project, Waves of Words: Mapping and Modelling Australia’s Pacific Past. This platform allows users to connect different geospatial datasets (for our purposes, from the humanities and social sciences) into layers that can then be explored by the use of natural gesture and body movement. This kind of interaction design in VR takes full advantage of the media’s affordances, without relying on metaphors from other interactive media, yet being familiar enough as to engender intuitive and meaningful use. We demonstrate how the platform is currently being used to connect linguistic data (word lists) with archaeological data (e.g. on the spread of bananas through the Asia-Pacific region, or canoe styles found in different locations) and anthropological data (e.g. shared cultural features like chieftainship systems or kinship systems). Taking into account what we also know about Pacific navigation and simulated canoe travel, we can therefore build a complex layered map of the region over time that allows us to better discover probable human migration and contact patterns.",data visualisation; gesture; interfaces; languages; leap motion; research platform; research through design; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2019,OORT: An Air-flow based Cooling System for Long-term Virtual Reality Sessions,VRST - Virtual Reality Software and Technology,A,"In this paper, we present OORT, a cooling system for head-mounted displays (HMDs) that improves wearing comfort by decreasing skin temperatures of the facial areas covered by the headset. The integrated cooling system consists of an electronically controlled fan blower. The fan compartment is integrated into an hmd padding element with custom-designed air flow channels that provide cool air circulation around the covered facial regions. We report on the design and implementation of OORT as a viable way to provide thermal comfort during long-term virtual reality experiences.",Cooling; HMDs; Oort; Thermal Comfort; Virtual Reality; VR,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,PillowVR: Virtual Reality in Bed,VRST - Virtual Reality Software and Technology,A,"We demonstrate PillowVR, virtual reality framework that integrates the smartphone, magnifier and sensors into a pillow/cushion for immersive VR experience in bed. PillowVR is applied for presenting immersive bed time stories to children to help them go to sleep and therefore its interaction was designed to minimize excessive bodily movements – only simple back-of-the head pressure events are used to browse the content. PillowVR illustrates shows how VR can be more woven into our daily lives inexpensively and naturally by customizing the set up and interaction for the specific task and experience.The actual demonstration of PillowVR would be very simple (as intended). In this paper, we can watch 360° video because viewpoint can be switching in “Non Ready” state. Our team will place an exercise pad or long picnic chair (instead of an actual bed) in the demo area. The user will enact the whole process as if being at home from the very start – sit/lie on the chair, insert the smartphone, wear the PillowVR, browse the content, pretend as if fallen to sleep, and assess the experience from the beginning to the very end (when one wakes up in the morning).",Interaction; Virtual Reality; VR Device,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2019,Spatially Accurate Generative Music with AR Drawing,VRST - Virtual Reality Software and Technology,A,"Recent experiments in semi-automatically generating ambient music have yielded emotionally affecting results, leading scientists and musicians alike to develop and experiment with computational systems for creating audible art with varying degrees of success. Most of these systems are based either in analogue technology such as classic tape-reel recording systems or digital systems like virtual synthesizers triggered by a combination of developer-defined values and random number generation. In this paper, I outline the conceptual reasoning behind and development of one such generative music system which uses a simple but versatile virtual synthesizer to generate sound and sequences of repeating randomly generated notes drawn by the user in augmented reality to formulate the patterns and spatial origin of each sound contributing to the entire generative piece.",art; augmented reality; education; interactive; museum; music; visualization,Abstract_Keywords,True,
Scopus,conferencePaper,2019,Text Entry Method for Immersive Virtual Environments Using Curved Keyboard,VRST - Virtual Reality Software and Technology,A,"In this paper, we introduce a curved QWERTY keyboard, bent spherically in front of the user, to facilitate 3D word-gesture text entry in immersive virtual environments. Using the curved keyboard, the number of candidate words in the 3D word-gesture text entry is reduced compared with that using a planar keyboard. In the pilot study, the text entry performance of the first author was 21.0&nbsp;WPM (SD = 5.06), with a total error rate of 26.0% (SD = 15.2).",Curved surface; spherical surface; text entry; virtual reality; WPM,Keywords,True,
Scopus,conferencePaper,2019,Virtual environment for processing medial axis representations of 3D nanoscale reconstructions of brain cellular structures,VRST - Virtual Reality Software and Technology,A,"We present a novel immersive environment for the interactive analysis of nanoscale cellular reconstructions of rodent brain samples acquired through electron microscopy. The system is focused on medial axis representations (skeletons) of branched and tubular structures of brain cells, and it is specifically designed for: i) effective semi-automatic creation of skeletons from surface-based representations of cells and structures ii) fast proofreading, i.e., correcting and editing of semi-automatically constructed skeleton representations, and iii) useful exploration, i.e., measuring, comparing, and analyzing geometric features related to cellular structures based on medial axis representations. The application runs in a standard PC-tethered virtual reality (VR) setup with a head mounted display (HMD), controllers, and tracking sensors. The system is currently used by neuroscientists for performing morphology studies on sparse reconstructions of glial cells and neurons extracted from a sample of the somatosensory cortex of a juvenile rat.",immersive neuroscience; nanoscale brain reconstruction,Abstract,True,
Scopus,conferencePaper,2019,VR Minecraft for Art,VRST - Virtual Reality Software and Technology,A,"Art museums are becoming very boring to many people especially to the younger generation. The purpose of this project is to try to make a new type of art museum, one that is engaging and interactive. This project aims to answer the research question: Can a VR Minecraft museum enhance the user experience by giving them something that a typical art museum can’t? To answer this question, we’ve create a VR art museum within Minecraft and added features to make it more interactive and interesting. A more engaging environment is a great atmosphere to want to learn more. A scavenger hunt was added to the art museum to give the player a reason to walk around the entire museum. The player can build a sculpture after he/she completes the scavenger hunt which allows the player to be creative and imaginative. The user is then provided with different colored blocks to create a painting of his/her own. In order to answer the research question, we demoed the museum to a few people and then interviewed them. Their answers were very positive towards the VR Minecraft museum which leads me to believe that a VR Minecraft museum can indeed enhance the user experience.",art; interactive; virtual reality,Keywords,True,
Scopus,conferencePaper,2020,DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback,VRST - Virtual Reality Software and Technology,A,"We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user’s hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user’s hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object’s surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device.",Haptics; Mass Perception; Vibration; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Docking Haptics: Extending the Reach of Haptics by Dynamic Combinations of Grounded and Worn Devices,VRST - Virtual Reality Software and Technology,A,"Grounded haptic devices can provide a variety of forces but have limited working volumes. Wearable haptic devices operate over a large volume but are relatively restricted in the types of stimuli they can generate. We propose the concept of docking haptics, in which different types of haptic devices are dynamically docked at run time. This creates a hybrid system, where the potential feedback depends on the user’s location. We show a prototype docking haptic workspace, combining a grounded six degree-of-freedom force feedback arm with a hand exoskeleton. We are able to create the sensation of weight on the hand when it is within reach of the grounded device, but away from the grounded device, hand-referenced force feedback is still available. A user study demonstrates that users can successfully discriminate weight when using docking haptics, but not with the exoskeleton alone. Such hybrid systems would be able to change configuration further, for example docking two grounded devices to a hand in order to deliver twice the force, or extend the working volume. We suggest that the docking haptics concept can thus extend the practical utility of haptics in user interfaces.",force feedback; haptics; virtual reality,Keywords,True,
Scopus,conferencePaper,2020,VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments,VRST - Virtual Reality Software and Technology,A,"Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen – a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1)&nbsp;pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2)&nbsp;vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1)&nbsp;pneumatic, (2)&nbsp;vibrotactile and (3)&nbsp;a combination of both with (4)&nbsp;snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.",3D User Interfaces; Haptics; Pneumatic Actuation; Sketching; Vibrotactile Actuation; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2020,The Impact of Missing Fingers in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Avatars in virtual reality (VR) can have body structures that differ from the physical self. Game designers, for example, often stylize virtual characters by reducing the number of fingers. Previous work found that the sensation of presence in VR depends on avatar realism and the number of limbs. However, it is currently unknown how the removal of individual fingers affects the VR experience, body perception, and how fingers are used instead. In a study with 24 participants, we investigate the effects of missing fingers and avatar realism on presence, phantom pain perception, and finger usage. Our results show that particularly missing index fingers decrease presence, show the highest phantom pain ratings, and significantly change hand interaction behavior. We found that relative usage of thumb and index fingers in contrast to middle, ring, and little finger usage was higher with abstract hands than with realistic ones – even when the fingers were missing. We assume that dominant fingers are firstly integrated into the own body schema when an avatar does not resemble one’s own appearance. We discuss cognitive mechanisms in experiencing virtual limb loss.",avatars; missing fingers; phantom pain; presence; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Moving Virtual Reality out of its Comfort Zone and Into the African Kalahari Desert Field: Experiences From Technological Co-Exploration With an Indigenous San Community in Namibia,VRST - Virtual Reality Software and Technology,A,"Indigenous people (IP) living in remote areas, at the margins of mainstream society, are often the last ones to experience emerging technologies and even less to shape those experiences. It could be argued technology exposure and experience is necessary for IP to gain agency in making informed decisions on the rejection or appropriation of novel technologies. In this paper, VR is introduced to a remote San community within a broader community-based research collaboration considering political and ethical perspectives of technology inclusion. The intent was to familiarise the community with the technology through the development and playthrough of a game, to explore future opportunities for joint co-designs of VR applications, meanwhile gauging the barriers for how VR operates outside of its intended setting. The community members expressed their excitement about the experience and the desire to re-create traditional San games in VR. The paper reflects on the community experiences, the setup and use of VR in remote settings, and the choices made to facilitate the familiarization of emerging technology.",Cultural Heritage; Indigenous Knowledge; Indigenous People; Namibia; San People; User Experiences; Virtual Reality,Title_Keywords,True,
Scopus,conferencePaper,2020,Investigating Immersive Virtual Reality as an Educational Tool for Quantum Computing,VRST - Virtual Reality Software and Technology,A,"Quantum computing (QC) is an intrinsically complex yet exciting discipline with increasing practical relevance. A deep understanding of QC requires the integration of knowledge across numerous technical fields, such as physics, computing and mathematics. This work aims to investigate how immersive Virtual Reality (VR) compares to a desktop environment (‘web-applet’) as an educational tool to help teach individuals QC fundamentals. We developed two interactive learning tutorials, one utilising the ‘Bloch sphere’ visualisation to represent a single-qubit system, and the other exploring multi-qubit systems through the lens of ‘quantum entanglement’. We evaluate the effectiveness of each medium to teach QC fundamentals in a user study with 24 participants. We find that the Bloch sphere visualisation was well-suited to VR over a desktop environment. Our results also indicate that mathematics literacy is an important factor in facilitating greater learning with this effect being notably more pronounced when using VR. However, VR did not significantly improve learning in a multi-qubit context. Our work provides valuable insights which contribute to the emerging field of Quantum HCI (QHCI) and VR for education.",Education; Learning; Quantum Computing; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Virtual Reality in Computer Science Education: A Systematic Review,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) technologies have become more affordable and accessible in recent years. This is opening up new methods and opportunities in the field of digital learning. VR can offer new forms of interactive learning and working, especially for subjects from the STEM (Science, technology, engineering, and mathematics) area. In this context we investigate the potential and application of VR for computer science education with a systematic review in this paper. We present a formal literature review on the use of VR technologies in computer science education. We focus on the identification of factors such as learning objectives, technologies used, interaction characteristics, and challenges and advantages of using fully immersive VR for computer science education.",Computer Science Education; Literature Review; Virtual Reality; VR,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Virtual Navigation considering User Workspace: Automatic and Manual Positioning before Teleportation,VRST - Virtual Reality Software and Technology,A,"Teleportation is a navigation technique widely used in virtual reality applications using head-mounted displays. Basic teleportation usually moves a user’s viewpoint to a new destination of the virtual environment without taking into account the physical space surrounding them. However, considering the user’s real workspace is crucial for preventing them from reaching its limits and thus managing direct access to multiple virtual objects. In this paper, we propose to display a virtual representation of the user’s real workspace before the teleportation, and compare manual and automatic techniques for positioning such a virtual workspace. For manual positioning, the user adjusts the position and orientation of their future virtual workspace. A first controlled experiment compared exocentric and egocentric manipulation techniques with different virtual workspace representations, including or not an avatar at the user’s future destination. Although exocentric and egocentric techniques result in a similar level of performance, representations with an avatar help the user to understand better how they will land after teleportation. For automatic positioning, the user selects their future virtual workspace among relevant options generated at runtime. A second controlled experiment shows that the manual technique selected from the first experiment and the automatic technique are more efficient than the basic teleportation. Besides, the manual technique seems to be more suitable for crowded scenes than the automatic one.",Locomotion; real workspace; spatial awareness.; teleportation; virtual object access; virtual workspace,Abstract,True,
Scopus,conferencePaper,2020,Towards Physically Interactive Virtual Environments: Reactive Alignment with Redirected Walking,VRST - Virtual Reality Software and Technology,A,"Interactions with the physical environment, such as passive haptic feedback, have been previously shown to provide richer and more immersive virtual reality experiences. A strict correspondence between the virtual and real world coordinate systems is a staple requirement for physical interaction. However, many of the commonly employed VR locomotion techniques allow for, or even require, this relationship to change as the experience progresses. The outcome is that experience designers frequently have to choose between flexible locomotion or physical interactivity, as the two are often mutually exclusive. To address this limitation, this paper introduces reactive environmental alignment, a novel framework that leverages redirected walking techniques to achieve a desired configuration of the virtual and real world coordinate systems. This approach can transition the system from a misaligned state to an aligned state, thereby enabling the user to interact with physical proxy objects or passive haptic surfaces. Simulation-based experiments demonstrate the effectiveness of reactive alignment and provide insight into the mechanics and potential applications of the proposed algorithm. In the future, reactive environmental alignment can enhance the interactivity of virtual reality systems and inform new research vectors that combine redirected walking and passive haptics.",alignment; locomotion; redirected walking; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Virtual environments with a wide range of scales are becoming commonplace in Virtual Reality applications. Methods to control locomotion parameters can help users explore such environments more easily. For multi-scale virtual environments, point-and-teleport locomotion with a well-designed distance control method can enable mid-air teleportation, which makes it competitive to flying interfaces. Yet, automatic distance control for point-and-teleport has not been studied in such environments. We present a new method to automatically control the distance for point-and-teleport. In our first user study, we used a solar system environment to compare three methods: automatic distance control for point-and-teleport, manual distance control for point-and-teleport, and automatic speed control for flying. Results showed that automatic control significantly reduces overshoot compared with manual control for point-and-teleport, but the discontinuous nature of teleportation made users prefer flying with automatic speed control. We conducted a second study to compare automatic-speed-controlled flying and two versions of our teleportation method with automatic distance control, one incorporating optical flow cues. We found that point-and-teleport with optical flow cues and automatic distance control was more accurate than flying with automatic speed control, and both were equally preferred to point-and-teleport without the cues.",Automatic control; multiscale virtual environments; Point-and-teleport; VR navigation,Abstract,True,
Scopus,conferencePaper,2020,Virtual Projection Planes for the Visual Comparison of Photogrammetric 3D Reconstructions with Photo Footage,VRST - Virtual Reality Software and Technology,A,"Image-based 3D reconstructions and their visualization in virtual reality promise novel opportunities to explore and analyze 3D reconstructions of real objects, buildings and places. However, the faithfulness of the presented data is not always obvious and, in most cases, a 3D reconstruction cannot be compared directly to its corresponding real world instance. However, in case of reconstruction methods based on structure from motion (SFM), a large number of raw photos is available. This motivated us to develop a novel interaction technique for the visual comparison of details of 3D models with projections of the corresponding image sections, e.g. in order to rapidly verify the authenticity of perceived features. The results of a formal user study (n=18) demonstrate the general usability of such visual provenance information as well as benefits of the comparison in vicinity of the features in question over a separate image gallery. Further observations informed our iterative design process and led to the development of an improved interactive visualization. Our final implementation provides a spatial and content-related overview while retaining the efficiency of the original approach.",image browsing; interaction design; magic lenses; spatially registered images; virtual reality; visual comparison,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Body LayARs: A Toolkit for Body-Based Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Technological advances are enabling a new class of augmented reality (AR) applications that use bodies as substrates for input and output. In contrast to sensing and augmenting objects, body-based AR applications track people around the user and layer information on them. However, prototyping such applications is complex, time-consuming, and cumbersome, due to a lack of easily accessible tooling and infrastructure. We present Body LayARs, a toolkit for fast development of body-based AR prototypes. Instead of directly programming for a device, Body LayARs provides an extensible graphical programming environment with a device-independent runtime abstraction. We focus on face-based experiences for headset AR, and show how Body LayARs makes a range of body-based AR applications fast and easy to prototype.",Augmented reality; body-based augmentation; toolkit,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,CorFix: Virtual Reality Cardiac Surgical Planning System for Designing Patient Specific Vascular Grafts,VRST - Virtual Reality Software and Technology,A,"Patients with single ventricle heart defect undergo Fontan surgery to reroute the blood flow from the lower body to the lung by connecting the inferior vena cava to the pulmonary artery using a vascular graft. Since each patient has an unique anatomical structure and blood flow dynamics, the graft design is a critical factor for maximizing the long-term survival rate of Fontan patients. Currently, designing and evaluating grafts involve computer aided design (CAD) and computational fluid dynamics (CFD) skills. CAD incorporates numerous tools for design but lacks depth perception, surgical features, and design parameters for creating vascular grafts while visualizing and modifying patient anatomies. These limitations may lead to long lead times, inconsistent workflow, and surgically infeasible graft designs. In this paper, we introduce a novel virtual reality vascular graft modeling software - CorFix, that provides solutions to these challenges. CorFix includes several visualization features for performing diagnostics and surgical features with design guidelines for creating patient specific tube-shaped grafts in 3D. The designed vascular graft can be exported into a 3D model, which can be utilized for performing computational fluid dynamic analysis and 3D printing. The patient specific vascular graft designs in CorFix were compared to an engineering CAD software, SolidWorks (Dassault Systèmes, Vélizy-Villacoublay, France), by 8 participants. Through all participants had only received one time 10-minute tutorial on CorFix, CorFix had a higher success rate and 3.4 times faster performance in designing surgically feasible grafts than CAD. CorFix also scored higher in usability and lower in perceived workload than CAD. CorFix may be the tool that can enable medical doctors without 3D modeling background to design patient specific grafts.",3D Modeling; Applications; Prototyping/Implementation; Usability Study; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,"A Call for Auditable Virtual, Augmented and Mixed Reality",VRST - Virtual Reality Software and Technology,A,"XR (Virtual, Augmented and Mixed Reality) technologies are growing in prominence. However, they are increasingly being used in sectors and in situations that can result in harms. As such, this paper argues the need for auditability to become a key consideration of XR systems. Auditability entails capturing information of a system’s operation to enable oversight, inspection or investigation. Things can and will go wrong, and information that helps unpack situations of failure or harm, and that enables accountability and recourse, will be crucial to XR’s adoption and acceptance. In drawing attention to the urgent need for auditability, we illustrate some risks associated with XR technology and their audit implications, and present some initial findings from a survey with developers indicating the current ‘haphazard’ approach towards such concerns. We also highlight some challenges and considerations of XR audit in practice, as well as areas of future work for taking this important area of research forward.",accountability; audit; responsibility; reviewability; transparency,Title_Abstract,True,
Scopus,conferencePaper,2020,The Effects of Visual Realism on Spatial Memory and Exploration Patterns in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Understanding the effects of environmental features such as visual realism on spatial memory can inform a human-centered design of virtual environments. This paper investigates the effects of visual realism on object location memory in virtual reality, taking account of individual differences, gaze, and locomotion. Participants freely explored two environments which varied in visual realism, and then recalled the locations of objects by returning the misplaced objects back to original locations. Overall, we did not find a significant relationship between visual realism and object location memory. We found, however, that individual differences such as spatial ability and gender accounted for more variance than visual realism. Gaze and locomotion analysis suggest that participants exhibited longer gaze duration and more clustered movement patterns in the low realism condition. Preliminary inspection further found that locomotion hotspots coincided with objects that showed a significant gaze time difference between high and low visual realism levels. These results suggest that high visual realism still provides positive spatial learning affordances but the effects are more intricate.",Spatial ability; Spatial memory; Virtual reality; Visual fidelity; Visual realism,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Eye-Hand Coordination Training for Sports with Mid-air VR,VRST - Virtual Reality Software and Technology,A,"A relatively recent application area for Virtual Reality (VR) systems is sports training and user performance assessment. One of these applications is eye-hand coordination training systems (EHCTSs). Previous research identified that VR-based training systems have great potential for EHCTSs. While previous work investigated 3D targets on a 2D plane, here we aim to study full 3D movements and extend the application of throughput analysis to EHCTSs. We conducted two user studies to investigate how user performance is affected by different target arrangements, feedback conditions, and handedness in VR-based EHCTSs. In the first study, we explored handedness as well as vertical and horizontal target arrangements, and showed that user performance increases with the dominant hand and a vertical target plane. In the second study, we investigated different combinations of visual and haptic feedback and how they affect user performance with different target and cursor sizes. Results illustrate that haptic feedback did not increase user performance when it is added to visual feedback. Our results inform the creation of better EHCTSs with mid-air VR systems.",Fitts’ task; haptic feedback; mid-air interaction; performance assessment; reaction test; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments.",Design; Gaze; Manual input; Menu; Pointing; Virtual Reality,Title_Keywords,True,
Scopus,conferencePaper,2020,Temporal Consistent Motion Parallax for Omnidirectional Stereo Panorama Video,VRST - Virtual Reality Software and Technology,A,"We present a new pipeline to enable head-motion parallax in omnidirectional stereo (ODS) panorama video rendering using a neural depth decoder. While recent ODS panorama cameras record short-baseline horizontal stereo parallax to offer the impression of binocular depth, they do not support the necessary translational degrees-of-freedom (DoF) to also provide for head-motion parallax in virtual reality (VR) applications. To overcome this limitation, we propose a pipeline that enhances the classical ODS panorama format with 6 DoF free-viewpoint rendering by decomposing the scene into a multi-layer mesh representation. Given a spherical stereo panorama video, we use the horizontal disparity to store explicit depth information for both eyes in a simple neural decoder architecture. While this approach produces reasonable results for individual frames, video rendering usually suffers from temporal depth inconsistencies. Thus, we perform successive optimization to improve temporal consistency by fine-tuning our depth decoder for both temporal and spatial smoothness. Using a consumer-grade ODS camera, we evaluate our approach on a number of real-world scene recordings and demonstrate the versatility and robustness of the proposed pipeline.",neural network; rendering; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Capture to Rendering Pipeline for Generating Dynamically Relightable Virtual Objects with Handheld RGB-D Cameras,VRST - Virtual Reality Software and Technology,A,"We present a complete end-to-end pipeline for generating dynamically relightable virtual objects captured using a single handheld consumer-grade RGB-D camera. The proposed system plausibly replicates the geometry, texture, illumination, and surface reflectance properties of non-Lambertian objects, making them suitable for integration within virtual reality scenes that contain arbitrary illumination. First, the geometry of the target object is reconstructed from depth images captured using a handheld camera. To get nearly drift-free texture maps of the virtual object, a set of selected images from the original color stream is used for camera pose optimization. Our approach further separates these images into diffuse (view-independent) and specular (view-dependent) components using low-rank decomposition. The lighting conditions during capture and reflectance properties of the virtual object are subsequently estimated from the computed specular maps. By combining these parameters with the diffuse texture, the reconstructed model can then be rendered in real-time virtual reality scenes that plausibly replicate real world illumination at the point of capture. Furthermore, these objects can interact with arbitrary virtual lights that vary in direction, intensity, and color.",content creation; reconstruction; scanning; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Investigating Effects and User Preferences of Extra- and Intradiegetic Virtual Reality Questionnaires,VRST - Virtual Reality Software and Technology,A,"Virtual realities (VR) are becoming an integral part of product development across many industries, for example to assess aesthetics and usability of new features in the automotive industry. The recording of the evaluation is typically conducted by filling out questionnaires after the study participants left the virtual environment. In this paper, we investigate how questionnaires can be best embedded within the virtual environment and compare how VR-questionnaires differ from classical post-test evaluations regarding preference, presence, and questionnaire completion time. In the first study (N = 11), experts rated four design concepts of questionnaires embedded in VR, of which two were designed as extradiegetic and two as intradiegetic user interfaces. We show that intradiegetic UIs have a significantly higher perceived user experience and presence while the usability remains similar. Intradiegetic UIs are preferred by the majority. Based on these findings, we compared intradiegetic VR-questionnaires with paper-based evaluations in a follow up study (N = 24). 67% of the participants preferred the evaluation in VR, even though it takes significantly longer. We found no effect on presence.",Evaluation; INVR- questionnaires; Presence; User Interface (UI); Virtual Reality (VR),Title_Keywords,True,
Scopus,conferencePaper,2020,Effects of Immersive Virtual Reality Content Type to Mindfulness and Physiological Parameters,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) has been applied as a complimentary way to conventional treatment for mental disorders successfully. On the other hand, it has not been clearly shown what type of immersive media such as VR can directly affect one’s physiological parameters, associated with the state of mindfulness. We sought to assess how being subjected to differently designed VR contents can affect and modulate one’s anxiety both psychologically and more importantly physiologically. We empirically tested the comparative effects of two polarizing VR content types to this effect: (1) “calm/soothing” content and (2) “disturbing”. Twenty-five adults participated and their mental state, anxiety level and physiological signals were measured before and after experiencing the respective VR content type. The experiment found a statistically significant effect of the content type to the changes in these measures and confirmed that the “calm” content was helpful for one to self-regulate to lower heart rate and blood pressure, stable GSR, and the “disturbing” content in the opposite way. We applied this result to calm down and stabilize vital signs of patients during actual coronary angiography and catheterization operations. We were able to observe the same effect with positive comments from the patients and operating team.",Blood Pressure; Galvanic Skin Response (Skin Conductance); Haptic feedback; Heart Rate; Nervous System; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Anonymity vs. Familiarity: Self-Disclosure and Privacy in Social Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Understanding how and why users reveal information about their self in online social spaces and what they perceive as privacy online is a central research agenda in HCI. Drawing on 30 in-depth interviews, in this paper we focus on what type of information users disclose, to whom they reveal information, and concerns they had regarding self-disclosure in social Virtual Reality (VR) - where multiple users can interact with one another through VR head-mounted displays in 3D virtual spaces. Our findings show that overall, users felt comfortable to disclose their emotions, personal experience, and personal information in social VR. However, they also acknowledged that disclosing personal information in social VR was an inevitable trade-off: giving up bio-metric information in order to better use the system. We contribute to existing literature on self-disclosure and privacy online by focusing on social VR as an emerging novel online social space. We also explicate implications for designing and developing future social VR applications.",digital privacy; online social interaction; self-disclosure; social virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,A Quest for Co-Located Mixed Reality: Aligning and Assessing SLAM Tracking for Same-Space Multi-User Experiences,VRST - Virtual Reality Software and Technology,A,"Current solutions for creating co-located Mixed Reality (MR) experiences typically rely on platform-specific synchronisation of spatial anchors or Simultaneous Localisation and Mapping (SLAM) data across clients, often coupled to cloud services. This introduces significant costs (in development and deployment), constraints (with interoperability across platforms often limited), and privacy concerns. For practitioners, support is needed for creating platform-agnostic co-located MR experiences. This paper explores the utility of aligned SLAM solutions by 1) surveying approaches toward aligning disparate device coordinate spaces, formalizing their theoretical accuracy and limitations; 2) providing skeleton implementations for audience-based, small-scale and large-scale co-location using said alignment approaches; and 3) detailing how we can assess the accuracy and safety of 6DoF/SLAM tracking solutions for any arbitrary device and dynamic environment without the need for an expensive ground truth optical tracking, by using trilateration and a $30 laser distance meter. Through this, we hope to further democratise the creation of cross-platform co-located MR experiences.",AR; Co-location; Mixed Reality; Multi-User; SLAM; VR,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,The Effects of Self- and External Perception of Avatars on Cognitive Task Performance in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) allows embodying any possible avatar. Known as the Proteus effect, avatars can change users’ behavior and attitudes. Previous work found that embodying Albert Einstein can increase cognitive task performance. The behavioral confirmation paradigm, however, predicts that our behavior is also affected by others’ perception of us. Therefore, we investigated the cognitive performance in collaborative VR when self-perception and external perception of the own avatar differ. 32 male participants performed a Tower of London task in pairs. One participant embodied Einstein or a young adult while the other perceived the participant as Einstein or a young adult. We show that the perception by others affects cognitive performance. The Einstein avatar also decreased the perceived workload. Results imply that avatars’ appearance to both, the user and the others must be considered when designing for cognitively demanding tasks.",avatar embodiment; body ownership; cognitive performance; Proteus effect; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,The Effects of Full-Body Avatar Movement Predictions in Virtual Reality using Neural Networks,VRST - Virtual Reality Software and Technology,A,"Motion tracking technologies and avatars in virtual reality (VR) showing the movements of the own body enable high levels of presence and a strong illusion of body ownership (IBO) – key features of immersive systems and gaming experiences in virtual environments. Previous work suggests using software-based algorithms that can not only compensate system latency but also predict future movements of the user to increase input performance. However, the effects of movement prediction in VR on input performance are largely unknown. In this paper, we investigate neural network-based predictions of full-body avatar movements in two scenarios: In the first study, we used a standardized 2D Fitts’ Law task to examine the information throughput in VR. In the second study, we utilized a full-body VR game to determine the users’ performance. We found that both performance and subjective measures in a standardized 2D Fitts’ law task could not benefit from the predicted avatar movements. In an immersive gaming scenario, however, the perceived accuracy of the own body location improved. Presence and body assessments remained more stable and were higher than during the Fitts’ task. We conclude that machine-learning-based predictions could be used to compensate system-related latency but participants only subjectively benefit under certain conditions.",avatars; movement prediction; neural networks.; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Realistic Virtual Humans from Smartphone Videos,VRST - Virtual Reality Software and Technology,A,"This paper introduces an automated 3D-reconstruction method for generating high-quality virtual humans from monocular smartphone cameras. The input of our approach are two video clips, one capturing the whole body and the other providing detailed close-ups of head and face. Optical flow analysis and sharpness estimation select individual frames, from which two dense point clouds for the body and head are computed using multi-view reconstruction. Automatically detected landmarks guide the fitting of a virtual human body template to these point clouds, thereby reconstructing the geometry. A graph-cut stitching approach reconstructs a detailed texture. Our results are compared to existing low-cost monocular approaches as well as to expensive multi-camera scan rigs. We achieve visually convincing reconstructions that are almost on par with complex camera rigs while surpassing similar low-cost approaches. The generated high-quality avatars are ready to be processed, animated, and rendered by standard XR simulation and game engines such as Unreal or Unity.",3D Reconstruction; Avatars; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2020,The Experience of Social Touch in Multi-User Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We present user study results on virtual body contact experience in a two-user VR scenario, in which participants performed different touches with a research assistant. The interaction evoked different emotional reactions in perceived relaxation, happiness, desire, anxiety, disgust, and fear. Congruent to physical social touch, the evaluation of virtual body contact was modulated by intimacy, touch direction, and sex. Further, individual comfort with interpersonal touch was positively associated with perceived relaxation and happiness. We discuss the results regarding implications for follow-up studies and infer implications for the use of social touch in social VR applications.",mediated social touch; multi-user VR; social touch; social VR; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2020,Virtual Humans in AR: Evaluation of Presentation Concepts in an Industrial Assistance Use Case,VRST - Virtual Reality Software and Technology,A,"Embedding virtual humans in educational settings enables the transfer of the approved concepts of learning by observation and imitation of experts to extended reality scenarios. Whilst various presentation concepts of virtual humans for learning have been investigated in sports and rehabilitation, little is known regarding industrial use cases. In prior work on manual assembly, Lampen et al.&nbsp;[21] show that three-dimensional (3D) registered virtual humans can provide assistance as effective as state-of-the-art HMD-based AR approaches. We extend this work by conducting a comparative user study (N=30) to verify implementation costs of assistive behavior features and 3D registration. The results reveal that the basic concept of a 3D registered virtual human is limited and comparable to a two-dimensional screen aligned presentation. However, by incorporating additional assistive behaviors, the 3D assistance concept is enhanced and shows significant advantages in terms of cognitive savings and reduced errors. Thus, it can be concluded, that this presentation concept is valuable in situations where time is less crucial, e.g. in learning scenarios or during complex tasks.",Augmented Reality; Expert-Based Learning; Virtual Human,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Don’t Bother Me: How to Handle Content-Irrelevant Objects in Handheld Augmented Reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we applied the concept of diminished reality to remove content-irrelevant pedestrian (i.e., real object) in the context of handheld augmented reality (AR). We prepared three view conditions: in Transparent (TP) condition, we removed the pedestrian entirely; in Semi-transparent (STP) condition, the pedestrian became semi-transparent; lastly, in Default (DF) condition, the pedestrian appeared as is. We conducted a user study to compare the effects of the three conditions on users’ engagement and perception of a virtual pet in the AR content. Our findings revealed that users felt less distracted to the AR content in TP and STP conditions, compared to the DF condition. Furthermore, users felt the virtual pet as more life-like, its behavior more plausible, and felt a higher spatial presence in the real environment, in the TP condition.",Diminished reality; Online user study; Perceptual issue,Title_Abstract,True,
Scopus,conferencePaper,2020,Bacterial Load of Virtual Reality Headsets,VRST - Virtual Reality Software and Technology,A,"As commodity virtual reality (VR) systems become more common, they are rapidly gaining popularity for entertainment, education, and training purposes. VR utilizes headsets which come in contact with or close proximity to the user’s eyes, nose, and forehead. In this study, the potential for these headsets to become contaminated with bacteria was analyzed. To the best of our knowledge, this study is the first to address the potential for microorganisms to be transmitted via VR headsets. The data discussed herein were collected roughly one year prior to the outbreak of the COVID-19 pandemic in the United States. We feel it is important to be clear that this study focuses exclusively on bacteria, as opposed to viruses like those responsible for the present pandemic. The nosepieces and foreheads of two HTC Vive headsets were sampled over the course of a seven-week period in a VR software development course. Serial dilutions were performed, and samples were plated on various culture media. Following incubation, counts of bacteria were determined. DNA was extracted from bacterial colonies and the 16S rRNA gene was sequenced to identify bacterial contaminates present on the headsets. Chief among these contaminates was Staphylococcus aureus. The results of these tests indicated that the Staphylococcus aureus strains isolated from the headsets possessed high levels of antibiotic resistance. Other notable bacterial isolates included Moraxella osloensis, the bacteria responsible for foul odors in laundry and, Micrococcus luteus, a communalistic bacterial species capable of causing opportunistic infections. Other bacterial isolates were detected in variable amounts throughout the trial.",bacteria; hygine; pathogen; sanitation; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Dynamic Projection Mapping of Deformable Stretchable Materials,VRST - Virtual Reality Software and Technology,A,"We present a method for dynamic projection mapping on deformable, stretchable and elastic materials (e.g. cloth) using a time of flight (ToF) depth camera (e.g. Azure Kinect or Pico-Flexx) that come equipped with an IR camera. We use Bezier surfaces to model the projection surface without explicitly modeling the deformation. We devise an efficient tracking method that tracks the boundary of the surface material using the IR-Depth camera. This achieves realistic mapping even in the interior of the surface, with simple markers (e.g. black dots or squares) or without markers entirely, such that the projection appears to be printed on the material. The surface representation is updated in real-time using GPU based computations. Further, we also show that the speed of these updates is limited by the camera frame rate and therefore can be adopted for higher speed cameras as well. This technique can be used to project on several stretchable moving materials to change their appearance.",Appearance Editing; Deformable Materials; Dynamic Projection Mapping; Spatially Augmented Reality,Keywords,True,
Scopus,conferencePaper,2020,AffectivelyVR: Towards VR Personalized Emotion Recognition,VRST - Virtual Reality Software and Technology,A,"We present AffectivelyVR, a personalized real-time emotion recognition system in Virtual Reality (VR) that enables an emotion-adaptive virtual environment. We used off-the-shelf Electroencephalogram (EEG) and Galvanic Skin Response (GSR) physiological sensors to train user-specific machine learning models while exposing users to affective 360° VR videos. Since emotions are largely dependent on interpersonal experiences and expressed in different ways for different people, we personalize the model instead of generalizing it. By doing this, we achieved an emotion recognition rate of 96.5% using the personalized KNN algorithm, and 83.7% using the generalized SVM algorithm.",EEG; Emotion Recognition; GSR; Machine Learning; Personalized; Physiology; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,An Examination of Effects of Fear Using VR Content on Time Estimation,VRST - Virtual Reality Software and Technology,A,"In recent years, the use of virtual reality (VR) attractions has been increasing in amusement facilities with the spread of head-mounted displays (HMDs) and the increase of VR content. VR attractions require less physical space than conventional attractions. However, because it takes a considerable time to adjust an HMD one by one, not many customers can play the attractions. In order to provide VR content with a long subjective time in a short time, this study presents VR content on an HMD that can evoke fear. Fear is one of the factors that influence psychological time. We investigate whether the VR content with fear influences time estimation.",fear; time estimation; VR,Abstract,True,
Scopus,conferencePaper,2020,An Examination of Influence on Weight Perception by Visual Change of the Own Arm,VRST - Virtual Reality Software and Technology,A,"The purpose of this research is to make a change in weight perception by utilizing the potential impression that a person has acquired from visual information. Recently, the illusion caused by changes in visual information, which is represented by the rubber hand illusion and the Proteus effect, has been reported. In this research, we make one’s self-awareness change by the appearance of the own arm through AR technology. We conduct an experiment to verify that the change of the self-awareness influences the weight perception of grasping an actual object.",augmented reality; visual perception; VR psychology,Keywords,True,
Scopus,conferencePaper,2020,An Immersive Decision Support System for Disaster Response,VRST - Virtual Reality Software and Technology,A,"This project introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be accessed via desktop and mobile devices as well as virtual and augmented reality headsets. The framework can generate realistic simulations of desired locations entailing the terrain, elevation model, infrastructures (e.g. buildings, roads, bridges), dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, surveillance data, occupancy, traffic, weather). The framework incorporates multiuser support to allow stakeholders to remotely work on the same VR environment, and thus, presenting the potential to be utilized as a virtual incident command platform or meeting room. To demonstrate the framework's usability and benefits, several case studies have been developed for flooding, wildfire, transportation, and active shooter response.",decision-support systems; environmental management; geospatial visualization; virtual reality; web-based interaction,Abstract_Keywords,True,
Scopus,conferencePaper,2020,An Open Framework for Infinite Walking With Saccadic Redirection,VRST - Virtual Reality Software and Technology,A,"In this project we created an expandable framework for allowing infinite walking in virtual reality in a closed play area. A saccade is a rapid eye movement with a unique property: the eye temporarily gathers reduced information – saccadic suppression. We leverage the suppression to redirect the user’s walking towards the center of the play area by rotating the virtual world around the camera’s location. With the VR environment and corresponding pre and post experience questions we could already show an improvement in understanding on a set of participants. Modern VR hardware such as the Vive Eye Pro allows a reasonable sample rate of eye movement measurements. A self-developed VR testing environment was used and with corresponding pre and post experience questions we tested a group of participants regarding general motion and VR- sickness parameters. We found a certain angle for the maximum saccade rotation which was base of further testing. We found, that our framework and the default settings successfully allow saccadic redirection with only marginal discomfort for the users.",eye tracking; mobility in virtual reality; virtual reality sickness,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Analyzing the Trade-off between Selection and Navigation in VR,VRST - Virtual Reality Software and Technology,A,"Navigation and selection are critical in very large virtual environments, such as a model of a whole city. In practice, many VR applications require both of these modalities to work together. We compare different combinations of two navigation and two selection methods in VR on selection tasks involving distant targets in a user study. The aim of our work is to discover the trade-off between navigation and selection techniques and to identify which combination leads to better interaction performance in large virtual environments. The results showed that users could complete the task faster with the fly/drive method and traveled less, compared to the teleportation method. Additionally, raycasting exhibited a better performance in terms of time and (less) distance traveled, however, it significantly increased the error rate for the selection of targets.",Interaction Design; Navigation; Selection; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2020,ARLS: An asymmetrical remote learning system for sharing anatomy between an HMD and a light field display,VRST - Virtual Reality Software and Technology,A,"VR remote learning is an environment-friendly approach for anatomy learning compared with the paper-based, physical model or prepared specimens. However, VR devices can potentially be costly pieces of equipment, and inexperienced users can experience unwanted symptoms like motion sickness. To solve these problems, an asymmetrical system has been developed to connect an experienced head-mounted-display user (lecturer) with light field display users (students) through the Internet. The scenes of lecturer’s end and students’ end are adjusted to match the corresponding displays technologies.",education; light field display; remote learning; scientific visualization; Virtual reality,Keywords,True,
Scopus,conferencePaper,2020,ARtist: Interactive Augmented Reality for Curating Children's Artworks,VRST - Virtual Reality Software and Technology,A,"ARtist is a mobile app that allows children to curate, display, and document their artworks through mobile augmented reality technology. This application aims to improve the traditional art display environment with augmented reality technology which enables users to utilize virtual space freely with interactive assets. It allows users to upload images of their artworks and decorate them with provided 3D frames and pedestals. Users can place their artworks and modify them in the augmented environment. ARtist is designed for children through a user-centered design process and developed using Unity and Google ARcore.",Art Education; Augmented Reality; Mobile Augmented Reality; Virtual Art Exhibition,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Assessing the Suitability and Effectiveness of Mixed Reality Interfaces for Accurate Robot Teleoperation,VRST - Virtual Reality Software and Technology,A,"In this work, a Mixed Reality (MR) system is evaluated to assess whether it can be efficiently used in teleoperation tasks that require an accurate control of the robot end-effector. The robot and its local environment are captured using multiple RGB-D cameras, and a remote user controls the robot arm motion through Virtual Reality (VR) controllers. The captured data is streamed through the network and reconstructed in 3D, allowing the remote user to monitor the state of execution in real time through a VR headset. We compared our method with two other interfaces: i) teleoperation in pure VR, with the robot model rendered with the real joint states, and ii) teleoperation in MR, with the rendered model of the robot superimposed on the actual point cloud data. Preliminary results indicate that the virtual robot visualization is better than the pure point cloud for accurate teleoperation of a robot arm.",Mixed Reality; Robot Teleoperation; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,"ATOM: HMD-VR Interface to Learn Atomic Structure, Bonding and Historical Research Experiments.",VRST - Virtual Reality Software and Technology,A,"Head-Mounted Displays (HMDs) based Virtual Reality (VR) has shown promising results in training and education. We present ATOM, an HMD-VR interface to educate students about atoms, atomic structures and historical research experiments conducted in understanding atomic structures. ATOM is designed to complement the classroom learning for grade 9 students through an interactive and practice-based learning experience. Preliminary evaluation with 10 students revealed higher interest, increase engagement and playfulness. The students also pointed out a few difficult user interactions.",Atomic structures; Chemistry education; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Augmented Reality World Editor,VRST - Virtual Reality Software and Technology,A,"Image inpainting allows for filling masked areas of an image with synthesized content that is indistinguishable from its environment. We present a video inpainting pipeline that enables users to “erase” physical objects in their environment using a mobile device. The pipeline includes an augmented reality application and an on-device conditional adversarial model for generating the inpainted textures. Users are able to interactively remove clutter in their physical space in realtime. The pipeline preserves frame to frame coherence, even with camera movements, using the Google ARCore SDK.",Augmented Reality; Conditional GANs; Video Inpainting,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,CasualVRVideos: VR videos from casual stationary videos,VRST - Virtual Reality Software and Technology,A,"Thanks to the ubiquity of devices capable of recording and playing back video, the amount of video files is growing at a rapid rate. Most of us have now video recordings of major events in our lives. However, until today, these videos are captured mainly in 2D and are mostly used for screen-based video replay. Currently there is no way for watching them in more immersive environments such as on a VR headset. They are simply not optimized for playback in stereoscopic displays or even tracked Virtual Reality devices. In this work, we present CasualVRVideos, a first approach that works towards solving these issues by extracting spatial information from video footage recorded in 2D, so that it can later be played back in VR displays to increase the immersion. We focus in particular on the challenging scenario when the camera itself is not moving.",content creation; single view geometry; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Computing Object Selection Difficulty in VR using Run-time Contextual Analysis,VRST - Virtual Reality Software and Technology,A,"This paper introduces a method for computing the difficulty of selection tasks in virtual environments using pointing metaphors by operationalizing an established human motor behavior model. In contrast to previous work, the difficulty is calculated automatically at run-time for arbitrary environments. We present and provide the implementation of our method within Unity 3D. The difficulty is computed based on a contextual analysis of spatial boundary conditions, i.e., target object size and shape, distance to the user, and occlusion. We believe our method will enable developers to build adaptive systems that automatically equip the user with the most appropriate selection technique according to the context. Further, it provides a standard metric to better evaluate and compare different selection techniques.",3DUI; Application; Performance Model; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2020,Creating a Smart Virtual-Reality based Contextual Diary for People with Persistent Postural-Perceptual Dizziness to Facilitate Habituation,VRST - Virtual Reality Software and Technology,A,"Postural-Perceptual Dizziness (PPPD) has variable levels of severity and triggers. Hence the use of an e-diary to capture triggers could be useful for both the patient and treating clinician. Virtual reality (VR) is not new to health sciences. This paper proposes a strategy that by using immersive VR environments at home, the technology could facilitate the user to identify baseline symptoms and record in an in-built virtual-reality based contextual diary (e-diary), plus an ability to alter the virtual environments to assess triggers, habituation of triggers and also treatment improvements. We discuss the type of VR designs that could be useful to incorporate a PPPD e-diary from the perspective of a VR designer. We also consider the development of the virtual reality environment that could be paired with e-diary responses.",artifical intelligence; habituation; persistent postural-perceptual dizziness; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Creating AR Applications for the IOT : a New Pipeline,VRST - Virtual Reality Software and Technology,A,"Prototyping Augmented Reality (AR) applications for smart environments is still a difficult task. Therefore, we propose a pipeline to help designers and developers to create AR applications for monitoring and controlling indoor environments equipped with connected objects. This pipeline starts with the capture (geometry and objects) of the real environment with an AR device. Then, it proposes a Virtual Reality (VR) tool to configure augmentations in this captured environment. This tool includes a feature to simulate AR devices to help anticipate the application’s rendering on real devices. The created application can then be seamlessly deployed on various AR devices including smartphones,tablets and glasses.",Augmented Reality; IoT; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Dual Phone AR: Exploring the use of Phones as Controllers for Mobile Augmented Reality,VRST - Virtual Reality Software and Technology,A,"The possible interactions with Mobile Augmented Reality applications today are largely limited to on-screen gestures and spatial movement. There is an opportunity to design new interaction methods that address common issues and go beyond the screen. Through this project, we explore the idea of using a second phone as a controller for mobile AR experiences. We develop prototypes that demonstrate the use of a second phone controller for tasks such as pointing, selecting, and drawing in 3D space. We use these prototypes and insights from initial remote evaluations to discuss the benefits and drawbacks of such an interaction method. We conclude by outlining opportunities for future research on Dual Phone AR for multiple usage configurations, and in collaborative settings.",Augmented Reality; Cross-Device Computing; Mobile Interaction,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Evaluation of Headset-based Viewing and Desktop-based Viewing of Remote Lectures in a Social VR Platform,VRST - Virtual Reality Software and Technology,A,"We study experiences of students attending classes remotely from home using a social VR platform, considering both desktop-based and headset-based viewing of remote lectures. Ratings varied widely. Headset viewing produced higher presence overall. Strong negative correlations between headset simulator sickness symptoms and overall experience ratings, and some other ratings, suggest that the headset experience was much better for comfortable users than for others. Reduced sickness symptoms, and no similar correlations, were found for desktop viewing. Desktop viewing appears to be a good alternative for students not comfortable with headsets. Future VR systems are expected to provide more stable and comfortable visuals, providing benefits to more users.",COVID-19; distance learning; educational VR; Mozilla Hubs; remote instruction; SARS-CoV-2; teleconferencing; virtual reality,Keywords,True,
Scopus,conferencePaper,2020,HexTouch: Affective Robot Touch for Complementary Interactions to Companion Agents in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"There is a growing need for social interaction in Virtual Reality (VR). Current social VR applications enable human-agent or interpersonal communication, usually by means of visual and audio cues. Touch, which is also an essential method for affective communication, has not received as much attention. To address this, we introduce HexTouch, a forearm-mounted robot that performs touch behaviors in sync with the behaviors of a companion agent, to complement visual and auditory feedback in virtual reality. The robot consists of four robotic tactors driven by servo motors, which render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy). We demonstrate HexTouch through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase the immersion of the VR experience. The player will receive affective haptic cues while collaborating with the agent to complete the mission in the game. The multisensory system for affective communication also has the potential to enhance sociality in the virtual world.",Emotion Communication; Expressive Robotics; Haptics; Physical Contact; Virtual Reality; Wearable,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,HMDPose: A large-scale trinocular IR Augmented Reality Glasses Pose Dataset,VRST - Virtual Reality Software and Technology,A,"Augmented Reality Glasses usually implement an Inside-Out tracking. In case of a driving scenario or glasses with less computation capabilities, an Outside-In tracking approach is required. However, to the best of our knowledge, no public datasets exist that collects images of users wearing AR glasses. To address this problem, we present HMDPose, an infrared trinocular dataset of four different AR Head-mounted displays captured in a car. It contains sequences of 14 subjects captured by three different cameras running at 60 FPS each, adding up to more than 3,000,000 labeled images in total. We provide a ground truth 6DoF-pose, captured by a submillimeter accurate marker-based tracker. We make HMDPose publicly available for non-profit, academic use and non-commercial benchmarking on ags.cs.uni-kl.de/datasets/hmdpose/.",AR Glasses; Dataset; Deep Learning; Object Pose Estimation; Tracking,Title_Abstract,True,
Scopus,conferencePaper,2020,Human Following Behavior In Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Redirected Walking (RDW) allows users to perform real walking in virtual worlds that are larger than the available physical space. Many RDW algorithms rely on the prediction of users’ possible paths in the virtual environment (VE) to calculate where users should be redirected to. This prediction could be obtained from the structure of the VE, where users look, or from existing path models. In this work, we examine users’ walking behaviors in the presence of a virtual agent acting as a tour guide. Results showed that users changed their speed significantly to match the agent’s walking speed. Furthermore, users also tend to adapt their trajectories to match with the agent’s path.",following behaviour; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2020,Impact of Social Distancing to Mitigate the Spread of COVID-19 in a Virtual Environment,VRST - Virtual Reality Software and Technology,A,"A novel strand of Coronavirus has spread in the past months to the point of becoming a pandemic of massive proportions. In order to mitigate the spread of this disease, many different policies have been adopted, including a strict national lockdown in some countries or milder government policies: one common aspect is that they mostly rely around keeping distance between individuals. The aim of this work is to provide means of visualizing the impact of social distancing in an immersive environment by making use of the virtual reality technology. To this aim, we create a virtual environment which resembles a university setting (we based it on the University of Derby), and populate it with a number of AI agents. We assume that the minimum social distance is 2 meters. The main contribution of this work is twofold: the multi-disciplinary approach that results from visualizing the social distancing in an effort to mitigate the spread of the COVID-19, and the digital twin application in which the users can navigate the virtual environment whilst receiving visual feedback in the proximity of other agents. We named our application SoDAlVR, which stands for Social Distancing Algorithm in Virtual Reality.",Digital Twin; Prototyping/Implementation; Simulation.; Usability Study; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Inconsistencies of Presence Questionnaires in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Presence in virtual reality (VR) is typically assessed through questionnaires in the real world and after leaving an immersive experience. Previous research suggests that questionnaires in VR reduce biases caused by the real-world setup. However, it remains unclear whether presence questionnaires still provide valid results when subjects are being surveyed while the construct is perceived. In a user study with 36 participants, two standardized presence questionnaires (IPQ, SUSa) were either completed in the real lab, in a virtual lab scene, or in the actual scene after a virtual gaming experience. Our results show inconsistencies between the measurements and that main scores, as well as subscales of the presence measures are significantly affected by the subjects’ environment. As presence questionnaires have been designed to be answered after an immersive experience, we recommend revising those tools for measuring presence in VR.",break in presence.; presence; questionnaire; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,"Introduction to AR-Bot, an AR system for robot navigation",VRST - Virtual Reality Software and Technology,A,"We introduce a system to assign navigation tasks to a self-moving robot using an Augmented Reality (AR) application running on a smartphone. The system relies on a robot controller and a central server hosted on a PC. The user points at a target location in the phone camera view and the robot moves accordingly. The robot and the phone are independently located in the 3D space thanks to registration methods running on the server, hence they do not need to be spatially registered to each other nor in direct line of sight.",Augmented Reality; Registration; Relocalization; Robot navigation; User interface,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Life-size Sequential Photography in a Mixed Reality Environment,VRST - Virtual Reality Software and Technology,A,"We visualize life-size sequential photographs of sports activities in a mixed reality (MR) environment. Wearing a video-see-through head-mounted display, an observer records the motions of a player using a handheld camera. Our system then places billboards in the three-dimensional MR space, on which the sequential photographs of the player’s motion are presented at life-size. In a user study, we found that the observers perceived the size of the motions more accurately than when viewing sequential photographs on a monitor display.",Life-size Sequential Photography; Mixed Reality; sports motion,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Mediated-Timescale Learning: Manipulating Timescales in Virtual Reality to Improve Real-World Tennis Forehand Volley,VRST - Virtual Reality Software and Technology,A,"In tennis training, beginner players can fail to return the ball when the ball moves faster than what they can react to. In this paper, we propose a new training process of mediated-timescale learning (MTL) to manipulate the incoming ball’s motion. The ball first moves in slow motion, allowing more time for players to react and develop skills. The ball then moves in faster motion, challenging players with improved skills. To evaluate MTL, we implemented it in a virtual reality (VR)-oriented tennis training system. We piloted the MTL implementations (N = 12) to study players’ physical enjoyment. We then conducted an efficacy study (N = 8) to evaluate MTL’s training effects on player’s real-world performance. We found that in comparison to real-world training, five participants improved more in hitting the sweet spot after training with MTL.",Tennis training; timescales; training effects; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Metachron: A framework for time perception research in VR,VRST - Virtual Reality Software and Technology,A,"The perception of time is closely related to our well-being. Psycho-pathological conditions such as depression, schizophrenia and autism are often linked to a disturbed sense of time. In this paper we present a novel framework called Metachron, which is intended to support research in the field of time perception and manipulation in Virtual Reality (VR). Our system allows the systematic modification of events in real time along the three main event axes i) Velocity, ii) Syncronicity and iii) Density. Our future work will investigate the influence of each dimension on the passage of time (varying velocity of time flow) and the structure of time (varying synchronicity of events), which should provide insights for the design of VR diagnostic and therapeutic tools.",framework; therapy; time perception manipulation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Molecular MR Multiplayer: a cross-platform collaborative interactive game for scientists,VRST - Virtual Reality Software and Technology,A,"We present Molecular MR Multiplayer – an interactive, collaborative solution for material design on the edge devices on flat screens and in Mixed Reality (MR)&nbsp;[3]. The application provides a concept of emerging gaming-like remote collaboration experience for researchers, helping explore and design chemical compounds, Both single-user and multi-user modes are implemented. The multi-user mode allows mixed collaborative sessions between local and distant users. A concept of the digital overlays over physical world known as Metaverse is developed.",Interactive; Material Design; Mixed Reality; Multiplayer,Abstract_Keywords,True,
Scopus,conferencePaper,2020,On the Interplay of Foveated Rendering and Video Encoding,VRST - Virtual Reality Software and Technology,A,Humans have sharp central vision but low peripheral visual acuity. Prior work has taken advantage of this phenomenon in two ways: foveated rendering (FR) reduces the computational workload of rendering by producing lower visual quality for peripheral regions and foveated video encoding (FVE) reduces the bitrate of streamed video through heavier compression of peripheral regions. Remote rendering systems require both rendering and video encoding and the two techniques can be combined to reduce both computing and bandwidth consumption. We report early results from such a combination with remote VR rendering. The results highlight that FR causes large bitrate overhead when combined with normal video encoding but combining it with FVE can mitigate it.,cloud rendering; foveated rendering; video encoding; virtual reality,Keywords,True,
Scopus,conferencePaper,2020,Physicalizing Virtual Objects with Affordances to Support Tangible Interactions in AR,VRST - Virtual Reality Software and Technology,A,"When interacting with virtual objects, we could discover some problems around us. Digital information has no physical properties, and sense organ(vision, tactile) does not match very well. These have a negative influence on the user’s interactive experience. In this paper, we propose to provide virtual objects with no physical properties with physical affordances to support tangible feedback via mechanical movement. In addition, we developed an interaction prototype system. The system could change the physical supports that are absorbed onto an electromagnet to adapt to the shape of different virtual objects and efficiently provide natural and consistent interactions when putting physical objects onto virtual objects in Augmented Reality (AR) scenarios.",affordance; Augmented Reality; physiaclizing virtual objects; tangible interaction,Abstract_Keywords,True,
Scopus,conferencePaper,2020,Portals With a Twist: Cable Twist-Free Natural Walking in Room-Scaled Virtual Reality,VRST - Virtual Reality Software and Technology,A,"To provide naturally walking in small virtual reality (VR) tracking spaces while preventing cables of head-mounted displays (HMDs) getting twisted, we developed Portals With A Twist (PorTwist), a redirected walking method using a portal metaphor. We compared PorTwist with a teleportation method in a 2m × 2.7m tracking space in a within-design user study (N = 34). PorTwist resulted in significantly longer natural walking distances and could prevent HMD cables from getting twisted while providing comparable levels of perceived presence and simulator sickness as teleportation. We further identified potential to improve usability in the future.",navigation; redirected walking; room-scaled virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,RoadVR: Mitigating the Effect of Vection and Sickness by Distortion of Pathways for In-Car Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We explore a method to reduce motion sickness and allow people to use virtual reality while moving in vehicles. We put forth a usage scenario where the target VR content is based on constant road navigation so that the actual motion can enhance the VR experience. The method starts with a virtual scene and objects around an infinitely straight road. The motion of the vehicle is sensed by the GPS and IMU module. The sensed motion is reflected in a way that the virtual scene is navigated according to the vehicle motion, and its pathways distorted such that the virtual motion has a near-identical optical flow pattern to the actual. This would align the user’s visual and vestibular sense and reduce the effect of vection and motion sickness. We ran an pilot experiment to validate our approach, comparing the before and after sickness levels with the VR content (1) not aligned to the motion of the vehicle and (2) aligned by our method. Our preliminary results have shown the sickness was reduced significantly (but not eliminated to a negligible level yet) with our approach.",Distortion; Motion Sickness; Navigation; Simulator Sickness; Vection; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2020,Smart Thermo-Haptic Bracelet for VR Environment,VRST - Virtual Reality Software and Technology,A,"We propose a lightweight smart haptic bracelet-based stimulation system for VR applications. This wireless system equipped with vibrotactile tactors and peltier actuator, generates different haptic/thermal levels for different materials being touched in a VR environment.",embedded system; haptic and thermal feedback system; virtual reality,Keywords,True,
Scopus,conferencePaper,2020,Teaching Scrum with a Virtual Sprint Simulation: Initial Design and Considerations,VRST - Virtual Reality Software and Technology,A,"Scrum is a well-developed and utilized agile project management framework, which requires extensive training and hands-on experience to master. The latter is not always possible, i.e. during the recent lockdown due to COVID-19. Thus, we propose the creation of a multi-user collaborative virtual simulation of a Scrum sprint that can provide an immersive training experience to remote trainees. Herein, we discuss the design considerations, elements of the virtual learning environment and the development process for the platform.",Education; Project Management; Software Engineering; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2020,Text Input Methods in Virtual Reality using Radial Layouts,VRST - Virtual Reality Software and Technology,A,"Currently, the most popular text input method in VR is Controller Pointing (CP). While this method is easy and intuitive to use, it requires users to have steady hands, and the overlaying of the keyboard onto the virtual scene occludes a part of the scene. In this work, we proposed two new text input methods: Sector Input and T9VR , that utilize a circular keyboard that is attached to the HTC Vive controller. A preliminary study with 24 subjects was conducted to explore the potential of the proposed methods, in comparison with CP. While CP performed significantly better than the proposed methods in terms of objective typing speed and error rate, T9VR was able to match with CP in a number of subjective measures.",,Title,True,
Scopus,conferencePaper,2020,The Digital Docent: XR storytelling for a Living History Museum,VRST - Virtual Reality Software and Technology,A,"In this work, we describe the use of a digital docent: a 3D avatar, presented using virtual and augmented reality, as a means for providing interactive storytelling experiences at a living history museum. To allow flexibility depending on the user’s location and access to technology, the app is designed to provide a common experience supporting a variety of different delivery modalities including AR devices, mobile AR, and VR on the Web.",Arts and humanities; Augmented and virtual realities; Living Museums; Media Arts,Abstract,True,
Scopus,conferencePaper,2020,Viewing Style of Augmented Reality/Virtual Reality Broadcast Contents while Sharing a Virtual Experience,VRST - Virtual Reality Software and Technology,A,"A conceptual space-sharing broadcasting service has been proposed, using augmented reality/virtual reality (AR/VR) with a head-mounted display. As proposed, virtual performers are displayed in their real-life sizes; the user experiences proximity to them. Family and friends living apart can also be displayed in this manner, and an individual can communicate with them in real time; this enables both the individual and their peer to enjoy the broadcast media together. In this study, we implemented this concept as a suitable style for daily use and confirmed the effect of the viewing experience. We developed a prototype of an environment for watching AR/VR mixed content along with a person in a distant place, which is expected to become a popular viewing style of future broadcast media. The individual is displayed as a live-action 3D point cloud image, and verbal and nonverbal communication with the individual are enabled. A demonstration showed that the system renders a sense of presence to a distant person and provides the feeling of sharing the same experience among all its users.",AR/VR; Broadcast; Telepresence; Virtual Space-sharing,Title_Abstract,True,
Scopus,conferencePaper,2020,Volumetric capture for narrative films,VRST - Virtual Reality Software and Technology,A,"Volumetric capture is a technique that allows to create “holographic” recordings of actors, sets and props. The technique can be used to create immersive stories that sometimes reflect aspects of reality better than realistic 3D models. For example, volumetric captures of actors do not seem to cause uncanny valley effect. In this paper, we provide an overview of volumetric capture technology and its application to narrative filmmaking.",augmented reality; interactive narrative; mixed reality; photogrammetry; virtual reality; volumetric capture; volumetric film,Keywords,True,
Scopus,conferencePaper,2020,VR Training for Warehouse Management,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) has evolved into a trending technology that has proven its worth in various application domains. VR is especially helpful for the training of complex tasks in special environments. In the area of logistics, warehouse management involves a complex workflow that consists of different order picking activities. Due to this complex workflow, stock discrepancies and misplaced wares are typical problems that often occur. To overcome this problem, we have developed a VR training application that integrates an existing warehouse management system and trains typical order picking processes. In our VR training demo, we simulate a real warehouse that is supplied with real stocks and real orders.",logistics; usability evaluation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,"Impostor-based Rendering Acceleration for Virtual, Augmented, and Mixed Reality",VRST - Virtual Reality Software and Technology,A,"This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.",image-based rendering; impostors; rendering acceleration,Title_Abstract,True,
Scopus,conferencePaper,2021,Inside-Out Instrument Tracking for Surgical Navigation in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Surgical navigation requires tracking of instruments with respect to the patient. Conventionally, tracking is done with stationary cameras, and the navigation information is displayed on a stationary display. In contrast, an augmented reality (AR) headset can superimpose surgical navigation information directly in the surgeon’s view. However, AR needs to track the headset, the instruments and the patient, often by relying on stationary infrastructure. We show that 6DOF tracking can be obtained without any stationary, external system by purely utilizing the on-board stereo cameras of a HoloLens 2 to track the same retro-reflective marker spheres used by current optical navigation systems. Our implementation is based on two tracking pipelines complementing each other, one using conventional stereo vision techniques, the other relying on a single-constraint-at-a-time extended Kalman filter. In a technical evaluation of our tracking approach, we show that clinically relevant accuracy of 1.70 mm/1.11°&nbsp;and real-time performance is achievable. We further describe an example application of our system for untethered end-to-end surgical navigation.",Augmented Reality; HoloLens 2; Surgical Navigation; Tracking,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Non-isomorphic Interaction Techniques for Controlling Avatar Facial Expressions in VR,VRST - Virtual Reality Software and Technology,A,"The control of an avatar’s facial expressions in virtual reality is mainly based on the automated recognition and transposition of the user’s facial expressions. These isomorphic techniques are limited to what users can convey with their own face and have recognition issues. To overcome these limitations, non-isomorphic techniques rely on interaction techniques using input devices to control the avatar’s facial expressions. Such techniques need to be designed to quickly and easily select and control an expression, and not disrupt a main task such as talking. We present the design of a set of new non-isomorphic interaction techniques for controlling an avatar facial expression in VR using a standard VR controller. These techniques have been evaluated through two controlled experiments to help designing an interaction technique combining the strengths of each approach. This technique was evaluated in a final ecological study showing it can be used in contexts such as social applications.",Avatar; Emoji; Emoticons; Emotion; Facial expression; VR,Abstract,True,
Scopus,conferencePaper,2021,Ubiq: A System to Build Flexible Social Virtual Reality Experiences,VRST - Virtual Reality Software and Technology,A,"While they have long been a subject of academic study, social virtual reality (SVR) systems are now attracting increasingly large audiences on current consumer virtual reality systems. The design space of SVR systems is very large, and relatively little is known about how these systems should be constructed in order to be usable and efficient. In this paper we present Ubiq, a toolkit that focuses on facilitating the construction of SVR systems. We argue for the design strategy of Ubiq and its scope. Ubiq is built on the Unity platform. It provides core functionality of many SVR systems such as connection management, voice, avatars, etc. However, its design remains easy to extend. We demonstrate examples built on Ubiq and how it has been successfully used in classroom teaching. Ubiq is open source (Apache License) and thus enables several use cases that commercial systems cannot.",avatars; communication tools; networking; open source; social virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Investigating the Effect of Sensor Data Visualization Variances in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This paper investigates the effect of real-time sensor data variances on humans performing straightforward assembly tasks in a Virtual Reality-based (VR-based) training system. A VR-based training system has been developed to transfer color and depth images, and constructs colored point clouds data to represent objects in real-time. Various parameters that affect sensor data acquisition and visualization of remotely operated robots in the real-world are varied. Afterward, the associated task performance is observed. Experimental results from 12 participants performed a total of 95 VR-guided puzzle assembly tasks demonstrated that a combination of low resolution and uncolored points has the most significant effect on participants’ performance. Participants mentioned that they needed to rely upon tactile feedback when the perceptual feedback was minimal. The most insignificant parameter determined was the resolution of the data representations, which, when varied within the experimental bounds, only resulted in a 5% average change in completion time. Participants also indicated in surveys that they felt their performance had improved and frustration was reduced when provided with color information of the scene.",Rendering; Usability Study; Virtual Reality; Visual Perception,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Spatial Augmented Reality Visibility and Line-of-Sight Cues for Building Design,VRST - Virtual Reality Software and Technology,A,"Despite the technological advances in building design, visualizing 3D building layouts can be especially difficult for novice and expert users alike, who must take into account design constraints including line-of-sight and visibility. Using CADwalk, a commercial building design tool that utilizes floor-facing projectors to show 1:1 scale building plans, this work presents and evaluates two floor-based visual cues for assisting with evaluating line-of-sight and visibility. Additionally, we examine the impact of using virtual cameras looking from the inside-out (from user’s location to objects of interest) and outside-in (looking from an object of interest’s location back towards the user). Results show that floor-based cues led to participants more correctly rating visibility, despite taking longer to complete the task. This is an effective tradeoff, given the final outcome (the building design) where accuracy is paramount.",augmented reality; CAD; construction; design; spatial augmented reality; XR,Title_Keywords,True,
Scopus,conferencePaper,2021,ImNDT: Immersive Workspace for the Analysis of Multidimensional Material Data From Non-Destructive Testing,VRST - Virtual Reality Software and Technology,A,"An analysis of large multidimensional volumetric data as generated by non-destructive testing (NDT) techniques, e.g., X-ray computed tomography (XCT), can hardly be evaluated using standard 2D visualization techniques on desktop monitors. The analysis of fiber-reinforced polymers (FRPs) is currently a time-consuming and cognitively demanding task, as FRPs have a complex spatial structure, consisting of several hundred thousand fibers, each having more than twenty different extracted features. This paper presents ImNDT, a novel visualization system, which offers material experts an immersive exploration of multidimensional secondary data of FRPs. Our system is based on a virtual reality (VR) head-mounted device (HMD) to enable fluid and natural explorations through embodied navigation, the avoidance of menus, and manual mode switching. We developed immersive visualization and interaction methods tailored to the characterization of FRPs, such as a Model in Miniature, a similarity network, and a histo-book. An evaluation of our techniques with domain experts showed advantages in discovering structural patterns and similarities. Especially novices can strongly benefit from our intuitive representation and spatial rendering.",fibre-reinforced polymers; immersive analytics; interaction techniques; Multidimensional data visualization; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Effects of Image Realism on the Stress Response in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Safety critical situations, as they occur in professions such as medicine, nursing, and aviation, are often trained in simulators to prevent damages to personnel and material. These jobs often come with a high amount of stress, to which prolonged exposure can have devastating effects. Over the past years, stress inoculation training in conjunction with Virtual Reality has become focus of the research community and software companies. Especially the nursing profession can benefit from it, since stress-related illnesses are often the reason for an early exit from the workforce. However, since training facilities often need to compromise on their simulations due to monetary reasons, trade-offs must be made in the degree of detail of such simulations in order to keep development and acquisition costs low. One such possibility is in using low graphical fidelity. We present a psycho-physiological study on the influence of image realism of virtual environments on the stress response. In a within subject design study, we ask participants to complete nursing related, virtually recreated tasks in an artificial intensive care unit, whilst exposed to different stress factors. We provide our findings in the form of objective and subjective measures. Results show that one can elicit different stress responses by manipulating image realism in a sufficiently drastic manner. However, a life-like reaction does not seem to depend on a highly realistic environment.",image realism; nursing; stress; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,opticARe - Augmented Reality Mobile Patient Monitoring in Intensive Care Units,VRST - Virtual Reality Software and Technology,A,"German Intensive Care Units (ICUs) are in crisis, struggling with an increasing shortage of skilled workers, ultimately putting patients’ safety at risk. To counteract this process, researchers are increasingly concerned with finding digital solutions which aim to support healthcare professionals by enhancing the efficiency of reoccurring critical caring tasks and thus, improve working conditions. In this regard, this paper evaluates the application of Augmented Reality (AR) for patient monitoring for critical care nursing. Grounded on an observational study, semi-structured interviews, as well as a quantitative analysis, mobile patient monitoring scenarios, present particularly during patient transport, were identified as an innovative context of use of AR in the field. Additionally, user requirements such as high wearability, hands-free operability, and clear data representation could be derived from the obtained study results. For validation of these and identification of further requirements, three prototypes differing in their data illustration format were subsequently developed and quantitatively, as well as qualitatively evaluated by conducting an online survey. Thereby, it became evident that future implementations of a corresponding system for patient monitoring ought to integrate a context-dependent data presentation in particular, as this combines high navigability and availability of required data. Identifying patient monitoring during patient transport as a potential context of use, as well as distinguishing a context-dependent design approach as favorable constitute two key contributions of this work and provide a foundation on which future implementations of AR systems in the nursing domain and other related contexts can be established.",augmented reality; critical care; head-mounted display; patient monitoring; wearable,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,The Influence of in-VR Questionnaire Design on the User Experience,VRST - Virtual Reality Software and Technology,A,"Researchers study the user experience in Virtual Reality (VR) typically by collecting either sensory data or using questionnaires. While traditional questionnaire formats present it through web-based survey tools (out-VR), recent studies investigate the effects of presenting questionnaires directly in the virtual environment (in-VR). The in-VR questionnaire can be defined as an implemented user-interface object that allows interaction with questionnaires in VR that do not break the immersion. Integrating questionnaires directly into the virtual environment, however, also challenges design decisions. While most previous research presents in-VR questionnaires in the form of 2D panels in the virtual environment, we want to investigate the difference from such traditional formats to a presentation of a questionnaire format in the form of an interactive object as part of the environment. Accordingly, we evaluate and compare two different in-VR questionnaire designs and a traditional web-based form (out-VR) to assess user experience, the effect on presence, duration of completing the questionnaires, and users’ preferences. As the means for achieving this goal, we developed an immersive questionnaire toolkit that provides a general solution for implementing in-VR questionnaires and exchanging data with popular survey services. This toolkit enables us to run our study both on-site and remotely. As a first small study, 16 users, either on-site or remotely, attended by completing the System Usability Scale, NASA TLX, and the iGroup Presence Questionnaire after a playful activity. The first results indicate that there is no significant difference in the case of usability and presence between different design layouts. Furthermore, we could not find a significant difference also for the task load except between 2D and web-based layout for mental demand and frustration as well as the duration of completing the questionnaire. The results also indicate that users generally prefer in-VR questionnaire designs to the traditional ones. The study can be expanded to include more participants in user studies as a means of gaining more concrete results. Furthermore, additional questionnaire design alternatives can also help to provide us with a more usable and accurate questionnaire design in VR.",3D User Interface; Presence; Questionnaires; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Towards Context-aware Automatic Haptic Effect Generation for Home Theatre Environments,VRST - Virtual Reality Software and Technology,A,"The application of haptic technology in entertainment systems, such as Virtual Reality and 4D cinema, enables novel experiences for users and drives the demand for efficient haptic authoring systems. Here, we propose an automatic multimodal vibrotactile content creation pipeline that substantially improves the overall hapto-audiovisual (HAV) experience based on contextual audio and visual content from movies. Our algorithm is implemented on a low-cost system with nine actuators attached to a viewing chair and extracts significant features from video files to generate corresponding haptic stimuli. We implemented this pipeline and used the resulting system in a user study (n = 16), quantifying user experience according to the sense of immersion, preference, harmony, and discomfort. The results indicate that the haptic patterns generated by our algorithm complement the movie content and provide an immersive and enjoyable HAV user experience. This further suggests that the pipeline can facilitate the efficient creation of 4D effects and could therefore be applied to improve the viewing experience in home theatre environments.",4D effect generation; automatic haptic effect authoring; Haptics; home theatre; immersive experience,Abstract,True,
Scopus,conferencePaper,2021,The Effect of Increased Body Motion in Virtual Reality on a Placement-Retrieval Task,VRST - Virtual Reality Software and Technology,A,"Previous work has shown that increased effort and use of one’s body can improve memory. When positioning windows inside a virtual reality, does the use of a larger volume, and using one’s legs to move around, improve ability to later find the windows? The results of our experiment indicate there can be a modest benefit for spatial memory and retrieval time, but at the cost of increased time spent initially positioning the windows.",controlled experiment; locomotion; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,VRGaitAnalytics: Visualizing Dual Task Cost for VR Gait Assessment,VRST - Virtual Reality Software and Technology,A,"Among its many promising applications, Virtual Reality (VR) can simulate diverse real-life scenarios and therefore help experimenters assess individuals’ gait performance (i.e., walking) under controlled functional contexts. VR-based gait assessment may provide low-risk, reproducible and controlled virtual environments, enabling experimenters to investigate underlying causes for imbalance by manipulating experimental conditions such as multi-sensory loads, mental processing loads (cognitive load), and/or motor tasks. We present a low-cost novel VR gait assessment system that simulates virtual obstacles, visual, auditory, and cognitive loads while using motion tracking to assess participants’ walking performance. The system utilizes in-situ spatial visualization for trial playback and instantaneous outcome measures which enable experimenters and participants to observe and interpret their performance. The trial playback can visualize any moment in the trial with embodied graphic segments including the head, waist, and feet. It can also replay two trials at the same time frame for trial-to-trial comparison, which helps visualize the impact of different experimental conditions. The outcome measures, i.e., the metrics related to walking performance, are calculated in real-time and displayed as data graphs in VR. The system can help experimenters get specific gait information on balance performance beyond a typical clinical gait test, making it clinically relevant and potentially applicable to gait rehabilitation. We conducted a feasibility study with physical therapy students, research graduate students, and licensed physical therapists. They evaluated the system and provided feedback on the outcome measures, the spatial visualizations, and the potential use of the system in the clinic. The study results indicate that the system was feasible for gait assessment, and the immediate spatial visualization features were seen as clinically relevant and useful. Limitations and considerations for future work are discussed.",gait balance; obstacle crossing; playback; Spatial visualization; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Catching Jellies in Immersive Virtual Reality: A Comparative Teleoperation Study of ROVs in Underwater Capture Tasks,VRST - Virtual Reality Software and Technology,A,"Remotely Operated Vehicles (ROVs) are essential to human-operated underwater expeditions in the deep sea. However, piloting an ROV to safely interact with live ecosystems is an expensive and cognitively demanding task, requiring extensive maneuvering and situational awareness. Immersive Virtual Reality (VR) Head-Mounted Displays (HMDs) could address some of these challenges. This paper investigates how VR HMDs influence operator performance through a novel telepresence system for piloting ROVs in real-time. We present an empirical user study [N=12] that examines common midwater creature capture tasks, comparing Stereoscopic-VR, Monoscopic-VR, and Desktop teleoperation conditions. Our findings indicate that Stereoscopic-VR can outperform Monoscopic-VR and Desktop ROV capture tasks, effectively doubling the efficacy of operators. We also found significant differences in presence, task load, usability, intrinsic motivation, and cybersickness. Our research points to new opportunities towards VR with ROVs.",Cybersickness; Head-Mounted Display; Human-Operated Vehicles; Immersive Applications; Immersive Virtual Reality; Remotely Operated Vehicle; Teleoperation; Telepresence; Usability,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,BreachMob: Detecting Vulnerabilities in Physical Environments Using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"BreachMob is a virtual reality (VR) tool that applies open design principles from information security to physical buildings and structures. BreachMob uses a detailed 3D digital model of a property owner's building. The model is then published as a virtual environment (VE), complete with all applicable security measures and released to the public to test the building's security and find any potential vulnerabilities by completing specified objectives. Our paper contributes a new method of applying VR to crowd source detection of physical environment vulnerabilities. We detail the technical realization of two BreachMob prototypes (a home and an airport) reflecting on static and dynamic vulnerabilities. Our design critique suggests that&nbsp;BreachMob&nbsp;promotes user immersion by allowing participants the freedom to behave in ways that align with the experience of breaching physical security protocols.",,Title_Abstract,True,
Scopus,conferencePaper,2021,EntangleVR: A Visual Programming Interface for Virtual Reality Interactive Scene Generation,VRST - Virtual Reality Software and Technology,A,"Entanglement is a unique phenomenon in quantum physics that describes a correlated relationship in the measurement of a group of spatially separated particles. In the fields of science fiction, game design, art and philosophy, it has inspired the creation of numerous innovative works. We present EntangleVR, a novel method to create entanglement-inspired virtual scenes with the goal to simplify representing this phenomenon in the design of interactive VR games and experiences. By providing a reactive visual programming interface, users can integrate entanglement into their design without requiring prior knowledge of quantum computing or quantum physics. Our system enables fast creation of complex scenes composed of virtual objects with manipulable correlated behaviors.",3D scene creation; art; creativity; entanglement; quantum computing; virtual reality; visual programming,Title_Keywords,True,
Scopus,conferencePaper,2021,Virtual Rotations for Maneuvering in Immersive Virtual Environments,VRST - Virtual Reality Software and Technology,A,"In virtual navigation, maneuvering around an object of interest is a common task which requires simultaneous changes in both rotation and translation. In this paper, we present Anchored Jumping, a teleportation technique for maneuvering that allows the explicit specification of a new viewing direction by selecting a point of interest as part of the target specification process. A first preliminary study showed that naïve Anchored Jumping can be improved by an automatic counter rotation that preserves the user’s relative orientation towards their point of interest. In our second, qualitative study, this extended technique was compared with two common approaches to specifying virtual rotations. Our results indicate that Anchored Jumping allows precise and comfortable maneuvering and is compatible with techniques that primarily support virtual exploration and search tasks. Equipped with a combination of such complementary techniques, seated users generally preferred virtual over physical rotations for indoor navigation.",3D navigation; Jumping; Maneuvering; Target-based travel; Teleportation; Virtual reality; Virtual rotation,Keywords,True,
Scopus,conferencePaper,2021,Using Gaze Behavior and Head Orientation for Implicit Identification in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Identifying users of a Virtual Reality (VR) headset provides designers of VR content with the opportunity to adapt the user interface, set user-specific preferences, or adjust the level of difficulty either for games or training applications. While most identification methods currently rely on explicit input, implicit user identification is less disruptive and does not impact the immersion of the users. In this work, we introduce a biometric identification system that employs the user’s gaze behavior as a unique, individual characteristic. In particular, we focus on the user’s gaze behavior and head orientation while following a moving stimulus. We verify our approach in a user study. A hybrid post-hoc analysis results in an identification accuracy of up to 75&nbsp;% for an explainable machine learning algorithm and up to 100&nbsp;% for a deep learning approach. We conclude with discussing application scenarios in which our approach can be used to implicitly identify users.",eye tracking; gaze-based authentication; implicit identification; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,InteractML: Making machine learning accessible for creative practitioners working with movement interaction in immersive media,VRST - Virtual Reality Software and Technology,A,"Interactive Machine Learning offers a method for designing movement interaction that supports creators in implementing even complex movement designs in their immersive applications by simply performing them with their bodies. We introduce a new tool, InteractML, and an accompanying ideation method, which makes movement interaction design faster, adaptable and accessible to creators of varying experience and backgrounds, such as artists, dancers and independent game developers. The tool is specifically tailored to non-experts as creators configure and train machine learning models via a node-based graph and VR interface, requiring minimal programming. We aim to democratise machine learning for movement interaction to be used in the development of a range of creative and immersive applications.",artists; creative virtual reality; dancers; machine learning; movement interaction,Keywords,True,
Scopus,conferencePaper,2021,Research and Practice Recommendations for Mixed Reality Design – Different Perspectives from the Community,VRST - Virtual Reality Software and Technology,A,"Over the last decades, different kinds of design guides have been created to maintain consistency and usability in interactive system development. However, in the case of spatial applications, practitioners from research and industry either have difficulty finding them or perceive such guides as lacking relevance, practicability, and applicability. This paper presents the current state of scientific research and industry practice by investigating currently used design recommendations for mixed reality (MR) system development. We analyzed and compared 875 design recommendations for MR applications elicited from 89 scientific papers and documentation from six industry practitioners in a literature review. In doing so, we identified differences regarding four key topics: Focus on unique MR design challenges, abstraction regarding devices and ecosystems, level of detail and abstraction of content, and covered topics. Based on that, we contribute to the MR design research by providing three factors for perceived irrelevance and six main implications for design recommendations that are applicable in scientific and industry practice.",Augmented Reality; Design Recommendations; Design Theory and Practice; Guidelines; Mixed Reality; User Interface Design,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Qualitative Dimensions of Technology-Mediated Reflective Learning: The Case of VR Experience of Psychosis,VRST - Virtual Reality Software and Technology,A,"Self-reflection is evaluation of one’s inferential processes often triggered by complex social and emotional experiences, characterized by their ambiguity and unpredictability, pushing one to re-interpret the experience, and update existing knowledge. Using immersive Virtual Reality (VR), we aimed to support social and emotional learning (SEL) through reflection in psychology education. We used the case of psychosis as it involves ambiguous perceptual experiences. With a codesign workshop, we designed a VR prototype that simulates the perceptual, cognitive, affective, and social elements of psychotic experiences, followed by a user-study with psychology students to evaluate the potential of this technology to support reflection. Our analyses suggested that technology-mediated reflection in SEL involves two dimensions: spontaneous perspective-taking and shared state of affect. By exploring the subjective qualities of reflection with the said dimensions, our work contributes to the literature on technology-supported learning and VR developers designing for reflection.",ambiguous design; experiential learning; reflection; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,RotoWrist: Continuous Infrared Wrist Angle Tracking using a Wristband,VRST - Virtual Reality Software and Technology,A,"We introduce RotoWrist, an infrared (IR) light based solution for continuously and reliably tracking 2-degree-of-freedom (DoF) relative angle of the wrist with respect to the forearm using a wristband. The tracking system consists of eight time-of-flight (ToF) IR light modules distributed around a wristband. We developed a computationally simple tracking approach to reconstruct the orientation of the wrist without any runtime training, ensuring user independence. An evaluation study demonstrated that RotoWrist achieves a cross-user median tracking error of 5.9° in flexion/extension and 6.8° in radial and ulnar deviation with no calibration required as measured with optical ground truth. We further demonstrate the performance of RotoWrist for a pointing task and compare it against ground truth tracking.",hand tracking; time-of-flight sensor; virtual and augmented reality; wearable device; wrist pose; wristband.,Keywords,True,
Scopus,conferencePaper,2021,PAIR: Phone as an Augmented Immersive Reality Controller,VRST - Virtual Reality Software and Technology,A,"Immersive head-mounted augmented reality allows users to overlay 3D digital content on a user’s view of the world. Current-generation devices primarily support interaction modalities such as gesture, gaze and voice, which are readily available to most users yet lack precision and tactility, rendering them fatiguing for extended interactions. We propose using smartphones, which are also readily available, as companion devices complementing existing AR interaction modalities. We leverage user familiarity with smartphone interactions, coupled with their support for precise, tactile touch input, to unlock a broad range of interaction techniques and applications - for instance, turning the phone into an interior design palette, touch-enabled catapult or AR-rendered sword. We describe a prototype implementation of our interaction techniques using an off-the-shelf AR headset and smartphone, demonstrate applications, and report on the results of a positional accuracy study.",Augmented Reality; Input Techniques; Smartphone; Touch,Abstract_Keywords,True,
Scopus,conferencePaper,2021,TangibleData: Interactive Data Visualization with Mid-Air Haptics,VRST - Virtual Reality Software and Technology,A,"In this paper, we investigate the effects of mid-air haptics in interactive 3D data visualization. We build an interactive 3D data visualization tool that adapts hand gestures and mid-air haptics to provide tangible interaction in VR using ultrasound haptic feedback on 3D data visualization. We consider two types of 3D visualization datasets and provide different data encoding methods for haptic representations. Two user experiments are conducted to evaluate the effectiveness of our approach. The first experimental results show that adding a mid-air haptic modality can be beneficial regardless of noise conditions and useful for handling occlusion or discerning density and volume information. The second experiment results further show the strengths and weaknesses of direct touch and indirect touch modes. Our findings can shed light on designing and implementing a tangible interaction on 3D data visualization with mid-air haptic feedback.",Data visualization; haptics; immersive analytics; virtual reality,Keywords,True,
Scopus,conferencePaper,2021,PneuMod: A Modular Haptic Device with Localized Pressure and Thermal Feedback,VRST - Virtual Reality Software and Technology,A,"Humans have tactile sensory organs distributed all over the body. However, haptic devices are often only created for one part (e.g., hands, wrist, or face). We propose PneuMod, a wearable modular haptic device that can simultaneously and independently present pressure and thermal (warm and cold) cues to different parts of the body. The module in PneuMod is a pneumatically-actuated silicone bubble with an integrated Peltier device that can render thermo-pneumatic feedback through shapes, locations, patterns, and motion effects. The modules can be arranged with varying resolutions on fabric to create sleeves, headbands, leg wraps, and other forms that can be worn on multiple parts of the body. In this paper, we describe the system design, the module implementation, and applications for social touch interactions and in-game thermal and pressure feedback.",fabrication; haptic communication; modular device; multimodal haptics; pneumatic feedback; thermal feedback; virtual reality,Keywords,True,
Scopus,conferencePaper,2021,Ellipses Ring Marker for High-speed Finger Tracking,VRST - Virtual Reality Software and Technology,A,"High-speed finger tracking is necessary for augmented reality and operation in human-machine cooperation without latency discomfort, but conventional markerless finger tracking methods are not fast enough and the marker-based methods have low wearability. In this paper, we propose an ellipses ring marker (ERM), a finger-ring marker consisting of multiple ellipses and its high-speed image recognition algorithm. The finger-ring shape has highly wearing continuity, and the surface shape is suitable for various viewing angle observation. The invariance of the ellipse in the perspective projection enables accurate and low-latency posture estimation. We have experimentally investigated the advantage in normal distribution, validated the sufficient accuracy and computational cost in the marker tracking, and showed a demonstration of dynamic projection mapping on a palm.",dynamic projection mapping; hand tracking; high-speed image processing,Abstract,True,
Scopus,conferencePaper,2021,Presenting Sense of Loud Vocalization Using Vibratory Stimuli to the Larynx and Auditory Stimuli,VRST - Virtual Reality Software and Technology,A,"In recent years, technologies related to virtual reality (VR) have continued to advance. As a method to enhance the VR experience, we focused on loud vocalization. This is because we believe that loud vocalization can enable us to engage with the VR environment in a more interactive way. Also, as loud vocalization is an action that is thought to be closely related to stress reduction and a sense of exhilaration, the stress reduction through VR with loud vocalization is also expected. But loud vocalization itself has disadvantages for physical, mental, and social reasons. Then, we hypothesized that loud vocalization itself is not necessary for such benefits; but the sense of loud vocalization plays an important role. Therefore, we focused on a method of substituting experience by presenting sensory stimuli. In this paper, we proposed a way to present the sense of loud vocalization through vibratory stimuli to the larynx and auditory stimuli to users who are actually vocalizing quietly with the expectation for the sense of loud vocalization. Our user study showed that the proposed method can extend the sense of vocalization and realize pseudo-loud vocalization. In addition, it was also shown that the proposed method can cause a sense of exhilaration. By contrast, excessively strong vibratory stimuli spoil the sense of loud vocalization, and thus the intensity of the vibration should be appropriately determined.",cross modal; loud vocalization; sense of vocalization; vibratory stimuli,Abstract,True,
Scopus,conferencePaper,2021,Analysis of Detection Thresholds for Hand Redirection during Mid-Air Interactions in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Avatars in virtual reality (VR) with fully articulated hands enable users to naturally interact with the virtual environment (VE). Interactions are often performed in a one-to-one mapping between the movements of the user’s real body, for instance, the hands, and the displayed body of the avatar. However, VR also allows manipulating this mapping to introduce non-isomorphic techniques. In this context, research on manipulations of virtual hand movements typically focuses on increasing the user’s interaction space to improve the overall efficiency of hand-based interactions. In this paper, we investigate a hand retargeting method for decelerated hand movements. With this technique, users need to perform larger movements to reach for an object in the VE, which can be utilized, for example, in therapeutic applications. If these gain-based redirections of virtual hand movements are small enough, users become unable to reliably detect them due to the dominance of the visual sense. In a psychophysical experiment, we analyzed detection thresholds for six different motion paths in mid-air for both hands. We found significantly different detection thresholds between movement directions on each spatial axis. To verify our findings, we applied the identified gains in a playful application in a confirmatory study.",avatar; detection thresholds; hand redirection,Title_Abstract,True,
Scopus,conferencePaper,2021,Virtual Reality platform for functional magnetic resonance imaging in ecologically valid conditions,VRST - Virtual Reality Software and Technology,A,"Functional magnetic resonance Brain Imaging (fMRI) is a key non-invasive imaging technique for the study of human brain activity. Its millimetric spatial resolution is at the cost of several constraints: participants must remain static and experience artificial stimuli, making it difficult to generalize neuroscientific results to naturalistic and ecological conditions. Immersive Virtual Reality (VR) provides alternatives to such stimuli through simulation, but still requires an active first-person exploration of the environment to evoke a strong sense of presence in the virtual environment. Here, we report how to compensate for the inability to freely move in VR by leveraging on principles of embodiment for a virtual avatar, to eventually evoke a strong sense of presence with a minimal motion of the participant. We validated the functionality of the platform in a study where healthy participants performed several basic research tasks in an MR-specific immersive virtual environment. Our results show that our approach can lead to high sense of presence, strong body ownership, and sense of agency for a virtual avatar, with low movement-related MRI artifacts. Moreover, to exemplify the versatility of the platform, we reproduced several behavioral and fMRI results in the perceptual, motor, and cognitive domains. We discuss how to leverage such technology for neuroscience research and provide recommendations on efficient ways to implement and develop it successfully.",Ecological; fMRI; Immersive Virtual Reality; Presence,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Enhancing In-game Immersion Using BCI-controlled Mechanics,VRST - Virtual Reality Software and Technology,A,"Due to multimodal approach, the virtual reality experiences become increasingly more immersive and entertaining. New control modalities, such as brain-computer interfaces (BCIs), enable the players to engage in the game with both their bodies and minds. In our work, we investigate the influence of employing BCI-driven mechanics on player’s in-game immersion. We designed and implemented an escape room-themed game which employed player’s mental states of focus and relaxation as input for selected game mechanisms. Through a between-subject user study, we found that controlling the game with mental states enhances the in-game immersion and attracts the player’s engagement. At the same time, using BCIs did not impose additional cognitive workload. Our work contributes qualitative insights on psychocognitive effects of using BCIs in gaming and describing immersive gaming experiences.",brain-computer interface; virtual reality games,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Perceived Realism of Pedestrian Crowds Trajectories in VR,VRST - Virtual Reality Software and Technology,A,"Crowd simulation algorithms play an essential role in populating Virtual Reality (VR) environments with multiple autonomous humanoid agents. The generation of plausible trajectories can be a significant computational cost for real-time graphics engines, especially in untethered and mobile devices such as portable VR devices. Previous research explores the plausibility and realism of crowd simulations on desktop computers but fails to account the impact it has on immersion. This study explores how the realism of crowd trajectories affects the perceived immersion in VR. We do so by running a psychophysical experiment in which participants rate the realism of real/synthetic trajectories data, showing similar level of perceived realism.",crowd simulation; perception; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Pressing a Button You Cannot See: Evaluating Visual Designs to Assist Persons with Low Vision through Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Partial vision loss occurs in several medical conditions and affects persons of all ages. It compromises many daily activities, such as reading, cutting vegetables, or identifying and accurately pressing buttons, e.g., on ticket machines or ATMs. Touchscreen interfaces pose a particular challenge because they lack haptic feedback from interface elements and often require people with impaired vision to rely on others for help. We propose a smartglasses-based solution to utilize the user’s residual vision. Together with visually-impaired individuals, we designed assistive augmentations for touchscreen interfaces and evaluated their suitability to guide attention towards interface elements and to increase the accuracy of manual inputs. We show that augmentations improve interaction performance and decrease cognitive load, particularly for unfamiliar interface layouts.",Accessibility; Augmented Reality; Low Vision,Title_Keywords,True,
Scopus,conferencePaper,2021,Flyables: Haptic Input Devices for Virtual Reality using Quadcopters,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) has made its way into everyday life. While VR delivers an ever-increasing level of immersion, controls and their haptics are still limited. Current VR headsets come with dedicated controllers that are used to control every virtual interface element. However, the controller input mostly differs from the virtual interface. This reduces immersion. To provide a more realistic input, we present Flyables, a toolkit that provides matching haptics for virtual user interface elements using quadcopters. We took five common virtual UI elements and built their physical counterparts. We attached them to quadcopters to deliver on-demand haptic feedback. In a user study, we compared Flyables to controller-based VR input. While controllers still outperform Flyables in terms of precision and task completion time, we found that Flyables present a more natural and playful way to interact with VR environments. Based on the results from the study, we outline research challenges that could improve interaction with Flyables in the future.",Drones; Flyables; Haptics; Quadcopter; Toolkit.; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Object Manipulations in VR Show Task- and Object-Dependent Modulation of Motor Patterns,VRST - Virtual Reality Software and Technology,A,"Humans can perform object manipulations in VR in spite of missing haptic and acoustic information. Whether their movements under these artificial conditions do still rely on motor programs based on natural experience or are impoverished due to the restrictions imposed by VR is unclear. We investigated whether reach-to-place and reach-to-grasp movements in VR can still be adapted to the task and to the specific properties of the objects being handled, or whether they reflect a stereotypic, task- and object-independent motor program. We analyzed reach-to-grasp and reach-to-place movements from participants performing an unconstrained ”set-the-table” task involving a variety of different objects in virtual reality. These actions were compared based on their kinematic features. We encountered significant differences in peak speed and the duration of the deceleration phase which are modulated depending on the action and on the manipulated object. The flexibility of natural human sensorimotor control thus is at least partially transferred and exploited in impoverished VR conditions. We discuss possible explanations of this behavior and the implications for the design of object manipulations in VR.",kinematics; motor control; motor skill; object manipulation phases; reach and place movements; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Modeling Pointing for 3D Target Selection in VR,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) allows users to interact similarly to how they do in the physical world, such as touching, moving, and pointing at objects. To select objects at a distance, most VR techniques rely on casting a ray through one or two points located on the user’s body (e.g., on the head and a finger), and placing a cursor on that ray. However, previous studies show that such rays do not help users achieve optimal pointing accuracy nor correspond to how they would naturally point. We seek to find features, which would best describe natural pointing at distant targets. We collect motion data from seven locations on the hand, arm, and body, while participants point at 27 targets across a virtual room. We evaluate the features of pointing and analyse sets of those for predicting pointing targets. Our analysis shows an 87% classification accuracy between the 27 targets for the best feature set and a mean distance of 23.56&nbsp;cm in predicting pointing targets across the room. The feature sets can inform the design of more natural and effective VR pointing techniques for distant object selection.",pointing; target selection; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Virtual Object Categorisation Methods: Towards a Richer Understanding of Object Grasping for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Object categorisation methods have been historically used in literature for understanding and collecting real objects together into meaningful groups and can be used to define human interaction patterns (i. e grasping). When investigating grasping patterns for Virtual Reality (VR), researchers used Zingg’s methodology which categorises objects based on shape and form. However, this methodology is limited and does not take into consideration other object attributes that might influence grasping interaction in VR. To address this, our work presents a study into three categorisation methods for virtual objects. We employ Zingg’s object categorisation as a benchmark against existing real and virtual object interaction work and introduce two new categorisation methods that focus on virtual object equilibrium and virtual object component parts. We evaluate these categorisation methods using a dataset of 1872 grasps from a VR docking task on 16 virtual representations of real objects and report findings on grasp patterns. We report on findings for each virtual object categorisation method showing differences in terms of grasp classes, grasp type and aperture. We conclude by detailing recommendations and future ideas on how these categorisation methods can be taken forward to inform a richer understanding of grasping in VR.",Grasping; Interaction; Object Categorisation; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,"Actions, not gestures: contextualising embodied controller interactions in immersive virtual reality",VRST - Virtual Reality Software and Technology,A,"Modern immersive virtual reality (IVR) often uses embodied controllers for interacting with virtual objects. However, it is not clear how we should conceptualise these interactions. They could be considered either gestures, as there is no interaction with a physical object; or as actions, given that there is object manipulation, even if it is virtual. This distinction is important, as literature has shown that in the physical world, action-enabled and gesture-enabled learning produce distinct cognitive outcomes. This study attempts to understand whether sensorimotor-embodied interactions with objects in IVR can cognitively be considered as actions or gestures. It does this by comparing verb-learning outcomes between two conditions: (1) where participants move the controllers without touching virtual objects (gesture condition); and (2) where participants move the controllers and manipulate virtual objects (action condition). We found that (1) users can have cognitively distinct outcomes in IVR based on whether the interactions are actions or gestures, with actions providing stronger memorisation outcomes; and (2) embodied controller actions in IVR behave more similarly to physical world actions in terms of verb memorization benefits.",cognition; embodiment; HCI; immersive virtual reality; learning; sensorimotor; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,3D Printing an Accessory Dock for XR Controllers and its Exemplary Use as XR Stylus,VRST - Virtual Reality Software and Technology,A,"This article introduces the accessory dock, a 3D printed multi-purpose extension for consumer-grade XR controllers that enables flexible mounting of self-made and commercial accessories. The uniform design of our concept opens new opportunities for XR systems being used for more diverse purposes, e.g., researchers and practitioners could use and compare arbitrary XR controllers within their experiments while ensuring access to buttons and battery housing. As a first example, we present a stylus tip accessory to build an XR Stylus, which can be directly used with frameworks for handwriting, sketching, and UI interaction on physically aligned virtual surfaces. For new XR controllers, we provide instructions on how to adjust the accessory dock to the controller’s form factor. A video tutorial for the construction and the source files for 3D printing are publicly available for reuse, replication, and extension (https://go.uniwue.de/hci-otss-accessory-dock).",3D modeling; 3D printing; augmented reality; handwriting; passive haptic feedback; sketching; stylus; virtual reality,Keywords,True,
Scopus,conferencePaper,2021,A Hat-shaped Pressure-Sensitive Multi-Touch Interface for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We developed a hat-shaped touch interface for virtual reality viewpoint control. The hat is made of conductive fabric and thus is lightweight. The user can touch, drag, and push the surface, enabling three-dimensional viewpoint control.",multi-touch; touch interface; viewpoint control; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,A Perceptual Evaluation of the Ground Inclination with a Simple VR Walking Platform,VRST - Virtual Reality Software and Technology,A,"We evaluate how highly realistic the inclination of the ground can be perceived with our simple VR walking platform. Firstly we prepared seven maps with different ground inclinations of -30 to 30 degrees and every 10 degrees. Then we conducted a perception experiment of the inclination feeling with each of the treadmill and our proposed platform, and questionnaire evaluation about the presence, the fatigue, and the exhilaration. As a result, it was clarified that even if our proposed platform is used, not only the feeling of presence equivalent to that of the treadmill can be felt, but also the inclination of the ground up and down can be perceived.",locomotion interface; redirection; virtual reality; walking platform,Keywords,True,
Scopus,conferencePaper,2021,A Pilot Study Examining the Unexpected Vection Hypothesis of Cybersickness.,VRST - Virtual Reality Software and Technology,A,"The relationship between vection (illusory self-motion) and cybersickness is complex. This pilot study examined whether only unexpected vection provokes sickness during head-mounted display (HMD) based virtual reality (VR). 20 participants ran through the tutorial of Mission: ISS (an HMD VR app) until they experienced notable sickness (maximum exposure was 15 minutes). We found that: 1) cybersickness was positively related to vection strength; and 2) cybersickness appeared to be more likely to occur during unexpected vection. Given the implications of these findings, future studies should attempt to replicate them and confirm the unexpected vection hypothesis with larger sample sizes and rigorous experimental designs.",Cybersickness; Oculus Rift; Perception; Vection; Virtual Reality; VR,Abstract_Keywords,True,
Scopus,conferencePaper,2021,A sharing system for the annoyance of menstrual symptoms using electrical muscle stimulation and thermal stimulations,VRST - Virtual Reality Software and Technology,A,,Electrical muscle stimulation (EMS); menstruation; virtual reality,Keywords,True,
Scopus,conferencePaper,2021,A System for Practicing Ball/Strike Judgment in VR Environment,VRST - Virtual Reality Software and Technology,A,"The purpose of this study is to develop an easy-to-use ball/strike judgment practice system for inexperienced baseball umpires. The main idea is to provide a practice environment in a Virtual Reality (VR) space. With our system, users observe a pitched ball, perform ball/strike judgment, and review their judgment in a VR space. Since the whole process is completed in VR, users can practice the judgments without preparing a pitcher and catcher. A user investigation in which participants practiced with our system and judged balls thrown by a pitching machine was conducted. The participants responded positively when asked about the usefulness of our system.",,Abstract,True,
Scopus,conferencePaper,2021,A Tangible Haptic Feedback Box for Mixed Reality Billiard in Tight Spaces,VRST - Virtual Reality Software and Technology,A,"This paper presents a system for a simulated billiard game with two players and an emphasis on haptic feedback. We devised a feedback box that is responsible for generating the inputs and providing immediate haptic feedback to the user. The simulation runs as an AR application and the player can use a real queue to hit the real ball. Although the haptic feedback is precise due to the usage of a real billiard ball and queue, the input accuracy of the angle and impulse measurement is limited.",Augmented Reality; Billiard; Haptic User Feedback; Mixed Reality; Tracking,Title_Keywords,True,
Scopus,conferencePaper,2021,"ALiSE: Non-wearable AR display through the looking glass, and what looks solid there",VRST - Virtual Reality Software and Technology,A,"With the Augmented Reality mirror display method using a half-mirror, there is a difference in the focal length between the mirror image and the AR image. Therefore, the observer perceives a mismatch in depth perception, which impairs usability. In this study, we developed an optical-reflection AR display, ALiSE (Augment Layer interweaved Semi-reflecting Existence), which enhances the depth perception experience of AR images by adding a gap zone with the same depth as the target depth between the display and the half-mirror. We conducted an experiment to view 3D objects and achieve virtual fitting using the existing AR with video synthesis and the proposed ALiSE method. As a result of the questionnaire survey, although the comfort of wearing virtual objects was below existing methods, we confirmed that the presence and solidity were superior with the proposed method to other approaches. This is an attempt to create a sense of the stereoscopic effect despite the 2D projection, as the object to be projected is simultaneously reflected in the mirror along with the observer themselves.",Augmented layer; Augmented mirror; Virtual fitting room,Abstract,True,
Scopus,conferencePaper,2021,An Evaluation of Methods for Manipulating Virtual Objects at Different Scales,VRST - Virtual Reality Software and Technology,A,"Immersive Virtual Reality enables users to experience 3D models and other virtual content in ways that cannot be achieved on a flat screen, and several modern Virtual Reality applications now give users the ability to include or create their own content and objects. With user-generated content however, objects may come in all shapes and sizes. This necessitates the use of object manipulation methods that are effective regardless of object size. In this work we evaluate two methods for manipulating virtual objects of varying sizes. World Pull enables the user to directly manipulate and scale the virtual environment, while Pivot Manipulation enables the user to rotate objects around a set of predefined pivot points. The methods were compared to a traditional 6 degree of freedom manipulation method during a user study and the results showed that World Pull performed better in terms of precision for small and large objects, while Pivot Manipulation performed better for large objects.",large objects; precise object manipulation; small objects; virtual docking; virtual object manipulation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,An Infant-Like Device that Reproduces Hugging Sensation with Multi-Channel Haptic Feedback,VRST - Virtual Reality Software and Technology,A,"Proximity interaction, such as hugging, plays an essential role in building relationships between parents and children. However, parents and children cannot freely interact in the neonatal intensive care unit due to visiting restrictions imposed by COVID-19. In this study, we develop a system of pseudo-proximity interaction with a remote infant through a VR headset by using an infant-like device that reproduces the haptic feedback features of the hugging sensation, such as weight, body temperature, breathing, softness, and unstable neck.",Infant-Like Device; Multimodal; Proximity Interaction; Telepresence; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2021,An Interactive Flight Operation with 2-DOF Motion Platform,VRST - Virtual Reality Software and Technology,A,"We propose an interactive flight operation with 2-DOF motion platform that enables user to tilt greatly according to the user posture and VR environment. In order to realize a flight like a hang glider, this work interactively controls the motion platform according to the attitude of the user. By tilting the body back and forth and left and right while keeping the body horizontal based on a posture like the planche exercise, the virtual aircraft tilts in that direction and the motion platform also rolling movements. In addition, since our motion platform with the balance board swings by rolling motion, it is possible to realize a large swing at low-cost and safely.",balance board; hang glider; motion platform; virtual reality,Keywords,True,
Scopus,conferencePaper,2021,Conference Talk Training With a Virtual Audience System,VRST - Virtual Reality Software and Technology,A,"This paper presents the first prototype of a virtual audience system (VAS) specifically designed as a training tool for conference talks. This system has been tailored for university seminars dedicated to the preparation and delivery of scientific talks. We describe the required features which have been identified during the development process. We also summarize the preliminary feedback received from lecturers and students during the first deployment of the system in seminars for bachelor and doctoral students. Finally, we discuss future work and research directions. We believe our system architecture and features are providing interesting insights on the development and integration of VR-based educational tools into university curriculum.",Education; Public Speaking; Training; Virtual Agent; Virtual reality,Keywords,True,
Scopus,conferencePaper,2021,Content-rich and Expansive Virtual Environments Using Passive Props As World Anchors,VRST - Virtual Reality Software and Technology,A,"In this paper, we present a system that allows developers to add passive haptic feedback into their virtual reality applications by making use of existing physical objects in the user’s real environment. Our approach has minimal dependence on procedural generation and does not limit the virtual space to the dimensions of the physical play-area.",,Abstract,True,
Scopus,conferencePaper,2021,Dealing with a Panic Attack: a Virtual Reality Training Module for Postgraduate Psychology Students,VRST - Virtual Reality Software and Technology,A,"In this paper we present a virtual reality training simulator for postgraduate psychology students. This simulator features an interaction between a clinical psychologist (student) and a patient (virtual agent) suffering from Obsessive Compulsive Disorder (OCD). Our simulation focuses on the form of OCD treatment called “Exposure Therapy”. The traditional way of learning how to perform Exposure Therapy (ET) currently involves watching video recordings and discussing those in the class. In our simulation we conduct an immersive exposure therapy session in VR. This session involves a live interaction with a patient that at one stage triggers a panic attack. Our hypothesis is that the immersive nature of the training session will affect the decision making process of the students so that they are more likely to cease the exposure task than those student participating in a less immersive form of learning (watching a video recording). We also hypothesise that participating in an immersive VR training session is more effective than watching videos, as far as information retention goes.",Exposure Therapy; OCD; Panic Attack; Psychology training; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Double-Layered Cup-Shaped Device to Amplify Taste Sensation of Carbonation by the Electrical Stimulation on the Human Tongue,VRST - Virtual Reality Software and Technology,A,"We show that electrical stimulation on the human tongue amplifies the taste sensation of carbonated beverages. We have developed a novel electric taste system with two components: a cup-shaped device and its circuit for stimulation. The cup-shaped device has a double-layer structure. The circuit has a constant current control circuit and a signal generator, which allow adjustment of the electrical parameters. The device is hygiene when we demonstrate electric taste because the device has two-layered. Thus we can change the inner layer that touches the user’s mouth. The device is also inexpensive and easy to manufacture so that many people can experience them.",Carbonated Beverage; Electric Taste; Electrical Stimulation; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2021,Effects of User’s Gaze on the Unintended Positional Drift in Walk-in-Place,VRST - Virtual Reality Software and Technology,A,"Walk-In-Place (WIP) is a technique in which users perform walking or jogging-like movements in a stationary place to move around in virtual environments (VEs). However, unintended positional drift (UPD) while performing WIP often occurs, thus weakening its benefits of keeping users in a fixed position in a physical space. In this paper, we present our preliminary study exploring whether users’ gaze while WIP affects the direction of the UPD. Participants of the study jogged in a VE five times. Each time, we manipulated their gaze direction by displaying visual information in 5 different locations in their view. Although a correlation between the gaze and UPD direction was not found, we report the results from this study, including the amount of observed drift and preferred location of visual information, and discuss future research directions.",unintended positional drift; virtual reality; walk-in-place,Keywords,True,
Scopus,conferencePaper,2021,Efficient Mapping Technique under Various Spatial Changes for SLAM-based AR Services,VRST - Virtual Reality Software and Technology,A,"Recently, many attempts have been made to apply real-time simultaneous localization and mapping (SLAM) technology to augmented reality (AR) applications. Such AR systems based on SLAM technology are generally implemented by augmenting virtual objects onto a diorama or three-dimensional sculpture. However, a new SLAM map needs to be generated if the space or lighting where the diorama is installed changes. This leads to the problem of updating the coordinate system each time a new SLAM map is generated. Updates to the coordinate system signify that the positions of the virtual objects placed in the AR space change as well. Therefore, we proposed a SLAM map regeneration technique in which the existing coordinate system is maintained even if a new map is generated.",Augmented Reality; Diorama; SLAM based AR,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Emotional Virtual Reality Stroop Task: Pilot Design,VRST - Virtual Reality Software and Technology,A,"Anxiety-inducing and assessment methods in Virtual Reality has been a topic of discussion in recent literature. The importance of the topic is related to the difficulty of getting accurate and timely measurements of anxiety without relying on self-report and breaking the immersion. To this end, the current study utilises the emotional version of a well-established cognitive task; the Stroop Color-Word Task and brings it to Virtual Reality. It consists of three levels; congruent which is used as control and corresponds with no anxiety, incongruent, which corresponds with mild anxiety and emotional, which corresponds with severe anxiety. This pilot serves two functions. The first is to validate the effects of the task using biosignal measurements. The second is to use the bio signal information and the labels to train a machine-learning algorithm. The information collected by the pilot will be used to decide what types of signals and devices to use in the final product, as well as what algorithm and time frame will be better suited for the purpose of accurately determining the user’s anxiety level within Virtual Reality without breaking the immersion.",anxiety; biosensors; biosignals; EEG; emotional stroop; GSR; PPG; VR,Title_Abstract,True,
Scopus,conferencePaper,2021,Evaluating the influence of interaction technology on procedural learning using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Within the context of industry 4.0, this paper studies the influence of interaction technology (Vive controller and Knuckles) on manufacturing assembly procedural training using Virtual Reality. To do so, an experiment with 24 volunteers have been conducted and these participants have been separated in two groups: one using Vive controller and the other using Knuckles. Our conclusions are based on two indicators: Time to realize all tasks and the number of manipulations. This study shows that, after get used to, volunteers using Knuckles are faster than the other group but for some very delicate tasks, they need more manipulations to succeed.",,Title_Abstract,True,
Scopus,conferencePaper,2021,Exploring Emotion Brushes for a Virtual Reality Painting Tool,VRST - Virtual Reality Software and Technology,A,"We present emoPaint, a virtual reality application that allows users to create paintings with expressive emotion-based brushes and shapes. While previous systems have introduced painting in 3D space, emoPaint focuses on supporting emotional characteristics by allowing users to use brushes corresponding to specific emotions or to create their own emotion brushes and paint with the corresponding visual elements. Our system provides a variety of line textures, shape representations and color palettes for each emotion to enable users to control expression of emotions in their paintings. In this work we describe our implementation and illustrate paintings created using emoPaint.",,Title_Abstract,True,
Scopus,conferencePaper,2021,Fishtank Sandbox: A Software Framework for Collaborative Usability Testing of Fish Tank Virtual Reality Interaction Techniques,VRST - Virtual Reality Software and Technology,A,"Human-computer interaction researchers have been studying how we can interact with virtual objects in a virtual environment efficiently. Many usability experiments do not have the same control parameters. The lack of consistency makes comparing different interaction techniques difficult. In this article, we present a software framework for usability study in FTVR interaction techniques. The software framework provides fixed control parameters (e.g., task, graphic settings, and measuring parameters), the ability for other researchers to incorporate their interaction techniques as an add-on, and enabling individuals to participate in the experiment over the internet. The article explores a new way for VR/AR researchers to approach usability experiments using the framework and discuss the challenges that it brings.",Fish Tank Virtual Reality; Framework; Usability Testing,Title_Keywords,True,
Scopus,conferencePaper,2021,Fluid3DGuides: A Technique for Structured 3D Drawing in VR,VRST - Virtual Reality Software and Technology,A,"We propose Fluid3DGuides, a drawing guide technique to help users draw structured sketches more accurately in VR. The prototype system continuously infers visual guide lines for the user based on the user’s instant stroke drawing intention and its potential constraint relationship with the existing strokes. We evaluated our prototype through a pilot user study with six participants by comparing the proposed guide technique against the non-guide drawing condition. Participants gave positive comments on ease of use and drawing accuracy. They found that the technique could reduce the time and effort required to find the corrected drawing perspective and obtain more accurate 3D structured sketches.",3D structured sketches; virtual reality; visual guidance,Keywords,True,
Scopus,conferencePaper,2021,Force-Based Foot Gesture Navigation in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Navigation is a primary interaction in virtual reality. Previous research has explored different forms of artificial locomotion techniques for navigation, including hand gestures and body motions. However, few studies have investigated force-based foot gestures as a locomotion technique. We present three force-based foot gestures (Foot Fly, Foot Step and Foot Teleportation) for navigation in a virtual environment, relying on surface electromyography sensors readings from leg muscles. A pilot study comparing our techniques with controller-based techniques indicates that force-based foot gestures can provide a fun and engaging alternative. Of all six input techniques evaluated, Foot Fly was often most preferred despite requiring more exertion than the Controller Fly technique.",Locomotion; Navigation; Other Hardware; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Freehand Interaction in Virtual Reality: Bimanual Gestures for Cross-Workspace Interaction,VRST - Virtual Reality Software and Technology,A,"This work presents the design and evaluation of three bimanual interaction modalities for cross-workspace interaction in virtual reality (VR), in which the user can move items between a personal workspace and a shared workspace. We conducted an empirical study to understand three modalities and their suitability for cross-workspace interaction in VR.",bimanual gestures; cross-workspace interaction; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,GazeMOOC: A Gaze Data Driven Visual Analytics System for MOOC with XR Content,VRST - Virtual Reality Software and Technology,A,"MOOC is widely used and more popular after COVID-19.In order to improve the learning effect, MOOC is evolving with XR technologies such as avatars, virtual scenes and experiments. This paper proposes a novel visual analytics system GazeMOOC, that can evaluate learners’ learning engagement in MOOC with XR content. For same MOOC content, gaze data of all learners are recorded and clustered. By differentiating gaze data of distracted learners and active learners, GazeMOOC can help evaluate MOOC content and learners’ learning engagement.",Data visualization; Eye tracking; Learning Engagement; Mixed Reality,Keywords,True,
Scopus,conferencePaper,2021,HapticPanel: An Open System to Render Haptic Interfaces in Virtual Reality for Manufacturing Industry,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) allows simulation of machine control panels without physical access to the machine, enabling easier and faster initial exploration, testing, and validation of machine panel designs. However, haptic feedback is indispensable if we want to interact with these simulated panels in a realistic manner. We present HapticPanel, an encountered-type haptic system that provides realistic haptic feedback for machine control panels in VR. To ensure a realistic manipulation of input elements, the user’s hand is continuously tracked during interaction with the virtual interface. Based on which virtual element the user intends to manipulate, a motorized panel with stepper motors moves a corresponding physical input element in front of the user’s hand, enabling realistic physical interaction.",Engineering Haptic Interactive Systems; Machine Interfaces; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,HoloKeys: Interactive Piano Education Using Augmented Reality and IoT,VRST - Virtual Reality Software and Technology,A,"The rise of online learning poses unique challenges in music education, where live demonstration and musical synchronization are critical for student success. We present HoloKeys, a music education interface which allows instructors to play remotely located pianos using an augmented reality headset and wifi-enabled microcontrollers. This approach allows students to receive distance education which is more direct, immersive, and comprehensive than conventional video conferencing allows for. HoloKeys enables remote students to observe live instructional demonstration on a physical keyboard in their immediate environment just as they would in traditional settings. HoloKeys consists of two separate components: an augmented reality user interface and a piano playing apparatus. Our system aims to extend online music education beyond desktop platforms into the physical world, thereby addressing crucial obstacles encountered by educators and students transitioning into online education.",,Title_Abstract,True,
Scopus,conferencePaper,2021,Immersive Visual Interaction with Autonomous Multi-Vehicle Systems,VRST - Virtual Reality Software and Technology,A,"With the emergence of multi-vehicular autonomous systems, such as AI controlled multiple fully autonomous vehicles, we need novel systems that provide tools for planning, executing, and reviewing of missions and keeping humans in the loop during all phases. We therefore present an immersive visualization system for interacting with these systems at a higher cognitive level than piloting of individual vehicles. Our system provides both desktop and VR modes for visual interaction with the robotic multi-vehicle AI system.",UAV; unmanned vehicle; USV; virtual reality; visualisation,Keywords,True,
Scopus,conferencePaper,2021,Incorporating Human Behavior in VR Compartmental Simulation Models,VRST - Virtual Reality Software and Technology,A,"A novel strand of Coronavirus has affected a large number of individuals worldwide, putting a considerable stress to national health services and causing many deaths. Many control measures have been put in place across different countries with the aim to save lives at the cost of personal freedom. Computer simulations have played a role in providing policy makers with critical information about the virus. However, despite their importance in applied epidemiology, general simulation models, are difficult to validate because of how hard it is to predict and model human behavior. To this end, we propose a different approach by developing a virtual reality (VR) multi-agent virus propagation system where a group of agents interact with the user in a university setting. We created a VR digital twin replica of a building in the University of Derby campus, to enhance the user’s immersion in our study. Our work integrates human behavior seamlessly in a simulation model and we believe that this approach is crucial to have a deeper understanding on how to control the spread of a virus such as COVID-19.",,Abstract,True,
Scopus,conferencePaper,2021,Managing a Crisis in Virtual Reality - Tackling a Wildfire,VRST - Virtual Reality Software and Technology,A,"In this paper we present a virtual reality application, where multiple users can observe and interact with a portion of geo-referenced terrain where a real wildfire took place. The application presents a layout with two maps, one is a three-dimensional view with terrain elevation and the other is a conventional two-dimensional view. The VR users can control different layers (roads, waterways, etc), control the wildfire’s playback, command vehicles to change positions and paint the terrain conveying information to one-another. This work explores how users interact with map visualizations and plan for a crisis management scenario within a virtual environment.",crisis management systems; geospatial visualization; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Miniature AR: Multi-view 6DOF Virtual Object Visualization for a Miniature Diorama,VRST - Virtual Reality Software and Technology,A,"We describe a miniature diorama AR system called ‘Miniature AR’ which can be applied to a mechanical diorama to extend the content’s feasibility by overlapping virtual objects on the complex diorama structure. The previous AR researches for the diorama are usually based on 2D planar recognition, and multiple user experience cannot be considered due to multi-devices synchronization. To overcome these constraints, in this paper, we show a new diorama AR system suitable for a tiny complex structure. The contributions of our work are i) design of the diorama AR system, ii) AR space generation and 6DOF view device tracking for diorama, iii) multiple view and event synchronization for multiple users. The utility of the approach has been demonstrated under a real diorama environment (miniature of a ski slope) using mobile devices.",6DOF view tracking; augmented reality; diorama; miniature AR,Keywords,True,
Scopus,conferencePaper,2021,Multi-Componential Analysis of Emotions Using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In this study, we propose our data-driven approach to investigate the emotional experience triggered using Virtual Reality (VR) games. We considered a full Component Process Model (CPM) which theorise emotional experience as a multi-process phenomenon. We validated the possibility of the proposed approach through a pilot experiment and confirmed that VR games can be used to trigger a diverse range of emotions. Using hierarchical clustering, we showed a clear distinction between positive and negative emotion in the CPM space.",,Title_Abstract,True,
Scopus,conferencePaper,2021,Multi-View AR Streams for Interactive 3D Remote Teaching,VRST - Virtual Reality Software and Technology,A,"In this work, we present a system that adds augmented reality interaction and 3D-space utilization to educational videoconferencing for a more engaging distance learning experience. We developed infrastructure and user interfaces that enable the use of an instructor’s physical 3D space as a teaching stage, promote student interaction, and take advantage of the flexibility of adding virtual content to the physical world. The system is implemented using hand-held mobile augmented reality to maximize device availability, scalability, and ready deployment, elevating traditional video lectures to immersive mixed reality experiences. We use multiple devices on the teacher’s end to provide different simultaneous views of a teaching space towards a better understanding of the 3D space.",AR; multi-view; remote teaching; telepresence; video conferencing,Abstract,True,
Scopus,conferencePaper,2021,Natural walking speed prediction in Virtual Reality while using target selection-based locomotion,VRST - Virtual Reality Software and Technology,A,"Travelling speed plays an essential role in the overall user experience while navigating inside a virtual environment. Researchers have used various travelling speed that matches the user speed profile in order to give a natural walking experience. However, predicting a user’s instantaneous walking speed can be challenging when there is no continuous input from the user. Target selection-based techniques are those where the user selects the target to reach there automatically. These techniques also lack naturalness due to their low interaction fidelity. In this work, we have proposed a mathematical model that can dynamically compute the instantaneous natural walking speed while moving from one point to another in a virtual environment. We formulated our model with the help of user studies.",Natural walking speed prediction; Target selection-based locomotion speed; Teleportation speed; Travelling speed in VR; Virtual Reality Locomotion,Title_Keywords,True,
Scopus,conferencePaper,2021,Of Leaders and Directors: A visual model to describe and analyse persistent visual cues directing to single out-of view targets,VRST - Virtual Reality Software and Technology,A,Researchers have come up with many visual cues that can guide Virtual (VR) and Augmented Reality (AR) users to out of view objects. The paper provides a classification of cues and tasks and visual model to describe and analyse cues to support their design.,,Abstract,True,
Scopus,conferencePaper,2021,ProMVR - Protein Multiplayer Virtual Reality Tool,VRST - Virtual Reality Software and Technology,A,"Due to the pandemic limitations caused by Covid-19, people need to work at home and carry on the meetings virtually. Virtual meeting tools start popularizing and thriving. Those tools allow users to see each other through screen and camera, chat through voice and text, and share content or ideas through screen share. However, screen sharing protein models through virtual meetings is not easy due to the difficulty of viewing protein 3D (Three Dimensional) structures from a 2D (Two Dimensional) screen. Moreover, interactions upon a protein are also limited.&nbsp;ProMVR is a tool the author developed to tackle the issue that protein designers may find limitations working in a traditional 2D or 3D environment and they may find it hard to communicate their ideas with other designers. Since ProMVR is a VR tool, it allows users to “jump into” a virtual environment, take a close look at protein models, and have intuitive interactions.",Multiplayer; Protein visualization; Virtual reality; Voice and text chat,Title_Keywords,True,
Scopus,conferencePaper,2021,Recreating a Medieval Mill as a Virtual Learning Environment,VRST - Virtual Reality Software and Technology,A,"Historic buildings shown in open-air museums often lack a good accessibility and visitors rarely can interact with them as well as displayed tools to learn about processes. Providing these buildings in Virtual Reality could be a great supplement for museums to provide accessible and interactive offers. To investigate the effectiveness of this approach and to derive design guidelines, we developed an interactive virtual replicate of a medieval mill. We present the design of the mill and the results of a preliminary usability evaluation.",immersive learning; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Remote Visual Line-of-Sight: A Remote Platform for the Visualisation and Control of an Indoor Drone using Virtual Reality,VRST - Virtual Reality Software and Technology,A,The COVID-19 pandemic has created the distinct challenge for the piloting of drones/other UAVs for researchers and educators who are restricted to working remotely. We propose a Remote Visual Line-of-Sight system that leverages the advantages of Virtual Reality (VR) and motion capture to allow users to fly a real-world drone from a remote location. The system was developed while our researcher (VR operator) was remotely working in Vietnam with the enclosed real-world environment located in Australia. Our paper will present the system design and the challenges found during the development of our system.,Drones Technology; Human-Robot Interaction; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Safety First: A Study of Users’ Perception of VR Adoption in Vehicles,VRST - Virtual Reality Software and Technology,A,"The increasing ubiquity and mobility of VR devices has introduced novel use cases, one of which is using VR while in dynamic, on-the-go environments. Hence, there is a need to examine the perceptual, cognitive, and behavioral aspects of both the driving experience and VR immersion, and how they influence each other. As an initial step towards this goal, we report on the results of an online survey that investigated users’ perceived safety of using VR in an AV. The results of the survey show a mix of expected and surprising attitudes towards VR-in-the-car.",automated vehicles; in-transit; perceived safety; virtual reality,Keywords,True,
Scopus,conferencePaper,2021,Study of Heart Rate Visualizations on a Virtual Smartwatch,VRST - Virtual Reality Software and Technology,A,"In this paper, we present three visualizations showing heart rate&nbsp;(HR) data collected over time. Two visualizations present a summary chart (bar or radial chart), summarizing the amount of time spent per HR zone (i.e., low, moderate, high intensity). We conducted a pilot study with five participants to evaluate the efficiency of the visualizations when monitoring the intensity of an activity while playing a tennis-like Virtual Reality game. Preliminary results show that participants were performing (with respect to time and accuracy) better with and preferred the bar chart summary.",heart rate visualization; micro visualization; virtual smartwatch,Abstract,True,
Scopus,conferencePaper,2021,Swaying Locomotion: A VR-based Locomotion System through Head Movements,VRST - Virtual Reality Software and Technology,A,"Locomotion systems used in virtual reality (VR) content have a significant impact on the content user experience. One of the most important factors of a walking system in VR is whether it can provide a plausible walking sensation because it is considered directly related to the user’s sense of presence. However, joystick-based and teleportation-based locomotion systems, which are commonly used today, can hardly provide an appropriate sense of presence to a user. To solve this problem, we present Swaying Locomotion, which is a novel VR-based locomotion system that uses head movements to support a user walking in a VR space while actually sitting in real space. Our user study suggests that Swaying Locomotion provides a better walking sensation than the traditional joystick-based approach.",locomotion; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,Technical Factors Affecting Augmented Reality User Experiences in Sports Spectating,VRST - Virtual Reality Software and Technology,A,"The maturity of augmented reality (AR) technology and research now paves the way for dissemination of AR outside of the laboratory. However, it is still under-explored which factors are influencing the user experience of an AR application. In this poster, we describe some of the technical factors that could influence the user experience. We focus on a use-case in the field of on-site sports spectating with mobile AR. We present a study design which analyzes the influence of latency, registration accuracy, and jitter as factors on AR user experience.",augmented reality; situated visualization; user experience,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,The Application of Virtual Reality in Student Recruitment,VRST - Virtual Reality Software and Technology,A,"In this paper we present details of a virtual tour and game for VR headset that are designed to investigate an interactive and engaging approach of applying VR to student recruitment for an undergraduate course. The VR tour employs a floating menu to navigate through a set of 360° panoramic photographs of the teaching environment and uses hotspot interaction to display further information about the course. The VR game is a fast-paced shooting game. The course information is embedded on cubes that the player needs to focus on and destroy. The game experience is expected to generate an engaging way to promote the course. This work in progress outlines the concept and development of the prototype, and discusses the next stages of testing in order to evaluate the effectiveness of applying VR to undergraduate student recruitment.",Student Recruitment; Virtual Reality; VR Games,Title_Keywords,True,
Scopus,conferencePaper,2021,The Effect of 2D Stylized Visualization of the Real World for Obstacle Avoidance and Safety in Virtual Reality System Usage,VRST - Virtual Reality Software and Technology,A,"Using virtual reality systems with the head-mounted display can incur interaction difficulties and safety problems because of the user’s view being isolated from the real world operating space. One possible solution is to super-impose the real world objects or environment information onto the virtual scene. A variety of such visualization methods have been proposed, all in hopes of minimizing the negative effects of introducing foreign elements to the original virtual scene. In this poster, we propose to apply the neural style transfer technique to blend in the real world operating environment in the style of the given virtual space to make the super-imposed resulting image as natural as possible, maintaining the sense of immersion with the least level of distraction. Our pilot experimental study has shown that the stylization obscured the clear presentation of the environment and worsened or did not improve the safe user performance, and was neither considered sufficiently natural.",Immersion; Neural style transfer; Obstacle avoidance; Presence; Safety; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,UGRA in VR: A Virtual Reality Simulation for Training Anaesthetists,VRST - Virtual Reality Software and Technology,A,"We present a virtual reality training simulator for medical interns practicing ultrasound-guided regional anaesthesia (UGRA). UGRA is a type of nerve block procedure performed commonly by critical care doctors such as anaesthetists, emergency medicine physicians, and paramedics. This procedure is complex and requires intense training. It is traditionally taught one-on-one by experts and is performed on simulated models long before attempting the procedure on live patients. We present our virtual reality application that allows for training this procedure in a simulated environment. The use of virtual reality makes training future doctors performing UGRA safer and more cost efficient than current approaches.",Ultrasound-guided regional anaesthesia; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,Using Hand Tracking and Voice Commands to Physically Align Virtual Surfaces in AR for Handwriting and Sketching with HoloLens 2,VRST - Virtual Reality Software and Technology,A,"In this paper, we adapt an existing VR framework for handwriting and sketching on physically aligned virtual surfaces to AR environments using the Microsoft HoloLens 2. We demonstrate a multimodal input metaphor to control the framework’s calibration features using hand tracking and voice commands. Our technical evaluation of fingertip/surface accuracy and precision on physical tables and walls is in line with existing measurements on comparable hardware, albeit considerably lower compared to previous work using controller-based VR devices. We discuss design considerations and the benefits of our unified input metaphor suitable for controller tracking and hand tracking systems. We encourage extensions and replication by providing a publicly available reference implementation (https://go.uniwue.de/hci-otss-hololens).",augmented reality; finger tracking; handwriting; holoLens; sketching; stylus; surface alignment; virtual reality,Keywords,True,
Scopus,conferencePaper,2021,Visualisation methods for patient monitoring in anaesthetic procedures using augmented reality,VRST - Virtual Reality Software and Technology,A,"In health care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in patients. Previous research has shown that the use of augmented reality can help to better monitor the condition of patients and better detect unforeseen events. The system created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.",augmented reality; health care; safety-critical-systems,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,VR Rehearse &amp; Perform - A platform for rehearsing in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose VR Rehearse &amp; Perform - a Virtual Reality application for enhancing the rehearsal efforts of performers by providing them access to accurate recreations - both visual and acoustical - of iconic concert venues.",Acoustic Environments; Musicians; Rehearsing; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2021,VRBT: A Non-pharmacological VR approach towards hypertension,VRST - Virtual Reality Software and Technology,A,"Hypertension is a prevalent disease that is known to affect the vascular system especially to the people with poor living habits and lifestyles. Virtual reality (VR) is effective to interact with people to release their pressure and cheer them up, which however is less conducted towards manipulating blood pressure and hypertension. In this paper, we consider how hypertension can be treated with VR devices and design virtual reality river bathing therapy (VRBT) with respect to a combination of traditional methods through sensory stimulation, audio interventions, and motor training.",Hypertension; non-pharmacological therapy; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2021,"XRSpectator: Immersive, Augmented Sports Spectating",VRST - Virtual Reality Software and Technology,A,"In-stadium sports spectating delivers a unique social experience in a variety of sports. However, in contrast to broadcast delivery, it lacks the provision of real-time information augmentation, like game statistics overlaid on screen. In an earlier iteration, we developed ARSpectator, a prototypical, mobile system which can be brought to the stadium to experience both, the live sport action and situated infographics spatially augmented into the scene. In some situations it is difficult or often impossible to go to the stadium though, for instance because of limited stadium access during pandemics or when wanting to conduct controlled user studies. We address this by turning our ARSpectator system into an indirect augmented reality experience deployed to an immersive, virtual reality head-mounted display: The live stadium experience is delivered by way of a surrounding 360 video recording while maintaining and extending the provision of interactive, situated infographics. With our XRSpectator demo prototype presented here, users can have an ARSpectator experience of a rugby game in our local stadium.",mixed reality; situated visualization; sports spectating,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Carousel: Improving the Accuracy of Virtual Reality Assessments for Inspection Training Tasks,VRST - Virtual Reality Software and Technology,A,"Training simulations in virtual reality (VR) have become a focal point of both research and development due to allowing users to familiarize themselves with procedures and tasks without needing physical objects to interact with or needing to be physically present. However, the increasing popularity of VR training paradigms raises the question: Are VR-based training assessments accurate? Many VR training programs, particularly those focused on inspection tasks, employ simple pass or fail assessments. However, these types of assessments do not necessarily reflect the user’s knowledge. In this paper, we present Carousel, a novel VR-based assessment method that requires users to actively employ their training knowledge by considering all relevant scenarios during assessments. We also present a within-subject user study that compares the accuracy of our new Carousel method to a conventional pass or fail method for a series of virtual object inspection tasks involving shapes and colors. The results of our study indicate that the Carousel method affords significantly more-accurate assessments of a user’s knowledge than the binary-choice method.",training assessments.; virtual inspections; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Leveraging VR Techniques for Efficient Exploration and Interaction in Large and Complex AR Space with Clipped and Small FOV AR Display,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose to take advantage of the digital twinned environment to interact more efficiently in the large and complex AR space in spite of the limited sized and clipped FOV of the AR display. Using the digital twin of the target environment, “magical” VR interaction techniques can be applied, as visualized and overlaid through the small window, while still maintaining the spatial association to the augmented real world. First we consider the use of amplified movement within the corresponding VR twinned space to help the user search, plan, navigate and explore efficiently by providing an effectively larger view and thereby better spatial understanding of the same AR space with less amount of physical movements. Secondly, we also apply the amplified movement and in addition, the stretchable arm to interact with relatively large objects (or largely spaced objects) which cannot be seen in their entirety at a time with the small FOV glass. The results of the experiment with the proposed methods have showed advantages with regards to the interaction performance as the scene became more complex and task more difficult. The work illustrates the concept of and potential for XR based interaction where the user can leverage the advantages of both VR and AR mode operations.",augmented reality; digital twin; extended reality; interaction; mixed reality; navigation; object manipulation; virtual reality,Keywords,True,
Scopus,conferencePaper,2022,VR Games for Chronic Pain Management,VRST - Virtual Reality Software and Technology,A,"Chronic pain is a continuous ailment lasting for long periods after the initial injury or disease has healed. Chronic pain is challenging to treat and affects the daily lives of patients. Distraction therapy is a proven method of relieving patients’ discomfort by taking their attention away from the pain. Virtual reality (VR) is a platform for distraction therapy by immersing the user in a virtual world detached from reality. However, there is little research on how physical interactions in VR affect pain management. We present a study to evaluate the effectiveness of physically active, mentally active, and passive interventions in VR using games with chronic pain patients. Our results indicate that physical and mental activities in VR are equally effective at reducing pain. Furthermore, These actively engage patients, while the effects of observing relaxing content persist outside VR. These findings can help inform the design of future VR games targeted at chronic pain management.",Chronic Pain Management; User Study; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,VISTA: User-centered VR Training System for Effectively Deriving Characteristics of People with Autism Spectrum Disorder,VRST - Virtual Reality Software and Technology,A,"Pervasive symptoms of people with autism spectrum disorder (ASD), such as a lack of social and communication skills, are major challenges to be embraced in the workplace. Although much research has proposed VR training programs, their effectiveness is somewhat unclear, since they provide limited, one-sided interactions through fixed scenarios or do not sufficiently reflect the characteristics of people with ASD (e.g., preference for predictable interfaces, sensory issues). In this paper, we present VISTA, a VR-based interactive social skill training system for people with ASD. We ran a user study with 10 people with ASD and 10 neurotypical people to evaluate user experience in VR training and to examine the characteristics of people with ASD based on their physical responses generated by sensor data. The results showed that ASD participants were highly engaged with VISTA and improved self-efficacy after experiencing VISTA. The two groups showed significant differences in sensor signals as the task complexity increased, which demonstrates the importance of considering task complexity in eliciting the characteristics of people with ASD in VR training. Our findings not only extend findings (e.g., low ROI ratio, EDA increase) in previous studies but also provide new insights (e.g., high utterance rate, large variation of pupil diameter), broadening our quantitative understanding of people with ASD.",Autism Spectrum Disorder (ASD); Social skills training system; User study; Virtual reality,Keywords,True,
Scopus,conferencePaper,2022,Exploring User Behaviour in Asymmetric Collaborative Mixed Reality,VRST - Virtual Reality Software and Technology,A,"A common issue for collaborative mixed reality is the asymmetry of interaction with the shared virtual environment. For example, an augmented reality (AR) user might use one type of head-mounted display (HMD) in a physical environment, while a virtual reality (VR) user might wear a different type of HMD and see a virtual model of that physical environment. To explore the effects of such asymmetric interfaces on collaboration we present a study that investigates the behaviour of dyads performing a word puzzle task where one uses AR and the other VR. We examined the collaborative process through questionnaires and behavioural measures based on positional and audio data. We identified relationships between presence and co-presence, accord and co-presence, leadership and talkativeness, head rotation velocity and leadership, and head rotation velocity and talkativeness. We did not find that AR or VR biased subjective responses, though there were interesting behavioural differences: AR users spoke more words, AR users had a higher median head rotation velocity, and VR users travelled further.",augmented reality; collaboration; mixed reality; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Marcus or Mira - Investigating the Perception of Virtual Agent Gender in Virtual Reality Role Play-Training,VRST - Virtual Reality Software and Technology,A,"Immersive virtual training environments are used in various domains. In this work we focus on role-play training in virtual reality. In virtual role-play training conversations and interactions with virtual agents are often fundamental to the training. Therefore, the appearance and behavior of the agents plays an important role when designing role-play training. We focus on the gender appearance of agents, as gender is an important aspect for differentiation between characters. We conducted a study with 40 participants in which we investigated how agents gender appearance influences the perception of the agents´ personality traits and the self-perception of a participants’ assumed role in a training for social skills. This work contributes towards understanding the design-space of virtual agent design, virtual agent gender identity, and the design and development of immersive virtual reality role-play training.",Gender; Training; Virtual Agents; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Evaluating the Effects of Virtual Human Animation on Students in an Immersive VR Classroom Using Eye Movements,VRST - Virtual Reality Software and Technology,A,"Virtual humans presented in VR learning environments have been suggested in previous research to increase immersion and further positively influence learning outcomes. However, how virtual human animations affect students’ real-time behavior during VR learning has not yet been investigated. This work examines the effects of social animations (i.e., hand raising of virtual peer learners) on students’ cognitive response and visual attention behavior during immersion in a VR classroom based on eye movement analysis. Our results show that animated peers that are designed to enhance immersion and provide companionship and social information elicit different responses in students (i.e., cognitive, visual attention, and visual search responses), as reflected in various eye movement metrics such as pupil diameter, fixations, saccades, and dwell times. Furthermore, our results show that the effects of animations on students differ significantly between conditions (20%, 35%, 65%, and 80% of virtual peer learners raising their hands). Our research provides a methodological foundation for investigating the effects of avatar animations on users, further suggesting that such effects should be considered by developers when implementing animated virtual humans in VR. Our findings have important implications for future works on the design of more effective, immersive, and authentic VR environments.",education; eye-tracking; immersive virtual reality; virtual human animation; visual attention,Keywords,True,
Scopus,conferencePaper,2022,Effect of Stereo Deficiencies on Virtual Distal Pointing,VRST - Virtual Reality Software and Technology,A,"Previous work has shown that the mismatch between disparity and optical focus cues, i.e., the vergence and accommodation conflict (VAC), affects virtual hand selection in immersive systems. To investigate if the VAC also affects distal pointing with ray casting, we ran a user study with an ISO 9241:411 multidirectional selection task where participants selected 3D targets with three different VAC conditions, no VAC, i.e., targets placed roughly at 75 cm, which matches the focal plane of the VR headset, constant VAC, i.e., at 400 cm from the user, and varying VAC, where the depth distance of targets changed between 75 cm and 400 cm. According to our results, the varying VAC condition requires the most time and decreases the throughput performance of the participants. It also takes longer for users to select targets in the constant VAC condition than without the VAC. Our results show that in distal pointing placing objects at different depth planes has detrimental effect on the user performance.",Distal Pointing; Ray Casting; Selection; Stereo Deficiencies; Vergence-Accommodation Conflict; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2022,"Rich virtual feedback from sensorimotor interaction may harm, not help, learning in immersive virtual reality",VRST - Virtual Reality Software and Technology,A,"Sensorimotor interactions in the physical world and in immersive virtual reality (IVR) offer different feedback. Actions in the physical world almost always offer multi-modal feedback: pouring a jug of water offers tactile (weight-change), aural (the sound of running water) and visual (water moving out the jug) feedback. Feedback from pouring a virtual jug, however, depends on the IVR’s design. This study examines if the richness of feedback from IVR actions causes a detectable cognitive impact on users. To do this, we compared verb-learning outcomes between two conditions in which participants make actions with objects and (1) audiovisual feedback is presented; (2) audiovisual feedback is not presented. We found that participants (n = 74) had cognitively distinct outcomes based on the type of audiovisual feedback experienced, with a high feedback experience harming learning outcomes compared with a low feedback one. This result has implications for IVR system design and theories of cognition and memorisation.",embodiment; gestural input; interactive virtual environments; language; learning; motivation; presence; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Exploration of Form Factor and Bimanual 3D Manipulation Performance of Rollable In-hand VR Controller,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) environments are expected to become future workspaces. An effective bimanual 3D manipulation technique would be essential to support this vision. A ball-shaped tangible input device that can be rolled in a hand is known to be useful for 3D object manipulation because such devices allow users to utilize their finger dexterity. In this study, we further explored the potential of a rollable in-hand controller. First, we evaluated the effects of its form factor on user behavior and performance. Although the size and shape of a rollable controller are expected to influence user behavior and performance, their effects have not been empirically explored in prior works. Next, we evaluated a rollable controller on bimanual 3D assembly tasks. A rollable controller may incur a high mental load as it requires users to use finger dexterity; therefore, the benefit of using such a device in each hand is not obvious. We found that a 5 cm-diameter ball-shaped controller was the most effective among the sizes and forms that we considered, and that a pair of in-hand rollable controllers showed significantly faster completion time than a pair of VR controllers for complex bimanual assembly tasks involving frequent rotations.",Bimanual Task; Finger Dexterity; Rollable In-hand Controller; Tangible Interface; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Eliciting Multimodal Gesture+Speech Interactions in a Multi-Object Augmented Reality Environment,VRST - Virtual Reality Software and Technology,A,"As augmented reality (AR) technology and hardware become more mature and affordable, researchers have been exploring more intuitive and discoverable interaction techniques for immersive environments. This paper investigates multimodal interaction for 3D object manipulation in a multi-object AR environment. To identify the user-defined gestures, we conducted an elicitation study involving 24 participants and 22 referents using an augmented reality headset. It yielded 528 proposals and generated a winning gesture set with 25 gestures after binning and ranking all gesture proposals. We found that for the same task, the same gesture was preferred for both one and two-object manipulation, although both hands were used in the two-object scenario. We present the gestures and speech results, and the differences compared to similar studies in a single object AR environment. The study also explored the association between speech expressions and gesture stroke during object manipulation, which could improve the recognizer efficiency in augmented reality headsets.",augmented reality; elicitation; gesture and speech interaction; multi-object AR environment; multimodal interaction,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Performance Analysis of Saccades for Primary and Confirmatory Target Selection,VRST - Virtual Reality Software and Technology,A,"In eye-gaze-based selection, dwell suffers from several issues, e.g., the Midas Touch problem. Here we investigate saccade-based selection techniques as an alternative to dwell. First, we designed a novel user interface (UI) for Actigaze and used it with (goal-crossing) saccades for confirming the selection of small targets (i.e., &lt; 1.5-2°). We compared it with three other variants of Actigaze (with button press, dwell, and target reverse crossing) and two variants of target magnification (with button press and dwell). Magnification-dwell exhibited the most promising performance. For Actigaze, goal-crossing was the fastest option but suffered the most errors. We then evaluated goal-crossing as a primary selection technique for normal-sized targets (≥ 2°) and implemented a novel UI for such interaction. Results revealed that dwell achieved the best performance. Yet, we identified goal-crossing as a good compromise between dwell and button press. Our findings thus identify novel options for gaze-only interaction.",Activation Methods; Eye-Gaze Tracking; Fitts’ Law; Saccade; Selection Techniques; Small Targets; Target Reverse Crossing; Throughput; Virtual Reality,Keywords,True,
Scopus,conferencePaper,2022,Precueing Sequential Rotation Tasks in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Augmented reality has been used to improve sequential-task performance by cueing information about a current task step and precueing information about future steps. Existing work has shown the benefits of precueing movement (translation) information. However, rotation is also a major component in many real-life tasks, such as turning knobs to adjust parameters on a console. We developed an AR testbed to investigate whether and how much precued rotation information can improve user performance. We consider two unimanual tasks: one requires a user to make sequential rotations of a single object, and the other requires the user to move their hand between multiple objects to rotate them in sequence. We conducted a user study to explore these two tasks using circular arrows to communicate rotation. In the single-object task, we examined the impact of number of precues and visualization style on user performance. Results show that precues improved performance and that arrows with highlighted heads and tails, with each destination aligned with the next origin, yielded the shortest completion time on average. In the multiple-object task, we explored whether rotation precues can be helpful in conjunction with movement precues. Here, using a rotation cue without rotation precues in conjunction with a movement cue and movement precues performed the best, implying that rotation precues were not helpful when movement was also required.",cueing; object rotation; precueing,Title_Abstract,True,
Scopus,conferencePaper,2022,Effects of Environmental Noise Levels on Patient Handoff Communication in a Mixed Reality Simulation,VRST - Virtual Reality Software and Technology,A,"When medical caregivers transfer patients to another person’s care (a patient handoff), it is essential they effectively communicate the patient’s condition to ensure the best possible health outcomes. Emergency situations caused by mass casualty events (e.g., natural disasters) introduce additional difficulties to handoff procedures such as environmental noise. We created a projected mixed reality simulation of a handoff scenario involving a medical evacuation by air and tested how low, medium, and high levels of helicopter noise affected participants’ handoff experience, handoff performance, and behaviors. Through a human-subjects experimental design study (N = 21), we found that the addition of noise increased participants’ subjective stress and task load, decreased their self-assessed and actual performance, and caused participants to speak louder. Participants also stood closer to the virtual human sending the handoff information when listening to the handoff than they stood to the receiver when relaying the handoff information. We discuss implications for the design of handoff training simulations and avenues for future handoff communication research.",environmental noise; human-subject research; Patient handoffs; virtual and mixed reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Timeline Design Space for Immersive Exploration of Time-Varying Spatial 3D Data,VRST - Virtual Reality Software and Technology,A,"Timelines are common visualizations to represent and manipulate temporal data. However, timeline visualizations rarely consider spatio-temporal 3D data (e.g. mesh or volumetric models) directly. In this paper, leveraging the increased workspace and 3D interaction capabilities of virtual reality (VR), we first propose a timeline design space for 3D temporal data extending the timeline design space proposed by Brehmer et al.&nbsp;[7]. The proposed design space adapts the scale, layout and representation dimensions to account for the depth dimension and how the 3D temporal data can be partitioned and structured. Moreover, an additional dimension is introduced, the support, which further characterizes the 3D dimension of the visualization. The design space is complemented by discussing the interaction methods required for the efficient visualization of 3D timelines in VR. Secondly, we evaluate the benefits of 3D timelines through a formal evaluation (n=21). Taken together, our results showed that time-related tasks can be achieved more comfortably using timelines, and more efficiently for specific tasks requiring the analysis of the surrounding temporal context. Finally, we illustrate the use of 3D timelines with a use-case on morphogenetic analysis in which domain experts in cell imaging were involved in the design and evaluation process.",3D temporal data; Multidimensional data; Timelines; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Virtual Air Conditioner’s Airflow Simulation and Visualization in AR,VRST - Virtual Reality Software and Technology,A,"This paper presents a mobile AR system for visualizing airflow and temperature change made by virtual air conditioners. Even though there have been efforts to integrate the results of airflow/temperature simulation into the real world via AR, they support neither interactive modeling of the environments nor real-time simulation. This paper presents an AR system, where 3D mapping and air conditioner installation are made interactively, and then airflow/temperature simulation and visualization are made at real time. The proposed system is designed in a client-server architecture, where the server is in charge of simulation and the rest is taken by the client.",Augmented reality; Flow visualization; HCI; Physics-based simulation,Keywords,True,
Scopus,conferencePaper,2022,Nebula: An Affordable Open-Source and Autonomous Olfactory Display for VR Headsets,VRST - Virtual Reality Software and Technology,A,"The impact of olfactory cues on user experience in virtual reality is increasingly studied. However, results are still heterogeneous and existing studies difficult to replicate, mainly due to a lack of standardized olfactory displays. In that context, we present Nebula, a low-cost, open-source, olfactory display capable of diffusing scents at different diffusion rates using a nebulization process. Nebula can be used with PC VR or autonomous head-mounted displays, making it easily transportable without the need for an external computer. The device was calibrated to diffuse at three diffusion rates: no diffusion, low and high. For each level, the quantity of delivered odor was precisely characterized using a repeated weighting method. The corresponding perceived olfactory intensities were evaluated by a psychophysical experiment on sixteen participants. Results demonstrated the device capability to successfully create three significantly different perceived odor intensities (Friedman test p &lt; 10− 6, Wilcoxon tests padj &lt; 10− 3), without noticeable smell persistence and with limited noise and discomfort. For reproducibility and to stimulate further research in the area, 3D printing files, electronic hardware schemes, and firmware/software source-code are made publicly available.",Autonomous VR experiment; Wearable olfactory display,Abstract,True,
Scopus,conferencePaper,2022,3D Reconstruction of Sculptures from Single Images via Unsupervised Domain Adaptation on Implicit Models,VRST - Virtual Reality Software and Technology,A,"Acquiring the virtual equivalent of exhibits, such as sculptures, in virtual reality (VR) museums, can be labour-intensive and sometimes infeasible. Deep learning based 3D reconstruction approaches allow us to recover 3D shapes from 2D observations, among which single-view-based approaches can reduce the need for human intervention and specialised equipment in acquiring 3D sculptures for VR museums. However, there exist two challenges when attempting to use the well-researched human reconstruction methods: limited data availability and domain shift. Considering sculptures are usually related to humans, we propose our unsupervised 3D domain adaptation method for adapting a single-view 3D implicit reconstruction model from the source (real-world humans) to the target (sculptures) domain. We have compared the generated shapes with other methods and conducted ablation studies as well as a user study to demonstrate the effectiveness of our adaptation method. We also deploy our results in a VR application.",3D Reconstruction; Domain Adaptation; Transfer Learning; Unsupervised Learning; VR,Abstract,True,
Scopus,conferencePaper,2022,The Relative Importance of Depth Cues and Semantic Edges for Indoor Mobility Using Simulated Prosthetic Vision in Immersive Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Visual neuroprostheses (bionic eyes) have the potential to treat degenerative eye diseases that often result in low vision or complete blindness. These devices rely on an external camera to capture the visual scene, which is then translated frame-by-frame into an electrical stimulation pattern that is sent to the implant in the eye. To highlight more meaningful information in the scene, recent studies have tested the effectiveness of deep-learning based computer vision techniques, such as depth estimation to highlight nearby obstacles (DepthOnly mode) and semantic edge detection to outline important objects in the scene (EdgesOnly mode). However, nobody has yet attempted to combine the two, either by presenting them together (EdgesAndDepth) or by giving the user the ability to flexibly switch between them (EdgesOrDepth). Here, we used a neurobiologically inspired model of simulated prosthetic vision (SPV) in an immersive virtual reality (VR) environment to test the relative importance of semantic edges and relative depth cues to support the ability to avoid obstacles and identify objects. We found that participants were significantly better at avoiding obstacles using depth-based cues as opposed to relying on edge information alone, and that roughly half the participants preferred the flexibility to switch between modes (EdgesOrDepth). This study highlights the relative importance of depth cues for SPV mobility and is an important first step towards a visual neuroprosthesis that uses computer vision to improve a user’s scene understanding.",bionic vision; indoor mobility; scene simplification; simulated prosthetic vision; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Adaptive Field-of-view Restriction: Limiting Optical Flow to Mitigate Cybersickness in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Dynamic field-of-view (FOV) restriction is a widely used software technique to mitigate cybersickness in commercial virtual reality (VR) applications. The classical FOV restrictor is implemented using a symmetric mask that occludes the periphery in response to translational and/or angular velocity. In this paper, we introduce adaptive field-of-view restriction, a novel technique that responds dynamically based on real-time assessment of optical flow generated by movement through a virtual environment. The adaptive restrictor utilizes an asymmetric mask to obscure regions of the periphery with higher optical flow during virtual locomotion while leaving regions with lower optical flow visible. To evaluate the proposed technique, we conducted a gender-balanced user study (N = 38) in which participants completed in a navigation task in two different types of virtual scenes using controller-based locomotion. Participants were instructed to navigate through either close-quarter or open virtual environments using adaptive restriction, traditional symmetric restriction, or an unrestricted control condition in three VR sessions separated by at least 24 hours. The results showed that the adaptive restrictor was effective in mitigating cybersickness and reducing subjective discomfort, while simultaneously enabling participants to remain immersed for a longer amount of time compared to the control condition. Additionally, presence ratings were significantly higher when using the adaptive restrictor compared to symmetric restriction. In general, these results suggest that adaptive field-of-view restriction based on real-time measurement of optical flow is a promising approach for virtual reality applications that seek to provide a better cost-benefit tradeoff between comfort and a high-fidelity experience.",cybersickness; field-of-view; optical flow; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Sweating Avatars Decrease Perceived Exertion and Increase Perceived Endurance while Cycling in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Avatars are used to represent users in virtual reality (VR) and create embodied experiences. Previous work showed that avatars’ stereotypical appearance can affect users’ physical performance and perceived exertion while exercising in VR. Although sweating is a natural human response to physical effort, surprisingly little is known about the effects of sweating avatars on users. Therefore, we conducted a study with 24 participants to explore the effects of sweating avatars while cycling in VR. We found that visualizing sweat decreases the perceived exertion and increases perceived endurance. Thus, users feel less exerted while embodying sweating avatars. We conclude that sweating avatars contribute to more effective exergames and fitness applications.",avatars; body ownership; exergames; perception of effort; Proteus effect; sweating; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Standing Balance Improvement Using Vibrotactile Feedback in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) users often encounter postural instability, i.e., balance issues, which can be a significant impediment to universal usability and accessibility, particularly for those with balance impairments. Prior research has validated imbalance issues, but little effort has been made to mitigate them. We recruited 39 participants (with balance impairments: 18, without balance impairments: 21) to examine the effect of various vibrotactile feedback techniques on balance in virtual reality, specifically spatial vibrotactile, static vibrotactile, rhythmic vibrotactile, and vibrotactile feedback mapped to the center of pressure (CoP). Participants completed standing visual exploration and standing reach and grasp tasks. According to within-subject results, each vibrotactile feedback enhanced balance in VR significantly (p &lt;.001) for those with and without balance impairments. Spatial and CoP vibrotactile feedback enhanced balance significantly more (p &lt;.001) than other vibrotactile feedback. This study presents strategies that might be used in future virtual environments to enhance standing balance and bring VR closer to universal usage.",accessibiliity; standing balance; vibrotactile feedback; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,The Rubber Hand Illusion in Virtual Reality and the Real World - Comparable but Different,VRST - Virtual Reality Software and Technology,A,"Feeling ownership of a virtual body is crucial for immersive experiences in VR. Knowledge about body ownership is mainly based on rubber hand illusion (RHI) experiments in the real world. Watching a rubber hand being stroked while one’s own hidden hand is synchronously stroked, humans experience the rubber hand as their own hand and underestimate the distance between the rubber hand and the real hand (proprioceptive drift). There is also evidence for a decrease in hand temperature. Although the RHI has been induced in VR, it is unknown whether effects in VR and the real world differ. We conducted a RHI experiment with 24 participants in the real world and in VR and found comparable effects in both environments. However, irrespective of the RHI, proprioceptive drift and temperature differences varied between settings. Our findings validate the utilization of the RHI in VR to increase our understanding of embodying virtual avatars.",avatars; body ownership illusion; disownership; proprioceptive drift; rubber hand illusion; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2022,Walk This Beam: Impact of Different Balance Assistance Strategies and Height Exposure on Performance and Physiological Arousal in VR,VRST - Virtual Reality Software and Technology,A,"Dynamic balance is an essential skill for the human upright gait; therefore, regular balance training can improve postural control and reduce the risk of injury. Even slight variations in walking conditions like height or ground conditions can significantly impact walking performance. Virtual reality is used as a helpful tool to simulate such challenging situations. However, there is no agreement on design strategies for balance training in virtual reality under stressful environmental conditions such as height exposure. We investigate how two different training strategies, imitation learning, and gamified learning, can help dynamic balance control performance across different stress conditions. Moreover, we evaluate the stress response as indexed by peripheral physiological measures of stress, perceived workload, and user experience. Both approaches were tested against a baseline of no instructions and against each other. Thereby, we show that a learning-by-imitation approach immediately helps dynamic balance control, decreases stress, improves attention focus, and diminishes perceived workload. A gamified approach can lead to users being overwhelmed by the additional task. Finally, we discuss how our approaches could be adapted for balance training and applied to injury rehabilitation and prevention.",balance control; physiological arousal; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,“Kapow!”: Studying the Design of Visual Feedback for Representing Contacts in Extended Reality,VRST - Virtual Reality Software and Technology,A,"In absence of haptic feedback, the perception of contact with virtual objects can rapidly become a problem in extended reality (XR) applications. XR developers often rely on visual feedback to inform the user and display contact information. However, as for today, there is no clear path on how to design and assess such visual techniques. In this paper, we propose a design space for the creation of visual feedback techniques meant to represent contact with virtual surfaces in XR. Based on this design space, we conceived a set of various visual techniques, including novel approaches based on onomatopoeia and inspired by cartoons, or visual effects based on physical phenomena. Then, we conducted an online preliminary user study with 60 participants, consisting in assessing 6 visual feedback techniques in terms of user experience. We could notably assess, for the first time, the potential influence of the interaction context by comparing the participants’ answers in two different scenarios: industrial versus entertainment conditions. Taken together, our design space and initial results could inspire XR developers for a wide range of applications in which the augmentation of contact seems prominent, such as for vocational training, industrial assembly/maintenance, surgical simulation, videogames, etc.",Contact; Design Space; Extended Reality; User Experience; Visual Feedback,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Understanding Perspectives for Single- and Multi-Limb Movement Guidance in Virtual 3D Environments,VRST - Virtual Reality Software and Technology,A,"Movement guidance in virtual reality has many applications ranging from physical therapy, assistive systems to sport learning. These movements range from simple single-limb to complex multi-limb movements. While VR supports many perspectives – e.g., first person and third person – it remains unclear how accurate these perspectives communicate different movements. In a user study (N=18), we investigated the influence of perspective, feedback, and movement properties on the accuracy of movement guidance. Participants had on average an angle error of 6.2° for single arm movements, 7.4° for synchronous two arm movements, and 10.3° for synchronous two arm and leg movements. Furthermore, the results show that the two variants of third-person perspectives outperform a first-person perspective for movement guidance (19.9% and 24.3% reduction in angle errors). Qualitative feedback confirms the quantitative data and shows users have a clear preference for third-person perspectives. Through our findings we provide guidance for designers and developers of future VR movement guidance systems.",body visualization; Movement guidance; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Design and Evaluation of Electrotactile Rendering Effects for Finger-Based Interactions in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"The use of electrotactile feedback in Virtual Reality (VR) has shown promising results for providing tactile information and sensations. While progress has been made to provide custom electrotactile feedback for specific interaction tasks, it remains unclear which modulations and rendering algorithms are preferred in rich interaction scenarios. In this paper, we propose a unified tactile rendering architecture and explore the most promising modulations to render finger interactions in VR. Based on a literature review, we designed six electrotactile stimulation patterns/effects (EFXs) striving to render different tactile sensations. In a user study (N=18), we assessed the six EFXs in three diverse finger interactions: 1) tapping on a virtual object; 2) pressing down a virtual button; 3) sliding along a virtual surface. Results showed that the preference for certain EFXs depends on the task at hand. No significant preference was detected for tapping (short and quick contact); EFXs that render dynamic intensities or dynamic spatio-temporal patterns were preferred for pressing (continuous dynamic force); EFXs that render moving sensations were preferred for sliding (surface exploration). The results showed the importance of the coherence between the modulation an the interaction being performed and the study proved the versatility of electrotactile feedback and its efficiency in rendering different haptic information and sensations.",electrotactile feedback; human computer interaction; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,A Method of Estimating the Object of Interest from 3D Object and User’s Gesture in VR,VRST - Virtual Reality Software and Technology,A,"In VR, gaze information is useful for directly or indirectly analyzing a user’s interest. However, there are inconveniences in using the eye tracking in the VR device. To overcome the drawback, we propose a method of estimating an object of interest from user’s gesture instead of eye tracking. LightGBM model is trained by using distance and angle-based features that are extracted from 3d information of the object and the position and rotation of the VR device. We compared accuracy of each feature for VR device combinations and found out that it is more efficient to use all devices instead of individual devices and to use angle-based feature instead of distance-based feature with accuracy of 79.36%.",the object of interest; virtual reality,Keywords,True,
Scopus,conferencePaper,2022,A Mixed Reality Platform for Collaborative Technical Assembly Training,VRST - Virtual Reality Software and Technology,A,"We have developed a mixed reality (MR)-based platform for basic mechanical engineering concepts as a learning environment for collaborative assembly tasks. In our platform, multiple co-located users interact with virtual objects simultaneously, and during that time, the platform collects data related to participants’ collaboration and team behavior. We implemented four main sections in the platform including setup, introduction, training, and assessment. The platform provides the opportunity for users to interact with virtual objects while also acquiring technical knowledge. Specifically, for the technical component of the platform, users are asked to assemble a hydraulic pump by manipulating and fitting various parts and pieces into a provided pre-assembled blueprint. We conducted a preliminary expert panel review composed of three experts and received positive feedback and suggestions for further development of the platform.",mixed reality; technical assembly; training,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,An Interactive Haptic Display System with Changeable Hardness Using Magneto-Rheological Fluid,VRST - Virtual Reality Software and Technology,A,"We present a haptic display system with changeable hardness using magneto-rheological (MR) fluid. The major component is the haptic device with layers of MR fluid, contact point and pressure sensors, and electromagnet. The system enables multi-modal interaction using this device with control circuits and a projector. We also developed two types of contents aiming for multi-modal virtual and mixed reality experiences.",display; haptics; hardness; Magneto-rheological fluid; surface,Abstract,True,
Scopus,conferencePaper,2022,Augmented Reality Patient-Specific Registration for Medical Visualization,VRST - Virtual Reality Software and Technology,A,"In recent years, medical research has made extensive use of Augmented Reality (AR) for visualization. These visualizations provide improved 3D understanding and depth perception for surgeons and medical staff during surgical planning, medical training, and procedures. Often, AR in medicine involves impractical and extensive instrumentation in order to provide the precision needed for clinical use. We propose a mobile AR 3D model registration system for use in a practical, non-instrumented hospital setting. Our registration system takes as input a patient-specific model and overlays it on the patient using an accurate pose registration technique that requires a single marker as a point of reference to initialize a point cloud-based pose refinement technique. Our method is automatic, easy to use, and runs in real-time on a mobile phone. We conduct quantitative and qualitative analysis of the registration. The results confirm that our AR pose registration system produces an accurate and visually correct overlay of the medical data in real-time.",3D Pose Estimation; Depth Map; Point Cloud; Visualization,Title_Abstract,True,
Scopus,conferencePaper,2022,Can Haptic Feedback on One Virtual Object Increase the Presence of Another Virtual Object?,VRST - Virtual Reality Software and Technology,A,"This paper investigated whether increased presence from experiencing haptic feedback on one virtual object can transfer to another virtual object. Two similar studies were run in different environments: an immersive virtual environment and a mixed environment. Results showed that participants reported a high presence of untouched virtual object after touching a virtual object in a virtual reality environment. On the other hand, it was difficult to confirm that such presence transfers occurred in an augmented reality environment.",Augmented Reality; Haptics; Presence; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Common Experience Sample 1.0: Developing a sample for comparing the characteristics of haptic displays,VRST - Virtual Reality Software and Technology,A,"Many haptic displays that provide haptic feedback to users have been proposed;however, differences in experimental environments make comparisons of displays difficult. Therefore, we categorized the characteristics of feedback based on existing research, and developed a common experience sample that includes virtual objects necessary for the expression of each characteristic. Additionally, we will study the methods of evaluating displays using the proposed sample, and aim at comparative evaluation of multiple displays.",crossmodal; evaluation; haptic; multimodal; sample; virtual reality,Keywords,True,
Scopus,conferencePaper,2022,CourseExpo: An Immersive Collaborative Learning Ecosystem,VRST - Virtual Reality Software and Technology,A,"Inspired by the need for remote learning technologies due to the Covid-19 pandemic and the isolated sense of lonely learners, we reimagined a remote classroom that fosters collaboration, builds community and yet without the constraints of the physical world. This paper presents a collaborative learning ecosystem that resembles a traditional city square where avatars of learners and facilitators wander, commingle, discover, and learn together. Buildings in the city square are learning modules which include typical knowledge units, assessment booths, or custom collaborative sketching studios. Our attempted prototype at realizing this conceptualization demonstrated initial success and we offer recommendations for future work.",Classroom; Collaboration; Learning Ecosystem; Virtual reality,Keywords,True,
Scopus,conferencePaper,2022,Covid Reflections: AR in Public Health Communications,VRST - Virtual Reality Software and Technology,A,"Augmented reality in public health communications is an under-explored field. Researchers forward Covid Reflections, a public health communications installation which employs augmented reality enhanced with AI LiDAR body tracking to engage public audiences in short duration health-oriented experiences. Covid Reflections helps audiences to visualize potential health outcomes of Covid-19 through depicting the process of disease contraction, sickness, and potential hospitalization on a virtual avatar which mirrors the user’s physical body in real-time. The user is immersed in a “virtual first-hand experience” of Covid-19, and is thus supported in drawing concrete conclusions about the potential personal implications of contracting Covid-19.",,Abstract,True,
Scopus,conferencePaper,2022,Dill Pickle: Interactive Theatre Play in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"“Dill Pickle”, is the first interactive immersive theatre experience in virtual reality that uses volumetric capture. In the play, a volumetrically captured actor plays the character of Robert. The user interacts with Robert through utterances that are memorized or prompted with text or audio. The set was recreated through a process of photogrammetry.",Immersive theatre; Virtual Reality; Volumetric Capture,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Dynamic X-Ray Vision in Mixed Reality,VRST - Virtual Reality Software and Technology,A,"X-ray vision, a technique that allows users to see through walls and other obstacles, is a popular technique for Augmented Reality (AR) and Mixed Reality (MR). In this paper, we demonstrate a dynamic X-ray vision window that is rendered in real-time based on the user’s current position and changes with movement in the physical environment. Moreover, the location and transparency of the window are also dynamically rendered based on the user’s eye gaze. We build this X-ray vision window for a current state-of-the-art MR Head-Mounted Device (HMD) – HoloLens 2 [5] by integrating several different features: scene understanding, eye tracking, and clipping primitive.",HoloLens; Mixed Reality; X-ray vision,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Evaluation of Pseudo-Haptics system feedbacking muscle activity,VRST - Virtual Reality Software and Technology,A,"Differences in perceptions between virtual reality (VR) and reality prevent immersion in VR. To improve immersion in VR, many methods have adopted haptic feedback in VR using pseudo-haptics. However, these methods have little evaluated the effect of force feedback on pseudo-haptics that reflect the user’s state. This paper proposes and evaluates the pseudo-haptics system that manipulates the control/display (C/D) ratio between reality and VR using muscle activity measured. We conducted a user study under three conditions: the C/D ratio is constant, large, or small, depending on the muscle activity. Our results indicated that pseudo-haptics were effective for small C/D ratio settings during low myoelectric intensity.",multisensory integration; muscle activity; pseudo-haptics; virtual weight,Abstract,True,
Scopus,conferencePaper,2022,Exploration of inter-marker interactions in Tangible AR,VRST - Virtual Reality Software and Technology,A,"Inter-marker interactions in marker based Augmented Reality mobile applications are limited to movement and placement. In this paper we explore multiple inter-marker interactions in the tangible AR space along with their use cases. We developed prototypes that demonstrate primarily five inter-marker interactions; namely, proximity of two or more markers, placement of makers over each other, flipping of markers, marker as a toggle and marker as a controller. These interactions are designed such that they would correlate with multiple contexts of application. To demonstrate their usage we have chosen lattice structures in Chemistry as the context. Using our prototypes and the insights from initial evaluations, we discuss the benefits and drawbacks of such interaction methods. We further outline the opportunities of using these interactions and extending these concepts in several other contexts.",augumented reality; inter-marker interaction; tangible AR,Abstract,True,
Scopus,conferencePaper,2022,Finger Kinesthetic Haptic Feedback Device Using Shape Memory Alloy-based High-Speed Actuation Technique,VRST - Virtual Reality Software and Technology,A,"Compared to the study on tactile feedback gloves, the kinesthetic feedback device, which has been studied for the past several decades, has difficulties in interacting with the user owing to various problems, such as large size, low portability, and high power consumption. Herein, we present a bidirectional finger kinesthetic feedback device that can provide an immersive virtual reality experience using a shape memory alloy (SMA). The proposed device provides kinesthetic feedback without heterogeneity by integrating efficient power control of the SMA actuator, fast cooling of the SMA within one second, and high-precision motion-tracking technology. The implemented device delivers a gripping and hand spreading force of up to 10N each to the index and middle fingers.",Kinesthetic haptic feedback; Shape memory alloy; Virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Haptic Interaction Module for VR Fishing Leisure Activity,VRST - Virtual Reality Software and Technology,A,This paper presents a tiny haptic interaction module which generates high resistive torque for VR fishing leisure activity. The presented haptic interaction module was developed by magnetic rheological fluids and optimizing its structure. The measured haptic torque was varied from 0.3 N·cm to 2.4 N·cm as the applied voltage increased from 0 V to 5 V. The performance of the proposed actuator was qualitatively evaluated by constructing virtual fishing environment where a user can feel not only the weight of a target object but also its motion.,Haptics; MR actuator; Virtual reality; VR leisure activity,Keywords,True,
Scopus,conferencePaper,2022,Immersive Analytics for Spatio-Temporal Data on a Virtual Globe: Prototype and Emerging Research Challenges,VRST - Virtual Reality Software and Technology,A,"We present our approach for the immersive analysis of spatio-temporal data, using a three-dimensional virtual globe. We display quantitative data as country-shaped elevated polygons and animate elevation levels over time to represent the temporal dimension. This approach allows us to investigate global patterns of behaviour, like pandemic infection data. By using a virtual reality setting, we intend to increase our understanding of spatial data and potential global relationships. Based on the development of our prototype, we outline research challenges we see emerging in this context.",globe visualisation; pandemic data visualisation; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Improving Pedestrian Safety around Autonomous Delivery Robots in Real Environment with Augmented Reality,VRST - Virtual Reality Software and Technology,A,"In recent years, the use of autonomous vehicles and autonomous delivery robots (ADR) has increased. This paper explores how pedestrian safety around moving ADRs can be improved. To reduce pedestrian anxiety, we proposed the display of various real-time information from the ADR in Augmented Reality (AR). A preliminary experiment was conducted in an outdoor environment where an ADR was running, within a 5G network. We found AR has a positive effect in alleviating user anxiety around the ADR.",5G network; Augmented Reality; Autonomous Delivery Robot; VPS,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Investigation of User Performance in Virtual Reality-based Annotation-assisted Remote Robot Control,VRST - Virtual Reality Software and Technology,A,"This poster investigates the use of point cloud processing algorithms to provide annotations for robotic manipulation tasks completed remotely via Virtual Reality (VR). A VR-based system has been developed that receives and visualizes the processed data from real-time RGB-D camera feeds. A real-world robot model has also been developed to provide realistic reactions and control feedback. The targets and the robot model are reconstructed in a VR environment and presented to users in different modalities. The modalities and available information are varied between experimental settings, and the associated task performance is recorded and analyzed. The results accumulated from 192 experiments completed by 8 participants showed that point cloud data is sufficient for completing the task. Additional information, either image stream or preliminary processes presented as annotations, was found to not have a significant impact on the completion time. However, the combination of image stream and colored point cloud data visualization modalities was found to greatly enhance a user’s performance accuracy, with the number of target centers missed being reduced by 40%.",,Title_Abstract,True,
Scopus,conferencePaper,2022,Leveraging multimodal sensory information in cybersickness prediction,VRST - Virtual Reality Software and Technology,A,"Cybersickness is one of the problems that undermines user experience in virtual reality. While many studies are trying to find ways to alleviate cybersickness, only a few have considered cybersickness through multimodal perspectives. In this paper, we propose a multimodal, attention-based cybersickness prediction model. Our model was trained based on a total of 24,300 seconds of data from 27 participants and yielded the F1-score of 0.82. Our study results highlight the potential to model cybersickness from multimodal sensory information with a high level of performance and suggest that the model should be extended using additional, diverse samples.",,Abstract,True,
Scopus,conferencePaper,2022,Mapping of Locomotion Paths between Remote Environments in Mixed Reality using Mesh Deformation,VRST - Virtual Reality Software and Technology,A,"Remote mixed reality (RMR) allows users to be present and interact in other users’ environments through their photorealistic avatars. Common interaction objects are placed on surfaces in each user's environments and interacting with these objects require users to walk towards them. However, since the user's and their avatar's room's spatial configuration are not exactly similar, for a particular user's walking path, an equivalent path must be found in the avatar's environment, according to its environment's spatial configuration. In this work, we use the concept of mesh deformation to obtain this path, where we deform the mesh associated with the user's environment to fit to the spatial configuration of the avatar's environment. This gives us the corresponding mapping of every point between the two environments from which the equivalent path can be generated.",,Title_Abstract,True,
Scopus,conferencePaper,2022,MetaTwin: Synchronizing Physical and Virtual Spaces for Seamless World,VRST - Virtual Reality Software and Technology,A,"This paper presents MetaTwin, a collaborative Metaverse platform that supports one-to-one spatiotemporal synchrony between physical and virtual spaces. The users can interact with other users and surrounding IoT devices without being tied to physical spaces. Resource sharing is implemented to allow users to share media, including presentation slides and music. We deploy MetaTwin in two different network environments (i.e., within the US, Korea-US international) and summarize users’ feedback about the experience.",Collaborative Platform; Digital Twin; IoT; Metaverse; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,PlayMeBack - Cognitive Load Measurement using Different Physiological Cues in a VR Game,VRST - Virtual Reality Software and Technology,A,"We present a Virtual Reality (VR) game, PlayMeBack, to investigate cognitive load measurement in interactive VR environments using pupil dilation, Galvanic Skin Response (GSR), Electroencephalogram (EEG) and Heart Rate (HR). The user is shown different patterns of tiles lighting up and is asked to replay the pattern back pressing the tiles in the same sequence they lit up. The task difficulty depends on the length of the observed pattern (3-6 keys). This task is designed to explore the effect of cognitive load on physiological cues, and if pupil dilation, EEG, GSR and HR can be used as measures of cognitive load.",Cognitive Load; Electroencephalogram; Galvanic Skin Response; Heart Rate.; Pupil Dilation; Virtual Reality,Abstract_Keywords,True,
Scopus,conferencePaper,2022,Selection of Expanded Data Points in Immersive Analytics,VRST - Virtual Reality Software and Technology,A,"We propose a novel technique to facilitate the selection of data points, a type of data representation we often work with in immersive analytics. We designed and implemented this technique based on the expansion of data points following Fitt’s law. A user study was conducted in an headset-based augmented reality environment. The results significantly highlight the performance of our technique in helping the user select data points and their subjective appreciation in working with the expendable data points.",,Abstract,True,
Scopus,conferencePaper,2022,"Sign Language in Immersive VR: Design, Development, and Evaluation of a Testbed Prototype",VRST - Virtual Reality Software and Technology,A,"Immersive Virtual Reality (IVR) systems support several modalities such as body, finger, eye, and facial expressions tracking, thus they can support sign-language-based communication. The combined utilization of tracking technologies requires careful evaluation to ensure high-fidelity transference of body posture, gestures, and facial expressions in real-time. This paper presents the design, development and evaluation of an IVR system utilizing state-of-the-art tracking options. The system is evaluated by certified sign language teachers to detect usability issues and examine appropriate methodology for large-scale follow-up evaluation by users fluent in sign language.",,Abstract,True,
Scopus,conferencePaper,2022,Size Does Matter: An Experimental Study of Anxiety in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"The emotional response of users induced by VR scenarios has become a topic of interest, however, whether changing the size of objects in VR scenes induces different levels of anxiety remains a question to be studied. In this study, we conducted an experiment to initially reveal how the size of a large object in a VR environment affects changes in participants’ (N = 38) anxiety level and heart rate. To holistically quantify the size of large objects in the VR visual field, we used the omnidirectional field of view occupancy (OFVO) criterion for the first time to represent the dimension of the object in the participant’s entire field of view. The results showed that the participants’ heartbeat and anxiety while viewing the large objects were positively and significantly correlated to OFVO. These study reveals that the increase of object size in VR environments is accompanied by a higher degree of user’s anxiety.",anxiety; large object; user experience; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2022,The Community Game Development Toolkit,VRST - Virtual Reality Software and Technology,A,"The Community Game Development Toolkit is a set of tools that provide an accessible, intuitive work-flow within the Unity game engine for students, artists, researchers and community members to create their own visually rich, interactive 3D stories and immersive environments. The toolkit is designed to support diverse communities to represent their own traditions, rituals and heritages through interactive, visual storytelling, drawing on community members’ own visual assets such as photos, sketches and paintings, without requiring the use of coding or other specialized game-design skills. Projects can be built for desktop, mobile and VR applications. This paper describes the background, implementation and planned future developments of the toolkit, as well the contexts in which it has been used.",manipulation; prototyping/implementation; virtual reality,Keywords,True,
Scopus,conferencePaper,2022,"The Effect of Training Communication Medium on the Social Constructs Co-Presence, Engagement, Rapport, and Trust: Explaining how training communication medium affects the social constructs co-presence, engagement, rapport, and trust",VRST - Virtual Reality Software and Technology,A,"Communication performance is highly context sensitive and difficult to quantify. In the SCOTTIE project (Systematic Communication Objectives and Telecommunications Technology Investigations and Evaluations), the goal is to investigate the impact of the communication medium on team performance and effectiveness. The human decision to travel or replace travel with telecommunications can be extracted from SCOTTIE, rather than relying on intuition and opinion. This poster analyzes four social communication constructs and compares them in Face-to-Face, Video Conferencing, and Virtual Reality training scenarios. Co-presence, engagement, rapport, and trust were the four constructs. Data from 105 participants across the three between-subject conditions showed that engagement was the only construct that had a statistically significant difference between the three training environments.",,Abstract,True,
Scopus,conferencePaper,2022,Using Virtual Reality Food Environments to Study Individual Food Consumer Behavior in an Urban Food Environment,VRST - Virtual Reality Software and Technology,A,"The objective of this research was to explore whether virtual reality can be used to study individual food consumer decision-making and behavior through a public health lens by developing a simulation of an urban food environment that included a street-level scene and three prototypical stores. Twelve participants completed the simulation and a survey. Preliminary results showed that 72.7% of participants bought food from the green grocer, 18.2% from the fast food store, and 9.1% from the supermarket. The mean presence score was 38.9 out of 49 and the mean usability score was 85.9 out of 100. This experiment demonstrates that virtual reality should be further considered as a tool for studying food consumer behavior within a food environment.",food choice; food environment; nutrition disparities; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2022,Visual Considerations for Augmented Reality in Urban Planning,VRST - Virtual Reality Software and Technology,A,"The design process in architecture and urban planning has always been accompanied by a discourse on suitable visual representation. This resulted in both a wealth of visualisation styles and high sensitivity among planners regarding the visual communication of their work. Representation of projects throughout phases of the planning process often adheres to established visual standards, from concept sketch to high-end rendering. A look at contemporary Augmented Reality (AR) apps for urban planning indicates however that the quality and precision of representation seem to lag somewhat behind, entailing risks that projects are misinterpreted. This poster describes our design research on three urban planning apps developed with Swiss municipalities and outlines results that improve visual representation in AR throughout different planning phases.",,Title_Abstract,True,
Scopus,conferencePaper,2022,Visualizing Perceptions of Non-Player Characters in Interactive Virtual Reality Environments,VRST - Virtual Reality Software and Technology,A,"Visual effects and elements to visualize the perceptions of one’s own virtual character (also referred to as Visual Delegates) are often used in video games, e.g., status bars visualize the character’s sense of health, filters on the interface layer visualize the character’s state of mind. It is still largely unexplored whether Visual Delegates can also be used to transfer the perception of non-player characters comprehensibly. Therefore, we developed a medical virtual reality scenario using five different types of Visual Delegates to visualize three different perceptions of a virtual non-player patient. We tested for character assignment in a qualitative user study (N = 20). Our results can be used to decide more effectively what types of Visual Delegates can be used to convey perceptions of non-player characters.",non-player character; NPC; perception; serious games; user interface design; virtual character; virtual reality; visual delegate; VR,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Planning Locomotion Techniques for Virtual Reality Games,VRST - Virtual Reality Software and Technology,A,"Locomotion is a fundamental component in many virtual reality (VR) games. However, few techniques have been designed with game’s demands in mind. In this paper, we propose two locomotion techniques for fast-paced VR games: Repeated Short-Range Teleports and Continuous Movement Pads. We conducted a user study with 27 participants using these techniques against Smooth Locomotion and Teleport in a game-like scenario. We found that Movement Pads can be a suitable alternative for games, with competitive performance on various criteria such as time, damage taken, usability, workload, and user preference. On the other hand, Repeated Short-Range Teleport displayed lower usability and higher mental workload.",Games; Human-Computer Interaction; Locomotion; Movement; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Versatile Mixed-method Locomotion under Free-hand and Controller-based Virtual Reality Interfaces,VRST - Virtual Reality Software and Technology,A,"Locomotion systems that allow the user to interact with large virtual spaces require precise input, competing with the same inputs available for performing a task in the virtual world. Despite extensive research on hand tracking input modalities, there is a lack of a widely adopted mechanism that offers general-purpose, high-precision locomotion across various applications. This research aims to address this gap by proposing a design that combines teleportation with a grab-pull locomotion scheme to bridge the divide between long-distance and high-precision locomotion in both a tracked-controller and free-hand environment. The implementation details for both tracked controller and tracked hand environments are presented and evaluated through a user study. The study findings indicate that each locomotion mechanism holds value for different tasks, with grab-pull providing more benefit in scenarios where smaller, more precise positioning is required. As found in prior research, controller tracking was found to be faster than hand tracking, but all participants were able to successfully use the locomotion system with both interfaces.",controller; free-hand; hand tracking; interaction; locomotion; virtual reality,Title_Keywords,True,
Scopus,conferencePaper,2023,Exploring User Engagement in Immersive Virtual Reality Games through Multimodal Body Movements,VRST - Virtual Reality Software and Technology,A,"User engagement in Virtual Reality (VR) games is crucial for creating immersive and captivating gaming experiences that meet the expectations of players. However, understanding and measuring these levels in VR games presents a challenge for game designers, as current methods, such as self-reports, may be limited in capturing the full extent of user engagement. Additionally, approaches based on biological signals to measure engagement in VR games present complications and challenges, including signal complexity, interpretation difficulties, and ethical concerns. This study explores body movements, as a novel approach to measure user engagement in VR gaming. We employ E4, emteqPRO, and off-the-shelf IMUs to measure the body movements from diverse participants engaged in multiple VR games. Further, we examine the simultaneous occurrence of player motivation and physiological responses to explore potential associations with body movements. Our findings suggest that body movements hold promise as a reliable and objective indicator of user engagement, offering game designers valuable insights on generating more engaging and immersive experiences.",Body Movements; Data-driven methods; Emotions; Engagement; Virtual Reality Games,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Cross-Reality Gaming: Comparing Competition and Collaboration in an Asymmetric Gaming Experience,VRST - Virtual Reality Software and Technology,A,"Due to the level of immersion and differences in the user interface, there can be a very large discrepancy in the user experience between users in immersive systems and non-immersive systems when playing games together. To investigate the impact of the cross-reality experience, which refers to the asymmetric use of eXtended Reality, we aim to understand the different affordances and experiences in an asymmetric setup, where one participant uses a desktop setup with a mouse and keyboard, and one uses a virtual reality (VR) headset and controller in two different task modes, Competition or Collaboration. In our research, a pair of participants played a game in real-time, using either the VR setup or the desktop setup. In Competition mode, the two participants were asked to defeat each other. In Collaboration mode, the pair of participants played as a team and were asked to defeat a pair of AI enemies. Our results show the VR group reported a better gaming experience and perceptual responses compared to the desktop group regardless of game mode, but that the desktop group showed superior gaming performance compared to the VR group in Competition mode.",asymmetric platform; collaboration; competition; cross-platform; cross-reality; gaming experience; shared experience; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Does One Keyboard Fit All? Comparison and Evaluation of Device-Free Augmented Reality Keyboard Designs,VRST - Virtual Reality Software and Technology,A,"Virtual keyboard designs are widely discussed with the increasing prevalence of head-mounted and lightweight Mixed Reality devices. However, isolated design suggestions with distinct implementations may lack comparability in terms of performance, learnability, and user preference. We compare three promising device-free text-entry solutions for Augmented Reality (AR) on the Microsoft HoloLens 2. The virtual keyboards comprise dwell-based eye-gaze input, eye-gaze with pinch-gesture-commit input, and mid-air tap typing on virtual QWERTY-keyboards. We conducted a controlled within-subjects lab experiment with 27 subjects measuring typing performance, task load, usability, and preference across the three keyboards. Users state distinct preferences for the respective keyboards and weight the advantages and disadvantages differently. Considering diverse usage scenarios, subjects would even prefer these input modes over speech or physical keyboard input. The results indicate that virtual keyboard design shall be tailored to individual user preferences. Therefore, this study provides essential insights into designing AR keyboards for heterogeneous user groups.",augmented reality; eye tracking; laboratory experiment; text entry,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Exploring Augmented Reality for Situated Analytics with Many Movable Physical Referents,VRST - Virtual Reality Software and Technology,A,"Situated analytics (SitA) uses visualization in the context of physical referents, typically by using augmented reality (AR). We want to pave the way toward studying SitA in more suitable and realistic settings. Toward this goal, we contribute a testbed to evaluate SitA based on a scenario in which participants play the role of a museum curator and need to organize an exhibition of music artifacts. We conducted two experiments: First, we evaluated an AR headset interface and the testbed itself in an exploratory manner. Second, we compared the AR headset to a tablet interface. We summarize the lessons learned as guidance for designing and evaluating SitA.",Augmented Reality; Immersive analytics; Situated analytics,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Exploring Users' Pointing Performance on Virtual and Physical Large Curved Displays,VRST - Virtual Reality Software and Technology,A,"Large curved displays have emerged as a powerful platform for collaboration, data visualization, and entertainment. These displays provide highly immersive experiences, a wider field of view, and higher satisfaction levels. Yet, large curved displays are not commonly available due to their high costs. With the recent advancement of Head Mounted Displays (HMDs), large curved displays can be simulated in Virtual Reality (VR) with minimal cost and space requirements. However, to consider the virtual display as an alternative to the physical display, it is necessary to uncover user performance differences (e.g., pointing speed and accuracy) between these two platforms. In this paper, we explored users’ pointing performance on both physical and virtual large curved displays. Specifically, with two studies, we investigate users’ performance between the two platforms for standard pointing factors such as target width, target amplitude as well as users’ position relative to the screen. Results from user studies reveal no significant difference in pointing performance between the two platforms when users are located at the same position relative to the screen. In addition, we observe users’ pointing performance improves when they are located at the center of a semi-circular display compared to off-centered positions. We conclude by outlining design implications for pointing on large curved virtual displays. These findings show that large curved virtual displays are a viable alternative to physical displays for pointing tasks.",Curved Display; Display Curvatures; Fitts Law; Large Physical Display; Large Virtual Display; Pointing Performance,Abstract,True,
Scopus,conferencePaper,2023,Re-investigating the Effect of the Vergence-Accommodation Conflict on 3D Pointing,VRST - Virtual Reality Software and Technology,A,"The vergence-accommodation conflict (VAC) limits user performance in current Virtual Reality (VR) systems. In this paper, we investigate the effects of the VAC in a single-focal VR system using three experimental conditions: with no VAC, with a constant VAC, and with a varying VAC. Previous work in this area had yielded conflicting results, so we decided to re-investigate this issue. Eighteen participants performed an ISO 9241:411 task in a study that closely replicates previous work, except that the angle of the task space was rotated 20 degrees downward, to make the task less fatiguing to perform, which addresses a potential confound in previous work. We found that the varying VAC condition had worse performance than the other conditions, which indicates that the contrasting results in previous work were very likely due to biomechanical factors. We hope that our work contributes to the understanding of the influence of the VAC in VR systems and potential strategies for improving user experience and performance in immersive virtual environments.",3D pointing; Fitts’ Law; vergence-accommodation conflict; VR,Abstract,True,
Scopus,conferencePaper,2023,Dialogues For One: Single-User Content Creation Using Immersive Record and Replay,VRST - Virtual Reality Software and Technology,A,"Non-player characters are an essential element of many 3D and virtual reality experiences. They can make the experiences feel more lively and populated. Animation for non-player characters is often motion-captured using expensive hardware and the post-processing steps are time-consuming, especially when capturing multiple people at once. Using record and replay techniques in virtual reality can offer cheaper and easier ways of motion capture since the user is already tracked. We use immersive record and replay to enable a single user to create stacked recordings of themselves. We provide tools to help the user interact with their previous recorded self and in doing so allow them to create believable interactive scenarios with multiple characters that can be used to populate virtual environments. We create a small dialogue dataset with two amateur actors who used our tool to record dialogues alone and together in virtual reality. To evaluate whether stacked recordings are qualitatively comparable to conventional multi-user recordings and whether people could tell the difference between the two, we conducted two user studies, one online and one in virtual reality with 89 participants in total. We found that participants could not tell the difference and even slightly preferred stacked recordings.",content creation; record and replay; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Dynascape : Immersive Authoring of Real-World Dynamic Scenes with Spatially Tracked RGB-D Videos,VRST - Virtual Reality Software and Technology,A,"In this paper, we present Dynascape, an immersive approach to the composition and playback of dynamic real-world scenes in mixed and virtual reality. We use spatially tracked RGB-D cameras to capture point cloud representations of arbitrary dynamic real-world scenes. Dynascape provides a suite of tools for spatial and temporal editing and composition of such scenes, as well as fine control over their visual appearance. We also explore strategies for spatiotemporal navigation and different tools for the in situ authoring and viewing of mixed and virtual reality scenes. Dynascape is intended as a research platform for exploring the creative potential of dynamic point clouds captured with mobile, tracked RGB-D cameras. We believe our work represents a first attempt to author and playback spatially tracked RGB-D video in an immersive environment, and opens up new possibilities for involving dynamic 3D scenes in virtual space.",Data Visualization; Human Computer Interaction; Immersive Authoring,Abstract,True,
Scopus,conferencePaper,2023,Exploring Unimodal Notification Interaction and Display Methods in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"As we develop computing platforms for augmented reality (AR) head-mounted display (HMDs) technologies for social or workplace environments, understanding how users interact with notifications in immersive environments has become crucial. We researched effectiveness and user preferences of different interaction modalities for notifications, along with two types of notification display methods. In our study, participants were immersed in a simulated cooking environment using an AR-HMD, where they had to fulfill customer orders. During the cooking process, participants received notifications related to customer orders and ingredient updates. They were given three interaction modes for those notifications: voice commands, eye gaze and dwell, and hand gestures. To manage multiple notifications at once, we also researched two different notification list displays, one attached to the user’s hand and one in the world. Results indicate that participants preferred using their hands to interact with notifications and having the list of notifications attached to their hands. Voice and gaze interaction was perceived as having lower usability than touch.",augmented reality; display methods; eye gaze; interaction; notifications; voice commands,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Intuitive User Interfaces for Real-Time Magnification in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Various reasons exist why humans desire to magnify portions of our visually perceived surroundings, e.g., because they are too far away or too small to see with the naked eye. Different technologies are used to facilitate magnification, from telescopes to microscopes using monocular or binocular designs. In particular, modern digital cameras capable of optical and/or digital zoom are very flexible as their high-resolution imagery can be presented to users in real-time with displays and interfaces allowing control over the magnification. In this paper, we present a novel design space of intuitive augmented reality (AR) magnifications where an AR head-mounted display is used for the presentation of real-time magnified camera imagery. We present a user study evaluating and comparing different visual presentation methods and AR interaction techniques. Our results show different advantages for unimanual, bimanual, and situated AR magnification window interfaces, near versus far vergence distances for the image presentation, and five different user interfaces for specifying the scaling factor of the imagery.",3D User Interfaces; Augmented Reality; Magnification,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,When Filters Escape the Smartphone: Exploring Acceptance and Concerns Regarding Augmented Expression of Social Identity for Everyday AR,VRST - Virtual Reality Software and Technology,A,"Mass adoption of Everyday Augmented Reality (AR) glasses will enable pervasive augmentation of our expression of social identity through AR filters, transforming our perception of self and others. However, despite filters’ prominent and often problematic usage in social media, research has yet to reflect on the potential impact AR filters might have when brought into everyday life. Informed by our survey of 300 existing popular AR filters used on Snapchat, Instagram and Tiktok, we conducted an AR-in-VR user study where participants (N=24) were exposed to 18 filters across six categories. We evaluated the social acceptability of these augmentations around others and attitudes towards an individual’s augmented self.Our findings highlight 1) how users broadly respected another individual’s augmented self; 2) positive use cases, such as supporting the presentation of gender identity; and 3) tensions around applying AR filters to others (e.g. censorship, changing protected characteristics) and their impact on self-perception (e.g. perpetuating unrealistic beauty standards). We raise questions regarding the rights of individuals to augment and be augmented that provoke the need for further consideration of AR augmentations in society.",AR Filters; Augmented Identity; Augmented Reality; Identity; Mediated Perception; Self-Presentation; Social Identity,Abstract_Keywords,True,
Scopus,conferencePaper,2023,From Clocks to Pendulums: A Study on the Influence of External Moving Objects on Time Perception in Virtual Environments,VRST - Virtual Reality Software and Technology,A,"This paper investigates the relationship between perceived object motion and the experience of time in virtual environments. We developed an application to measure how the motion properties of virtual objects and the degree of immersion and embodiment may affect the time experience. A first study (n = 145) was conducted remotely using an online video survey, while a second study (n = 60) was conducted under laboratory conditions in virtual reality (VR). Participants in both studies experienced seven different virtual objects in a randomized order and then answered questions about time experience. The VR study added an ""embodiment"" condition in which participants were either represented by a virtual full body or lacked any form of virtual body representation. In both studies, time was judged to pass faster when viewing oscillating motion in immersive and non-immersive settings and independently of the presence or absence of a virtual body. This trend was strongest when virtual pendulums were displayed. Both studies also found a significant inverse correlation between the passage of time and boredom. Our results support the development of applications that manipulate the perception of time in virtual environments for therapeutic use, for instance, for disorders such as depression, autism, and schizophrenia. Disturbances in the perception of time are known to be associated with these disorders.",embodiment; extended reality; mixed reality; time perception; virtual reality; virtual time; virtual zeitgeber,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Music Therapy in Virtual Reality for Autistic Children with Severe Learning Disabilities,VRST - Virtual Reality Software and Technology,A,"Music Therapy (MT) has shown many benefits in helping autistic children, but some challenges remain due to children’s social anxiety and sensory issues. Yet, very few studies have investigated how Virtual Reality (VR) could help to increase the accessibility of MT approaches. This paper presents an exploratory study investigating the use of VR to perform MT sessions for autistic children with severe learning disabilities and complex needs. The study is performed in terms of acceptability, usability, and social communication. A collaborative MT approach was designed in close collaboration with music therapists from Denmark and psychologists from France, using head-mounted display-based VR. Testing were conducted with thirteen children with various neurodevelopmental conditions and intellectual disabilities at a children’s day hospital in Paris. The results indicate positive acceptability and usability for these children, and suggest a positive effect of MT in VR regarding communication.",autism; intellectual disabilities; music therapy; virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Gaze Assistance for Older Adults during Throwing in Virtual Reality and its Effects on Performance and Motivation,VRST - Virtual Reality Software and Technology,A,"Initial motivation when starting exergaming is a key factor towards enabling long-term engagement and adherence, especially among older adults. To increase, in particular, the initial motivation of older adults, we introduce the concept of diminishing gaze assistance (GA), assess its feasibility for virtual reality (VR) exergames, and investigate the effects on motor learning, performance, and motivation in older adult users. First, we conducted a focus group followed by a pre-study on the development of VR exergames for older adults and VR gaze assistance. The results informed the design and implementation of our gaze-assisted throwing exergame, which was then evaluated in a follow-up main study. Participants of the main study were randomly assigned to the GA and Motor (control) group, and had to complete a VR throwing task, in which participants had to aim and throw at three targets at varying angles. The GA group received declining gaze assistance, in which the ball trajectory was initially guided by their gaze (rather than their physical (motor) throwing) before guidance was gradually reduced until their physical (motor) throwing ability was solely responsible for hitting the target. Motivation and user experience were assessed using the Questionnaire on Current Motivation before and during, and the short scale of intrinsic motivation questionnaire after the task. The results show that the GA was generally perceived positively. In particular, the initial confidence of the GA group was rated higher, and we observed evidence suggesting increased confidence throughout the trial.",errorless learning; eyetracking; gaze assistance; motivation,Title_Abstract,True,
Scopus,conferencePaper,2023,GazeRayCursor: Facilitating Virtual Reality Target Selection by Blending Gaze and Controller Raycasting,VRST - Virtual Reality Software and Technology,A,"Raycasting is a common method for target selection in virtual reality (VR). However, it results in selection ambiguity whenever a ray intersects multiple targets that are located at different depths. To resolve these ambiguities, we estimate object depth by projecting the closest intersection between the gaze and controller rays onto the controller ray. An evaluation of this method found that it significantly outperformed a previous eye convergence depth estimation technique. Based on these results, we developed GazeRayCursor, a novel selection technique that enhances Raycasting, by leveraging gaze for object depth estimation. In a second study, we compared two variations of GazeRayCursor with RayCursor, a recent technique developed for a similar purpose, in a dense target environment. The results indicated that GazeRayCursor decreased selection time by 45.0% and reduced manual depth adjustments by a factor of 10 in a dense target environment. Our findings showed that GazeRayCursor is an effective method for target disambiguation in VR selection without incurring extra effort.",controller; disambiguation; gaze; object selection; raycasting; VR,Title_Abstract,True,
Scopus,conferencePaper,2023,Evaluating Augmented Reality Communication: How Can We Teach Procedural Skill in AR?,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) has great potential for use in healthcare applications, especially remote medical training and supervision. In this paper, we analyze the usage of an AR communication system to teach a medical procedure, the placement of a central venous catheter (CVC) under ultrasound guidance. We examine various AR communication and collaboration components, including gestural communication, volumetric information, annotations, augmented objects, and augmented screens. We compare how teaching in AR differs from teaching through videoconferencing-based communication. Our results include a detailed medical training steps analysis in which we compare how verbal and visual communication differs between video and AR training. We identify procedural steps in which medical experts give visual instructions utilizing AR components. We examine the change in AR usage and interaction over time and recognize patterns between users. Moreover, AR design recommendations are given based on post-training interviews.",Augmented Reality; Remote Collaboration; Telehealth; Volumetric Communication,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Hands-on DNA: Exploring the Impact of Virtual Reality on Teaching DNA Structure and Function,VRST - Virtual Reality Software and Technology,A,"Molecular biology is a demanding subject, requiring students to master abstract, three-dimensional (3D) concepts across a range of spatial scales. Virtual reality (VR) is a medium that excels at portraying scale and 3D concepts, and allows people to have tangible experiences of otherwise intangible subjects. This paper describes Hands-on DNA, a virtual reality learning experience for teaching undergraduate university students about the scale and structure of deoxyribose nucleic acid (DNA), a central molecule in molecular biology. The intention of Hands-on DNA is to leverage the advantages of virtual reality against specific challenges faced in teaching molecular biology. We derive design requirements motivated by pedagogy, provide guidelines, and discuss lessons learned during development. Our user study shows that students perceive Hands-on DNA as a fun, engaging, effective learning tool, and that it addresses some of the weaknesses in molecular biology education. Our results also suggest that new interaction techniques to support learning in VR need to be developed (e.g., for note taking) and that the increasing penetration of recreational VR increases students’ expectations and hence the risk of students being disappointed of VR learning tools.",constructivism; DNA; education; gamification; molecular biology; multimedia education; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Measuring and Comparing Collaborative Visualization Behaviors in Desktop and Augmented Reality Environments,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) provides a significant opportunity to improve collaboration between co-located team members jointly analyzing data visualizations, but existing rigorous studies are lacking. We present a novel method for qualitatively encoding the positions of co-located users collaborating with head-mounted displays (HMDs) to assist in reliably analyzing collaboration styles and behaviors. We then perform a user study on the collaborative behaviors of multiple, co-located synchronously collaborating users in AR to demonstrate this method in practice and contribute to the shortfall of such studies in the existing literature. Pairs of users performed analysis tasks on several data visualizations using both AR and traditional desktop displays. To provide a robust evaluation, we collected several types of data, including software logging of participant positioning, qualitative analysis of video recordings of participant sessions, and pre- and post-study questionnaires including the NASA TLX survey. Our results suggest that the independent viewports of AR headsets reduce the need to verbally communicate about navigating around the visualization and encourage face-to-face and non-verbal communication. Our novel positional encoding method also revealed the overlap of task and communication spaces vary based on the needs of the collaborators.",Augmented reality; Co-located collaboration; Visualization,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Vicarious: Context-aware Viewpoints Selection for Mixed Reality Collaboration,VRST - Virtual Reality Software and Technology,A,"Mixed-perspective, combining egocentric (first-person) and exocentric (third-person) viewpoints, have been shown to improve the collaborative experience in remote settings. Such experiences allow remote users to switch between different viewpoints to gain alternative perspectives of the remote space. However, existing systems lack seamless selection and transition between multiple perspectives that better fit the task at hand. To address this, we present a new approach called Vicarious, which simplifies and automates the selection between egocentric and exocentric viewpoints. Vicarious employs a context-aware method for dynamically switching or highlighting the optimal viewpoint based on user actions and the current context. To evaluate the effectiveness of the viewpoint selection method, we conducted a user study (n = 27) using an asymmetric AR-VR setup where users performed remote collaboration tasks under four distinct conditions: No-view, Manual, Guided, and Automatic selection. The results showed that Guided and Automatic viewpoint selection improved users’ understanding of the task space and task performance, and reduced cognitive load compared to Manual or No-view selection. The results also suggest that the asymmetric setup had minimal impact on spatial and social presence, except for differences in task load and preference. Based on these findings, we provide design implications for future research in mixed reality collaboration.",360-degree Panoramic Video; Mixed Reality; Perspective Sharing.; Remote Collaboration; Telepresence; Viewpoint Sharing,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Ready Worker One? High-Res VR for the Home Office,VRST - Virtual Reality Software and Technology,A,"Many employees prefer to work from home, yet struggle to squeeze their office into an already fully-utilized space. Virtual Reality (VR) seemingly offered a solution with its ability to transform even modest physical spaces into spacious, productive virtual offices, but hardware challenges—such as low resolution—have prevented this from becoming a reality. Now that hardware issues are being overcome, we are able to investigate the suitability of VR for daily work. To do so, we (1) studied the physical space that users typically dedicate to home offices and (2) conducted an exploratory study of users working in VR for one week. For (1) we used digital ethnography to study 430 self-published images of software developer workstations in the home, confirming that developers faced myriad space challenges. We used speculative design to re-envision these as VR workstations, eliminating many challenges. For (2) we asked 10 developers to work in their own home using VR for about two hours each day for four workdays, and then interviewed them. We found that working in VR improved focus and made mundane tasks more enjoyable. While some subjects reported issues—annoyances with the fit, weight, and umbilical cord of the headset—the vast majority of these issues seem to be addressable. Together, these studies show VR technology has the potential to address many key problems with home workstations, and, with continued improvements, may become an integral part of creating an effective workstation in the home.",Field Study; Remote Work; Virtual Reality; Workstations,Abstract_Keywords,True,
Scopus,conferencePaper,2023,UniteXR: Joint Exploration of a Real-World Museum and its Digital Twin,VRST - Virtual Reality Software and Technology,A,"The combination of smartphone Augmented Reality (AR) and Virtual Reality (VR) makes it possible for on-site and remote users to simultaneously explore a physical space and its digital twin through an asymmetric Collaborative Virtual Environment (CVE). In this paper, we investigate two spatial awareness visualizations to enable joint exploration of a space for dyads consisting of a smartphone AR user and a head-mounted display VR user. Our study revealed that both, a mini-map-based method and an egocentric compass method with a path visualization, enabled the on-site visitors to locate and follow a virtual companion reliably and quickly. Furthermore, the embodiment of the AR user by an inverse kinematics avatar allowed the use of natural gestures such as pointing and waving which was preferred over text messages by the participants of our study. In an expert review in a museum and its digital twin we observed an overall high social presence for on-site AR and remote VR visitors and found that the visualizations and the avatar embodiment successfully facilitated their communication and collaboration.",asymmetric exploration; cross-device collaboration; digital twin; mixed reality; smartphone augmented reality; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Comparing Mixed Reality Agent Representations: Studies in the Lab and in the Wild,VRST - Virtual Reality Software and Technology,A,"Mixed-reality systems provide a number of different ways of representing users to each other in collaborative scenarios. There is an obvious tension between using media such as video for remote users compared to representations as avatars. This paper includes two experiments (total n = 80) on user trust when exposed to two of three different user representations in an immersive virtual reality environment that also acts as a simulation of typical augmented reality simulations: full body video, head and shoulder video and an animated 3D model. These representations acted as advisors in a trivia quiz. By evaluating trust through advisor selection and self-report, we found only minor differences between representations, but a strong effect of perceived advisor expertise. Unlike prior work, we did not find the 3D model scored poorly on trust, perhaps as a result of greater congruence within an immersive context.",avatars; collaboration; mixed reality; trust; Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,"Dynamic Theater: Location-Based Immersive Dance Theater, Investigating User Guidance and Experience",VRST - Virtual Reality Software and Technology,A,"Dynamic Theater explores the use of augmented reality (AR) in immersive theater as a platform for digital dance performances. The project presents a locomotion-based experience that allows for full spatial exploration. A large indoor AR theater space was designed to allow users to freely explore the augmented environment. The curated wide-area experience employs various guidance mechanisms to direct users to the main content zones. Results from our 20-person user study show how users experience the performance piece while using a guidance system. The importance of stage layout, guidance system, and dancer placement in immersive theater experiences are highlighted as they cater to user preferences while enhancing the overall reception of digital content in wide-area AR. Observations after working with dancers and choreographers, as well as their experience and feedback are also discussed.",Immersive Theater; Mobile Augmented Reality; User Study; Wide-Area,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Revisiting Consumed Endurance: A NICE Way to Quantify Shoulder Fatigue in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) is increasingly being adopted in fitness, gaming, and workplace productivity applications for its natural interaction with body movement. A widely accepted method for quantifying the physical fatigue caused by VR interactions is through metrics such as Consumed Endurance (CE). Proposed in 2014, CE calculates the shoulder torque to infer endurance time (ET)—i.e. the maximum amount of time a pose can be maintained—during mid-air interactions. This model remains widely cited but has not been closely examined beyond its initial evaluation, leaving untested assumptions about exertion from low-intensity interactions and its basis on torque. In this paper, we present two VR studies where we (1) collect a baseline dataset that replicates the foundation of CE and (2) extend the initial evaluation in a pointing task from a two-dimensional (2D) screen to a three-dimensional (3D) immersive environment. Our baseline dataset collected from a high-precision tracking system found that the CE model overestimates ET for low-exertion interactions. Further, our studies reveal that a biomechanical model based on only torque cannot account for additional exertion measured when the shoulder angle exceeds 90° elevation. Based on these findings, we propose a revised formulation of CE to highlight the need for a hybrid approach in future fatigue modelling.",Consumed Endurance; Endurance; Ergonomics; Interaction design; VR interactions,Title_Abstract,True,
Scopus,conferencePaper,2023,Cognitive Load Measurement with Physiological Sensors in Virtual Reality during Physical Activity,VRST - Virtual Reality Software and Technology,A,"Many Virtual Reality (VR) experiences, such as learning tools, would benefit from utilising mental states such as cognitive load. Increases in cognitive load (CL) are often reflected in the alteration of physiological responses, such as pupil dilation (PD), electrodermal cctivity (EDA), heart rate (HR), and electroencephalography (EEG). However, the relationship between these physiological responses and cognitive load are usually measured while participants sit in front of a computer screen, whereas VR environments often require a high degree of physical movement. This physical activity can affect the measured signals, making it unclear how suitable these measures are for use in interactive Virtual Reality (VR). We investigate the suitability of four physiological measures as correlates of cognitive load in interactive VR. Suitable measures must be robust enough to allow the learner to move within VR and be temporally responsive enough to be a useful metric for adaptation. We recorded PD, EDA, HR, and EEG data from nineteen participants during a sequence memory task at varying levels of cognitive load using VR, while in the standing position and using their dominant arm to play a game. We observed significant linear relationships between cognitive load and PD, EDA, and EEG frequency band power, but not HR. PD showed the most reliable relationship but has a slower response rate than EEG. Our results suggest the potential for use of PD, EDA, and EEG in this type of interactive VR environment, but additional studies will be needed to assess feasibility under conditions of greater movement.",virtual reality; EEG; physical activity; pupil dilation; heart rate; cognitive load; galvanic skin response,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Exploring the Stability of Behavioral Biometrics in Virtual Reality in a Remote Field Study: Towards Implicit and Continuous User Identification through Body Movements,VRST - Virtual Reality Software and Technology,A,"Behavioral biometrics has recently become a viable alternative method for user identification in Virtual Reality (VR). Its ability to identify users based solely on their implicit interaction allows for high usability and removes the burden commonly associated with security mechanisms. However, little is known about the temporal stability of behavior (i.e., how behavior changes over time), as most previous works were evaluated in highly controlled lab environments over short periods. In this work, we present findings obtained from a remote field study (N = 15) that elicited data over a period of eight weeks from a popular VR game. We found that there are changes in people’s behavior over time, but that two-session identification still is possible with a mean F1-score of up to 71%, while an initial training yields 86%. However, we also see that performance can drop by up to over 50 percentage points when testing with later sessions, compared to the first session, particularly for smaller groups. Thus, our findings indicate that the use of behavioral biometrics in VR is convenient for the user and practical with regard to changing behavior and also reliable regarding behavioral variation.",Virtual Reality; Field Study; Continuous Identification.; Implicit User Identification,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Beyond Mirrors: Exploring Behavioral Changes through Comparative Avatar Design in VR Taiko Drumming,VRST - Virtual Reality Software and Technology,A,"Most studies on the Proteus Effect, which examines how avatars can influence users’ behavior through evoked stereotypes, have primarily manipulated only participants’ own avatars as the independent variable. However, in reality, there are numerous scenarios where individuals recognize their uniqueness by comparing themselves to others. Therefore, this study aimed to explore the impact of recognizing one’s distinctiveness by comparing one’s own avatar’s appearance with others on behavioral changes. In our experiment, participants and non-player characters engaged in playing the Japanese drum ‘Taiko’ together within a virtual environment. They utilized avatars dressed in suits or ‘Happi,’ which is a traditional Japanese festival costume. The results demonstrated that both the uniformity/distinctiveness and the type of avatar appearance played a joint role in influencing the speed and amplitude of arm swings during the taiko performance. This finding provides valuable insights into comprehending the mechanisms of behavior change in settings where multiple avatars interact, such as social virtual reality, and aids in designing virtual spaces that foster appropriate interactions among individuals.",virtual reality; avatar; identification; proteus effect; social comparison; behavioral changes; comparative avatar design,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Effect of Virtual Hand's Fingertip Deformation on the Stiffness Perceived Using Pseudo-Haptics,VRST - Virtual Reality Software and Technology,A,"In this study, using a novel method for haptic presentation based on pseudo-haptics, the perceived stiffness was visually altered by changing the fingertips shape of a virtual hand, as users engaged with objects in a VR environment. While past approaches have primarily focused on instigating such sensations through object deformation, we focused on how an individual’s fingertips deform upon making contact with an object. In this study, we investigated pseudo-haptics based on the deformation of the fingertips of a virtual hand. In Experiment 1, we determined how the shape deformation of a virtual hand’s fingertip affected the sense of body ownership. The experiment determined that the maximum change in the fingertip width should be 2.25 times. In Experiment 2, subjects touched a virtual object in the VR space and evaluated the perception of the stiffness of the virtual object. The results confirmed that when the deformation of the fingertip shape of the virtual hand was small, the object was perceived as hard, whereas when it was large, the object was perceived as soft. These results indicated that a haptic presentation is possible without using a haptic device that restricts user movement, which will users could broaden the range of natural interactions in VR spaces.",virtual reality; pseudo-haptics; haptics illusions,Keywords,True,
Scopus,conferencePaper,2023,Exploring Real-time Precision Feedback for AR-assisted Manual Adjustment in Mechanical Assembly,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) based manual assembly nowadays enables to guide the process of physical tasks, providing intuitive instructions and detailed information in real-time. However, very limited studies have explored AR manual adjustment tasks with precision requirements. In this paper, we develop an AR-assisted guidance system for manual adjustments with relatively high-precision requirements. We first assessed the accuracy of the special-set OptiTrack system to determine the threshold of precision requirements for our user study. We further evaluated the performance of Number-based and Bar-based precision feedback by comparing orienting assembly errors and task completion time, as well as the usability in the user study. We found that the assembly errors of orientation in the Number-based and Bar-based interfaces were significantly lower than the baseline condition, while there was no significant difference between the Number-based and Bar-based interfaces. Furthermore, the Number-based showed faster task completion time, lower workload, and higher usability than the Bar-based condition.",Augmented Reality; manual adjustment; OptiTrack; precision feedback,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Exploring Visual Augmentations for Improving the Operation of a Hydraulic Excavator using Expert Operation Replay,VRST - Virtual Reality Software and Technology,A,"Hydraulic excavators are widely used in construction work owing to their versatility. However, the general operation of these excavators is complex and novice operators require extensive training to operate them. In this study, we propose a virtual reality (VR)-based training system with three types of visual augmentations using pre-recorded expert operations to support the skill acquirement for handling a hydraulic excavator. To evaluate the effectiveness of the proposed visual augmentations in terms of skill improvement, we compared the scores of the trainees before and after training including combinations of visual augmentations. The results indicated that the display of the lever movement significantly improved the trajectory of the bucket tip, while ghost animation and slow motion did not show significant effects. Furthermore, by showing the lever input and excavator movement of the expert in slow motion, the task completion time increased because of the aftereffect. Our findings not only provide a design guideline for VR-based excavator operation training but can also be applied to augmented reality (AR)/mixed reality (MR) support systems for supporting practical excavator operations.",Virtual Reality; Training; Visualization; Augmentations; Excavator,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Redirected Placement: Evaluating the Redirection of Passive Props during Reach-to-Place in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Hand redirection is an effective technique that can provide users with haptic feedback in virtual reality (VR) when a disparity exists between virtual objects and their physical counterparts. Psychophysiological research has revealed the distinct motion profiles of different kinematic phases when people operate hand-object interaction. In this paper, we proposed the Redirected Placement (RP), which determines the new placement of a physical prop using a constrained optimization problem. The visual illusion is used during the ""reach-to-place"" kinematic phase in the proposed RP method rather than the ""reach-to-grasp"" phase in the typical Redirected Reach (RR) method. We conducted two experiments based on the proposed RP method. Our first experiment showed that detection thresholds are generally higher with the proposed method compared to the RR method. The second experiment evaluated the embodiment experience with hand redirection using RR-only, RP-only, and RR&amp;RP methods. The results report an enhanced sense of embodiment with the combined use of both RR and RP techniques. Our study further indicates that a 1:1 combination ratio of RR&amp;RP resulted in the closest subjective experience to the baseline.",Virtual Reality; hand interaction; passive haptic feedback; hand redirection,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Instant Hand Redirection in Virtual Reality Through Electrical Muscle Stimulation-Triggered Eye Blinks,VRST - Virtual Reality Software and Technology,A,"In this paper we investigate the use of electrical muscle stimulation (EMS) to trigger eye blinks for instant hand redirection in virtual reality (VR). With the rapid development of VR technology and increasing user expectations for realistic experiences, maintaining a seamless match between real and virtual objects becomes crucial for immersive interactions. However, hand movements are fast and sometimes unpredictable, increasing the need for instantaneous redirection. We introduce EMS to the field of hand redirection in VR through precise stimulation of the eyelid muscles. By exploiting the phenomenon of change blindness through natural eye blinks, our novel stimulation model achieves instantaneous, imperceptible hand redirection without the need for eye tracking. We first empirically validate the efficiency of our EMS model in eliciting full eye closure. In a second experiment, we demonstrate the feasibility of using such a technique for seamless instantaneous displacement in VR and its particular impact for hand redirection. Among other factors, our analysis also delves into the under-explored domain of gender influence on hand redirection techniques, revealing significant gender-based performance disparities.",virtual reality; VR; eye blinks; EMS; redirection; Hand redirection,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Redirecting Rays: Evaluation of Assistive Raycasting Techniques in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Raycasting-based interaction techniques are widely used for object selection in immersive environments. Despite their intuitive use, they come with challenges due to small or far away objects, hand tremor, and tracking inaccuracies. Previous adaptations for raycasting, such as directly snapping the ray to the closest target, extruding the ray to a cone, or multi-step selection techniques, require additional time for users to become familiar with them. To address these issues, we propose three assistive techniques in which the visible selection ray is subtly redirected towards a target, with a proximity and gain based increase in the redirection amount. In a user study (N = 26), we compared these redirection techniques with a baseline condition based on a Fitts’ law task and collected performance measures as well as comprehensive subjective feedback. The results indicate that the three redirection techniques are significantly faster and have higher effective throughput than the baseline condition. Participants retained a high sense of agency with all redirection techniques and reported significantly lower total workload compared to the baseline. The majority of participants preferred selection with assistive ray redirection and perceived it as not distracting or intrusive. Our findings support that assistive redirected raycasting techniques can improve object selection performance and user experience in virtual environments.",virtual reality; selection; interaction techniques; raycast redirection,Title_Keywords,True,
Scopus,conferencePaper,2023,Stay Vigilant: The Threat of a Replication Crisis in VR Locomotion Research,VRST - Virtual Reality Software and Technology,A,"The ability to reproduce previously published research findings is an important cornerstone of the scientific knowledge acquisition process. However, the exact details required to reproduce empirical experiments vary depending on the discipline. In this paper, we summarize key replication challenges as well as their specific consequences for VR locomotion research. We then present the results of a literature review on artificial locomotion techniques, in which we analyzed 61 papers published in the last five years with respect to their report of essential details required for reproduction. Our results indicate several issues in terms of the description of the experimental setup, the scientific rigor of the research process, and the generalizability of results, which altogether points towards a potential replication crisis in VR locomotion research. As a countermeasure, we provide guidelines to assist researchers with reporting future artificial locomotion experiments in a reproducible form.",Virtual Reality; Reproducibility; Teleportation; Locomotion; Replication Crisis; Steering,Keywords,True,
Scopus,conferencePaper,2023,A Pilot Study on the Impact of Discomfort Relief Measures on Virtual Reality Sickness and Immersion,VRST - Virtual Reality Software and Technology,A,"While there are several theories of virtual reality (VR) sickness causes and pertinent methods suggested for mitigation, it remains an important problem. One possible solution might be to prescribe measures for just relieving the immediate symptoms (vs. addressing the very root causes). Understanding that the severity of the sickness may affect individuals differently, we examined three methods: (1) reducing the weight of the headset (using a suspension mechanism); (2) refreshing the user with a gentle breeze of wind (using a fan); (3) accompanying the VR viewing experience with mindful breathing. We assess the relative sickness reduction effect, if any, of these three measures through a comparative pilot experiment and individual case analysis. The preliminary results point to rather the importance of system usability and how it affects the relationship between the perceived immersion and the extent of sickness. The initial proposition to enhance the user’s physical condition as a way to better withstand VR sickness symptoms could not be established.",Virtual Reality; Multi-modal; Head-mounted Display; VR sickness,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,A Virtualized Augmented Reality Simulation for Exploring Perceptual Incongruencies,VRST - Virtual Reality Software and Technology,A,"When blending virtual and physical content, certain incongruencies emerge from hardware limitations, inaccurate tracking, or different appearances of virtual and physical content. They restrain us from perceiving virtual and physical content as one experience. Hence, it is crucial to investigate these issues to determine how they influence our experience. We present a virtualized augmented reality simulation that can systematically examine single incongruencies or different configurations.",Augmented Reality; Mixed Reality; Virtual Reality; Extended Reality; Visualization; Perception; Congruence,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Audio-based Vibrotactile Feedback in Multimodal VR Interactions,VRST - Virtual Reality Software and Technology,A,"While consumer-grade virtual reality (VR) hardware can deliver immersive audiovisual experiences, these systems often lack the ability to display realistic haptic feedback, or incorporate cost-efficient vibrotactile actuators with very limited abilities to provide tactile feedback. To overcome these limitations, we introduce an approach based on audio-based vibrotactile actuators. Due to their wide frequency response and multiple resonant frequencies, they can provide more tactile details. In our implementation, every VR interaction uses standard audio clips to provide simultaneous auditory and tactile feedback, as well as coupled realistic physics simulations for the visual feedback. We evaluate our approach to assess the benefits on the user’s experience regarding various interaction scenarios in VR, comparing our approach to a simulated fixed-frequency actuator as a baseline. The results confirmed the benefits of our approach in terms of user preference, perceived realism, comfort, sense of agency, and texture perception. Furthermore, multimodal feedback resulted in the best user experience.",virtual reality; haptics; multimodal; vibrotactile; audio-based,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Augmented Aroma: The Influence of Augmented Particles' Movement and Color on Emotion during Olfactory Perception,VRST - Virtual Reality Software and Technology,A,"This study investigates the impact of visual augmentation on the olfactory system by analyzing users’ emotional responses. Augmented particles were presented using HoloLens through five methods, involving adjustment in color and movement, alongside six odors. Through the experiments with 30 participants, we discovered that augmented particles could intensify or reduce emotional reactions based on their colors and movement directions.",Augmented reality; emotion analysis; olfactory perception; odor,Keywords,True,
Scopus,conferencePaper,2023,Combining embodiment and 360 video for teaching protection of civilians to military officers,VRST - Virtual Reality Software and Technology,A,"This demo presents an innovative use of embodiment in combination with 360º video to support teaching the threat-based approach to protection of civilians at a military university. To create a realistic and emotionally appealing XR experience and at the same time save on developing time and costs, scanned 3D objects and avatar were integrated in 360º videos. The app also includes interactions with a virtual perpetrator and collaborative map exercise and received positive feedback from end users.",Extended Reality; Embodiment; 360º video; Interactive Learning Environment,Keywords,True,
Scopus,conferencePaper,2023,Comparing Performance of Dry and Gel EEG Electrodes in VR using MI Paradigms,VRST - Virtual Reality Software and Technology,A,"Brain–computer interfaces (BCIs) are an emerging technology with numerous applications. Electroencephalogram (EEG) motor imagery (MI) is among the most common BCI paradigms and has been used extensively in healthcare applications such as post-stroke rehabilitation. Using a Virtual Reality (VR) game, Push Me, we conducted a pilot study to compare MI accuracy with Gel or active-dry EEG electrodes. The motivation was to (1) investigate the MI paradigm in a VR environment and (2) compare MI accuracy using active dry and gel electrodes with different Machine Learning (ML) classifications (SVM, KNN and RF). The results indicate that while gel-based electrodes, in combination with SVM, achieved the highest accuracy, dry electrode EEG caps achieved similar outcomes, especially with SVM and KNN models.",Virtual Reality; Machine Learning; Electroencephalogram; Brain Computer Interface; Motor Imagery,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Directional Multimodal Flow to Help Mitigate VR Sickness,VRST - Virtual Reality Software and Technology,A,"One way to alleviate VR sickness is to reduce the sensory mismatch between the visual and vestibular organ regarding the motion perception. Mixing in the motion trail in the reverse direction to the original has been suggested as one such method. However, as such visual feedback can be content intrusive, we consider supplementing it by the non-visual multimodal reverse flow. In particular, we have devised methods to supply sound effects as if heard from the reverse direction, and vibration and air flow likewise. Our validation experiment has shown that the multimodal feedback was effective in significantly reducing the sickness, but its direction (reverse or not) did not have an effect as hypothesized.",Virtual Reality; Multi-modal; Head-mounted Display; VR sickness,Keywords,True,
Scopus,conferencePaper,2023,Diving Into The Twilight Zone VR for Marine Biology,VRST - Virtual Reality Software and Technology,A,Teaching students about underwater marine science is difficult due to the limitations required to access underwater environments. Marine science is typically not taught until tertiary education levels. We have developed a Virtual Reality experience for teaching marine science activities focusing on high school students. Our education programme and VR tool can help train the next generation of students into learning and being aware about marine science.,,Abstract,True,
Scopus,conferencePaper,2023,Early User Feedback on a VR Interface Draft for Interaction with a Multi-Robot System in Ship Hull Inspection,VRST - Virtual Reality Software and Technology,A,"The use of multi-robot systems is a field that can benefit from VR by strengthening understanding of the situation and enabling seamless interaction with the actors involved. This work investigates how the usability of a design for interaction with a multi-robot system for ship maintenance is assessed. Furthermore, comments from the participants are consulted as impulses for improving the design.",virtual reality; human-robot interaction; multi-robot,Keywords,True,
Scopus,conferencePaper,2023,Earnormous: An educational VR game about how humans hear,VRST - Virtual Reality Software and Technology,A,"We present a demo of Earnormous, a virtual reality game to teach about the human ear.",Virtual Reality; Hearing,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Effects of Vibrotactile Feedback on Aim-and-Throw Tasks in Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Vibrotactile feedback has been actively utilized in many virtual reality applications to provide the sense of touch. In this preliminary work, we investigated the effects of vibrotactile feedback in the dart throwing task in a virtual environment. The user study compared the task performance, as well as observed the participants’ behavior in throwing tasks with vibrotactile feedback or not. The results showed that the participants made larger body movements during the task when vibrotactile feedback was on, while the feedback did not affect the task performance.",Virtual Reality; Hand Tracking; Vibrotactile Feedback,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Enhancing VR Based Serious Games and Simulations Design: Bayesian Knowledge Tracing and Pattern-Based Approaches,VRST - Virtual Reality Software and Technology,A,"This paper explores how Bayesian Knowledge Tracing (BKT) can be integrated with a pattern-based approach to enhance the development of virtual reality (VR) based serious games and simulations. These technologies allow for the prediction of user progress and the utilization of Artificial Intelligence (AI) methods to tailor difficulty levels based on individual needs. By combining BKT, pattern-based mechanics, and affective feedback, comprehensive data on user interactions, skills, and emotional states can be collected. This data enables the estimation of learners’ knowledge levels and the prediction of their progress.",virtual reality; serious games; design patterns; simulations; Bayesian Knowledge Tracing,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Enhancing User Experience in VR using Wearable Olfactory System,VRST - Virtual Reality Software and Technology,A,"This paper introduces a mounted olfactory device prototype that instantly emits and quickly switches scents. Earlier approaches, such as fixed olfactory devices, have limitations in terms of the time it took for users to perceive a scent in Virtual Reality (VR) due to dissipating lingering scents in physical space, as well as the lack of instantaneous scent switching. To evaluate its effectiveness, we conducted a pilot user study comparing mounted and fixed olfactory devices.The results show that the mounted olfactory device provides a better VR experience than the fixed olfactory device.",,Abstract,True,
Scopus,conferencePaper,2023,Estimating mechanical properties of soft objects using surface measurements from AR headsets,VRST - Virtual Reality Software and Technology,A,Physics-driven predictions of soft tissue mechanics are vital for various medical interventions. Insights on the mechanical properties of soft tissues are essential for obtaining personalised predictions from these models. This study aims to provide a workflow to identify the material parameters of soft homogeneous materials under gravity loading using 3D surface geometrical measurements acquired from a wearable augmented reality (AR) headset’s depth camera. Preliminary results show that the parameter estimation procedure can successfully recover the ground truth material parameter C1 of a cantilever beam using synthetic surface data. This workflow could be used for real-time navigational guidance during soft tissue treatment procedures.,augmented reality; Mechanical parameter estimation,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Exploratory Study on the Reinstatement Effect Under 360-Degree Video-Based Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Episodic memory incorporates environmental contexts, and memory retrieval is aided by matching the retrieval context to the encoding context. This study tested whether similar context-dependency of memory could be confirmed in virtual environments. Participants learned words in a 360-degree video-based virtual environment depicting either natural or urban landscapes. Immediately, they completed a test in the same virtual environment. After two days, half of the participants underwent a final test with the same context as that on the initial day, whereas the other half underwent it with a different context. Surprisingly, participants tested in a different context exhibited significantly lower forgetting than those tested in the same context, which contradicted our hypothesis.",virtual reality; 360-degree video; context-dependent memory,Keywords,True,
Scopus,conferencePaper,2023,Fabric Electrodes for Physiological Sensing in a VR HMD,VRST - Virtual Reality Software and Technology,A,"This paper explores the development and testing of fabric electrodes to collect a range of physiological measures. The aim is to integrate these sensors into a Virtual Reality (VR) headset to collect physiological and muscular motion data that will help detect emotion, cognitive load and facial expressions. As part of an on-going project, we have already developed prototypes of the EMG and GSR sensors. A head phantom has been developed for the purpose of testing and validating electrode performance.",Virtual Reality; Empathic Computing; Physiological Sensors,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Immersive Climate Narratives: Using Extended Reality to Raise Climate Change Awareness,VRST - Virtual Reality Software and Technology,A,"Shadows of Tomorrow is an innovative public art installation utilizing LiDAR body tracking and augmented reality to promote climate change awareness. The installation projects depictions of real-world climate change scenarios from Australia, Kuwait, the United States, and Greenland on a large display. Shadows utilizes the Azure Kinect to capture and integrate audience member silhouettes as a simple user interface for the display. As audiences move in front of the display, their silhouette reveals the impact of climate change on the projected environment. By bringing global climate change stories to local audiences, we emphasize the universal, yet highly localized impacts of climate crisis. These interactive visualizations encourage audiences to engage with and understand the stark realities of climate change in regions far removed from their own. Created for display in high-transit public areas like museums, airports, and city centers, Shadows of Tomorrow aims to create a global dialogue around our shared responsibility for climate action.",,Title_Abstract,True,
Scopus,conferencePaper,2023,Immersive visualization for ecosystem services analysis,VRST - Virtual Reality Software and Technology,A,"Ecosystem services are benefits provided to humans by ecosystems through the natural processes and conditions which occur&nbsp;[7]. Interviews with land use scientists identified problems with currently available software applied to their ecosystem services analysis. A user centred design process is adopted and a visualization system, Immersive ESS Visualizer, is presented for visualizing data relating to ecosystem services analysis. Immersive ESS Visualizer is designed for both experts and non-experts and allows users to compare data visualized with multiple hand-manipulated maps. Users can glide over a landscape with data layers draped to analyse areas of interest. Immersive ESS Visualizer could augment a process for presenting ecosystem services analysis results.",Virtual Reality; Visualization; Ecosystems Services,Keywords,True,
Scopus,conferencePaper,2023,Listen again: virtual reality based training for children with hearing impairments,VRST - Virtual Reality Software and Technology,A,"Although hearing loss is treated with hearing technology and rehabilitation, children with hearing loss still face challenges. Factors such as distance to the sound source and noise from the surroundings are the children’s biggest enemies. In the ""Listen Again"" project, a listening- and spatial awareness training application was co-designed together with deaf and hard-of-hearing children who use cochlear implants. This paper presents quantitative and qualitative results from a two-month evaluation where 22 children were asked to play with the VR solution for two months, three times a week.",spatial awareness; hearing aids; virtual reality.,Title_Keywords,True,
Scopus,conferencePaper,2023,Navigating in VR using free-hand gestures and embodied controllers: A comparative evaluation,VRST - Virtual Reality Software and Technology,A,"While natural body-based movements are essential features for immersive VR (Virtual Reality) experiences, most of the available input techniques for navigation in VR involve the use of hand-held controllers. Alternatively, while body-based input for VR navigation has previously been explored in HCI using external tracking devices, there is little to no work that utilizes the in-built tracking functionalities of the predominant VR headsets (such as Meta Quest 2) for gesture-based navigation in VR. This paper addresses this research gap by proposing five free-hand gestures for 3-D navigation in VR using internal gesture-tracking functionality of Quest 2 headset. Additionally, a qualitative and quantitative comparison is presented between free-hand and controller-based navigation in VR using a custom designed task (with 10 users). Overall, the findings from the task-analysis indicate that while in-built tracking functionalities in VR headsets open doors for inexpensive gesture-based VR navigation, the mid-air hand-gestures result into greater fatigue as compared to using controllers for navigation in VR.",virtual reality; navigation; motion-capture; free-hand gestures; gesture-recognition,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Pain Distraction for Children Through VR- or Audio-haptic Soundscapes in Situ,VRST - Virtual Reality Software and Technology,A,"In this pilot study we compare two prototype applications developed in collaboration with Rigshospitalet, the main hospital in Denmark, aimed to evaluate the effectiveness of a virtual reality (VR)- versus an audio-haptic based solution, as a pain distraction tool for children aged 5 to 8 during needle-related medical procedures. Both prototypes were developed with a narrative where children help a farmer find hidden animals. The final prototype underwent testing in situ, at Rigshospitalet’s clinic for blood tests. Here, participants’ pain levels were assessed using the Wong-Baker FACES Scale [9] and the Visual Analogue Scale [5]. Both prototypes saw participants report reduced pain perception, skewing more in favor of the VR prototype. However, the audio-haptic prototype showed similar levels of reduction in pain perception when effective. The study concludes that both VR- and audio-haptic based distraction are viable methods, that each cover a group’s needs within medical procedures involving young children (those who need to not see the procedure, and those who do), and that these should be further developed and implemented in said medical procedures.",,Abstract,True,
Scopus,conferencePaper,2023,Performing Tasks in Virtual Reality. Interplay between Realism and Visual Imagery,VRST - Virtual Reality Software and Technology,A,The main aims of the presented study are to verify whether the amount of textures in a virtual scene affects task performance and to test whether visual imagery changes the relationship between realism and task performance. An experimental study with three groups differed in visual realism was conducted (n=100). Participants were asked to perform a task: taking on the role of a marshaller and positioning the plane on the airport apron. Results indicate that texturing does not affect task performance. Visual imagery is a moderator of the relationship between perceived realism and task performance. A high level of imagery interferes with a high realism assessment decreasing task performance.,virtual reality; task performance; scene realism; visual imagery,Title_Keywords,True,
Scopus,conferencePaper,2023,Pigments of Imagination: An Interactive Virtual Reality Composition,VRST - Virtual Reality Software and Technology,A,"Pigments of Imagination is an artistic interactive virtual reality experience based on an original fixed media composition. It is designed to reimagine the popular music video in a virtual space as a dynamic, emotionally engaging experience through exploration of novel approaches toward audiovisual reactivity and interactivity. In this piece the user can interact, directly affect, and build upon prior user interpolations of the environment’s sonic and visual qualities, allowing a narrative immersion that maintains a structured arc and conclusion but unique experience with each use.",virtual reality; haptics; audiovisual; spatial audio; interactivity; pentimento; VR music video,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Reducing Sensing Errors in a Mixed Reality Musical Instrument,VRST - Virtual Reality Software and Technology,A,"This paper describes the design and evaluation of Netz, a novel mixed reality musical instrument that leverages artificial intelligence for reducing errors in gesture interpretation by the system. We followed a participatory design approach over three months through regular sessions with a professional musician. We explain our design process and discuss technological sensing errors in mixed reality devices, which emerged during the design sessions. We investigate the use of interactive machine learning techniques to mitigate such errors. Results from statistical analyses indicate that a deep learning model based on interactive machine learning can significantly reduce the number of technological errors in a set of musical performance tasks with the mixed reality musical instrument. Based on our findings, we argue that the application of interactive machine learning techniques can be beneficial for embodied, hand-controlled musical instruments in the mixed reality domain.",participatory design; hand-pose estimation; mixed reality musical instruments,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,ShadowPlayVR: Understanding Traditional Shadow Puppetry Performance Techniques Through Non-Intuitive Embodied Interactions.,VRST - Virtual Reality Software and Technology,A,"""ShadowPlayVR"" is a virtual reality system designed to introduce the intricate art of Chinese shadow puppetry into Virtual Reality (VR), focusing on the non-intuitive embodied interactions that emulate puppetry performance. By incorporating an immersive, experiential learning approach, ShadowPlayVR offers users a hands-on understanding of this art form. Preliminary testing reveals the significant role of contextual information in facilitating understanding and mastering these complex interactions. The work also presented showcases how VR can serve as a powerful tool to preserve and engage with traditional cultural heritage in a contemporary digital context.",Interaction Design; Virtual Reality (VR); Embodied Interactions; Experiential Learning,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Sickness Reduction in FPV Drone Control: Improved Effects of Reverse Optical Flow with Static Landmarks Only,VRST - Virtual Reality Software and Technology,A,"Drones are controlled remotely through the on-board live camera often using a headset to immersively (without external distraction) situate the drone operator with a first-person view. The highly dynamic imagery of drone piloting can elicit significant VR sickness. In this poster, we demonstrate the application of mixing the reverse optical flow pattern into the drone piloting imagery for mitigating VR sickness, using only the features of the static landmarks in the scene. We compare it to the cases of applying no mitigation technique (baseline) and mixing in the optical flow pattern from “all” object features. The results show that the suggested method was significantly more effective in reducing sickness than when considering all object features, and had a higher preference for improved visibility and controllability.",Virtual Reality; Object Detection; Optical Flow; Drone; VR Sickness,Keywords,True,
Scopus,conferencePaper,2023,Simple and Practical Dual Rendering for Reducing Eye Fatigue from Vergence-Accomodation Conflict in Stereoscopic Viewing,VRST - Virtual Reality Software and Technology,A,"Eye fatigue and unpleasant symptoms caused by the vergence and accommodation conflict with stereoscopic rendering pose a substantial usability issue in virtual reality. To address this problem, dual rendering of the stereoscopic imagery is proposed, aimed to alleviate such stress on the user. First, the scene is divided into two regions: the front proximal zone and the rest behind it. The back layer is rendered first for objects in the rest, with the conventional stereoscopic viewing parameter values. Then, remaining objects in the proximal zone are rendered using viewing parameters adjusted to reduce the VAC. The validation experiment confirmed that the proposed approach significantly reduced the overall eye fatigue without compromising the user’s depth perception ability to manipulate objects in the parameters-altered proximal zone.",Virtual Reality; Dual Rendering; Vergence Accommodation Conflict,Abstract_Keywords,True,
Scopus,conferencePaper,2023,Slingshot: A Novel Gesture Locomotion System for Fast-paced Gameplay in Virtual Reality,VRST - Virtual Reality Software and Technology,A,,,Title,True,
Scopus,conferencePaper,2023,SpaceVR: Virtual Reality Space Science Outreach Experience,VRST - Virtual Reality Software and Technology,A,Teaching people about Space concepts is challenging with traditional text book and teaching methods. It is hard to encourage prospective students with these traditional methods as they lack engagement and interactivity. We have developed SpaceVR which is a VR application that provides high school students with an engaging outreach experience about Space Science. The application uses real images of the sun from NASA’s solar dynamics observatory satellite to create an interactive digital Sun for students to explore. The project investigates if adding gamification elements will increase student engagement with Space Science outreach efforts.,,Title,True,
Scopus,conferencePaper,2023,Stress visualization in geometrically complex structures using Thermoelastic Stress Analysis and Augmented Reality,VRST - Virtual Reality Software and Technology,A,"We present a framework for the visualization of mechanical stress using augmented reality (AR) using Thermoelastic Stress Analysis (TSA). The 2D stress images generated by TSA are converted to a 3D stress map using computer vision technology and then superimposed on the real object using AR. Our framework enables in-situ visualization of stress in geometrically complex structural components, which can assist in the design, manufacture, test, and through-life sustainment of failure-critical engineering assets. We also discuss the challenges of such a TSA-AR combination and present a case study that demonstrates the performance and significance of our system.",,Title_Abstract,True,
Scopus,conferencePaper,2023,Temporal Foveated Rendering for VR on Modern Mobile Architecture,VRST - Virtual Reality Software and Technology,A,"We introduce Temporal Foveated Rendering (TFR), a method of achieving GPU savings for VR content by reducing rendering frequency in the periphery of a fixed or eye tracked mobile VR headset utilizing tiled rendering. TFR saves GPU compute by rendering a peripheral “outset” at half rate, while maintaining full frame rate in a smaller ""inset"" centered at the gaze position. Judder is mitigated in the peripheral outset by applying asynchronous space warp, driven by System on Chip (SoC) derived motion vectors. This technique saves up to 17% more GPU compute on a mobile device, compared to a spatial foveated rendering technique called Fixed Foveated Rendering (FFR).",mobile devices; virtual reality; foveated rendering,Keywords,True,
Scopus,conferencePaper,2023,The Detectability of Saccadic Hand Offset in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"On the way towards novel hand redirection (HR) techniques that make use of change blindness, the next step is to take advantage of saccades for hiding body warping. A prerequisite for saccadic HR algorithms, however, is to know how much the user’s virtual hand can unnoticeably be offset during saccadic suppression. We contribute this knowledge by conducting a psychophysical experiment, which lays the ground for upcoming HR techniques by exploring the perceptual detection thresholds (DTs) of hand offset injected during saccades. Our findings highlight the pivotal role of saccade direction for unnoticeable hand jumps, and reveal that most offset goes unnoticed when the saccade and hand move in opposite directions. Based on the gathered perceptual data, we derived a model that considers the angle between saccade and hand offset direction to predict the DTs of saccadic hand jumps.",virtual reality; saccades; detection thresholds; hand redirection,Title_Keywords,True,
Scopus,conferencePaper,2023,The Effect of False but Stable Heart Rate Feedback via Sound and Vibration on VR User Experience,VRST - Virtual Reality Software and Technology,A,"Vital signals tend to become destabilized and generally increase when one’s physical condition is not well. For example, experiencing virtual reality (VR) sickness brings about a deteriorated physical condition, often accompanied by an increased heart rate. Several research have shown that providing feedback of false heart rate can induce various altered perceptions, such as increased effort and anxiety. In this poster, we propose to provide false but “stable” heart rate feedback through sound and vibration while navigating a sickness-inducing VR scene. We hypothesize that the false but stable heart rate feedback will have an induced effect of calming the user down (even stabilizing the heart rate itself) and reducing the unpleasant VR sickness symptoms. A pilot study was conducted to compare three conditions, namely viewing a sickness eliciting VR content, (1) as is, (2) with the false but stable heart rate feedback through sound, and (3) with the false but stable heart rate feedback through vibration. Results showed that the level of sickness was significantly reduced by sound and vibration feedback, respectively.",Virtual Reality; False Heart Rate; VR Sickness; Calming Effects,Abstract_Keywords,True,
Scopus,conferencePaper,2023,The Effect of Virtual Reality Level of Immersion on Spatial Learning Performance and Strategy Usage,VRST - Virtual Reality Software and Technology,A,"The utilization of the immersive and ecological nature of head-mounted displayed (HMD) virtual reality (VR) has been increasing in studies of human spatial learning, and various aspects of VR's impact on navigation have been examined. Nevertheless, the effect the VR level of immersion has on spatial learning strategy usage is yet to be determined. Here, we addressed this gap by comparing spatial learning properties and experience measures in three modality settings. We translated a classic spatial learning task from animals to humans, where three spatial learning strategies were observed (place/cue/response). We compared 3 conditions: wearing a VR headset while physically walking vs. using a controller, and a 2D screen display using a mouse and a keyboard for navigation. We examined various learning properties and used presence questionnaires to analyze experience measures. Our results show that learning measures including strategy usage were affected by the VR level of immersion, suggesting that modality characteristics should be considered during VR task design.",Navigation; Learning Performance; Level of Immersion; Spatial Learning Strategy,Title_Abstract,True,
Scopus,conferencePaper,2023,The Effects of Customized Strategies for Reducing VR Sickness,VRST - Virtual Reality Software and Technology,A,"The extent of virtual reality (VR) sickness varies widely among users, as each user has different sensitivities to diverse causes of VR sickness. This can make prescribing a single particular reduction technique difficult and ineffective. In this poster, we present preliminary work examining the more effective and preferred sickness-reduction techniques for a given user under varied sickness-inducing motions. Based on the user-specific information collected using VR content, a customized strategy is developed for a given user and applied to the same VR content. We report the experimental results for testing the effectiveness of the customized reduction technique, comparing it to a single particular reduction method.",Virtual Reality; VR Sickness; Personalized Experiences.; Restricted Field-of-view; Reverse Optical Flow; Virtual Nose,Abstract_Keywords,True,
Scopus,conferencePaper,2023,The Staircase Procedure Toolkit: Psychophysical Detection Threshold Experiments Made Easy,VRST - Virtual Reality Software and Technology,A,"We propose a novel open-source software toolkit to support researchers in the domains of human-computer interaction (HCI) and virtual reality (VR) in conducting psychophysical experiments. Our toolkit is designed to work with the widely-used Unity engine and is implemented in C# and Python. With the toolkit, researchers can easily set up, run, and analyze experiments to find perceptual detection thresholds using the adaptive weighted up/down method, also known as the staircase procedure. Besides being straightforward to integrate in Unity projects, the toolkit automatically stores experiment results, features a live plotter that visualizes answers in real time, and offers scripts that help researchers analyze the gathered data using statistical tests.",Python; Unity; detection threshold; psychophysical experiments; staircase procedure; up/down method,Abstract,True,
Scopus,conferencePaper,2023,Utilizing AR as a Tool for Assessing Accessibility in the Home,VRST - Virtual Reality Software and Technology,A,"Home modification interventions can remedy deficiencies in the home environments of the growing number of older adults that want to age in place. Unfortunately, performing an assessment of a home environment is a difficult process, often requiring numerous measurements from a skilled practitioner. To address these gaps in practice, we used an iterative co-design process to develop a first prototype of a novel augmented reality home assessment tool (ARHAT) that can be utilized more rapidly by both individuals in and outside of health care, as well as performed either on or off-site. The aim of this work is to create a tool for major stakeholders involved in supporting housing design and aging in place, thereby reaching and making a difference in the lives of more older adults.",Augmented Reality; Assessments; Measurements,Abstract_Keywords,True,
Scopus,conferencePaper,2023,VR Experiences of Pregnant Women During Antenatal Care,VRST - Virtual Reality Software and Technology,A,"Pregnant women use a range of non-pharmacological pain relief methods to help manage and reduce pain intensity and to induce relaxation. We conducted a study with 18 pregnant women to explore VR experiences as a non-pharmacological method of pain relief to determine the effect on pain intensity. The results of the study identified several themes: evoking emotion with sub-themes, memory, and imagination. The theme presence, with sub-themes of relatability, realism, immersion, interactivity, and narration. Finally, the escape and anchoring themes were descriptions of how women envisaged using VR antenatally. This study provides a novel contribution to the field of VR and antenatal and labour care which can help inform the design of VR experiences for pregnant women.",Virtual Reality; Analgesia; Antenatal; Birth; Labour Pain; Relaxation,Keywords,True,
Scopus,conferencePaper,2023,Waddle: using virtual penguin embodiment as a vehicle for empathy and informal learning,VRST - Virtual Reality Software and Technology,A,"This paper presents, Waddle, a virtual experience to promote informal learning by embodying the user as an Adélie Penguin to partake in a narrative-based virtual reality application that shares the story of the lives of these unique animals. We test the effects of this experience on informal learning and empathy, an important component for fostering social engagement with ecology. The research demonstrates that the developed experience is able to support informal learning, virtual embodiment, and is able to create a positive change in empathy.",Informal Learning; Empathy; Virtual Embodiment,Abstract,True,
Scopus,conferencePaper,2023,Walking-in-Flat-Place on Non-flat Virtual Environment can be Sickening!,VRST - Virtual Reality Software and Technology,A,"It is well-known that employing the Walking-in-Place (WIP) interface can significantly reduce VR sickness in addition to promoting the sense of presence, immersion, and natural interaction. In this poster, we re-examine the conditions for which WIP will effectively reduce VR sickness. In particular, we investigate and compare the cases of applying WIP to navigating on flat terrain vs. up-and-down ramps with respect to the sickness reduction effect. We point out that naively designed WIP/navigation content has the possibility of actually worsening the VR sickness due to the sensory and reality mismatch between the flat real operating environment and the inclined virtual terrain.",Virtual Reality; User Study; Cybersickness; Locomotion; Walking-in-Place (WIP),Keywords,True,
Scopus,conferencePaper,2023,XR for Improving Cardiac Catheter Ablation Procedure,VRST - Virtual Reality Software and Technology,A,"Arrhythmia refers to abnormalities of the heart rhythm, and it is considered a life-threatening pathology. Catheter ablation is a minimally invasive procedure which provides the best therapeutic outcomes to cure the arrhythmia. The procedure consists of a series of intraoperative and training challenges that could potentially affect the procedure outcome. This study examines how Extended Reality (XR) technologies (AR/VR) can be used to improve the cardiac catheter ablation procedure for electrophysiologists.",,Abstract,True,
Scopus,conferencePaper,2024,Context-Relevant Locations as an Alternative to the Place Illusion in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Presence is a powerful aspect of Virtual Reality (VR). However, there has been no consensus on how to achieve presence in Augmented Reality (AR) or whether it exists at all. The Place Illusion, a key component in presence as defined in VR, cannot be obtained in AR as there is no way to make the user feel as though they are transported somewhere else when they are limited to what they can physically see in front of them. However, recently it has been argued that coherence or congruence are important parts of the Place and Plausibility Illusions. The implication for AR is that the AR content might invoke a higher Plausibility Illusion if it is consistent with the physical place the content is situated in. In this study, we define the concept of a Context-Relevant Location (CRL), a physical place that is congruent with the experience. We present a study with a between-subjects design that allowed users to interact with AR objects in a CRL and in a generic environment. The results indicate that presence was higher in the CRL setting than the generic environment, contribute to the debate about providing a concrete description of presence-like phenomena in AR, and posit that CRLs play a similar role to the Place Illusion in an AR setting.",presence; Augmented reality; context-relevant location; plausibility,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,"Lifter for VR Headset: Enhancing Immersion, Presence, Flow, and Alleviating Mental and Physical Fatigue during Prolonged Use",VRST - Virtual Reality Software and Technology,A,"The virtual reality (VR) headset is still relatively heavy, causing a significant physical and mental burden and negatively affecting the VR user experience, particularly during extended periods of use. In this paper, we present a prototype design of the “Lifter,” which utilizes a counterbalanced wire-pulley mechanism to partially relieve the weight of the VR headset (between 50% and 85%). The human subject study has confirmed that the Lifter relieved not only physical fatigue but also significantly improved mental burden, sense of immersion, presence, and flow (perception of time passing) during prolonged usage (30 minutes or more).",Head-Mounted Display; Headset Weight; Weight Reduction,Abstract,True,Duplicate
Scopus,conferencePaper,2024,"MeetingBenji: Tackling Cynophobia with Virtual Reality, Gamification, and Biofeedback",VRST - Virtual Reality Software and Technology,A,"Phobias, particularly animal phobias like cynophobia (fear of dogs), disrupt the lives of those affected by, for instance, limiting outdoor activities. While virtual reality exposure therapy (VRET) has emerged as a potential treatment for this phobia, these efforts have been limited by high dropout rates and a lack of ability to handle stressful situations in people who suffer from cynophobia. Inspired by these challenges, we present MeetingBenji, a VRET system for cynophobia that uses (i) gamification to enhance motivation and engagement, and (ii) biofeedback to facilitate self-control and reduce physiological responses. In a study (N=10) that compared the effects of displaying dogs in 3D scenes and 360º videos using the Behavioral Approach Test (BAT) – in which participants are increasingly exposed to the source of phobia – participants reported a high level of immersion to the exposure sequence. Further, they reported feeling more anxiety with 3D content than 360º video (60%), lower heart rates in the presence of biofeedback (between 1.71% and 7.46%), and improved self-control across the three exposure levels. They appreciated our gamified elements – completing all exposure levels. This study suggests that VRET with gamification and biofeedback is an effective approach to stimulate the habituation of people with cynophobia.",biofeedback; gamification; cynophobia; VR exposure therapy,Title_Abstract,True,Duplicate
Scopus,conferencePaper,2024,iStrayPaws: Immersing in a Stray Animal's World through First-Person VR to Bridge Human-Animal Empathy,VRST - Virtual Reality Software and Technology,A,"While Virtual Reality Perspective-Taking (VRPT) demonstrates its efficiency in inducing empathy, its application primarily focuses on vulnerable humans, not animals. Existing animal-related works mainly targets farm animals and wildlife. In this work, we focus on stray animals and introduce iStrayPaws, a VRPT system that simulates stray animals’ challenging lives. The system offers users an immersive first-person journey into the world of stray animals encountering different difficulties like inclement weather, hunger, and illnesses. Enriched with audio-visual and kinesthetic design, the system seeks to deepen users’ understanding of stray animals’ life and foster profound emotional connections. To evaluate the system, a user study was conducted, which showed that VRPT recipients exhibited significant improvement in both state and trait empathy compared to traditional method. Our research not only delivers a novel, accessible, and interactive animal empathy experience but also provides innovative solutions for addressing stray animal issues and advancing broader animal welfare work.",Virtual Reality; Empathy; Embodied Experience; Hand Mocap; Stray Animals,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Exploring Presence in Interactions with LLM-Driven NPCs: A Comparative Study of Speech Recognition and Dialogue Options,VRST - Virtual Reality Software and Technology,A,"Combining modern technologies like large-language models (LLMs), speech-to-text, and text-to-speech can enhance immersion in virtual reality (VR) environments. However, challenges exist in effectively implementing LLMs and educating users. This paper explores implementing LLM-powered virtual social actors and facilitating user communication. We developed a murder mystery game where users interact with LLM-based non-playable characters (NPCs) through interrogation, clue-gathering, and exploration. Two versions were tested: one using speech recognition and another with traditional dialog boxes. While both provided similar social presence, users felt more immersed with speech recognition but found it overwhelming, while the dialog version was more challenging. Slow NPC response times were a source of frustration, highlighting the need for faster generation or better masking for a seamless experience.",NPC; VR; Presence; Speech Recognition; Immersive systems; Large Language Models (LLM); Social Actors,Abstract,True,Duplicate
Scopus,conferencePaper,2024,Effects of Different Tracker-driven Direction Sources on Continuous Artificial Locomotion in VR,VRST - Virtual Reality Software and Technology,A,"Continuous artificial locomotion in VR typically involves users selecting their direction using controller input, with the forward direction determined by the Head, Hands, or less commonly, the Hip. The effects of these different sources on user experience are under-explored, and Feet have not been used as a direction source. To address these gaps, we compared these direction sources, including a novel Feet-based technique. A user study with 22 participants assessed these methods in terms of performance, preference, motion sickness, and sense of presence. Our findings indicate high levels of presence and minimal motion sickness across all methods. Performance differences were noted in one task, where the Head outperformed the Hand. The Hand method was the least preferred, feeling less natural and realistic. The Feet method was found to be more natural than the Head and more realistic than the Hip. This study enhances understanding of direction sources in VR locomotion and introduces Feet-based direction as a viable alternative.",Virtual Reality; User Studies; Continuous Locomotion,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Influence of Rotation Gains on Unintended Positional Drift during Virtual Steering Navigation in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Unintended Positional Drift (UPD) is a phenomenon that occurs during navigation in Virtual Reality (VR). It is characterized by the user’s unconscious or unintentional physical movements in the workspace while using a locomotion technique (LT) that does not require physical displacement (e.g., steering, teleportation). Recent work showed that some factors, such as the LT used and the type of trajectory, can influence UPD. However, little is known about the influence of rotation gains (commonly used in redirection-based LTs) on UPD during navigation in VR. In this paper, we conducted two user studies to assess the influence of rotation gains on UPD. In the first study, participants had to perform consecutive turns in a corridor virtual environment. In the second study, participants had to explore a large office floor and collect spheres freely. We compared the conditions between rotation gains and without gains, and we also varied the turning angle to perform the turns while considering factors such as sensitivity to cybersickness and the learning effect. We found that rotation gains and lower turning angles decreased UPD during the first study, but the presence of rotation gains increased UPD in the second study. This work contributes to the understanding of UPD, which tends to be an overlooked topic and discusses the design implications of these results for improving navigation in VR.",Virtual Reality; Locomotion Techniques; Rotation Gains; Unintended Positional Drift,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Semi-Automated Guided Teleportation through Immersive Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Immersive knowledge spaces like museums or cultural sites are often explored by traversing pre-defined paths that are curated to unfold a specific educational narrative. To support this type of guided exploration in VR, we present a semi-automated, hands-free path traversal technique based on teleportation that features a slow-paced interaction workflow targeted at fostering knowledge acquisition and maintaining spatial awareness. In an empirical user study with 34 participants, we evaluated two variations of our technique, differing in the presence or absence of intermediate teleportation points between the main points of interest along the route. While visiting additional intermediate points was objectively less efficient, our results indicate significant benefits of this approach regarding the user’s spatial awareness and perception of interface dependability. However, the user’s perception of flow, presence, attractiveness, perspicuity, and stimulation did not differ significantly. The overall positive reception of our approach encourages further research into semi-automated locomotion based on teleportation and provides initial insights into the design space of successful techniques in this domain.",Virtual Reality; 3D Navigation; Teleportation; 3D User Interfaces; Head-Mounted Display; Guided Navigation; Guided Tour,Keywords,True,Duplicate
Scopus,conferencePaper,2024,The Effects of Electrical Stimulation of Ankle Tendons on Redirected Walking with the Gradient Gain,VRST - Virtual Reality Software and Technology,A,"As a redirected walking technique, a method has been proposed to enable users to walk in an undulating virtual space even in a flat physical environment by setting the slope of the floor in the virtual environment to be different from that in the physical environment without causing discomfort. However, the slope range in which discrepancies between visual and proprioceptive sensations are not perceived is limited, restricting the slopes that can be presented. In this study, we proposed redirected walking using electrical stimulation of the Achilles and tibialis anterior muscle tendons, extending the applicable slope range of redirected walking without compromising the natural gait sensation. Electrical stimulation of the ankle tendons affects the proprioceptive sensation and gives the illusion of tilting in the standing posture, expanding the applicable slope range. Two experiments showed that the proposed method improved the experience of uphill and downhill walking in terms of the range of the virtual slope where a high naturalness of gait and a high congruency of visual and proprioceptive sensations are maintained. Notably, electrical stimulation of the Achilles tendons significantly improved the naturalness of the walking experience during virtual downhill walking, which has been considered more challenging in previous studies.",Virtual reality; Redirected walking; Electrical stimulation of ankle tendons; Locomotion technique; Transcutaneous electrical stimulation,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Neural Motion Tracking: Formative Evaluation of Zero Latency Rendering,VRST - Virtual Reality Software and Technology,A,"Low motion-to-photon latencies between physical movement and rendering updates are crucial for an immersive virtual reality (VR) experience and to avoidusers’ discomfort and sickness. Current methods aim to minimize the delay between the motion measurement and rendering at the cost of increasing technical complexity and possibly decreasing accuracy. By relying on capturing physical motion, these strategies will, by nature, not result in zero latency rendering or will be based on prediction and resulting uncertainty. This paper presents and evaluates a novel alternative and proof of principle for VR motion tracking that could enable motion-to-photon latencies of zero and below zero in time. We termed our concept Neural Motion Tracking, which we define as the sensing and assessment of motion through human neural activation of the somatic nervous system. In contrast to measuring physical activity, the key principle is that we aim to utilize the physiological timeframe between a user’s intention and the execution of motion. We aim to foresee upcoming motion ahead of the physical movement, by sampling preceding electromyographic signals before the muscle activation. The electromechanical delay (EMD) between potential change in the muscle activation and actual physical movement opens a gap in which measurement can be taken and evaluated before the physical motion. In a first proof of principle, we evaluated the concept with two activities, arm bending and head rotation, measured with a binary activation measure. Our results indicate that it is possible to predict movement and update a rendering up to 2&nbsp;ms before its physical execution, which is assessed by optical tracking after approximately 4&nbsp;ms. However, to make the best use of this advantage, electromyography (EMG) sensor data should be as high quality as possible (i.e., low noise and from muscle-near electrodes). Our results empirically quantify this characteristic for the first time when compared to state-of-the-art optical tracking systems for VR. We discuss our results and potential pathways to motivate further work toward marker- and latency-less motion tracking.",mixed reality; augmented reality; Virtual reality; latency; tracking; electromyography,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Investigation of Redirection Algorithms in Small Tracking Spaces,VRST - Virtual Reality Software and Technology,A,"In virtual reality, redirected walking lets users walk in larger virtual spaces than the physical tracking space set aside for their movements. This benefits immersion and spatial navigation compared to virtual locomotion techniques such as teleportation or joystick control. Different algorithms have tried to optimise redirected walking. These algorithms have been tested in simulation in large spaces and with small user studies. However, few studies have looked at the user experience of these algorithms in small tracking spaces. We conducted a user study to compare the performance of different redirected walking algorithms in a small tracking space of 3.5m x 3.5m. Three algorithms were chosen based on their approaches to redirection – Reset Only, Steer to Centre and Alignment Based Redirection Control. 36 people participated in the study. It was found users preferred Reset Only in the tracking space. Reset Only redirects users less and is easier to implement than Steer to Centre or Alignment Based Redirection Control. Additionally, Reset Only had similar performance to Steer to Centre and better task performance than Alignment Based Redirection Control despite resetting users more often. Based on these findings, we provide guidelines for developers working in small tracking spaces.",virtual reality; user study; user experience; locomotion; redirected walking,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Interactive Multi-GPU Light Field Path Tracing Using Multi-Source Spatial Reprojection,VRST - Virtual Reality Software and Technology,A,"Path tracing combined with multiview displays enables progress towards achieving ultrarealistic virtual reality. However, multiview displays based on light field technology impose a heavy workload for real-time graphics due to the large number of views to be rendered. In order to achieve low latency performance, computational effort can be reduced by path tracing only some views (source views), and synthesizing the remaining views (target views) through spatial reprojection, which reuses path traced pixels from source views to target views. Deciding the number of source views with respect to the computational resources is not trivial, since spatial reprojection introduces dependencies in the otherwise trivially parallel rendering pipeline and path tracing multiple source views increases the computation time. In this paper, we demonstrate how to reach near-perfect linear multi-GPU scalability through a coarse-grained distribution of the light field path tracing workload. Our multi-source method path traces a single source view per GPU, which helps decreasing the number of dependencies. Reducing dependencies reduces the overhead of image transfers and G-Buffers rasterization used for spatial reprojection. In a node of 4 × RTX A6000 GPUs, given 4 source views, we reach a light field rendering frequency of 3–19 Hz, which corresponds to interactive rate. On four test scenes, we outperform state-of-the-art multi-GPU light field path tracing pipelines, achieving a speedup of 1.65 × up to 4.63 × for 1D light fields of dimension 100 × 1, each view having a resolution of 768 × 432, and 1.51 × up to 3.39 × for 2D stereo near-eye light fields of size 12 × 6 (left eye: 6 × 6 views and right eye: 6 × 6 views), 1024 × 1024 per view.",Dependencies; Multiview; Parallel Rendering; View Synthesis,Abstract,True,Duplicate
Scopus,conferencePaper,2024,Exploring Visual Conditions in Virtual Reality for the Teleoperation of Robots,VRST - Virtual Reality Software and Technology,A,"In the teleoperation of robots, the absence of proprioception means that visual information plays a crucial role. Previous research has investigated methods to offer optimal vantage points to operators during teleoperation, with virtual reality (VR) being proposed as a mechanism to give the operator intuitive control over the viewpoint for improved visibility and interaction. However, the most effective perspective for robot operation and the optimal portrayal of the robot within the virtual environment remain unclear. This paper examines the impact of various visual conditions on users’ efficiency and preference in controlling a simulated robot via VR. We present a user study that compares two operating perspectives and three robot appearances. The findings indicate mixed user preferences and highlight distinct advantages associated with each perspective and appearance combination. We conclude with recommendations on selecting the most beneficial perspective and appearance based on specific application requirements.",Virtual Reality; 3D User Interfaces; Teleoperation,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Choose Your Reference Frame Right: An Immersive Authoring Technique for Creating Reactive Behavior,VRST - Virtual Reality Software and Technology,A,"Immersive authoring enables content creation for virtual environments without a break of immersion. To enable immersive authoring of reactive behavior for a broad audience, we present modulation mapping, a simplified visual programming technique. To evaluate the applicability of our technique, we investigate the role of reference frames in which the programming elements are positioned, as this can affect the user experience. Thus, we developed two interface layouts: ""surround-referenced"" and ""object-referenced"". The former positions the programming elements relative to the physical tracking space, and the latter relative to the virtual scene objects. We compared the layouts in an empirical user study (n = 34) and found the surround-referenced layout faster, lower in task load, less cluttered, easier to learn and use, and preferred by users. Qualitative feedback, however, revealed the object-referenced layout as more intuitive, engaging, and valuable for visual debugging. Based on the results, we propose initial design implications for immersive authoring of reactive behavior by visual programming. Overall, modulation mapping was found to be an effective means for creating reactive behavior by the participants.",Virtual Reality; Immersive Authoring; Visual Programming; Empirical Evaluation; Spatial Reference Frames,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Motion Passwords,VRST - Virtual Reality Software and Technology,A,"This paper introduces “Motion Passwords”, a novel biometric authentication approach where virtual reality users verify their identity by physically writing a chosen word in the air with their hand controller. This method allows combining three layers of verification: knowledge-based password input, handwriting style analysis, and motion profile recognition. As a first step towards realizing this potential, we focus on verifying users based on their motion profiles. We conducted a data collection study with 48 participants, who performed over 3800 Motion Password signatures across two sessions. We assessed the effectiveness of feature-distance and similarity-learning methods for motion-based verification using the Motion Passwords as well as specific and uniform ball-throwing signatures used in previous works. In our results, the similarity-learning model was able to verify users with the same accuracy for both signature types. This demonstrates that Motion Passwords, even when applying only the motion-based verification layer, achieve reliability comparable to previous methods. This highlights the potential for Motion Passwords to become even more reliable with the addition of knowledge-based and handwriting style verification layers. Furthermore, we present a proof-of-concept Unity application demonstrating the registration and verification process with our pretrained similarity-learning model. We publish our code, the Motion Password dataset, the pretrained model, and our Unity prototype on https://github.com/cschell/MoPs",Biometrics; Extended Reality; Authentication; Verification,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Out-Of-Virtual-Body Experiences: Virtual Disembodiment Effects on Time Perception in VR,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel experiment investigating the relationship between virtual disembodiment and time perception in Virtual Reality (VR). Recent work demonstrated that the absence of a virtual body in a VR application changes the perception of time. However, the effects of simulating an out-of-body experience (OBE) in VR on time perception are still unclear. We designed an experiment with two types of virtual disembodiment techniques based on viewpoint gradual transition: a virtual body’s behind view and facing view transitions. We investigated their effects on forty-four participants in an interactive scenario where a lamp was repeatedly activated and time intervals were estimated. Our results show that, while both techniques elicited a significant virtual disembodiment perception, time duration estimations in the minute range were only shorter in the facing view compared to the eye view condition. We believe that reducing agency in the facing view is a key factor in the time perception alteration. This provides first steps towards a novel approach to manipulating time perception in VR, with potential applications for mental health treatments such as schizophrenia or depression and for improving our understanding of the relation between body, virtual body, and time.",Virtual Reality; Plausibility; Presence; Avatar; Embodiment; Time Perception; Disembodiment; Virtual Body,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Some Times Fly: The Effects of Engagement and Environmental Dynamics on Time Perception in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"An hour spent with friends seems shorter than an hour waiting for a medical appointment. Many physiological and psychological factors, such as body temperature and emotions, have been shown to correlate with our subjective perception of time. Experiencing virtual reality (VR) has been observed to make users significantly underestimate the duration. This paper explores the effect of virtual environment characteristics on time perception, focusing on two key parameters: user engagement and environmental dynamics. We found that increased presence and interaction with the environment significantly decreased the users’ estimation of the VR experience duration. Furthermore, while a dynamic environment lacks significance in shifting perception toward one specific direction, that is, underestimation or overestimation of the durations, it significantly distorts perceived temporal length. Exploiting these two factors’ influence smartly constitutes a powerful tool in designing intelligent and adaptive virtual environments that can reduce stress, alleviate boredom, and improve well-being by adjusting the pace at which we experience the passage of time.",Virtual Reality; User Engagement; Time Perception; Environmental Dynamics,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Enhancing VR Sketching with a Dynamic Shape Display,VRST - Virtual Reality Software and Technology,A,"Sketching on virtual objects in Virtual Reality (VR) can be challenging due to the lack of a physical surface that constrains the movement and provides haptic feedback for contact and movement. While using a flat physical drawing surface has been proposed, it creates a significant discrepancy between the physical and virtual surfaces when sketching on non-planar virtual objects. We propose using a dynamic shape display that physically mimics the shape of a virtual surface, allowing users to sketch on a virtual surface as if they are sketching on a physical object’s surface. We demonstrate this using VRScroll, a shape-changing device that features seven independently controlled flaps to imitate the shape of a virtual surface automatically. Our user study showed that participants exhibited higher precision when tracing simple shapes with the dynamic shape display and produced clearer sketches. We also provided several design implications for dynamic shape displays aimed at enabling precise sketching in VR.",virtual reality; dynamic shape display; on-surface interactions,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Simulating Object Weight in Virtual Reality: The Role of Absolute Mass and Weight Distributions,VRST - Virtual Reality Software and Technology,A,"Weight interfaces enable users of Virtual Reality (VR) to perceive the weight of virtual objects, significantly enhancing realism and enjoyment. While research on these systems primarily focused on their implementation, little attention has been given to determining the weight to be rendered by them: As the perceived weight of objects is influenced not only by their absolute mass, but also by their weight distribution and prior expectations, it is currently unknown which simulated mass provides the most realistic representation of a given object. We conducted a study, in which 30 participants chose the best fitting weight for a virtual object in 54 experimental trials. Across these trials, we systematically varied the virtual objects’ visual mass (three levels), their weight distribution (six levels), and the position of the physical mass on the grip (three levels). Our Bayesian analysis suggests that the visual weight distribution of objects does not affect which absolute physical mass best represents them, whereas the position of the provided physical mass does. Additionally, participants overweighted virtual objects with lower visual mass while underweighting objects with higher visual mass. We discuss how these findings can be leveraged by designers of weight interfaces and VR experiences to optimize realism.",virtual reality; weight perception; multisensory integration; weight simulation; weight interfaces,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Enriching Industrial Training Experience in Virtual Reality with Pseudo-Haptics and Vibrotactile Stimulation,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) technology facilitates effective, flexible, and safe industrial training for novice technicians when on-site training is not feasible. However, previous research has shown that training in VR may be less successful than traditional learning approaches in real-world settings, and haptic interaction may be the key to improving virtual training. In this study, we integrated pseudo-haptic feedback from motion delay with vibrotactile stimulation to enhance the sense of presence, enjoyment, and the perception of physical properties in VR, which may be crucial for achieving faithful simulations. The impact of combined haptic support was assessed in a complex industrial training procedure completing a variety of tasks such as component assembly and cleaning. The results indicate that vibrotactile cues are beneficial for presence and enjoyment, whereas pseudo-haptic illusions effectively enable kinesthetic sensations. Furthermore, multimodal haptic feedback that mixed the two yielded the most advantageous outcomes. Our findings highlight the potential of the pseudo-haptic and vibrotactile fusion in industrial training scenarios, presenting practical implications of the state-of-the-art haptic technologies for virtual learning.",Haptics; Virtual reality; User study; Multimodal interaction; Industrial training,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Investigating the Impact of Odors and Visual Congruence on Motion Sickness in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Motion sickness is a prevalent side effect of exposure to virtual reality (VR). Previous work found that pleasant odors can be effective in alleviating symptoms of motion sickness such as nausea. However, it is unknown whether pleasant odors that do not match the anticipated scent of the virtual environment are also effective as they could, in turn, amplify symptoms such as disorientation. Therefore, we conducted a study with 24 participants experiencing a pleasant odor (rose) and an unpleasant odor (garlic) while being immersed in a virtual environment involving either virtual roses or garlic. We found that participants had lower motion sickness when experiencing the rose odor, however, only in the rose environment. Accordingly, we also showed that the sense of disorientation was lower for the rose odor, however, only while being immersed in the rose environment. Results indicate that whether pleasant odors are effective in alleviating motion sickness symptoms depends on the visual appearance of the virtual environment. We discuss possible explanations for such effects to occur. Our work contributes to the goal of mitigating visually induced motion sickness in VR.",virtual reality; olfaction; motion sickness; odor; visually induced motion sickness,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Generative Terrain Authoring with Mid-air Hand Sketching in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Terrain generation and authoring in Virtual Reality (VR) offers unique benefits, including 360-degree views, improved spatial perception, immersive and intuitive design experience and natural input modalities. Yet even in VR it can be challenging to integrate natural input modalities, preserve artistic controls and lower the effort of landscape prototyping. To tackle these challenges, we present our VR-based terrain generation and authoring system, which utilizes hand tracking and a generative model to allow users to quickly prototype natural landscapes, such as mountains, mesas, canyons and volcanoes. Via positional hand tracking and hand gesture detection, users can use their hands to draw mid-air strokes to indicate desired shapes for the landscapes. A Conditional Generative Adversarial Network trained by using real-world terrains and their height maps then helps to generate a realistic landscape which combines features of training data and the mid-air strokes. In addition, users can use their hands to further manipulate their mid-air strokes to edit the landscapes. In this paper, we explore this design space and present various scenarios of terrain generation. Additionally, we evaluate our system across a diverse user base that varies in VR experience and professional background. The study results indicate that our system is feasible, user-friendly and capable of fast prototyping.",Virtual Reality; Generative Terrain Authoring; Hand Gesture Control; Hand Sketching,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,How Different Is the Perception of Vibrotactile Texture Roughness in Augmented versus Virtual Reality?,VRST - Virtual Reality Software and Technology,A,"Wearable haptic devices can modify the haptic perception of an object touched directly by the finger in a portable and unobtrusive way. In this paper, we investigate whether such wearable haptic augmentations are perceived differently in Augmented Reality (AR) vs. Virtual Reality (VR) and when touching with a virtual hand instead of one’s own hand. We first designed a system for real-time rendering of vibrotactile virtual textures without constraints on hand movements, integrated with an immersive visual AR/VR headset. We then conducted a psychophysical study with 20 participants to evaluate the haptic perception of virtual roughness textures on a real surface touched directly with the finger (1) without visual augmentation, (2) with a realistic virtual hand rendered in AR, and (3) with the same virtual hand in VR. On average, participants overestimated the roughness of haptic textures when touching with their real hand alone and underestimated it when touching with a virtual hand in AR, with VR in between. Exploration behaviour was also slower in VR than with real hand alone, although subjective evaluation of the texture was not affected. We discuss how the perceived visual delay of the virtual hand may produce this effect.",Augmented Reality; Virtual Reality; Haptic Perception; Psychophysical Study; Roughness Textures; Virtual Hands; Wearable Haptics,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,TeenWorlds: Supporting Emotional Expression for Teenagers with their Parents and Peers through a Collaborative VR Experience,VRST - Virtual Reality Software and Technology,A,"Adolescence is a period of growth and exploration, marked by influential relationships with peers and parents. These relationships are essential for teenagers’ well-being, highlighting the need to support their interpersonal interactions. Emotional expression is key in resolving conflicts that can frequently arise. This paper investigates the potential of TeenWorlds, a Virtual Reality (VR) application, to facilitate emotional expression and shared understanding among teenagers and their peers and parents. In our study, teenagers, accompanied by either a peer or a parent (total n=42), used TeenWorlds to visually represent their emotions during a shared conflict, discuss them, and collaborate on a joint VR drawing. Our findings indicate that TeenWorlds can foster communication, reflection, and strengthen interpersonal relationships. However, notable differences were observed in interactions with peers versus parents. We contribute insights into designing VR systems that support reflective experiences and meaningful family interactions, ultimately enhancing the well-being of adolescents, parents, and families.",Virtual Reality; reflection; collaboration; parents; adolescent; family; youth; emotional expression; teenager; CCI,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,HistoLab VR: A User Elicitation Study Exploring the Potential of Virtual Reality Game-based Learning for Hazard Awareness,VRST - Virtual Reality Software and Technology,A,"Occupational medicine is a vital field for workplace safety and health but often encounters challenges in engaging students and effectively communicating subtle yet critical workplace hazards. To tackle these issues, we developed HistoLab VR, a Virtual Reality (VR) game that immerses participants in a histology lab environment based on real-world practice. Our comprehensive user study with 17 students and experts assessed the game’s impact on hazard awareness, interest in occupational medicine, and user experience through quantitative and qualitative measures. Our findings show that HistoLab VR not just immersed participants in a relatable histology lab worker experience but that it effectively raised awareness about subtle hazards and conveyed the inherent stress of the job. We discuss our results and highlight the potential of VR as a valuable educational tool for occupational medicine training.",education; workplace; anxiety; ergonomics; hazard awareness; histology laboratory; occupational medicine; serious games.,Title_Abstract,True,Duplicate
Scopus,conferencePaper,2024,Game-Based Motivation: Enhancing Learning with Achievements in a Customizable Virtual Reality Environment,VRST - Virtual Reality Software and Technology,A,"Digital learning experiences that promote interactive learning and engagement are becoming increasingly relevant. Educational games can be used to create an engaging learning atmosphere that allows knowledge acquisition through hands-on activities. Combining it with virtual reality (VR) allows users to interact with virtual environments, leading to a highly immersive learning experience. In this study, we explore how game achievements impact motivation and learning in a customizable VR learning environment. Using an A/B test involving 50 students, we utilized an interactive wave simulation to assess motivation, engagement, and the overall learning experience. Data collection involved standardized questionnaires, along with tracking interaction time and interactions within the virtual environment. The findings revealed that users who earned game achievements to unlock customization features felt significantly more accomplished when they mastered challenges and obtained all achievements. However, it was observed that adding achievements could also create pressure on students, leading to feelings of embarrassment when facing task failures. While achievements have the potential to enhance engagement and motivation, their excessive use may lead to distractions, anxiety, and reduced overall engagement. It shows that is crucial to find a good balance in employing game achievements within educational environments to ensure they contribute positively to the learning experience without causing undue stress or deterring learners.",virtual reality; STEM education; interactive simulations; immersive learning; customizable learning,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Hands or Controllers? How Input Devices and Audio Impact Collaborative Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Advancing virtual reality technologies are enabling real-time virtual-face to virtual-face communication. Hand tracking systems that are integrated into Head-Mounted Displays (HMD) enable users to directly interact with their environments and with each other using their hands as opposed to using controllers. Due to the novelties of these technologies our understanding of how they impact our interactions is limited. In this paper, we investigate the consequences of using different interaction control systems, hand tracking or controllers, when interacting with others in a virtual environment. We design and implement NASA’s Survival on the Moon teamwork evaluation exercise in virtual reality (VR) and test for effects with and without allowing verbal communication. We evaluate social presence, perceived comprehension, team cohesion, group synergy, task workload, as well as task performance and duration. Our findings reveal that audio communication significantly enhances social presence, perceived comprehension, and team cohesion, but it also increases effort workload and negatively impacts group synergy. The choice of interaction control systems has limited impact on various aspects of virtual collaboration in this scenario, although participants using hand tracking reported lower effort workload, while participants using controllers reported lower mental workload in the absence of audio.",collaboration; Communication; avatars; gestures,Title_Abstract,True,Duplicate
Scopus,conferencePaper,2024,Exploring User Placement for VR Remote Collaboration in a Constrained Passenger Space,VRST - Virtual Reality Software and Technology,A,"Extended Reality (XR) offers the potential to transform the passenger experience by allowing users to inhabit varied virtual spaces for entertainment, work or social interaction, whilst escaping the constrained transit environment. XR allows remote collaborators to feel like they are together and enables them to perform complex 3D tasks. However, the social and physical constraints of the passenger space pose unique challenges to productive and socially acceptable collaboration. Using a collaborative VR puzzle task, we examined the effects of five different f-formations of collaborator placement and orientation in an interactive workspace on social presence, task workload, and implications for social acceptability. Our quantitative and qualitative results showed that face-to-face formations were preferred for tasks with a high need for verbal communication but may lead to social collisions, such as inadvertently staring at a neighbouring passenger, or physical intrusions, such as gesturing in another passenger’s personal space. More restrictive f-formations, however, were preferred for passenger use as they caused fewer intrusions on other passengers’ visual and physical space.",Mixed Reality; Virtual Reality; Collaboration; Social Acceptability; Constrained Spaces; Passenger Spaces,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Stand Alone or Stay Together: An In-situ Experiment of Mixed-Reality Applications in Embryonic Anatomy Education,VRST - Virtual Reality Software and Technology,A,"Where traditional media and methods reach their limits in anatomy education, mixed-reality (MR) environments can provide effective learning support because of their high interactivity and spatial visualization capabilities. However, the underlying design and pedagogical requirements are as diverse as the technologies themselves. This paper examines the effectiveness of individual- and collaborative learning environments for anatomy education, using embryonic heart development as an example. Both applications deliver the same content using identical visualizations and hardware but differ in interactivity and pedagogical approach. The environments were evaluated in a user study with medical students (n = 90) during their examination phase, assessing usability, user experience, social interaction/co-presence, cognitive load, and personal preference. Additionally, we conducted a knowledge test before and after an MR learning session to determine educational effects compared to a conventional anatomy seminar. Results indicate that the individual learning environment was generally preferred. However, no significant difference in learning effectiveness could be shown between the conventional approach and the MR applications. This suggests that both can effectively complement traditional seminars despite their different natures. Our study contributes to understanding how different MR settings could be tailored for anatomical education.",Mixed Reality; Collaborative Learning; Medical Education; Immersive Learning Environments; Individual Adaptive Learning,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Contextual Matching Between Learning and Testing Within VR Does Not Always Enhance Memory Retrieval,VRST - Virtual Reality Software and Technology,A,"Episodic memory is influenced by environmental contexts, such as location and auditory stimuli. The most well-known effect is the reinstatement effect, which refers to the phenomenon where contextual matching between learning and testing enhances memory retrieval. Previous studies have investigated whether the reinstatement effect can be observed within immersive virtual environments. However, only a limited number of studies have reported a significant reinstatement effect using virtual reality, while most have failed to detect it. In this study, we re-examined the reinstatement effect using 360-degree video-based virtual environments. Specifically, we carefully selected virtual environments to elicit different emotional responses, which has been suggested as a key factor in inducing a robust reinstatement effect in the physical world. Surprisingly, we found a significant reversed reinstatement effect with a large effect size. This counter-intuitive result suggests that contextual congruence does not necessarily enhance memory and may even interfere with it. This outcome may be explained by the retrieval-induced forgetting phenomenon, but further exploration is needed. This finding is particularly important for virtual reality-based, educational applications and highlights the need for a deeper understanding of the complex interactions between memory and contextual cues within virtual environments.",virtual reality; 360-degree video; environmental context-dependent memory; reinstatement; retrieval-induced forgetting,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Toward Facilitating Search in VR With the Assistance of Vision Large Language Models,VRST - Virtual Reality Software and Technology,A,"While search is a common need in Virtual Reality (VR) applications, current approaches are cumbersome, often requiring users to type on a mid-air keyboard using controllers in VR or remove VR equipment to search on a computer. We first conducted a literature review and a formative study, identifying six common search needs: knowing about one object, knowing about the object’s partial details, knowing objects with environmental context, knowing about interactions with objects, and finding objects within field of view (FOV) and out of FOV in the VR scene. Informed by these needs, we designed technology probes that leveraged recent advances in Vision Large Language Models and conducted a probe-based study with users to elicit feedback. Based on the findings, we derived design principles for VR designers and developers to consider when designing a user-friendly search interface in VR. While prior work about VR search tended to address specific aspects of search, our work contributes design considerations aimed at enhancing the ease of search in VR and potential future directions.",participatory design; Virtual reality; vision large language model; VR search,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Evaluating Gaze Interactions within AR for Nonspeaking Autistic Users,VRST - Virtual Reality Software and Technology,A,"Nonspeaking autistic individuals often face significant inclusion barriers in various aspects of life, mainly due to a lack of effective communication means. Specialized computer software, particularly delivered via Augmented Reality (AR), offers a promising and accessible way to improve their ability to engage with the world. While research has explored near-hand interactions within AR for this population, gaze-based interactions remain unexamined. Given the fine motor skill requirements and potential for fatigue associated with near-hand interactions, there is a pressing need to investigate the potential of gaze interactions as a more accessible option. This paper presents a study investigating the feasibility of eye gaze interactions within an AR environment for nonspeaking autistic individuals. We utilized the HoloLens 2 to create an eye gaze-based interactive system, enabling users to select targets either by fixating their gaze for a fixed period or by gazing at a target and triggering selection with a physical button (referred to as a ‘clicker’). We developed a system called HoloGaze that allows a caregiver to join an AR session to train an autistic individual in gaze-based interactions as appropriate. Using HoloGaze, we conducted a study involving 14 nonspeaking autistic participants. The study had several phases, including tolerance testing, calibration, gaze training, and interacting with a complex interface: a virtual letterboard. All but one participant were able to wear the device and complete the system’s default eye calibration; 10 participants completed all training phases that required them to select targets using gaze only or gaze-click. Interestingly, the 7 users who chose to continue to the testing phase with gaze-click were much more successful than those who chose to continue with gaze alone. We also report on challenges and improvements needed for future gaze-based interactive AR systems for this population. Our findings pave the way for new opportunities for specialized AR solutions tailored to the needs of this under-served and under-researched population.",augmented reality; assistive technology; eye tracking; nonspeaking autistic people,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Exploring Immersive Debriefing in Virtual Reality Training: A Comparative Study,VRST - Virtual Reality Software and Technology,A,"Simulation and debriefing are two essential and inseparable phases of virtual reality training. With the widespread adoption of these training tools, it is crucial to define the best pedagogical approaches for trainers and learners to maximize their effectiveness. However, despite their educational benefits, virtual reality-specific debriefing methods remain underexplored in research. This article proposes an architecture and interface for an all-in-one immersive debriefing module that is adaptable to different types of training, including a complete system for recording, replaying, and redoing actions. A study with 36 participants compared this immersive debriefing system with traditional discussion-based and video-supported debriefing. Participants were divided into three groups to evaluate the effectiveness of each method. The results showed no significant differences between these debriefing methods across several criteria, such as satisfaction, motivation, or information retention. Immersive debriefing is as usable and retentive as traditional or video debriefing in this context. The next step will be to evaluate the redo system in other training courses involving more dynamic scenarios.",Virtual Reality; Simulation; Debriefing; Immersive Learning; Trainer,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,"A Critical Review of Virtual and Extended Reality Immersive Police Training: Application Areas, Benefits &amp; Vulnerabilities",VRST - Virtual Reality Software and Technology,A,"Virtual and Extended Reality (VR/XR) headsets have promised to enhance police training through the delivery of immersive simulations able to be conducted anywhere, anytime. However, little consideration has been given to reviewing the evidenced benefits and potential issues posed by XR police training. In this paper, we summarise the evidenced usage and benefits of XR police training through a formative targeted literature review (n=41 publications). We then reflect on the prospective technical, security, social and legal issues posed by XR police training, identifying four areas where issues or vulnerabilities exist: training content, trainees and trainers, systems and devices, and state and institutional stakeholders. We highlight significant concerns around e.g. the validity of training; the psychological impact and risks of trauma; the safety and privacy risks posed to trainees and trainers; and the risks to policing institutions. We aim to encourage end-user communities (e.g. police forces) to more openly reflect on the risks of immersive training, so we can ultimately move towards transparent, validated, trusted training that is evidenced to improve policing outcomes.",Virtual Reality; Extended Reality; Police Training,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,The Impact of Task-Responsibility on User Experience and Behaviour under Asymmetric Knowledge Conditions,VRST - Virtual Reality Software and Technology,A,"Virtual Reality presents a promising tool for knowledge transfer, allowing users to learn in different environments and with the help of three-dimensional visualizations. At the same time, having to learn new ways of interacting with their environment can present a significant hurdle for novice users. When users enter a virtual space to receive knowledge from a more experienced person, the question arises as to whether they benefit from learning VR-specific interaction techniques instead of letting the expert take over some or all interactions. Based on related work about expert-novice interaction in virtual spaces, this paper presents a user study comparing three different distributions of interaction responsibilities between participants and an expert user. The Role-Based interaction mode gives the expert the full interaction responsibility. The Shared interaction mode gives both users the same interaction capabilities, allowing them to share the responsibility of interacting with the virtual space. Finally, the Parallel interaction mode gives participants full interaction responsibility, while the expert can provide guidance through oral communication and visual demonstration. Our results indicate that assuming interaction responsibility led to higher task loads but also increased the participant’s engagement and feeling of presence. For most participants, sharing interaction responsibilities with the expert represented the best trade-off between engagement and challenge. While we did not measure a significant increase in learning success, participant comments indicated that they also paid more attention to details when assuming more interaction responsibility.",Virtual Reality; Collaboration; 3D User Interfaces; Head-Mounted Display; Instruction; Knowledge-Transfer,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Evaluating the effects of Situated and Embedded Visualisation in Augmented Reality Guidance for Isolated Medical Assistance,VRST - Virtual Reality Software and Technology,A,"One huge advantage of Augmented Reality (AR) is its numerous possibilities of displaying information in the physical world, especially when applying Situated Analytics (SitA). AR devices and their respective interaction techniques allow for supplementary guidance to assist an operator carrying out complex procedures such as medical diagnosis and surgery, for instance. Their usage promotes user autonomy by presenting relevant information when the operator may not necessarily possess expert knowledge of every procedure and may also not have access to external help such as in a remote or isolated situation (e.g., International Space Station, middle of an ocean, desert). In this paper, we propose a comparison of two different forms of AR visualisation: An embedded visualisation and a situated projected visualisation, with the aim to assist operators with the most appropriate visualisation format when carrying out procedures (medical in our case). To evaluate these forms of visualisation, we carried out an experiment involving 23 participants possessing latent/novice medical knowledge. These participant profiles were representative of operators who are medically trained yet do not apply their knowledge every day (e.g., an astronaut in orbit or a sailor out at sea). We discuss our findings which include the advantages of embedded visualised information in terms of precision compared to situated projected information with the accompanying limitations in addition to future improvements to our proposition. We conclude with the prospects of our work, notably the continuation and possibility of evaluating our proposition in a less controlled and real context in collaboration with our national space agency.",Augmented Reality; Immersive Analytics; Situated Analytics; AR Guidance; Isolated Situation; Medical Assistance; Procedure Execution; Situated Visualisation,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,An Evaluation of Targeting Methods in Spatial Computing Interfaces with Visual Distractions,VRST - Virtual Reality Software and Technology,A,"In modern spatial computing devices, users are confronted with diverse methods for object selection, including eye gaze (cf. Apple Vision Pro), hand gestures (cf. Microsoft HoloLens 2), touch gestures (cf. Google Glass Enterprise Edition 2), and external controllers (cf. Magic Leap 2). Although there are a plethora of empirical studies on which selection techniques perform best, a common limiting factor stems from the partly artificial setups. These typically exclude practical influences such as visual distraction. In this paper, we present a user study comparing two hand-based and two gaze-based state-of-the-art selection methods, using the HoloLens 2. We extended a traditional Fitts’ law-inspired study design by incorporating a visual task that simulates changes in the user interface after a successful selection. Without a visual task, gaze-based techniques were on average faster than hand-based techniques. This performance gain was eliminated (for head gaze) or even reversed (for eye gaze) when the visual task was active. These findings underscore the value of continued practice-oriented research of targeting methods in virtual environments.",Augmented Reality; Interaction Techniques; Selection,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Evaluation of AR Pattern Guidance Methods for a Surface Cleaning Task,VRST - Virtual Reality Software and Technology,A,"Cleanroom cleaning is a surface coverage task where the pattern should be followed correctly, and the entire surface should be covered. We investigate the efficacy of augmented reality (AR) by implementing various pattern guidance designs to enhance a cleanroom cleaning task. We developed an AR guidance system for cleaning procedures and evaluated four distinct pattern guidance methods: (1) breadcrumbs, (2) examples, (3) middle lines, and (4) outlines. We vary the instructions on the entire surface or as a single step. To measure performance, accuracy, and user satisfaction associated with each guidance method, we conducted a large-scale (n=864) between-subjects study. Our findings indicate that single step instructions proved to be more intuitive and efficient than full instructions, especially for the breadcrumbs. We also discussed the implications of our results for the development of AR applications for surface coverage and pattern optimization.",Augmented Reality; Motion control.; Pattern guidance,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Editing Immersive Recordings: An Elicitation Study,VRST - Virtual Reality Software and Technology,A,"Immersive recordings capture virtual reality interactions and are used in various contexts such as education and entertainment. However, there has been only limited research on requirements and techniques for editing such recordings. We interviewed expert editors of video recordings to understand their workflows, familiarised them with immersive recordings, and asked them about what editing challenges and capabilities they can envision for immersive recordings. The experts identified several functionalities they considered relevant for editing, including viewer placement, control over the viewer’s size, support for live and asynchronous collaboration, and different transition types.",Virtual Reality; Immersive Editing; Immersive Recordings,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Towards an Avatar Customization System for Semi-realistic Ethnically-diverse Virtual Reality Avatars,VRST - Virtual Reality Software and Technology,A,"Due to the Proteus effect, in which people modify their behaviour based on their avatar, participant avatar representation is an important factor in virtual reality (VR) studies. We develop an open source prototype avatar customization system that enables quick customization of semi-realistic, ethnically-diverse avatars. The prototype provides options for customizing body and face shape, hairstyle, glasses, religious clothing, and skin, eye, and hair colour. The prototype generates avatar assets that are fully rigged and textured for incorporation into VR study code, and it serves as a step towards designing more inclusive VR research studies.",avatar customization; Proteus effect; virtual avatars,Title_Abstract,True,Duplicate
Scopus,conferencePaper,2024,"Comparing Tracking Accuracy in Standalone MR-HMDs: Apple Vision Pro, Hololens 2, Meta Quest 3, and Pico 4 Pro",VRST - Virtual Reality Software and Technology,A,"Modern Mixed Reality Head-Mounted Displays (MR-HMDs) can track user movements across large spaces without external markers. This study evaluates the tracking accuracy and the loop closure capabilities of four commercially available MR-HMDs across four distinct scenarios. We found consistent tracking performance in well-lit and expansive environments for all devices. Tracking accuracy remained stable even in outdoor nighttime conditions. Furthermore, most HMDs demonstrated effective error correction during loop closure, with errors in non-loop scenarios consistently exceeding those in loop scenarios.",Mixed Reality; Tracking Accuracy; Visual Inertial Odometry,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Exploring Alternative Text Input Modalities in Virtual Reality: A Comparative Study,VRST - Virtual Reality Software and Technology,A,"Text input in Virtual Reality (VR) is crucial for communication, search, and productivity. We compared four keyboard designs for VR text entry, leveraging the flexibility and the tracking options of a 3D environment. We used the Dvorak layout to control for experience differences. The designs were: (a) a floating keyboard with touch input, (b) a keyboard attached on the back of the hand with touch input, (c) a floating keyboard with eye tracking and pinch input, and (d) a keyboard laid out over a rolling shape with touch input. Designs (b), (c), and (d) can move in 3D space, while design (a) is static. Design (d) had similar efficiency to design (a) but with better usability and lower Physical Demand. Design (b) led to higher Physical Demand, Effort, and Frustration. Design (c) had lower Physical Demand but higher Mental Demand, Effort, and error rates. Typing speeds averaged 6.51 WPM (1.24% error rate) for (a), 5.56 WPM (3.82% error rate) for (b), 5.33 WPM (1.43% error rate) for (c), and 6.70 WPM (1.64% error rate) for (d).",Virtual Reality; Text input; Interface design,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,A Study on the Effectiveness of Augmented Reality Signal-Integrated Camera Monitor Systems for Safe Lane Changing,VRST - Virtual Reality Software and Technology,A,"This study investigates the effectiveness of augmented reality (AR) signals in camera monitor systems (CMS) for enhancing safety during lane changes. Seventy participants used seven side mirror conditions, including traditional side mirrors and six CMS conditions with and without AR signals. Results showed that CMS with AR signals significantly reduced the number of collisions and reaction time compared to CMS without AR signals.",Augmented Reality; Virtual Reality; Camera Monitor Systems; Driving Safety; Driving Simulation,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Exploring Influencers' and Users' Experiences in Douyin's Virtual Reality Live-Streaming,VRST - Virtual Reality Software and Technology,A,"VR live-streaming has become an emerging part on Douyin. This study aims to explore the technical modes, content strategies, user experiences in Douyin‘s VR live-streaming. Through interviews and focus groups, we found that VR technology is recognized by influencers and has become an essential part of their creative practice. For some influencers, VR technology is a key factor in enhancing audience engagement and immersive experiences, although technical literacy barriers may arise when setting up VR scenes. We also provide dimensions for improving and developing user adoption and experience of VR technology in social media environments.",Virtual Reality; Douyin; Influencers; Live-Streaming; Users,Title_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Digital Eyes: Social Implications of XR EyeSight,VRST - Virtual Reality Software and Technology,A,"The EyeSight feature, introduced with the new Apple Vision Pro XR headset, promises to revolutionize user interaction by simulating real human eye expressions on a digital display. This feature could enhance XR devices’ social acceptability and social presence when communicating with others outside the XR experience. In this pilot study, we explore the implications of the EyeSight feature by examining social acceptability, social presence, emotional responses, and technology acceptance. Eight participants engaged in conversational tasks in three conditions to contrast experiencing the Apple Vision Pro with EyeSight, the Meta Quest 3 as a reference XR headset, and a face-to-face setting. Our preliminary findings indicate that while the EyeSight feature improves perceptions of social presence and acceptability compared to the reference headsets, it does not match the social connectivity of direct human interactions.",Extended Reality; Social Acceptability; Social Presence,Keywords,True,Duplicate
Scopus,conferencePaper,2024,SOLDAR: Supporting Low-Volume PCB Prototyping Using Collaborative Robots and Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Printed circuit boards (PCBs) are fundamental to modern electronics and are present in almost every electronic device. However, despite their ubiquity, current PCB assembly methods can be time-consuming and lack flexibility for one-off designs. This poster investigates how low-volume PCB prototyping can be enhanced by integrating collaborative robots (cobots) and Augmented Reality (AR). Specifically, we introduce SOLDAR, a system that facilitates the soldering of electronic through-hole components on PCBs. By using a cobot for optimal PCB positioning and AR glasses for step-by-step guidance, SOLDAR aims to streamline the assembly process. The expected outcomes are increased efficiency, reduced assembly time, and greater flexibility for low-volume PCB prototyping designs. To validate these hypotheses, user experiments are necessary.",Augmented Reality; Prototyping; Collaborative Robots,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Earscape: A VR Auditory Educational Escape Room,VRST - Virtual Reality Software and Technology,A,"According to the World Health Organisation’s World Report on Hearing, there is a strong need to provide better education on hearing loss from a young age. This project aims to educate the Danish young population (13 to 17-year-olds) about the hearing sense through an educational multiplayer virtual reality-based escape room with the benefits of educational escape rooms. In collaboration with relevant audiologist stakeholders, this project follows an iterative process of design, implementation, and evaluation of the application. The developed solution will undergo several user studies in the following months.",Virtual Reality; Educational Escape Room; Hearing Loss; Multiplayer,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Haptic and Auditory Feedback on Immersive Media in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In Virtual Reality (VR), visual and auditory sensations are effectively leveraged to create immersive experiences. However, touch is significantly underutilized in immersive media. We enhance the VR image viewing experience by integrating haptic and auditory feedback into 3D environments constructed from immersive media. We address the challenges of utilizing depth maps from various image formats to create intractable environments. The VR experience is enhanced using vibrohaptic feedback and audio cues triggered by controller collisions with haptic materials.",Virtual Reality; Haptic Feedback; Auditory Feedback; Immersive Media,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,From Ground to Sky: Flying-motion Generation via Motion Dataset Adaptation,VRST - Virtual Reality Software and Technology,A,"We conducted a study utilizing a lightweight generative network to create flying motions. The existing datasets used for training did not include any data on flying motions. Therefore, we selected certain classes from the existing motion datasets and transformed these motions to resemble flying actions. By training the existing generative network with the modified dataset, we were able to generate motions that closely resemble flying. The results of this study demonstrate the potential for generating flying motions. The generation of flying motions for human avatars is expected to be a critical technology not only in 3D animation or game industry but also in virtual environments, enabling users to experience various activities through their avatars.",Virtual Reality; dataset; Avatar motion,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Walking of uphill slopes in immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"We explore three visual manipulation techniques aiming to create a realistic feeling of walking an uphill slope while in reality being on flat ground. The techniques are based on real physical visual perception and consist of modification of height and display of virtual shoes, modification of speed, and modification of view pitch. Quantitative and qualitative evaluation indicated that modification of speed, and pitch contributed to user discomfort, as well as a general increase in discomfort correlating with the slope’s increasing inclination. However, height manipulation was well received and can be used in future projects for more realistic landscape.",Virtual reality; walking; human factors,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Enhanced Wayfinding Insights Through VR and Eye-Tracking Analysis,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel method for evaluating wayfinding within a public building to provide meaningful insights for stakeholders. Our approach features unique methods for both data collection and evaluation, with a holistic digital capture of the entire virtual environment experienced by participants, maintained in an interactive format for in-depth analysis. We also captured and output data in point cloud formats, raw data text files, and task-specific metrics, which support interactive replays of participants’ experiences. We developed algorithms to extract meaningful insights from the raw data based on assumptions about wayfinding characteristics. The contribution is a flexible framework that can be easily adapted for future projects with adjustable variables to suit specific applications.",virtual reality; point cloud; wayfinding; gaze tracking; signage; unreal engine,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Pipelining Processors for Decomposing Character Animation,VRST - Virtual Reality Software and Technology,A,"This paper presents an openly available implementation of a modular pipeline architecture for character animation. It effectively decomposes frequently necessary processing steps into dedicated character processors, such as copying data from various motion sources, applying inverse kinematics, or scaling the character. Processors can easily be parameterized, extended (e.g., with AI), and freely arranged or even duplicated in any order necessary, greatly reducing side effects and fostering fine-tuning, maintenance, and reusability of the complex interplay of real-time animation steps.",Virtual Reality; Avatars; Embodiment; Agents; Extended Reality.; Humanoid Characters; Open-Source; Virtual Humans,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Study of inpainting based on generative AI for noise-canceling HMDs,VRST - Virtual Reality Software and Technology,A,"Entering a small space such as an elevator or a crowded train with a stranger can cause discomfort and suffocation. This is because the stranger is invading the individual’s personal space. However, it is difficult to maintain an appropriate interpersonal distance from others at all times in various situations. Therefore, a noise-canceling HMD [2][3] that uses AR to change the size of the person in the field of vision has been proposed as a means of reducing noise such as discomfort caused by inappropriate interpersonal distance. In this paper, we propose an improvement method using generative AI for background completion in noise-canceling HMDs.",Augmented Reality; Noise-canceling HMD,Keywords,True,Duplicate
Scopus,conferencePaper,2024,A Comparison between Vibrotactile Error-correction Feedback on Upper and Lower Body in the VR Snowboard Balancing Task,VRST - Virtual Reality Software and Technology,A,"This study investigated the effect of vibrotactile stimulus location on the balancing task in virtual reality (VR). Using a virtual snowboarding system with wearable haptic devices, we conducted a between-subject user study comparing the effectiveness of two different body locations–upper body (UB; torso vibrations) and lower body (LB; ankle vibrations). The real-time vibrotactile balance-correction feedback was generated by the Center of Pressure (CoP) calculated from the sensor array on insoles. The initial results showed that UB feedback is better than LB to improve users’ balance ability.",virtual reality; balancing; center-of-pressure; Vibrotactile wearables,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,The MASTER XR Platform for Robotics Training in Manufacturing,VRST - Virtual Reality Software and Technology,A,"The MASTER project introduces an open Extended Reality (XR) platform designed to enhance human-robot collaboration and train workers in robotics within manufacturing settings. It includes modules for creating safe workspaces, intuitive robot programming, and user-friendly human-robot interactions (HRI), including eye-tracking technologies. The development of the platform is supported by two open calls targeting technical SMEs and educational institutes to enhance and test its functionalities. By employing the learning-by-doing methodology and integrating effective teaching principles, the MASTER platform aims to provide a comprehensive learning environment, preparing students and professionals for the complexities of flexible and collaborative manufacturing settings.",Robotics; Industry 4.0; Eye Tracking; Extended Reality (XR); Manufacturing; Human-Robot Collaboration; Worker Training,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,VR4UrbanDev: An Immersive Virtual Reality Experience for Energy Data Visualization,VRST - Virtual Reality Software and Technology,A,"In this demonstration paper, we present our interactive virtual reality (VR) experience, which has been designed to facilitate interaction with energy-related information. This experience consists of two main modes: the world in miniature for large-scale and first-person for real-world scale visualizations. Additionally, we presented our approach to potential target groups in interviews. The results of these interviews can help developers for future implementation considering the requirements of each group.",Virtual Reality; Data Visualization; Building Information Modeling,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,UXR-kit: An Ideation Kit and Method for Collaborative and User-Centered Design about Extended Reality systems.,VRST - Virtual Reality Software and Technology,A,"Emerging kits and methods about Extended Reality (XR) systems are mainly centered on the prototyping phase. The ideation phase, which comes before prototyping, is currently still under-explored. In this work, we propose UXR-kit: a toolkit and a method for the co-design of ideas for XR systems. UXR-kit is based on an approach inspired by design studios and generative techniques and highlights the specificities of XR systems. Results from an experimental study suggest that UXR-kit allows the emergence of ideas for XR designs through both World-In-Miniature representations and first-person representations at scale 1:1.",Mixed Reality; Extended Reality; Ideation; Design toolkit,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Cultural Windows: Towards Immersive Journeys into Global Living Spaces,VRST - Virtual Reality Software and Technology,A,"“Cultural Windows” is a research initiative aimed at enhancing cross-cultural understanding through immersive extended reality (XR) experiences. The project deploys AR and VR platforms to allow users to explore diverse living spaces, bridging the gap between preconceived notions and the actual appearance of these spaces. By using 3D scanning to create accurate models of culturally significant objects and integrating them into immersive systems, the project provides insights into the use of immersive technologies in cultural education, promoting engagement with global living designs.",Extended Reality (XR); Cross-Cultural Visualization; Cultural Awareness in Design,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,A Volumetric Video Application to Enhance Museum Experiences,VRST - Virtual Reality Software and Technology,A,"Volumetric video (VV) is an emerging 3D format that allows the integration of real people into XR (extended reality) applications. Recent cost-effective AI-based methods have enabled VV capture using single handheld cameras or mobile phones. This study addresses the quality, integration, and acceptance of AI-based VV content creation in an augmented reality (AR) application designed to enhance museum experiences. The main result reveals that, although the current VV quality is lower than professional standards, users still find significant added value and enjoy its immersive experience.",,Abstract,True,Duplicate
Scopus,conferencePaper,2024,Effectiveness of Adaptive Difficulty Settings on Self-efficacy in VR Exercise,VRST - Virtual Reality Software and Technology,A,"The difficulty is a fundamental factor of the user’s motivation and engagement in some tasks. Dynamic difficulty adjustment (DDA) systems provide users with an optimal level of challenge. Previously, some studies developed a DDA system that can set the task’s difficulty to any level. However, these studies lack the investigation of the influence of the difficulty levels on the psychological aspect. For this purpose, we consider a difficulty setting that consists of stepwise difficulty levels (e.g., hard, normal, and easy) set to adapt to each user’s skill and evaluate it using self-efficacy. In the experiment, we employ a Kendama task in a VR space where the difficulty level can be easily adjusted. The result shows that the difficulty levels in our method can be set according to the user’s skill. Moreover, we experimentally clarify a strong correlation between successful experiences in imagination and the enhancement of self-efficacy in the difficulty setting, which means that adapting difficulty levels to the user’s skill has the potential to enhance self-efficacy effectively.",Virtual reality; User experience; Self-efficacy; Difficulty adjustment,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Investigation of Simulator Sickness in Walking with Multiple Locomotion Technologies in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"With the increasing development of Virtual Reality, locomotion has become an essential component of interaction in VR. Currently, various locomotion technologies have been developed to provide users with a natural walking experience in virtual environments. However, the multiple walking techniques impact users’ walking experience in different ways. Simulator sickness is a common issue in VR experiences. Since different walking methods may influence simulator sickness differently, we conducted a user study to evaluate simulator sickness in walking with three relevant walking methods: real walking, arm-swing, and omnidirectional treadmill, and the results indicated that these three walking methods caused different levels of simulator sickness, and people perceived stronger sickness when they walked on the omnidirectional treadmill.",Virtual Reality; Simulator Sickness; Locomotion Technologies; Natural Walking Techniques,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Exploring an XR Indoor Navigation System for Remote Collaboration,VRST - Virtual Reality Software and Technology,A,"While collaboration in shared extended reality spaces has been extensively explored, larger environments like entire floors or buildings have garnered less attention. To address this gap, spatial navigation and collaboration across realities must be made possible so that users can find each other and foster shared spatial understanding independent from reality. Current developments target either navigation or collaboration but lack the combination. In this poster, we present an extended reality remote collaboration system using an augmented reality (AR) based indoor navigation for on-site and a Building Information Model (BIM) of the physical environment Virtual Reality (VR) system for remote users. We conducted a user study with ten participants (five pairs) to gather initial insights into the system’s usability and preferences for collaborative tools. The results offer initial insights into creating shared spatial understanding across realities. Our work contributes to a collaborative XR navigation system for extensive shared spaces.",AR; VR; indoor navigation; remote; XR collaboration,Abstract,True,Duplicate
Scopus,conferencePaper,2024,Wheel-Based Attachable Footwear for VR: Challenges and Opportunities in Seated Walking-in-Place Locomotion,VRST - Virtual Reality Software and Technology,A,"This poster explores the potential of Cybershoes, a foot-based consumer input device, used with a swivel chair to enable seated walking-in-place (WIP) locomotion in virtual reality (VR). Through a qualitative study with 12 participants, we investigated the effects of Cybershoes on user comfort, presence, motion sickness, and overall experience during various sightseeing tasks. Our findings reveal both opportunities and challenges for Cybershoes as a seated-WIP solution. Participants perceived Cybershoes as more natural for navigation compared to handheld controllers, with most reporting reduced motion sickness. However, challenges included perceived slower movement speed, ergonomic issues, and limited action detection. Our work also highlights Cybershoes’ potential beyond gaming, including applications in exercise, professional training, remote work, and accessibility.",VR; locomotion; shoes; input.; seated walking; virtual travel; wheel,Abstract,True,Duplicate
Scopus,conferencePaper,2024,White Lies in Virtual Reality: Impact on Enjoyment and Fatigue,VRST - Virtual Reality Software and Technology,A,"This study examined the impact of a ""white lie"" designed to boost motivation during virtual reality exercise on enjoyment and mental fatigue. Participants engaged in a ball-throwing or ball-targeting task and were randomly assigned to groups with or without the white lie. Results indicated that both groups experienced similar levels of enjoyment and fatigue, suggesting the white lie had minimal effect on these factors. All participants, regardless of group, reported high levels of enjoyment, with 17 out of 18 indicating they had fun, no significant differences in mental fatigue were found between groups while participants generally favored the white lie. However, the positive experience across all participants highlights the potential of Virtual Reality for promoting exercise engagement.",Virtual Reality; Fatigue; Enjoyment; White Lies,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Dynamic Difficulty Adjustment in Virtual Reality Exergaming to Regulate Exertion Levels via Heart Rate Monitoring,VRST - Virtual Reality Software and Technology,A,"By regulating exertion levels, Dynamic difficulty adjustment (DDA) has the potential to enhance user experience and optimize exercise in Virtual Reality (VR) exergames. This pilot study assesses the effectiveness of adjusting the difficulty of gameplay challenges based on heart rate (HR) data to control the intensity of physical activity in VR exergaming. Observational results from 13 participants indicate that the HR-based DDA more effectively maintained target heart rate zones compared to randomized adjustments. Improved perceived exertion, and increased enjoyment underlines the potential of this approach for VR-based exercise and rehabilitation programs.",heart rate; dynamic difficulty adjustment; virtual reality exergames,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Rendering diffraction Phenomena on rough surfaces in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Wave-optical phenomena, such as diffraction, significantly impact the visual appearance of surfaces. Despite their importance, wave-optical reflection models are rare and computationally expensive. Recently, we presented a real-time model that accounts for diffraction-induced color shifts and speckle. Given that diffraction phenomena are highly dependent on illumination and viewing directions, as well as stereoscopic vision, we developed a VR demo to evaluate the new model. This demo shows the substantial impact of diffraction on the appearance of rough surfaces, particularly in stereoscopic viewing.",Virtual Reality; Modeling; Diffraction; Predictive Rendering,Title_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Supporting Wildfire Evacuation Preparedness through a Virtual Reality Simulation,VRST - Virtual Reality Software and Technology,A,This demo presents a virtual reality simulation of a wildfire evacuation. Players are tasked with going through a home environment and collecting items they believe they would need and want to take if they were under an evacuation notice. The experience is playable on the Meta Quest 2 headset.,Virtual Reality; Training; Evacuation; Wildfires,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Row your boat in VR and solve thinking exercises on the way: The Brain-Row Challenge,VRST - Virtual Reality Software and Technology,A,"In this demo, we showcase Brain-Row Challenge. Brain-Row Challenge is a research prototype for dual-task training in Virtual Reality (VR). Dual-task training combines a mental and a physical task. This training is relevant in neurodegenerative diseases, especially in Parkinson’s disease. The user is rowing with a Concept 2 ergometer over a Nordic lake, must follow a marked route and answers multiple-choice questions by rowing through gates. Steering is done with an inertial measurement unit that is attached to the handlebar. The VR experience can also be compared to a less immersive representation of the rowing course on a TV screen.",Dual Tasking; Ergometers; Excer Game; Medical Application,Abstract,True,Duplicate
Scopus,conferencePaper,2024,ChronoShore: Diegetic Temporal Exploration in a Simulated Virtual Coast Environment,VRST - Virtual Reality Software and Technology,A,"This paper introduces ChronoShore, an immersive virtual reality (VR) experience designed to explore diegetic time manipulation mechanics within a semi-realistic coastal environment. Traditional 2D video scrubbing methods fall short in immersive settings, particularly for understanding time-bound processes such as simulations of geology or biology. ChronoShore addresses this by allowing users to interact with celestial bodies to dynamically control and experience the passage of time, currently showcasing different weather events and atmospheric phenomena.",virtual reality; simulation; Time manipulation,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Hands-On Plant Root System Reconstruction in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"VRoot is an immersive extended reality reconstruction tool for root system architectures from 3D volumetric scans of soil columns. We have conducted a laboratory user study to assess the performance of new users with our software in comparison to established software. We utilize a plant model to derive a synthetic root architecture, providing a baseline for reconstruction. This demo showcases the processes and techniques contributing to exact and efficient manual root architecture reconstruction in Virtual Reality. The extraction task typically is the sparse graph-structure extraction from a 3D magnetic-resonance imaging (MRI) data set. We visualize the RSA directly within the MRI and offer selection-set-based methods of adapting and augmenting the root architecture. This application is in productive use at our partner institute, where it is used to analyze complex root images.",virtual reality; 3D imaging; root reconstruction,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Off-The-Shelf: Exploring 3D Arrangements of See-Through Masks to Switch between Virtual Environments,VRST - Virtual Reality Software and Technology,A,"This demo explores prioritization techniques to arrange see-through masks in virtual reality (VR). The oval masks show live previews of different virtual environments (VEs) and allow for seamless teleportation into a corresponding VE by putting the mask on the face. Each environment includes a mini-game (e.g., basketball and archery) in which the user has to perform a small task. The arrangement of the masks changes depending on a calculated rating, which considers the time since the game was last played and the game score. We envision this system to help users to multitask in VR. For example, to control multiple characters in VR games, to experience multi-strand (nonlinear) narratives, and to supervise semi-autonomous agents in different VEs.",Virtual Reality; Multiverse; Transitions; Mask,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,EcoDive: Enhancing Presence and Ambient Environmental Awareness in a Virtual Reality Experience for Underwater Marine Debris Collection,VRST - Virtual Reality Software and Technology,A,"This paper presents a VR-based serious game. The game aims to raise awareness about ocean pollution by immersing players in a virtual underwater world where they collect trash to prevent coral bleaching and save marine life. Despite their efforts, players inevitably face game over, highlighting the futility of merely collecting trash and underscoring the need to prevent waste from entering oceans. The game uses various diegetic feedback mechanisms and enhanced user presence features to deepen emotional engagement and promote pro-environmental behavior.",Serious Game; Virtual Reality (VR); Coral Bleaching; Diegetic Feedback; Environmental Awareness; Environmental Education; Marine Conservation; Ocean Pollution; Pro-Environmental Behavior,Title_Keywords,True,Duplicate
Scopus,conferencePaper,2024,GazeLock: Gaze- and Lock Pattern-Based Authentication,VRST - Virtual Reality Software and Technology,A,"Password entry is common authentication approach in Extended Reality (XR) applications for its simplicity and familiarity, but it faces challenges in public and dynamic environments due to its cumbersome nature and susceptibility to observation attacks. Manual password input can be disruptive and prone to theft through shoulder surfing or surveillance. While alternative knowledge-based approaches exist, they often require complex physical gestures and are impractical for frequent public use. We present GazeLock, an eye-tracking and lock pattern-based authentication method. This method aims to provide an easy-to-learn and efficient alternative by leveraging familiar lock patterns operated through gaze. It ensures resilience to external observation, as physical interaction is unnecessary and eyes are obscured by the headset. Its hands-free, discreet nature makes it suitable for secure public use. We demonstrate this method by simulating the unlocking of a smart lock via an XR headset, showcasing its potential applications and benefits in real-world scenarios.",Eye Tracking; Extended Reality (XR); Authentication; Gaze-based Interaction,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Us Xtended - Tracking and Sensing through Embedded and Embodied Design in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This short paper presents an embodied and embedded design method via biometric data tracking on the example of the virtual reality prototype Us Xtended. Users are taken through different immersive worlds and their task is to manipulate the environments via a certain type of physiological interaction (i.e. heart rate, gaze, voice, cognitive load). By employing biofeedback, the system tailors the immersive environment via audiovisual and haptic stimuli to user’s psycho-physiological responses and reflects them on its scale which is part of the virtual environment. By recording their voice, users can self-assess their own affects. In the finale, users stand in a pastiche-like world filled with different artifacts of psycho-physiological evaluations they co-created with the biofeedback system throughout their journey.",virtual reality; biometrics; affect; embedded and embodied design; psycho-physiology; self-quantification,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Make America Great Again and Again: How to Adapt Interactive Installation Art for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Make America Great Again and Again features a large, fluttering American flag accompanied by the Star-Spangled Banner, with sixty small screens displaying one-minute video clips in sequence. This one-hour loop continues until participants upload their own videos, transforming the flag into a collage of visitor selfies. By providing a public sphere for local visitors, this interactive art project encourages them to share their opinions on this controversial issue. To capture global perspectives on the topic, the project was adapted into a virtual reality environment using the metaverse platform Styly. This paper outlines the process of converting the installation into virtual reality artwork.",Installation art; Interactive art; VR conversion,Title_Abstract,True,Duplicate
Scopus,conferencePaper,2024,Real-Time Scent Prediction and Release for Video Games,VRST - Virtual Reality Software and Technology,A,"This demo explores the use of computer vision technologies for the integration of scent in video games and interactive applications. We present an extendable system that is domain-independent and allows for customization and debugging based on the targeted game. Using Minecraft as a case study, we optimized the system configuration and evaluated its performance. Our aim is to advance the exploration of scent integration in gaming and inspire future designs for olfactory experiences.",Virtual Reality; Computer Vision; Scent Integration,Keywords,True,Duplicate
Scopus,conferencePaper,2024,ICELab Demo: an industrial digital-twin and simulator in VR,VRST - Virtual Reality Software and Technology,A,"In this demo we present an application featuring the integration of Virtual Reality (VR) technologies with the demonstration laboratory (ICELab) built around Industry 4.0/5.0 concepts. In particular, we showcase a digital twin of the real laboratory that allows the user to explore its environment in VR and interact with the different machinery to obtain several data and information.",Digital Twin; Computer Graphics; Cyber-Physical Factory,Abstract,True,Duplicate
Scopus,conferencePaper,2024,"Travel Speed, Spatial Awareness, And Implications for Egocentric Target-Selection-Based Teleportation - A Replication Design",VRST - Virtual Reality Software and Technology,A,"Virtual travel in Virtual Reality experiences is common, offering users the ability to explore expansive virtual spaces. Various interfaces exist for virtual travel, with speed playing a crucial role in user experience and spatial awareness. Teleportation-based interfaces provide instantaneous transitions, whereas continuous and semi-continuous methods vary in speed and control. Prior research by Bowman et al. highlighted the impact of travel speed on spatial awareness demonstrating that instantaneous travel can lead to user disorientation. However, additional cues, such as visual target selection, can aid in reorientation. This study replicates and extends Bowman’s experiment, investigating the influence of travel speed and visual target cues on spatial orientation.",,Abstract,True,Duplicate
Scopus,conferencePaper,2024,Walking &gt; Walking-in-Place &gt; Flying/Steering &gt; Teleportation? Designing Locomotion Research for Replication and Extension,VRST - Virtual Reality Software and Technology,A,"In this abstract, we discuss the demand for replication and extension efforts related to two seminal studies focused on virtual reality (VR) locomotion interfaces, initially centered around a VR implementation of the Visual Cliff, often referred to as Virtual Pit. The original experiments by Slater et al. (1995) and Usoh et al. (1999) compared different locomotion methods, including Real Walking, Walking-in-Place, and Flying/Steering, with a focus on presence and ease of use. We discuss the importance of these studies for the field, motivate replication efforts focused on these studies, discuss potential confounding factors, and present considerations for a concerted effort to reproduce the findings with state-of-the-art VR systems and measures, extensions to locomotion methods like Teleportation, and means to support future replications and extensions.",presence; Virtual reality; user study; locomotion; walking; replication; teleportation; steering,Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Fade-to-Black Duration in Egocentric Target-Selection-Based Teleport - A Replication Design,VRST - Virtual Reality Software and Technology,A,"Fade-to-black animations are a commonly used technique to visualize transitions during teleportation. However, their duration varies across different implementations and has not been extensively researched. This abstract details a study design to understand how the level of environmental detail affects the preferred duration of fade-to-black animations. We propose a within-subject study, comparing participants’ preferred duration across three virtual environments with varying levels of detail. We discuss improvements to the task design of an existing study. Other than the level of environmental detail, we motivate research into the effects of different tasks (i.e. hurried or calm) on the preferred duration.",Virtual reality; user study; locomotion; transitions; replication; teleportation,Keywords,True,Duplicate
Scopus,conferencePaper,2024,Generative Multi-Modal Artificial Intelligence for Dynamic Real-Time Context-Aware Content Creation in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"We introduce a framework that uses generative Artificial Intelligence (AI) for dynamic and context-aware content creation in Augmented Reality (AR). By integrating Vision Language Models (VLMs), our system detects and understands the physical space around the user, recommending contextually relevant objects. These objects are transformed into 3D models using a text-to-3D generative AI techniques, allowing for real-time content inclusion within the AR space. This approach enhances user experience by enabling intuitive customization through spoken commands, while reducing costs and improving accessibility to advanced AR interactions. The framework’s vision and language capabilities support the generation of comprehensive and context-specific 3D objects.",Augmented reality; generative AI; 3D object generation; vision language models,Title_Abstract_Keywords,True,Duplicate
Scopus,conferencePaper,2024,Usable Authentication in Virtual Reality: Exploring the Usability of PINs and Gestures,ACNS - International Conference on Applied Cryptography and Network Security,B,"Virtual Reality (VR) is becoming increasingly popular with its ability to offer new forms of interaction, user interface, and immersion not only for recreation but also for work, therapy, arts, or education. These new spaces need to be safeguarded by authentication similar to conventional IT systems. However, porting conventional interfaces to VR has often been found to be less than optimal as it fails to fully embrace the technology’s potential and potentially disrupt the immersive experience. This paper evaluates and compares the usability of two major authentication methods for VR: 2D Personal Identification Number (PIN) and gesture-based authentication - with 40 participants. While prior research has shown promising results in authentication security, there is a lack of studies specifically on usability in VR. Our findings indicate that the type of authentication and the user’s experience level affect usability, with gesture-based authentication having a higher usability score than a PIN and having faster authentication times. Hereby, users with less VR experience profited the most from a natural interaction mode for VR. The results suggest that developers should rather choose a native interaction mode in VR than try to port a familiar conventional interaction such as number pads for PINs. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Authentication; Gestures; PINs; Usability; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2024,Leveraging Overshadowing for Time-Delay Attacks in 4G/5G Cellular Networks: An Empirical Assessment,"ARES - International Conference on Availability, Reliability and Security",B,"Ensuring both reliable and low-latency communications over 4G or 5G Radio Access Network (RAN) is a key feature for services such as smart power grids and the metaverse. However, the lack of appropriate security mechanisms at the lower-layer protocols of the RAN–a heritage from 4G networks–opens up vulnerabilities that can be exploited to conduct stealthy Reduction-of-Quality attacks against the latency guarantees. This paper presents an empirical assessment of a proposed time-delay attack that leverages overshadowing to exploit the reliability mechanisms of the Radio Link Control (RLC) in Acknowledged Mode. By injecting falsified RLC Negative Acknowledgements, an attacker can maliciously trigger retransmissions at the victim User Equipment (UE), degrading the uplink latency of application flows. Extensive experimental evaluations on open-source and commercial off-the-shelf UEs demonstrate the attack’s effectiveness in increasing latency, network load, and buffer occupancy. The attack impact is quantified by varying the bitrate representing different applications and the number of injected negative acknowledgments controlling the attack intensity. This work studies a realistic threat against the latency quality of service in 4G/5G RANs and highlights the urgent need to revisit protocol security at the lower-RAN layers for 5G (and beyond) networks.",Deny of Service; Latency; Man on the Side; Overshadowing; Radio Access Network; Reduction of Quality; Time-delay,Abstract,True,
Scopus,conferencePaper,2024,A Domain-Specific Language for Augmented Reality Games,SAC - Selected Areas in Cryptography,B,"Augmented Reality (AR) applications have become popular over the last few years, with significant impact on video games. AR does not require advanced technology, but a mobile device with a camera is enough. However, building AR games is time-consuming and requires deep expertise in the tools, technologies and programming languages of the field, as well as on mathematical concepts related to the graphics and physics of the virtual objects. We attack this problem by means of a Domain-Specific Language (DSL) named argDSL, tailored to create AR games. It offers primitives to customise the domain and logic of the game, the physics of the virtual objects, and their graphical representation. We provide an Eclipse environment enabling the definition of AR games using the DSL, and an iOS client able to run the defined games.",augmented reality; domain-specific languages; games,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2024,High Energy Efficiency Mobile AR Applications under Adaptive Object Detection Engine and Self-learning Governor,SAC - Selected Areas in Cryptography,B,"Augmented reality (AR) applications aim to enhance user interactions by integrating virtual and real worlds, leveraging deep learning. However, implementing AR on mobile devices is challenging due to the high computational demands of deep learning, causing substantial energy drain on batteries. Server-assisted mobile augmented reality (MAR) systems address this by offloading computation, but network delays lead to worst object detection accuracy, degrading the user experience. We propose an adaptive object detection engine and self-learning governor framework. The engine maintains accuracy across network conditions and scenes, while the governor optimizes energy efficiency via self-learning models, dynamic voltage and frequency scaling (DVFS), and thread management. Our framework implemented on real platforms, significantly reduces energy consumption compared to state-of-the-art approaches.",energy efficiency; mobile augmented reality; mobile computing; object detection; scheduling,Abstract_Keywords,True,
Scopus,conferencePaper,2024,An Adaptive Transmission Strategy for Tiled 360-Degree VR Videos in NOMA Systems,SAC - Selected Areas in Cryptography,B,"Recently, virtual reality (VR) has become popular in many fields. Users can view a 360-degree video in all directions and have an immersive experience with a VR device. However, the transmission of a 360-degree video, which is compared to a traditional video, is a significant burden on wireless networks. In other words, users may suffer poor quality of experience (QoE) under constrained power. In order to improve transmission efficiency and ensure the QoE of users, in this paper, we propose an adaptive transmission strategy (ATS) for resource allocation in 360-degree videos to multiple users using a non-orthogonal multiple access (NOMA) scheme. In ATS, the delivery mode (multicast or unicast) of tiles is based on the users' field of views (FoVs) and their channel conditions, and the quality of tiles is dynamically determined by their priorities. The simulation results show that ATS significantly improves the QoE of users under constrained power.",360-degree video; multicast; NOMA; unicast; virtual reality,Abstract_Keywords,True,
Scopus,conferencePaper,2024,Securing Contrastive mmWave-based Human Activity Recognition against Adversarial Label Flipping,WiSec - Security and Privacy in Wireless and Mobile Networks,B,"Wireless Human Activity Recognition (HAR), leveraging their non-intrusive nature, has the potential to revolutionize various sectors, including healthcare, virtual reality, and surveillance. The advent of millimeter wave (mmWave) technology has significantly enhanced the capabilities of wireless HAR systems. This paper presents the first systematic study on the vulnerabilities of mmWave-based HAR to label flipping poisoning attacks in the context of supervised contrastive learning. We identify three label poisoning attacks on the contrastive mmWave-based HAR and propose corresponding countermeasures. The efficacy of the attacks and also our countermeasures are experimentally validated on a prototype system. The attacks and countermeasures can be easily extended to other wireless HAR systems, thereby promoting security considerations in system design and deployment.",human activity recognition; label poisoning; millimeter-wave (mmwave) technology; supervised contrastive learning (scl),Abstract,True,
Scopus,conferencePaper,2024,De-anonymizing VR Avatars using Non-VR Motion Side-channels,WiSec - Security and Privacy in Wireless and Mobile Networks,B,"Virtual Reality (VR) technology offers an immersive audio-visual experience to users through which they can interact with a digitally represented 3D space (i.e., a virtual world) using a headset device. By (visually) transporting users from their physical world to realistic virtual spaces, VR systems enable interactive and true-to-life versions of traditional applications such as gaming, remote conferencing and virtual tourism. However, VR applications also present significant user-privacy challenges. This paper studies a new type of privacy threat targeting VR users which attempts to connect their activities visible in the virtual world to their physical state sensed in the real world. Specifically, this paper analyzes the feasibility of carrying out a de-anonymization or identification attack on VR users by correlating visually observed movements of users' avatars in the virtual world with some auxiliary data (e.g., motion sensor data from mobile/wearable devices) representing their context/state in the physical world. To enable this attack, the paper proposes a novel framework which first employs a learning-based activity classification approach to translate the disparate visual movement data and motion sensor data into an activity-vector to ease comparison, followed by a filtering and identity ranking phase outputting an ordered list of potential identities corresponding to the target visual movement data. A comprehensive empirical evaluation of the proposed framework is conducted to study the feasibility of such a de-anonymization attack.",de-anonymization; motion; side channel; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Securing Bystander Privacy in Mixed Reality While Protecting the User Experience,SPM - Security & Privacy Magazine,B,"The modern mixed-reality devices that make the Metaverse viable require vast information about the physical world and can also violate the privacy of unsuspecting or unwilling bystanders. We provide an introduction to the problem, existing solutions, and avenues for future research.  © 2003-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,Security and Privacy in the Metaverse,SPM - Security & Privacy Magazine,B,[No abstract available],,Title,True,
Scopus,journalPaper,2024,Augmenting Security and Privacy in the Virtual Realm: An Analysis of Extended Reality Devices,SPM - Security & Privacy Magazine,B,We present a device-centric analysis of security and privacy attacks and defenses on extended reality (XR) devices. We present future research directions and propose design considerations to help ensure the security and privacy of XR devices.  © 2003-2012 IEEE.,,Title_Abstract,True,
Scopus,journalPaper,2024,A Viewpoint on the Societal Impact of Everyday Augmented Reality and the Need for Perceptual Human Rights,SPM - Security & Privacy Magazine,B,"Everyday augmented reality will become as fundamental to our daily lives as smartphones are today. — empowering users, communities, businesses, governments, and more to alter or mediate our perception of reality. But is society prepared for a world where a common objective reality that we all perceive and experience no longer exists? © 2003-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,Understanding Privacy in Virtual Reality Classrooms: A Contextual Integrity Perspective,SPM - Security & Privacy Magazine,B,"We outline privacy concerns and challenges associated with adopting virtual reality technologies in established social contexts using the theory of contextual integrity, examining information flows within and in between the real and virtual environments that could violate existing privacy norms.  © 2003-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,"Shockvertising, Malware, and a Lack of Accountability: Exploring Consumer Risks of Virtual Reality Advertisements and Marketing Experiences",SPM - Security & Privacy Magazine,B,"Companies increasingly use virtual reality (VR) for advertising. This begs the question, What risks does VR advertising pose for consumers? We analyze VR marketing experiences to identify risks and discuss opportunities to address those and future risks in VR advertising.  © 2003-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,Handling Identity and Fraud in the Metaverse,SPM - Security & Privacy Magazine,B,"Given the metaverse&#x2019;s potential realism and immersive nature, users may find it difficult to detect fraud. We explore the design space and technical aspects of verification and identification and how they may apply to fraud prevention in the metaverse. IEEE",Avatars; Fraud; Internet; Metaverse; Museums; Privacy; Security,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Truth in Motion: The Unprecedented Risks and Opportunities of Extended Reality Motion Data,SPM - Security & Privacy Magazine,B,"Motion-tracking telemetry data lie at the core of most modern extended reality (XR) and metaverse experiences. Recent studies have demonstrated that motion data have the potential to profile and deanonymize XR users, posing a threat to privacy in the metaverse.  © 2003-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2023,Examining the training and education potential of the metaverse: Results from an empirical study of next generation SAFe training,JSEP - Journal of software: Evolution and Process,B,"Restrictions imposed by the COVID-19 pandemic have forced many to seek alternative means of training and learning, which ended up with increasing investment in the notion of the metaverse. Metaverse is envisioned as the next iteration of the Internet in which the virtual and the real world are blended to materialize a highly immersive experience. Not surprisingly, perhaps, next-generation training and education systems are concerned with methods to integrate themselves into metaverse environments. In particular, participants are looking for more interactive and flexible training while maintaining a degree of educational content and high quality for their training plans and interactive workflows. In this paper, we conducted research to explore the role of metaverse in employee training. To this end, we utilized a variant of PlaySAFe (i.e., a 3D game) to investigate its metaverse adoption and usage. A qualitative design was adopted, using semistructured interviews to explore practitioners' experiences using the new version of PlaySAFe. After having it played in an industrial setting, we interviewed a group of software practitioners to compare the actual and expected features. This research has explored the pros and cons of using the current technologies for the practical groundwork of SAFe training. Findings from this research suggest that the metaverse holds the potential to deliver improved practical alignment in training and education programs, but that at the present time, practitioners expect more metaverse compatible features. © 2023 John Wiley & Sons Ltd.",industrial case study; metaverse; SAFe; serious games; software engineering processes,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,"Integration of properties of virtual reality, artificial neural networks, and artificial intelligence in the automation of software tests: A review",JSEP - Journal of software: Evolution and Process,B,"The complete automation of software tests has been considered to be an unattainable goal. This article discusses the potential to achieve this goal with recent discoveries and innovations in the areas of virtual reality (VR), artificial neural networks (ANNs), and artificial intelligence (AI). In this study, a theoretical proposal is described to integrate the properties of each of these areas using a process of automation of software tests. This process is based on a classification and description of the properties after consulting the literature, interviews, and dialogs with specialists from Australia, the United States, Germany, and Colombia. In addition to the experiences of the researchers, the construction of two tools is proposed: (1) a robot to design and apply functional tests, and (2) a virtual machine to identify errors in the logical structure of the code. Both tools are expected to replace human factors; the advantage is that the first tool identifies procedural flaws and the second errors of operation. © 2019 John Wiley & Sons, Ltd.",artificial intelligence; artificial neural networks; automation; software testing; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Model-based testing, test case prioritization and testing of virtual reality applications",STVR - Software Testing Verification and Reliability,B,[No abstract available],,Title,True,
Scopus,journalPaper,2023,Exploiting deep reinforcement learning and metamorphic testing to automatically test virtual reality applications,STVR - Software Testing Verification and Reliability,B,"Despite the rapid growth and popularization of virtual reality (VR) applications, which have enabled new concepts for handling and solving existing problems through VR in various domains, practices related to software engineering have not kept up with this growth. Recent studies indicate that one of the topics that is still little explored in this area is software testing, as VR applications can be built for practically any type of purpose, making it difficult to generalize knowledge to be applied. In this paper, we present an approach that combines metamorphic testing, agent-based testing and machine learning to test VR applications, focusing on finding collision and camera-related faults. Our approach proposes the use of metamorphic relations to detect faults in collision and camera components in VR applications, as well as the use of intelligent agents for the automatic generation of test data. To evaluate the proposed approach, we conducted an experimental study on four VR applications, and the results showed an (Formula presented.) of the solution ranging from 93% to 69%, depending on the complexity of the application tested. We also discussed the feasibility of extending the approach to identify other types of faults in VR applications. In conclusion, we discussed important trends and opportunities that can benefit both academics and practitioners. © 2023 John Wiley & Sons Ltd.",metamorphic testing; software testing; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,An automated functional testing approach for virtual reality applications,STVR - Software Testing Verification and Reliability,B,"Software testing is regarded as an important method for fault revealing. Despite this advantage, it has been poorly used within the scope of virtual reality (VR) applications because they are highly complex and have peculiar features. Most testing performed of this VR applications are usability, which is conducted manually and only at final of the development process. Although some works try to propose criteria for this domain, there are no approaches that automatize the generation of test data from requirements specification in the VR domain. This paper proposes an approach called virtual reality—requirements specification and testing (VR-ReST) to assist the requirements specification through a semiformal language and uses structural test criteria to generate test requirements and test data automatically for VR applications using scene graph concepts. The paper also examines the empirical results concerning the cost-effectiveness of the approach for three different VR applications through two experiments. Mutation testing was used to evaluate effectiveness. We found that the approach achieved a high mutation score outperforming random testing, by 20%, on average. Our results also demonstrate that the approach is promising since it assists in writing and validating the requirements, as well as in reducing the risks of requirement specification by adopting a semiformal language. © 2018 John Wiley & Sons, Ltd.",requirements engineering; software testing; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Proteus effect or bodily affordance? The influence of virtual high-heels on gait behavior,VRS - Virtual Reality,B,"Shoes are an important part of the fashion industry, stereotypically affect our self-awareness as well as external perception, and can even biomechanically modify our gait pattern. Immersive Virtual Reality (VR) enables users not only to explore virtual environments, but also to control an avatar as a proxy for themselves. These avatars can wear any kind of shoe which might similarly affect self-awareness due to the Proteus Effect and even cause a bodily affordance to change the gait pattern. Bodily affordance describes a behavioral change in accordance with the expected constraints of the avatar a user is embodied with. In this article, we present the results of three user studies investigating potential changes in the gait pattern evoked by wearing virtual high-heels. Two user studies targeted female participants and one user study focused male participants. The participants wore either virtual sneakers or virtual high-heels while constantly wearing sneakers or socks in reality. To measure the gait pattern, the participants walked on a treadmill that also was added to the virtual environment. We measured significant differences in stride length and in the flexion of the hips and knees at heel strike and partly at toe off. Also, participants reported to walk more comfortably in the virtual sneakers in contrast to the virtual high-heels. This indicates a strong acceptance of the virtual shoes as their real shoes and hence suggests the existence of a bodily affordance. While sparking a discussion about the boundaries as well as aspects of the Proteus Effect and providing another insight into the effects of embodiment in VR, our results might also be important for researchers and developers. © The Author(s) 2024.",Embodiment; Gait; Intermodal illusion; Proteus Effect; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Correction to: Cognitive differences in product shape evaluation between real settings and virtual reality: case study of two-wheel electric vehicles (Virtual Reality, (2024), 28, 3, (136), 10.1007/s10055-024-01034-8)",VRS - Virtual Reality,B,"The Original Publication, Table 9 was published with text format errors. This has been corrected now in the original publication. © The Author(s) 2024.",,Title,True,
Scopus,journalPaper,2024,Developing English language learners’ speaking skills through applying a situated learning approach in VR-enhanced learning experiences,VRS - Virtual Reality,B,"A situated learning environment is crucial for language learners to develop speaking skills as learners can apply their speaking skills in context, helping them adapt language use to various situations and improve their language proficiency and communication effectiveness. Although various features of situated learning environments have been explored across different subject areas and in many platforms, there is limited research on their application to language learning within VR environments. This convergent mixed-method study adopts a situated learning framework and examines the impact of situated learning on learners’ English-speaking performance, specifically in areas of fluency, vocabulary, pronunciation, and grammar, and explores learners’ perception of the instruction based on the situated learning approach. Sixteen first-year English majors at a university in China participated in eight role-play speaking classes using the desktop-based VR application, Immerse. The study involved pre- and post-assessments of speaking performance and semi-structured interviews with six participants. Paired samples t-tests were used to assess the difference in the speaking performance and respective areas, and a thematic analysis was adopted to explore learners’ perceptions of the instruction based on the situated learning approach. Quantitative findings show a significant improvement in learners’ speaking performance (t(15) = 7.41, p <.001, Cohen’s d = 1.82), with notable progress in fluency, vocabulary, pronunciation, and grammar. Thematic analysis of the qualitative data indicated the authenticity of the context and activities, the collaborative nature of the tasks, the expert guidance, and the opportunities for reflection all contribute to a comprehensive learning experience that aligns well with the principles of situated learning. © The Author(s) 2024.",English speaking skill; Role-play speaking activities; Situated learning approach; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,New proxemics in new space: proxemics in VR,VRS - Virtual Reality,B,"With the development of computer technology, it is possible to design virtual reality (VR) media that provides services to multiple users. Hall’s proxemics theory, which holds that the distance varies depending on the relationship between people, has been applied when designing VR in such media. However, this concept was usually applied to designs without criticism and without confirming whether proxemic distances established in physical space are equally valid in VR. This study investigated how proxemics in VR activate differently from those in a physical space. We measured the distance and the number of instances of direct contact between people, with 69 participants from Korea and Turkiye. As a result, a proxemics pattern similar to that of a physical space appeared in VR. However, the average distance between participants in the VR was about 160% greater than in the physical space. Also, we could observe direct contact up to 260% more in the VR than in the physical space. We analyzed the collected data using Bayesian ANOVA and t-tests. We could clarify the difference between the two proxemics in physical space and VR, but the reason for the phenomenon has yet to be discovered. However, this study is meaningful because any industry designing VR, such as those in digital games, can directly apply the findings to manipulate multiple users’ emotions and experiences more efficiently. Additionally, this study provides directions for any future studies discussing VR design. © The Author(s) 2024.",Human-computer interaction; Proxemics; Virtual space; VR,Abstract,True,
Scopus,journalPaper,2024,"Publisher Correction: Adoption of immersive-virtual reality as an intrinsically motivating learning tool in parasitology (Virtual Reality, (2024), 28, 3, (123), 10.1007/s10055-024-01016-w)",VRS - Virtual Reality,B,"In the Original publication, Figure 5 and Figure 6 were swapped mistakenly during typesetting. This has been corrected in the original article. © The Author(s) 2024.",,Title,True,
Scopus,journalPaper,2024,Cognitive differences in product shape evaluation between real settings and virtual reality: case study of two-wheel electric vehicles,VRS - Virtual Reality,B,"Product shape evaluation is an important part of new product development. In the shape design stage, design schemes are often presented through visual images. The presentation of visual images causes evaluators to form different cognitive experiences and evaluation results. In recent years, virtual reality (VR) technology has been widely used in the field of industrial design, enriching the presentation forms of design scheme images. Although VR technology has shown the potential to improve evaluators’ perception and cognitive experiences in product shape design, research comparing it with traditional methods remains relatively scattered. This study used two-wheel electric vehicles as an example to examine the difference in evaluators’ cognition of product shape in VR and a real setting (RS). First, we established a semantic scale comprising seven pairs of opposite adjectives to evaluate the shape scheme. Second, we built VR and RS evaluation environments using head-mounted displays and paper renderings, respectively. The participants evaluated the vehicle shape design in alternating viewing and underwent semi-structured interviews on cognitive experience. We analyzed the experimental and interview results based on three aspects of product shape cognition. The results demonstrated that volume cognition was significantly more accurate in VR environments. Furthermore, graphic cognition, particularly regarding shape details, differed partially between environments. VR provided a better sense of immersion and more variable viewing angles than RS. Treatment cognition did not exhibit significant differences between environments, as it depended on human experience rather than visualization. These findings suggest that VR tools are more suited for shaping design evaluations early. Selecting suitable visual presentation tools based on evaluators’ cognitive characteristics at different evaluation nodes to display design schemes is a practical, economical, and efficient strategy. © The Author(s) 2024.",Cognitive differences; Design evaluation; Product shape design; Two-wheel electric vehicle; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Virtual reality platform for teacher training on classroom climate management: evaluating user acceptance,VRS - Virtual Reality,B,"Enhancing the educational experience through Immersive Virtual Reality (IVR) is a promising avenue, elevating the authenticity and responsiveness of simulations. Particularly in educational settings, IVR holds the potential to augment accessibility and engagement in learning. However, one pivotal aspect lies in assessing the learners' acceptance of such environments to ensure optimal and effective utilization of these technologies. This paper delves into the Didascalia Virtual-ClassRoom usability testing —an immersive IVR environment tailored for pre-service secondary school teachers. The platform transports users into a simulated classroom, where they are invited to play the role of a teacher. During the simulation, three scenarios are recreated, reproducing disruptive behaviours commonly faced in real classrooms. 84 participants (28 teachers and 56 pre-service teachers) engaged in decision-making to manage the classroom climate influenced by the simulated situations. To collect data, we used a questionnaire based on the Technology Acceptance Model (TAM) to assess and gauge users' inclinations and attitudes towards embracing the technology in question. To gain deeper insights into the user experience, participants were further invited to participate in semi-structured interviews, offering reflections and suggestions for potential enhancements. The evaluation process encompassed the perceived usefulness of the Didascalia Virtual-ClassRoom, shedding light on factors that could either facilitate or impede the adoption of this platform to enhance classroom management competence. The participants' perspectives serve as a valuable foundation for refining the tool's functionality, and their feedback fuels recommendations for its seamless integration into initial teacher training programs. © The Author(s) 2024.",Classroom management; Immersive virtual reality; Initial teacher education; Secondary school; Usability evaluation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The effect of target and background texture on relative depth discrimination in a virtual environment,VRS - Virtual Reality,B,"The spatial frequency (SF) content of an object’s texture is an important cue for depth perception, although less is known about the role of background texture. Here, we used bandpass-filtered noise patterns to systematically study the interactions between target and background textures in a virtual environment. During the trials, three square targets were presented at 3 m against a background wall 6 m away from the observer. One of the squares was presented closer than the other two, and the subjects had to indicate it with a key press. The threshold distance from the two reference tiles was determined using a staircase procedure. Both the target and background were tested with different combinations of SF textures and a non-textured gray, which were rendered onto the flat surfaces. Against a gray background, the distance thresholds were smallest when the targets were presented with a mid-SF texture. Performance declined significantly with a non-textured target against a textured background. With different combinations of target-background texture, the background texture significantly affected the performance. We propose several hypotheses to explain the behavioral result. Understanding the effect of surrounding texture can be useful in improving the depth perception experience in virtual reality. © The Author(s) 2024.",Depth perception; Figure-background; Spatial frequency; Texture; Virtual environment,Abstract,True,
Scopus,journalPaper,2024,"Virtual reality games for cognitive rehabilitation of older adults: a review of adaptive games, domains and techniques",VRS - Virtual Reality,B,"In recent decades, the senior adults population worldwide has increased, as well as the medical conditions related to aging, such as cognitive decline. Virtual reality (VR) games are a valuable addition to conventional cognitive rehabilitation as they increase engagement to the therapy through customization, socialization, immersion, and feedback. This review, performed according to PRISMA protocol, addresses the following questions: How VR games have been used for cognitive rehabilitation?, What cognitive domains have been addressed by VR games and in which populations have these games been used?, Which features have been considered to improve engagement in VR games for cognitive rehabilitation?, How is the difficulty adjustment of exercises carried out in VR games for cognitive rehabilitation?. We found 25 scientific works related to these questions, 92% of them treating one cognitive domain at a time, despite the fact that the related literature recognizes the value of training multiple domains simultaneously. Our review indicates that, despite the existence of serious VR games for working memory training, such as those described in Flak et al. (Front Psychol 10:807, 2019. https://doi.org/10.3389/fpsyg.2019.00807), to our knowledge, there are no applications that simultaneously address multiple cognitive domains and incorporate dynamic difficulty adjustment, which are important to ensure ecological validity of therapy and therapy adherence, respectively. In addition, we found that games themselves could be used to monitor the user’s progression. It is also important to determine the impact of multiplayer interactions in the game, test difficulty adjustment approaches that use physiological variables, and define difficulty-skill relationships aligned with the user’s preferences. This paper concludes that the main barriers to implement dynamic difficulty adjustment in VR games for cognitive rehabilitation are: (i) the absence of metrics to estimate when the game offers to the players a challenge adapted their skills, and (ii) the lack of a conceptual framework that integrates relevant theories such as state of flow, cognitive load, cognitive rehabilitation, and feedback systems. © The Author(s) 2024.",Cognitive domains; Cognitive rehabilitation; Dynamic difficulty adjustment; Serious games; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Design and implementation of an interactive virtual library based on its physical counterpart,VRS - Virtual Reality,B,"The rapid technological advancements and the widespread adoption of the internet have diminished the role of the physical library as a main information resource. As the Metaverse is evolving, a revolutionary change is anticipated in how social relationships are perceived, within an educational context. It is therefore necessary for libraries to upgrade the services they provide to keep in line with the technological trends and be a part of this virtual revolution. It is believed that the design and development of a Virtual Reality (VR) library can be the community and knowledge hub the society needs. In this paper, the process of creating a partially digital replica of the Limassol Municipal University Library, a landmark for the city of Limassol, is examined by using photogrammetry and 3D modelling. A 3D platform was developed, where users have the perception that they are experiencing the actual library. To that end, a perceptual study was conducted, to understand the current usage of physical libraries, examine the users’ experience in VR, and identify the requirements and expectations in the development of a virtual library counterpart. Following the suggestions and observations from the perceptual study, five key scenarios were implemented that demonstrate the potential use of a virtual library. This work incorporates the fundamental VR attributes, such as immersiveness, realism, user interactivity and feedback as well as other features, such as animated NPCs, 3D audio, ray-casting and GUIs, that significantly augment the overall VR library user experience, presence as well as navigation autonomy. The main effort of this project was to produce a VR representation of an existing physical library, integrated with its key services, as a proof-of-concept, with emphasis on easy 24/7 access, functionality, and interactivity. The above attributes differentiate this work from existing studies. A detailed user evaluation study was conducted upon completion of the final VR library implementation, which firmly confirmed all its key attributes and future viability. © The Author(s) 2024.",Digital library; Digital replica; Metaverse; Virtual humans; Virtual library; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Mapping the landscape of research on 360-degree videos and images: a network and cluster analysis,VRS - Virtual Reality,B,"The recent emergence of low-cost virtual reality technologies, like 360° videos and images is attracting the attention of researchers suggesting it could be the next significant step in technological innovation. The birth of 360° videos and images is quite young, it goes back to the middle of the nineteenth century and then spread more and more in many areas. In recent years, 360° videos and images have grown in popularity because they provide a great number of advantages compared to traditional virtual reality computer-generated technology. The aim of this research is to map scientific works in the area of 360° technology using advanced scientometric techniques. We collected all the existent articles about 360° contents in the Scopus database, and the resultant dataset contained 3319 records. The bibliographic record encompassed all categories of scientific articles retrieved from Scopus, considering parameters such as countries, institutions, journals, authors, citation counts, and publication years. The network and cluster analysis of the literature showed a composite panorama characterized by changes and evolutions over time of the use of 360° contents. We discuss these aspects in the main areas of application with an emphasis on the future expected 360° capacities, increases, and challenges. As already happened with the advent of virtual reality, the future of 360° technology will be an increasing shift from engineering to clinical use, by improving the use and the development of scientific applications in clinical areas and by modifying social communication and interaction among people. © The Author(s) 2024.",360° Videos and images; Cluster analysis; Psychometrics; Scientometrics; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,DTP: learning to estimate full-body pose in real-time from sparse VR sensor measurements,VRS - Virtual Reality,B,"For virtual reality (VR) applications, estimating full-body pose in real-time is becoming increasingly popular. Previous works have reconstructed full-body motion in real time from an HTC VIVE headset and five VIVE Tracker measurements by solving the inverse kinematics (IK) problem. However, an IK solver may yield unnatural poses and shaky motion. This paper introduces Deep Tracker poser (DTP): a method for real-time full-body pose estimation in VR. This task is difficult due to the ambiguous mapping from the sparse measurements to full-body pose. The data obtained from VR sensors is calibrated, normalized and fed into the deep neural networks (DNN). To learn from sufficient data, we propose synthesizing a VR sensor dataset called AMASS-VR from the AMASS, a collection of various motion capture datasets. Furthermore, feet tracking loss is a common problem of VIVE Tracker. To improve the accuracy and robustness of DTP to the occlusion noise, we simulate the occlusion noise by Gaussian random noise. Then we synthesize an occlusion dataset AMASS-OCC and fine-tune DTP on that. We evaluate DTP by comparing with other popular methods in terms of the accuracy and computational cost. The results indicate that DTP outperforms others in terms of the positional error (1.04 cm) and rotational error (4.22 °). The quantitative and qualitative results show that DTP reconstructs accurate and natural full-body pose even under serious feet occlusion, which indicates the superiority of the DTP in modelling the mapping from sparse joint data to the full-body pose. © The Author(s) 2024.",Deep learning; Full-body pose estimation; HTC VIVE tracker; Transformer; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Correction to: Cybersickness with passenger VR in the aircraft: Influence of turbulence and VR content (Virtual Reality, (2024), 28, 2, (112), 10.1007/s10055-024-01008-w)",VRS - Virtual Reality,B,"In the Original publication, Fig. 1, Fig. 3 and Fig. 2, Fig. 4 are swapped. The correct figures are given below, and the original publication has been corrected (Figs. 1, 2, 3, 4). (Figure presented.) (Figure presented.) (Figure presented.) (Figure presented.) Experimental protocol of the VR flight The outside view of the Air Vehicle Simulator (AVES) Interior view of aircraft cabin simulator with participants wearing VR headsets Typical view of the VR clips, in this case a clip from the staticcondition © The Author(s) 2024.",,Title,True,
Scopus,journalPaper,2024,A framework for virtual learning in industrial engineering education: development of a reconfigurable virtual learning factory application,VRS - Virtual Reality,B,"Advances in digital factory technologies are offering great potential to innovate higher education, by enabling innovative learning approaches based on virtual laboratories that increase the involvement of students while delivering realistic experiences. This article introduces a framework for the development of virtual learning applications by addressing multidisciplinary requirements. The implementation of the framework can be eased by the use of the proposed virtual learning factory application (VLFA), an open-source solution that takes advantage of virtual reality to support innovative higher-education learning activities in industrial engineering. A complete design and development workflow is described, starting from the identification of the requirements, to the design of software modules and underlying technologies, up to the final implementation. The framework and the VLFA have been tested to implement a serious game related to the design and analysis of manufacturing systems, also collecting the feedback of students and teachers. © The Author(s) 2024.",Digital factory; Higher-education; Virtual laboratory; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Virtual reality exposure effect in acrophobia: psychological and physiological evidence from a single experimental session,VRS - Virtual Reality,B,"In recent years, virtual reality (VR) has gained attention from researchers in diverse fields, particularly in therapy of phobias. Currently, virtual reality exposure therapy therapy (VRET) is considered a promising cognitive-behavioral therapy technique. However, specific psychological and physiological responses of VR users to virtual exposure in such a context are still only vaguely explored. In this experimental study, we mapped VR exposure in a height environment in people with a moderate fear of heights–acrophobia. Thirty-six participants were divided into experimental and control groups–with and without psychological guidance during exposure. Participants' subjective level of anxiety was examined, and objective physiological response was captured via heart rate variability (HRV) measurement. Psychological assessments recorded an anticipated rise in participant anxiety following exposure to height; nevertheless, no distinctions were observed in self-reported anxiety concerning psychological guidance. Notably, objective physiological measures revealed that VR exposure prompts physiological responses akin to real-world scenarios. Moreover, based on the analysis of heart rate variability, participants who received psychological guidance were identified as better at compensating for anxiety compared to those without such support. These findings support VRET as a promising tool for psychotherapy and advocate for psychological guidance as beneficial in reducing anxiety and managing stress during exposure. The results may help improve our understanding of anxiety during exposure to phobic stimuli. © The Author(s) 2024.",Acrophobia; Cognitive-behavioral therapy; HRV; iVR; Virtual reality; VRET,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Physiology-driven cybersickness detection in virtual reality: a machine learning and explainable AI approach,VRS - Virtual Reality,B,"One of the major obstacles to the widespread adoption of Virtual Reality (VR) is Cybersickness. It is a sense of physical discomfort akin to motion sickness experienced by the users either during or subsequent to VR utilization. Typically, it is detected through explicit methods such as self-reported questionnaires, which are not ideal for online and implicit monitoring of users’ well-being. This study tackles the challenge of implicitly detecting and measuring Cybersickness in VR through physiological signals. Therefore, we propose a multimodal approach that integrates physiological signals with self-reported measures. We utilize a mixed-methods design combining quantitative and qualitative analyses, using a roller coaster simulation as the experimental paradigm. The research analyzes physiological data collected from a group of 22 participants exposed to a simulated VR rollercoaster experience, through statistical analysis and machine learning algorithms. The physiological markers studied include Electroencephalograms (EEG), Electrodermal Activity, Blood Volume Pulse, and skin Temperature signals. Additionally, the study elucidates the complex interplay among physiological markers using Explainable AI (XAI). We found out significant correlations between high Simulator Sickness Questionnaire scores and physiological measures such as brain rhythms and EEG indices related to engagement, visual fatigue, and drowsiness, as well as heart rate variability. Moreover, the proposed machine learning model achieves an accuracy of 86.66 % in detecting elevated Cybersickness symptoms, which is higher than the accuracy achieved by existing techniques. Further exploration using the XAI technique confirms the elevated levels of drowsiness and reduced engagement in participants experiencing elevated Cybersickness. By providing a comprehensive, multimodal approach for quantitative assessment, this study fills a gap in the existing literature and paves the path for the development of adaptive systems that modulate their behavior based on physiological data. © The Author(s) 2024.",Brain–computer interface; Cybersickness; Explainable AI; Physiological signals; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,VR interventions aimed to induce empathy: a scoping review,VRS - Virtual Reality,B,"To assess the methods and outcomes of virtual reality (VR), interventions aimed at inducing empathy and to evaluate if VR could be used in this manner for disability support worker (DSW) training, as well as highlight areas for future research. The authors conducted a scoping review of studies that used VR interventions to induce empathy in participants. We searched three databases for articles published between 1960 and 2021 using “virtual reality” and “empathy” as key terms. The search yielded 707 articles, and 44 were reviewed. VR interventions largely resulted in enhanced empathy skills for participants. Most studies agreed that VR’s ability to facilitate perspective-taking was key to inducing empathy for participants. Samples were often limited to the context of healthcare, medicine, and education. This literature provides preliminary evidence for the technology’s efficacy for inducing empathy. Identified research gaps relate to limited studies done, study quality and design, best practice intervention characteristics, populations and outcomes of interest, including lack of transfer and data across real-world settings. © The Author(s) 2024.", Disability support worker; Empathy; Scoping review; Technology; Training; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Acoustic characteristics of voice production in virtual reality-simulated and physical environments: a comparative study in university professors,VRS - Virtual Reality,B,"This study investigated the reliability of a virtual reality-simulated classroom to generate a comparable self-perception of voice quality and acoustic effects of phonation to a real classroom in a group of teachers, and sense of presence. Thirty university professors participated in the study and were required to produce loud connected speech by reading a 100-word text in two conditions: (1) in a real classroom including a group of students, and (2) in a virtual replica of the classroom consisting of a 360-degree video of the same classroom and students, which was displayed using a head mounted display. Ambient noise was controlled in both conditions by playing classroom noise through headphones. The self-perception of voice quality, the long-term average spectrum and smooth cepstral peak prominence were estimated in both conditions. The sense of presence generated by virtual reality was measured after interacting with the virtual classroom. There were no statistically significant differences in the self-perception of voice quality or in the acoustic measures of voice production between conditions. The sense of presence in the virtual classroom was high. Our findings suggest that a virtual reality-simulated classroom generate comparable self-perception of voice quality and acoustic effects of phonation to the real classroom, and a high sense of presence, in a group of teachers. Additionally, it is important to highlight the potential of virtual reality to enhance the ecological validity of acoustic assessment of voice production in laboratories and clinical settings. © The Author(s) 2024.",Acoustic analysis; Ecological validity; Virtual reality; Voice assessment,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Towards overcoming barriers to the clinical deployment of mixed reality image-guided navigation systems supporting percutaneous ablation of liver focal lesions,VRS - Virtual Reality,B,"In recent years, we have observed a rise in the popularity of minimally invasive procedures for treating liver tumours, with percutaneous thermoablation being one of them, conducted using image-guided navigation systems with mixed reality technology. However, the application of this method requires adequate training in using the employed system. In our study, we assessed which skills pose the greatest challenges in performing such procedures. The article proposes a training module characterized by an innovative approach: the possibility of practicing the diagnosis, planning, execution stages and the physical possibility of performing the execution stage on the radiological phantom of the abdominal cavity. The proposed approach was evaluated by designing a set of 4 exercises corresponding to the 3 phases mentioned. To the research group included 10 radiologists and 5 residents in the study. Based on 20 clinical cases of liver tumors subjected to percutaneous thermoablation, we developed assessment tasks evaluating four skill categories: head-mounted display (HMD), ultrasound (US)/computed tomography (CT) image fusion interpretation, tracking system use, and the ability to insert a needle. The results were presented using the Likert scale. The results of our study indicate that the most challenging aspect for radiology specialists is adapting to HMD gesture control, while residents point to intraoperative images of fusion and respiratory movements in the liver as the most problematic. In terms of improving the ability to perform procedures on new patients, the module also allows you to create a new hologram for a different clinical case. © The Author(s) 2024.",Acquiring the ability to use image-guided navigation; Image-guided navigation; Mixed reality; Percutaneous liver tumour ablation; Training of interventional radiologists,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Simulating vision impairment in virtual reality: a comparison of visual task performance with real and simulated tunnel vision,VRS - Virtual Reality,B,"In this work, we explore the potential and limitations of simulating gaze-contingent tunnel vision conditions using Virtual Reality (VR) with built-in eye tracking technology. This approach promises an easy and accessible way of expanding study populations and test groups for visual training, visual aids, or accessibility evaluations. However, it is crucial to assess the validity and reliability of simulating these types of visual impairments and evaluate the extend to which participants with simulated tunnel vision can represent real patients. Two age-matched participant groups were acquired: The first group (n = 8, aged 20–60, average 49.1 ± 13.2) consisted of patients diagnosed with Retinitis pigmentosa (RP). The second group (n = 8, aged 27–59, average 46.5 ± 10.8) consisted of visually healthy participants with simulated tunnel vision. Both groups carried out different visual tasks in a virtual environment for 30 min per day over the course of four weeks. Task performances as well as gaze characteristics were evaluated in both groups over the course of the study. Using the ’two one-sided tests for equivalence’ method, the two groups were found to perform similar in all three visual tasks. Significant differences between groups were found in different aspects of their gaze behavior, though most of these aspects seem to converge over time. Our study evaluates the potential and limitations of using Virtual Reality technology to simulate the effects of tunnel vision within controlled virtual environments. We find that the simulation accurately represents performance of RP patients in the context of group averages, but fails to fully replicate effects on gaze behavior. © The Author(s) 2024.",Disability; Retinitis pigmentosa; Tunnel vision; Virtual reality; Vision impairment simulation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Intraoperative use of virtual reality decreases anxiety during surgery under local anaesthesia in the head and neck region,VRS - Virtual Reality,B,"The availability of virtual reality (VR) in the medical field has been rapidly increasing in the past years. Here we investigate to which extent the VR headset can lead to a reduction in anxiety and pain in patients during surgical procedures under local anesthesia in the head and neck region. Patients were divided into a study group (N = 67) and a control group (N = 28). The study group used a VR headset during surgical procedures in the head and neck region under local anaesthesia. Before and after surgery, the influence of the VR headset on perioperative anxiety was assessed using the State-Trait-Anxiety-Inventory (STAI) in both groups. The use of a VR headset leads to a significant reduction in perioperative anxiety. The anxiety scores measured by means and ranks of the STAI were significantly decreased (p =.002). However, 14/67 (20.9%) of the patients wearing the VR headset also reported higher intraoperative tension. No technical complications occurred intraoperatively. 48/67 (71.7%) of the patients would be less apprehensive about a future operation when using a VR headset and 58/67 (86.6%) would further recommend the use of a VR headset to other patients. In addition to a trusting surgeon-patient relationship and the use of sufficient local anaesthesia, the use of a VR headset as a method of distraction can further reduce the intraoperative anxiety of patients. © The Author(s) 2024.",Anxiety; ENT; Local anaesthesia; Surgery; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Non-deterministic method for semi-automatic calibration of smartphone-based OST HMDs,VRS - Virtual Reality,B,"An Augmented Reality (AR) system must show real and virtual elements as if they coexisted in the same environment. The tridimensional aligment (registration) is particularly challenging on specific hardware configurations such as Head Mounted Displays (HMDs) that use Optical See-Through (OST) technology. In general, the calibration of HMDs uses deterministic optimization methods. However, non-deterministic methods have been proposed in the literature with promising results in distinct research areas. In this work, we developed a non-deterministic optimization method for the semi-automatic calibration of smartphone-based OST HMDs. We tested simulated annealing, evolutionary strategy, and particle swarm algorithms. We also developed a system for calibration and evaluated it through an application that aligned a virtual object in an AR environment. We evaluated our method using the Mean Squared Error (MSE) at each calibration step, considering the difference between the ideal/observed positions of a set of reference points and those estimated from the values determined for the calibration parameters. Our results show an accurate OST HMD calibration for the peripersonal space, with similar MSEs for the three tested algorithms. © The Author(s) 2024.",Calibration; Head mounted displays; Non-deterministic optimization; Optical see-through; Smartphone-based AR,Abstract,True,
Scopus,journalPaper,2024,Exploring the user’s gaze during product evaluation through the semantic differential: a comparison between virtual reality and photorealistic images,VRS - Virtual Reality,B,"Advanced product presentation methods can enhance the product evaluation experience both during the design process and online shopping, as static images often fail to convey essential product details. Virtual Reality (VR) technologies hold great potential in this regard, becoming increasingly accessible to all users. However, the influence of display mediums on emotional responses and product assessment needs further investigation, especially using physiological measures to obtain more objective insights. In this study, we investigate the influence of VR and photorealistic images on assessing and observing virtual prototypes of game controllers. The Semantic Differential technique was employed for product assessment, while built-in eye-tracking was used to measure participants’ viewing time on various areas of interest (AOIs). Our findings show that the medium significantly affects not only product evaluation and confidence in the response but also how the user observes it, with sensory-related features being particularly influenced. These findings hold practical implications for product design and vendors, as understanding the relationship between visualization mediums and product evaluation enhances the design process and improves consumer experiences. © The Author(s) 2024.",Eye-tracking; Game controllers; Gaze bias; Immersion; Interaction; Product evaluation; Semantic differential; Subjective impressions; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The user experience of distal arm-level vibrotactile feedback for interactions with virtual versus physical displays,VRS - Virtual Reality,B,"Haptic feedback, a natural component of our everyday interactions in the physical world, requires careful design in virtual environments. However, feedback location can vary from the fingertip to the finger, hand, and arm due to heterogeneous input/output technology used for virtual environments, from joysticks to controllers, gloves, armbands, and vests. In this work, we report on the user experience of touch interaction with virtual displays when vibrotactile feedback is delivered on the finger, wrist, and forearm. In a first controlled experiment with fourteen participants and virtual displays rendered through a head-mounted device, we report a user experience characterized by high perceived enjoyment, confidence, efficiency, and integration as well as low perceived distraction, difficulty, and confusion. Moreover, we highlight participants’ preferences for vibrotactile feedback on the finger compared to other locations on the arm or through the VR controller, respectively. In a follow-up experiment with fourteen new participants and physical touchscreens, we report a similar preference for the finger, but also specific nuances of the self-reported experience, not observed in the first experiment with virtual displays. Overall, our results depict an enhanced user experience when distal vibrotactile feedback is available over no vibrations at all during interactions with virtual and physical displays, for which we propose future work opportunities for augmented interactions in virtual worlds. © The Author(s) 2024.",Touchscreens; User experience; Vibrotactile feedback; Virtual displays; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,Exploring the viability of Virtual Reality as a teaching method for knee aspiration,VRS - Virtual Reality,B,"Knee arthrocentesis is a simple procedure commonly performed by general practitioners and junior doctors. As such, doctors should be competent and comfortable in performing the technique by themselves; however, they need to be adequately trained. The best method to ensure practitioner proficiency is by optimizing teaching at an institutional level, thus, educating all future doctors in the procedure. However, the Coronavirus Disease 19 (COVID-19) pandemic caused significant disruption to hospital teaching for medical students which necessitated investigating the effectiveness of virtual reality (VR) as a platform to emulate hospital teaching of knee arthrocentesis. A workshop was conducted with 100 fourth year medical students divided into three Groups: A, B and C, each receiving a pre-reading online lecture. Group A was placed in an Objective Structured Clinical Examination (OSCE) station where they were assessed by a blinded orthopaedic surgeon using the OSCE assessment rubric. Group B undertook a hands-on practice station prior to assessment, while Group C received a VR video (courtesy of the University of Adelaide’s Health Simulation) in the form of VR headset or 360° surround immersion room and hands-on station followed by the OSCE. Upon completion of the workshop, students completed a questionnaire on their confidence with the procedure and the practicality of the VR station. OSCE scores were compared between Groups B and C to investigate the educational value of VR teaching. On average, students with VR headsets reported higher confidence with the procedure and were more inclined to undertake it on their own. Students in Group C who used the VR station prior to assessment scored higher than the non-VR Groups (Group A, 56%; Group B, 67%; Group C 83%). Students in Group A had statistically significant results on average compared to those in Group B (t(69) = 3.003, p = 0.003), as do students in Group B compared to Group C (t(62) = 5.400, p < 0.001). Within Group C students who were given VR headsets scored higher than immersion room students. The VR headset was beneficial in providing students with a representation of how knee arthrocentesis may be conducted in the hospital setting. While VR will not replace conventional in-hospital teaching, given current technological limitations, it serves as an effective teaching aid for arthrocentesis and has many other potential applications for a wide scope of medicine and surgical training. © Crown 2024.",Knee aspiration; Medical education; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Effect of vibrotactile feedback and perception induced by transcranial magnetic stimulation over the primary motor cortex in virtual hand illusion,VRS - Virtual Reality,B,"Noninvasive brain stimulation has the potential to revolutionize the way we interact with virtual environments by inducing tactile sensations. In this study, we conducted a virtual hand illusion experiment to investigate how the induced embodiment in virtual reality using brain stimulation differs from that using traditional haptic devices. The haptic glove was used to provide vibrotactile feedback, while the noninvasive brain stimulation was achieved through transcranial magnetic stimulation (TMS) to elicit hand movement and sensation. We compared the proprioceptive drifts resulting from the changes in ownership and body representation. Both the TMS and vibrotactile methods showed a similar tendency of moving toward the virtual hand, indicating the successful induction of the illusion of embodiment. The drifts were 21.9 mm and 30.7 mm for the vibrotactile and the TMS-evoked perception, respectively. Questionnaire results about ownership significantly favored the synchronous condition of visual and haptic sensations compared to the asynchronous condition. Our results demonstrate the feasibility of using TMS-based virtual reality to enhance the illusion effect, offering new possibilities for improved interactions in virtual environments. © The Author(s) 2024.",Primary motor cortex; Proprioceptive drift; Robotic coil positioning; Transcranial magnetic stimulation; Vibrotactile feedback; Virtual hand illusion,Abstract,True,
Scopus,journalPaper,2024,Augmented reality navigation in external ventricular drain insertion—a systematic review and meta-analysis,VRS - Virtual Reality,B,"External ventricular drain (EVD) insertion using the freehand technique is often associated with misplacements resulting in unfavorable outcomes. Augmented Reality (AR) has been increasingly used to complement conventional neuronavigation. The accuracy of AR guided EVD insertion has been investigated in several studies, on anthropomorphic phantoms, cadavers, and patients. This review aimed to assess the current knowledge and discuss potential benefits and challenges associated with AR guidance in EVD insertion. MEDLINE, EMBASE, and Web of Science were searched from inception to August 2023 for studies evaluating the accuracy of AR guidance for EVD insertion. Studies were screened for eligibility and accuracy data was extracted. The risk of bias was assessed using the Cochrane Risk of Bias Tool and the quality of evidence was assessed using the Newcastle-Ottawa-Scale. Accuracy was reported either as the average deviation from target or according to the Kakarla grading system. Of the 497 studies retrieved, 14 were included for analysis. All included studies were prospectively designed. Insertions were performed on anthropomorphic phantoms, cadavers, or patients, using several different AR devices and interfaces. Deviation from target ranged between 0.7 and 11.9 mm. Accuracy according to the Kakarla grading scale ranged between 82 and 96%. Accuracy was higher for AR compared to the freehand technique in all studies that had control groups. Current evidence demonstrates that AR is more accurate than free-hand technique for EVD insertion. However, studies are few, the technology developing, and there is a need for further studies on patients in relevant clinical settings. © The Author(s) 2024.",Accuracy; Augmented reality; External ventricular drain; Neuronavigation; Systematic review; Ventriculostomy,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Design and verification of universal virtual shopping store application,VRS - Virtual Reality,B,"The aim of this study was to design the user experience for a head-mounted display (HMD)-based universal virtual shopping store by conducting a literature analysis and employing video ethnography techniques. Subsequently, a prototype was developed to validate the outcomes. Despite the growing trend of contactless shopping, with various brands launching virtual store platforms, these often lack the elements and experiences that users find satisfying. To address this gap, we analysed users’ offline shopping experiences using video ethnography. The insights gained informed the development of a prototype that adopts a universal design approach, incorporating key features and interface designs tailored for a virtual store environment. The prototype includes innovative features include tutorials, avatar-based virtual fitting, user location/orientation control, and voice guidance. It also incorporates design features such as colour customisation, high contrast, central interface placement, and visual cues for differentiating product types. To validate the prototype’s usability, a study was conducted with 30 university students in their 20s (mean age 23.3 years, SD = 1.8), revealing high levels of satisfaction with its functionality and interface design. Further, in-depth interviews revealed that the appropriate design of features and interfaces, aligned with shopping goals and intentions, significantly enhanced interest in and engagement with virtual shopping. The prototype’s key features, representative of the main outcomes of this study, provide valuable insights for the future development of related services. © The Author(s) 2024.",Universal design; User experience; Virtual reality; Virtual shopping store,Keywords,True,
Scopus,journalPaper,2024,"A comparison of balance between real and virtual environments: differences, role of visual cues and full-body avatars, a quasi-experimental clinical study",VRS - Virtual Reality,B,"Virtual rehabilitation using Virtual Reality (VR) technology is a promising novel approach to rehabilitation. However, postural responses in VR differ significantly from real life. The introduction of an avatar or visual cues in VR could help rectify this difference. An initial session was used to assess static and dynamic balance performances between VR and real life to set the reference values. A second session involved three VR conditions applied in a randomised order: i.e. full-body avatar, enhanced visual cues, or a combination of both conditions. Performances of the centre of pressure (COP) were recorded on a force plate. Seventy (70) people took part in the first session and 74 in the second. During the first session, a significant difference was observed in left static, right static and right dynamic COP distance (respectively SMD = − 0.40 [− 0.73, − 0.06], p = 0.02, − 0.33 [− 0.67, 0.00], p = 0.05, SMD = − 0.61 [− 0.95, − 0.27], p < 0.001) and a non-significant difference in the left dynamic, SMD = − 0.22 [− 0.56, 0.11], p = 0.19). During the second session it was observed that this difference was corrected mainly by reinforced visual information and to a lesser extent by the presence of a full-body avatar. Balance disruption triggered by the use of virtual reality can be offset by vertical visual information and/or by the presence of a full-body avatar. Further research is required on the effects of a full-body avatar. © The Author(s) 2024.",Avatar; Balance; Rehabilitation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Robot remote control using virtual reality headset: studying sense of agency with subjective distance estimates,VRS - Virtual Reality,B,"Mobile robots have many applications in the modern world. The autonomy of robots is increasing, but critical cases like search and rescue missions must involve the possibility of human intervention for ethical reasons and safety. To achieve effective human–robot interaction, the operator needs to have a sense of agency (SoA) over the activities of the robot. One possible way to increase one's SoA in remote control could be the use of VR technology. The remote control situation has some important features, so indicators of SoA need to be reproduced there independently. In our study, participants controlled a mobile robot using either a monitor or a VR-headset as an output device. In both cases, active control was contrasted with passive observation of the robot's movement. In each trial, participants estimated the distance traveled by the robot—a putative implicit indicator of SoA. A significant difference between subjective distance estimates was found in the active and passive conditions with the monitor, but not in the active and passive conditions with VR. The effect obtained in the monitor conditions suggests that distance estimates can be used as an implicit indicator of SoA in robot remote control. We believe that the lack of difference between the active and passive conditions in VR was caused by motion sickness due to a mismatch of visual and vestibular sensory cues, leading to a weakened SoA. © The Author(s) 2024.",Distance estimation; Intentional binding; Remote control; Sense of agency; Sense of control; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,PublicVR: a virtual reality exposure therapy intervention for adults with speech anxiety,VRS - Virtual Reality,B,"Speech anxiety, or Glossophobia, currently affects approximately 75% of the population with potentially severe negative effects on those with this condition. There are several treatments currently available with research showing that the use of Virtual Reality (VR) as a non-pharmacologic treatment can have positive effects on individuals suffering from such social phobias. However, there is a significant lack of treatments currently available for speech anxiety, even though such a large number of the population are affected by it. In this paper, we aim to contribute to efforts to improve the effects of speech anxiety through a VR intervention. Our VR solution was designed following the Exposure Therapy approach for treating social anxiety disorders. The evaluation of this work was twofold: A. to assess the ability of our solution to positively change participants’ perception of factors related to non-verbal communication contributing to anxiety toward public speaking, and B. to determine whether it is able to induce a sense of presence. We carried out an empirical evaluation study that measured participants’ self-reported anxiety level towards public speaking using the Personal Report of Public Speaking Anxiety and their perceived sense of presence using the iGroup Presence Questionnaire. Our results demonstrate the potential of VR Exposure Therapy solutions to assist towards positively changing perception of factors related to non-verbal communication skills that contribute to increasing public speaking anxiety for participants suffering from self-reported speech anxiety symptoms. Our findings are of wider importance as they contribute to ongoing efforts to improve social anxiety-related phobias. © The Author(s) 2024.",Phobias; Public speaking; Social anxiety; Speech anxiety; Virtual Reality; VR; VRET,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Enhancing music rhythmic perception and performance with a VR game,VRS - Virtual Reality,B,"This study analyzes the effect of using a virtual reality (VR) game as a complementary tool to improve users’ rhythmic performance and perception in a remote and self-learning environment. In recent years, remote learning has gained importance due to various everyday situations; however, the effects of using VR in such situations for individual and self-learning have yet to be evaluated. In music education, learning processes are usually heavily dependent on face-to-face communication with a teacher and are based on a formal or informal curriculum. The aim of this study is to investigate the potential of gamified VR learning and its influence on users’ rhythmic sensory and perceptual abilities. We developed a drum-playing game based on a tower defense scenario designed to improve four aspects of rhythmic perceptual skills in elementary school children with various levels of music learning experience. In this study, 14 elementary school children received Meta Quest 2 headsets for individual use in a 14-day individual training session. The results showed a significant increase in their rhythmical skills through an analysis of their rhythmic performance before and after the training sessions. In addition, the experience of playing the VR game and using the HMD setup was also assessed, highlighting some of the challenges of currently available affordable headsets for gamified learning scenarios. © The Author(s) 2024.",E-learning; Music theory; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,The effect of virtual reality interventions on reducing pain intensity in chronic pain patients: a systematic review,VRS - Virtual Reality,B,"The use of virtual reality (VR) for the management of chronic pain is an intriguing topic. Given the abundance of VR stuies and the numerous opportunities presented by this technology in healthcare, a systematic review that focuses on VR and its applications in chronic pain is necessary to shed light on the various modalities available and their actual effectiveness. This systematic review aims to explore the efficacy of reducing pain and improving pain management through CR interventions for people suffering from chronic pain. Following the PRISMA guidelines, data collection was conducted between December 2020 and February 2021 from the following databases: Cochrane Evidence, JSTOR, Science Direct, PubMed Medline, PubMed NIH, Springer Link, PsychNET, PsychINFO - OVID and PsycARTICLES, Wiley Online Library, Web of Science, ProQuest - MEDLINE®, Sage Journals, NCBI – NLM catalog, Medline OVID, Medline EBSCO, Oxford Handbooks Online, PSYNDEX OVID, Google Scholar. Seventeen articles were included in the qualitative synthesis. Our results highlight that VR interventions, on a global scale, lead to an improvement in pain-related variables, particularly in reducing pain intensity. However, the analyzed articles vary significantly, making them challenging to compare. Future studies could focus on specific types of VR interventions to reduce heterogeneity and conduct a more specific analysis. In conclusion, VR interventions have demonstrated their validity and adaptability as a method for managing chronic pain. Nevertheless, further studies are needed to delve into the various categories of VR interventions in more detail. © The Author(s) 2024.",Adult; Chronic pain; Pain; Pain management; Virtual reality; VR,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Not just cybersickness: short-term effects of popular VR game mechanics on physical discomfort and reaction time,VRS - Virtual Reality,B,"Uncomfortable sensations that arise during virtual reality (VR) use have always been among the industry’s biggest challenges. While certain VR-induced effects, such as cybersickness, have garnered a lot of interest from academia and industry over the years, others have been overlooked and underresearched. Recently, the research community has been calling for more holistic approaches to studying the issue of VR discomfort. Focusing on active VR gaming, our article presents the results of two user studies with a total of 40 participants. Incorporating state-of-the-art VR-specific measures (the Simulation Task Load Index—SIM-TLX, Cybersickness Questionnaire—CSQ, Virtual Reality Sickness Questionnaire—VRSQ) into our methodology, we examined workload, musculoskeletal discomfort, device-related discomfort, cybersickness, and changes in reaction time following VR gameplay. Using a set of six different active VR games (three per study), we attempted to quantify and compare the prevalence and intensity of VR-induced symptoms across different genres and game mechanics. Varying between individuals, as well as games, the diverse symptoms reported in our study highlight the importance of including measures of VR-induced effects other than cybersickness into VR gaming user studies, while questioning the suitability of the Simulator Sickness Questionnaire (SSQ)—arguably the most prevalent measure of VR discomfort in the field—for use with active VR gaming scenarios. © The Author(s) 2024.",Cybersickness; Discomfort; Reaction time; User studies; VR game mechanics; VR gaming; Workload,Abstract,True,
Scopus,journalPaper,2024,Design and emotional responses: is there coherence between what is said and what is felt? A study using biofeedback and virtual reality,VRS - Virtual Reality,B,"Identifying users’ experience when using products is one of the major challenges for design. Analyzing users’ psychophysiological reactions to an experience using biofeedback can produce more reliable results than using subjective evaluations, such as structured interviews and questionnaires. Two case studies were conducted to identify emotions users actually felt and to check whether there is some correspondence with what they reported after using two computational systems. The first system investigated users’ emotions during training on a vehicle driving simulator, and the second analyzed the emotions experienced during a car racing game, both in a virtual reality environment. User’s opinions about their emotional state were obtained using self-report techniques (using the Geneva Emotions Wheel—GEW and Positive and Negative Affective Schedule—PANAS questionnaires) and applying EEG (brain activity with Frontal Alpha Asymmetry Index—FAAI) and infrared thermography (facial thermograms). The training experiment presented the greater concordance between the psychophysiological and the self-report responses. Results evidenced the importance of undertaking multimodal studies in design research to determine users’ emotional experiences in a virtual reality context. © The Author(s) 2024.",Emotion; Game design; Infrared thermography; Psychophysiological aspects; Simulator; User experience,Title_Abstract,True,
Scopus,journalPaper,2024,Neurophysiological evidence for the overview effect: a virtual reality journey into space,VRS - Virtual Reality,B,"The Overview Effect is a complex experience reported by astronauts after viewing Earth from space. Numerous accounts suggest that it leads to increased interconnectedness to other human beings and environmental awareness, comparable to self-transcendence. It can cause fundamental changes in mental models of the world, improved well-being, and stronger appreciation of, and responsibility for Earth. From a cognitive perspective, it is closely linked to the emotion of awe, possibly triggered by the overwhelming perceived vastness of the universe. Given that most research in the domain focuses on self-reports, little is known about potential neurophysiological markers of the Overview Effect. In the experiment reported here, participants viewed an immersive Virtual Reality simulation of a space journey while their brain activity was recorded using electroencephalography (EEG). Post-experimental self-reports confirmed they were able to experience the Overview Effect in the simulated environment. EEG recordings revealed lower spectral power in beta and gamma frequency bands during the defining moments of the Overview Effect. The decrease in spectral power can be associated with reduced mental processing, and a disruption of known mental structures in this context, thereby providing more evidence for the cognitive effects of the experience. © The Author(s) 2024.",Awe; EEG; Overview effect; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The effect of distance on audiovisual temporal integration in an outdoor virtual environment,VRS - Virtual Reality,B,"In this study, we explore the influence of stimulus distance on human tolerance for (physical) asynchronies in virtual reality (VR). A repeated audiovisual (AV) stimulus with sound and light bursts was presented to the participants in an outdoor virtual environment (VE) using a head-mounted display (HMD). The investigation focused on quantifying the point of subjective simultaneity (PSS) for both visual and auditory stimuli. A synchrony judgment method (SJ-3) was used for 11 stimulus onset asynchronies (SOA) and five egocentric distances from 10 m up to 50 m with 10 m increments. The data analysis showed negative PSS values that decreased with distance, resulting in a negative slope (-3 ms/m) of the regression line between PSS values and simulated distances. In contrast to the recent study conducted in the indoor VE, we conclude that the presented study in the outdoor VE does not incorporate a distance compensation mechanism and refutes the hypothesis of an ‘implicit estimation’ of sound-arrival time. The reasons behind the observed contrast are discussed in this paper. Moreover, the negative slope of the regression line (−3 ms/m) is similar to previous research, concluding that there is a temporal integration of auditory-visual information within human neural processes without distance compensation. © The Author(s) 2024.",Audiovisual and temporal integration; Multisensory perception; Synchrony judgements; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Adoption of immersive-virtual reality as an intrinsically motivating learning tool in parasitology,VRS - Virtual Reality,B,"Veterinary parasitology is study of parasitic diseases, treatment and prevention. It is a major component of animal health courses due to impacts parasites have on production and companion animals. Extant tertiary education in parasitology typically involves theory sessions coupled with practical experience. In this study we propose tertiary parasitology teaching would be enhanced through adoption of immersive Virtual Reality (I-VR) as an intrinsically motivating learning tool to complement their studies. To evaluate this adoption, a custom I-VR parasitology game was developed that tertiary veterinary science students experienced (n = 109), with feedback assessed using the Hedonic-Motivation System Adoption Model (HMSAM). HMSAM proved appropriate for measuring student’s hedonistic and utilitarian perspectives of I-VR experience with perceived ease of use, perceived usefulness, joy, ability to control, immersion levels and intention to use displaying significant positive relationships in derived model. However, in a departure from similar studies, the curiosity construct was not a useful predictor of intention to use in this context of a scaffolded, instructional application. This study highlights suitability of I-VR and provides a statistically robust evaluation method using a modified HMSAM to evaluate acceptance, usefulness, and ease of use of I-VR in tertiary education. © The Author(s) 2024.",HMSAM; Immersive; Immersive virtual reality; Motivation; Parasitology; Quantitative; SEM,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Virtual reality based rehabilitation in adults with chronic neck pain: a systematic review and meta-analysis of randomized clinical trials,VRS - Virtual Reality,B,"Chronic neck pain is one of the most frequent musculoskeletal disorders, with high prevalence worldwide. Rehabilitation is an essential component of therapeutic strategy. Virtual reality based rehabilitation (VRBR) is a powerful distraction technique that could be beneficial for chronic neck pain patients. The objective of this systematic review was to analyse the effectiveness of VRBR in chronic neck pain treatment. We followed the PRISMA guidelines and used four databases (CINAHL, Medline (Via PubMed), Scopus and Web of Science) from their inception to August 2023. Eligibility criteria were established using PICOS. Methodological quality was evaluated with the Downs and Black scale and the risk of bias with the Revised Cochrane risk-of-bias tool. The meta-analysis was performed using the RevMan software. Six studies were included in the systematic review and the meta-analysis. We observed significant differences in favour of VRBR for pain intensity (SMD = − 0.46; 95% CI = − 0.74, − 0.19; p = 0.001), disability (MD = − 2.84; 95% CI = − 4.23, − 1.45; p < 0.0001), global perceived effect (MD = 0.49; 95% CI = 0.25, 0.72; p < 0.0001) and patient satisfaction (MD = 0.62; 95% CI = 0.38, 0.86; p < 0.00001). However, at short-term follow-up significant differences were only obtained for disability (MD = − 3.52; 95% CI = − 5.85, − 1.20; p = 0.003). VRBR can significantly improve pain intensity, disability, global perceived effect and patient satisfaction. The small number of articles included in the analysis is a limitation, even considering the good methodological quality of these studies. Investigating the effects of VRBR on mid and long-term follow-up and exploring different types of VR are needed. PROSPERO database, registration number ID: CRD42020222129. © The Author(s) 2024.",Chronic neck pain; Disability; Physical therapy; Rehabilitation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Enhancing hand-object interactions in virtual reality for precision manual tasks,VRS - Virtual Reality,B,"The realism and immersion of Virtual Reality (VR) experiences depend on the quality of interactions between the virtual hand and virtual objects. The current drawbacks, such as seemingly artificial hand-object interactions and erratic post-collision behaviors of both virtual objects and the virtual hand, curtail the effectiveness of VR in tasks requiring precise manipulation, precluding sustained and successful adoption of VR in precision manual tasks. To address these limitations, we advocate a strategic approach across three related domains: (1) Developing a sufficiently realistic virtual hand model (i.e., a set of rigid bodies or deformable meshes) that can implement the complex movements of a biological hand. (2) Exploiting synergistic patterns of multi-digit motion and contact forces revealed by research on neuroscience, psychophysics, and manual actions to develop hand-object collision handling algorithms. (3) Implementing seamless and fluid releases of whole-hand virtual grasps, especially involving complex grasps and in-hand manipulation tasks. This article explores various aspects of virtual grasping that go beyond traditional physics simulations, proposing innovative solutions to overcome technical barriers across these domains. © The Author(s) 2024.",Collision detection; Collision handling; Grasping; Haptic-free VR; Haptics; Interpenetration; Manipulating virtual object; Multi-digit motion; Object release,Title_Abstract,True,
Scopus,journalPaper,2024,Training transfer validity of virtual reality simulator assessment,VRS - Virtual Reality,B,"This study utilises computer-based simulations to explore the transfer effects of competency training in maritime education, addressing the current lack of research on their transferability to real-world scenarios. The research explores the accuracy of procedural knowledge assessment using virtual reality (VR), positing that head-mounted display (HMD) VR offers stronger concurrent validity through training transfer measures than 3D desktop VR. This is evaluated by regression on a training transfer condition. It also investigates motivation’s influence on training transfer and the regression model of this relationship. Fifteen marine engineering students were divided into two experimental groups using 3D desktop VR and HMD VR systems, with eight experts in the control group. The students had previously received traditional lecture-based instruction and were given practical training using a 2D desktop simulator in the same scenario as in the VR treatment and in the training transfer condition. The ANCOVA design experiment involved two levels of technical immersion before the operation of real-life equipment. Neither technical immersion nor expertise level as independent variables were found to have a significant effect in the relationship of the assessment predicting the training transfer. The direct relationship was significant (R2adj = 0.436) and further analysed with the influence of motivation, resulting in a moderation model with a decent effect size (R2 = 0.740). Based on these findings, we can infer that both types of VR simulations used for assessment demonstrate concurrent validity in predicting real-life performance before we discuss and define the characteristics of the observed transfer according to theory. © The Author(s) 2024.",Computer-Based Simulation; Experimental Design; Marine Engineering; Maritime Education and Training; Simulator Training and Assessment; Training Transfer,Title_Abstract,True,
Scopus,journalPaper,2024,Unlocking the potential of virtual reality to expand treatment frontiers for bulimia nervosa: a pilot study to explore the impact of virtual reality-enhanced cognitive-behavioral therapy,VRS - Virtual Reality,B,"The primary objective of this study is to assess the efficacy of a Virtual Reality (VR) intervention when compared to an integrated multimodal medically managed Inpatient Program (IP) in a cohort of 24 female patients diagnosed with Bulimia Nervosa (BN). Psychological measures (i.e., EDI-2) were assessed at three points: pre-treatment, post-treatment, and at 1-month follow-up. Behavioral measures (i.e., BMI) were evaluated at 6 different time points, instead (i.e., pre-treatment, post-treatment, 3, 6, 9, and 12 months from the discharge date). The VR treatment was more effective in improving the EDI subscales EDI-DT (i.e., drive for thinness) and EDI-BU (i.e., binging-purging behaviors). In particular, patients in the VR condition showed a reduced EDI-BU score at 1-month follow-up and post-test in comparison to the pre-test, as well as a lower EDI-DT score at 1-month follow-up compared to the pre-test. Conversely, no significant changes were noted in the IP group for either subscale. Regarding the behavioral measures, the group undergoing the VR condition reported the maintenance of the BMI in the long term compared to the IP. Specifically, in the VR group BMI decreased from the pre- to post-test, and from the pre-test to the 12-month follow-up. In the IP group, BMI improved from the pre- to the post-test, and from the pre-test to the 12-month follow-up. However, a relapse pattern was observed in the IP condition during the follow-up period, with a significant BMI increase from the post-test to the 9-month follow-up, from the 3 to the 9-month follow-up, from the 6 to the 9-month follow-up, and a decrease of BMI between the 9 and the 12-month follow-up. In conclusion, these results suggest that integrating VR treatment into the care of individuals with BN could enhance both immediate and sustained treatment outcomes. This may offer valuable insights for future studies to expand and delve deeper into the field of EDs. © The Author(s) 2024.",Allocentric Lock Theory; Body Image; Body Memory; Bulimia Nervosa; Eating Disorder; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Multisensory experiences of affective touch in virtual reality enhance engagement, body ownership, pleasantness, and arousal modulation",VRS - Virtual Reality,B,"When engaging in physical contact, our emotional response hinges not only on the nuanced sensory details and the receptive properties of the skin but also on contextual cues related to the situation and interpersonal dynamics. The consensus is that the nature of the affective interactive experience in social touch is shaped by a combination of ascending, C-tactile (CT) afferents mediated somatosensory information, and modulatory, top-down information. The question we pose here is whether, in the absence of somatosensory input, multisensory cues alone can suffice to create a genuinely pleasant, authentic, and engaging experience in virtual reality. The study aims to explore how affective touch is perceived in immersive virtual environments, considering varied social norms in neutral settings or settings like a physiotherapy room where the touch provider is a healthcare professional. We conducted an experiment with 58 male and female healthy adults, where we employed a within-group counterbalanced design featuring two factors: (a) visuo-tactile affective touch, and (B) visual-only affective touch. Findings, drawn from questionnaires and collected physiological data, shed light on how contextual factors influence implicit engagement, self-reported embodiment, co-presence, as well as the perceived realism and pleasantness of the touch experience. Our findings, in line with the literature, indicate that to experience the advantages of touch in immersive virtual worlds, it is essential to incorporate haptic feedback, as depending solely on visual input may not be adequate for fully realising the optimal benefits of interpersonal touch. Furthermore, in contradiction with our hypothesis, a less ambiguous context (specifically, the physiotherapy room and touch from a physiotherapist) is not linked to heightened touch pleasantness. © The Author(s) 2024.",Affective touch; Copresence; Mediated touch; Social virtual reality; Virtual embodiment,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Virtual reality: towards a better prediction of full body illusion — a mediation model for healthy young women,VRS - Virtual Reality,B,"The application of advanced embodied technologies, particularly virtual reality (VR), has been suggested as a means to induce the full-body illusion (FBI). This technology is employed to modify different facets of bodily self-consciousness, which involves the sense of inhabiting a physical form, and is influenced by cognitive inputs, affective factors like body dissatisfaction, individual personality traits and suggestibility. Specifically, VR-based Mirror Exposure Therapies are used for the treatment of anorexia nervosa (AN). This study aims to investigate whether the “Big Five” personality dimensions, suggestibility, body dissatisfaction and/or body mass index can act as predictors for FBI, either directly or acting as a mediator, in young women of similar gender and age as most patients with AN. The FBI of 156 healthy young women immersed in VR environment was induced through visuomotor and visuo-tactile stimulations, and then assessed using the Avatar Embodiment Questionnaire, comprising four dimensions: Appearance, Ownership, Response, and Multi-Sensory. Data analysis encompassed multiple linear regressions and SPSS PROCESS macro’s mediation model. The findings revealed that the “Big Five” personality dimensions did not directly predict FBI in healthy young women, but Openness to experience, Agreeableness, and Neuroticism exerted an indirect influence on some FBI components through the mediation of suggestibility. © The Author(s) 2024.",Anorexia nervosa; Body dissatisfaction; Eating disorders; Embodiment; Full body ownership illusion; Personality; Suggestibility; Virtual reality exposure,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Enabling personalized VR experiences: a framework for real-time adaptation and recommendations in VR environments,VRS - Virtual Reality,B,"The personalization of user experiences through recommendation systems has been extensively explored in Internet applications, but this has yet to be fully addressed in Virtual Reality (VR) environments. The complexity of managing geometric 3D data, computational load, and natural interactions poses significant challenges in real-time adaptation in these immersive experiences. However, tailoring VR environments to individual user needs and interests holds promise for enhancing user experiences. In this paper, we present Virtual Reality Environment Adaptation through Recommendations (VR-EAR), a framework designed to address this challenge. VR-EAR employs customizable object metadata and a hybrid recommendation system modeling implicit user feedback in VR environments. We utilize VR optimization techniques to ensure efficient performance. To evaluate our framework, we designed a virtual store where product locations dynamically adjust based on user interactions. Our results demonstrate the effectiveness of VR-EAR in adapting and personalizing VR environments in real time. domains. © The Author(s) 2024.",E-commerce; Recommendation systems; Virtual reality; Virtual shopping,Abstract_Keywords,True,
Scopus,journalPaper,2024,Phone-based virtual exploration of green space increases positive affect in students with test anxiety: a pre-post experimental study with qualitative insights,VRS - Virtual Reality,B,"Nature confers a host of benefits including recovering from stress, replenishing attentional resources, improving mood, and decreasing negative thinking. Virtual nature, i.e. exposure to natural environments through technological means, has proven to also be efficacious in producing benefits, although more limitedly. Previous studies with immersive virtual reality with university students have shown that one bout of virtual nature can reduce negative affect in students with high test anxiety and can reduce feeling of worry and panic after several weeks of daily exposure. The present study aimed at replicating the effect of one bout of virtual nature on affect and extend it to cognition in a sample of university students with different levels of test anxiety. An inexpensive goggle + phone apparatus was utilized and the one bout of virtual nature was self-administered. 48 university students took part in the study, randomized between viewing a 360 degrees video of nature or of an urban environment. They completed the Positive and Negative Affect Schedule and the Cognitive Reflection Test before and after the exposure to the virtual environments and responded to open-ended questions about their experience of the intervention. Results showed improvements in positive affect in students with higher anxiety were obtained in the nature condition, no other effects were found. Qualitative appraisal indicated that participants in the nature condition felt more relaxed and focused, however the technical issues were detrimental to the benefits. In conclusion one bout of virtual nature could support students with higher test anxiety when confronted with examinations. © The Author(s) 2024.",Affect; Attention restoration; Nature benefits; Stress recovery; Test anxiety; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Usability and user expectations of a HoloLens-based augmented reality application for learning clinical technical skills,VRS - Virtual Reality,B,"The application of augmented reality in training health science students is increasingly widespread. The aim of this work was to assess the usability and user expectations of an augmented reality application for smart glasses (Microsoft HoloLens) that can be used to train on four invasive procedures (i.e. intramuscular injection, nasogastric tube insertion, endotracheal intubation and suctioning via tracheostomy tube). A descriptive study was conducted with nursing students from three Spanish universities. Participants answered a questionnaire to assess the use of the ARSim2care application. This application offers the possibility of visualizing the internal anatomical structures during the training of the clinical technical skills for the performance of the mentioned invasive techniques. The questionnaire included demographic data, the System Usability Scale and questions about the user expectations in relation to learning with the use of augmented reality. In total, 61 participants responded to the questionnaire after using the ARSim2care application. The mean score of the System Usability Scale was 73.15 (standard deviation: 15.04) and 62.4% (n = 38) of the participants considered their experience with the application as excellent or good. In relation to user expectations, more than 90% of students indicated that the use of the application could improve their motivation and stimulation in learning, their content retention and their anatomical understanding. The developed ARSim2care application for Microsoft HoloLens showed a high level of usability and acceptance as a learning tool for training certain clinical procedures by visualizing the internal structures of the body. © The Author(s) 2024.",Academic training; Augmented reality; Clinical skills; HoloLens; Invasive procedures; Medical students; Nursing students,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Effects of cybersickness mitigation methods on behavior: a comparative study based on the skill–rule–knowledge model,VRS - Virtual Reality,B,"Cybersickness (CS) is a pressing issue in virtual reality (VR) systems. While various mitigation methods (MMs) have been developed to counteract CS, their effects on human behavior remain largely unexplored, raising concerns about their potential applications. Using Jens Rasmussen’s skill–rule–knowledge (SRK) model as a framework, our study investigated the effects of two widely adopted MMs—dynamic field of view and dynamic blurring—in VR. We compared these methods to a baseline condition where no MM was applied. We designed three VR tasks that align with the behavioral levels of the SRK model. In a within-subject study (N = 22), participants completed each task using these MMs. We measured task performance, CS symptoms, and locomotion control. Additionally, qualitative feedback was collected. Our results revealed that neither MM significantly alleviated CS across different VR scenarios. Furthermore, while some participants found MMs helpful, a larger portion reported visual hindrances, and a significant performance drop was measured in the skill-based task. More critically, participants indicated behavioral adaptations in response to the MMs, including changes in locomotion strategies and viewing behavior. Potential causes and implications were discussed. In conclusion, MMs offer promise, but their application necessitates a nuanced understanding of their impacts. We recommend a context-sensitive approach when designing and integrating MMs, prioritizing both maximizing CS mitigation and minimizing interference with the natural behaviors of users. © The Author(s) 2024.",Cybersickness; Human factors; Mitigation methods; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Evaluation of visual, auditory, and olfactory stimulus-based attractors for intermittent reorientation in virtual reality locomotion",VRS - Virtual Reality,B,"In virtual reality, redirected walking (RDW) enables users to stay within the tracking area while feeling that they are traveling in a virtual space that is larger than the physical space. RDW uses a visual attractor to the user’s sight and scene manipulation for intermittent reorientation. However, repeated usage can hinder the virtual world immersion and weaken the reorientation performance. In this study, we propose using sounds and smells as alternative stimuli to draw the user’s attention implicitly and sustain the attractor’s performance for intermittent reorientation. To achieve this, we integrated visual, auditory, and olfactory attractors into an all-in-one stimulation system. Experiments revealed that the auditory attractor caused the fastest reorientation, the olfactory attractor induced the widest angular difference, and the attractor with the combined auditory and olfactory stimuli induced the largest angular speed, keeping users from noticing the manipulation. The findings demonstrate the potential of nonvisual attractors to reorient users in situations requiring intermittent reorientation. © The Author(s) 2024.",Attractor; Auditory; Multi-modality; Olfactory; Redirected walking; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The geometry of the vergence-accommodation conflict in mixed reality systems,VRS - Virtual Reality,B,"Mixed reality technologies, such as virtual (VR) and augmented (AR) reality, present promising opportunities to advance education and professional training due to their adaptability to diverse contexts. Distortions in the perceived distance in such mediated conditions, however, are well documented and have imposed nontrivial challenges that complicate and limit transferring task performance in a virtual setting to the unmediated reality (UR). One potential source of the distance distortion is the vergence-accommodation conflict—the discrepancy between the depth specified by the eyes’ accommodative state and the angle at which the eyes converge to fixate on a target. The present study involved the use of a manual pointing task in UR, VR, and AR to quantify the magnitude of the potential depth distortion in each modality. Conceptualizing the effect of vergence-accommodation offset as a constant offset to the vergence angle, a model was developed based on the stereoscopic viewing geometry. Different versions of the model were used to fit and predict the behavioral data for all modalities. Results confirmed the validity of the conceptualization of vergence-accommodation as a device-specific vergence offset, which predicted up to 66% of the variance in the data. The fitted parameters indicate that, due to the vergence-accommodation conflict, participants’ vergence angle was driven outwards by approximately 0.2°, which disrupted the stereoscopic viewing geometry and produced distance distortion in VR and AR. The implications of this finding are discussed in the context of developing virtual environments that minimize the effect of depth distortion. © The Author(s) 2024.",Augmented reality; Depth perception; Manual pointing; Vergence-accommodation conflict; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Replicating outdoor environments using VR and ambisonics: a methodology for accurate audio-visual recording, processing and reproduction",VRS - Virtual Reality,B,"This paper introduces a methodology tailored to capture, post-process, and replicate audio-visual data of outdoor environments (urban or natural) for VR experiments carried out within a controlled laboratory environment. The methodology consists of 360∘ video and higher order ambisonic (HOA) field recordings and subsequent calibrated spatial sound reproduction with a spherical loudspeaker array and video played back via a head-mounted display using a game engine and a graphical user interface for a perceptual experimental questionnaire. Attention was given to the equalisation and calibration of the ambisonic microphone and to the design of different ambisonic decoders. A listening experiment was conducted to evaluate four different decoders (one 2D first-order ambisonic decoder and three 3D third-order decoders) by asking participants to rate the relative (perceived) realism of recorded outdoor soundscapes reproduced with these decoders. The results showed that the third-order decoders were ranked as more realistic. © The Author(s) 2024.",Ambisonics; Decoder equalisation; Perceptual evaluation; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,Analysis of MR–VR tele-operation methods for legged-manipulator robots,VRS - Virtual Reality,B,"The development of immersive technologies in recent years has facilitated the control and execution of tasks at a high level of complexity in robotic systems. On the other hand, exploration and manipulation tasks in unknown environments have been one of the main challenges in search and rescue (SAR) robotics. Due to the complexity and uncertainty involved in autonomous manipulation tasks in unstructured environments, these are usually tele-operated initially. This article addresses a comparative study between Mixed Reality (MR—Hololens) and Virtual Reality (VR—HTC-Vive) methods for teleoperating legged-manipulator robots in the context of search and rescue. For this purpose, a teleoperation robotics method was established to address the comparison, developing VR–MR interfaces with the same contextualization and operational functionality for mission management and robot control of a robotic set composed of a quadrupedal robot equipped with a 6 degrees of freedom (6DoF) manipulator, by a user using hand gestures. A set of metrics is proposed for the comparative evaluation of the interfaces considering parameters that allow analyzing operability in the context of the mission (latencies, physical parameters of the equipment, etc.), as well as from the aspect of operator performance (required training, confidence levels, etc.). The experimental phase was conducted using both on-site and remote operations to evaluate and categorize the advantages and disadvantages of each method. © The Author(s) 2024.",Legged-manipulator robot; Mixed reality; Quadruped robot; ROS; Search and rescue; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Using attentional guidance methods in virtual reality laboratories reduces students’ cognitive load and improves their academic performance,VRS - Virtual Reality,B,"Learning in virtual reality laboratories (VR labs) has become an important method in experimental teaching but can increase individuals’ cognitive load compared with traditional laboratories. This study analysed the effect of introducing an attentional guidance mechanism into a VR lab on students’ cognitive load and academic performance. We designed and developed two VR labs, one with and one without this attentional guidance stimulus (a 3D yellow arrow). A quasi-experimental design was adopted, and the data obtained were analysed using one-way ANOVA and linear regression. The experiment was conducted with 80 students majoring in digital media art at two universities. The results indicated that the students in the VR lab with the attentional guidance mechanism included exhibited lower cognitive load and higher academic performance than the control group. The regression analyses revealed that cognitive load negatively predicted learning outcomes; that is, academic performance improved as cognitive load decreased. In conclusion, as VR labs are increasingly used in education, supplementing them with attentional guidance stimuli can improve students’ academic performance by reducing their cognitive load. © The Author(s) 2024.",Academic performance; Attentional guidance; Cognitive load; Virtual reality lab,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Behavioral intention, perception and user assessment in an immersive virtual reality environment with CFD simulations",VRS - Virtual Reality,B,"This study explores technology acceptance, perception and user assessment of an immersive virtual reality environment with computational fluid dynamics simulations in engineering education. 57 participants from three different institutions tested the virtual reality application. Partial least squares structural equation modeling and interferential statistics were performed to predict and assess interrelations among constructs. Results show that the learning value, content value, intrinsic motivation and personal innovativeness are underlying factors behind students’ intention to use virtual reality. Pair-wise analysis indicates that users’ perceptions matter and positively affect their attitudes. In addition, the virtual reality application helps students perform significantly better in the post-knowledge test. Findings also highlight that prior experience and interest can affect students’ attitudes and behavioral intentions to accept the virtual reality application in education. Our study can guide lecturers and developers to achieve on-target immersive virtual reality learning environments in higher education. © The Author(s) 2024.",Assessment; Computational fluid dynamics; Immersive learning; Technology acceptance; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Head-locked, world-locked, or conformal diminished-reality? An examination of different AR solutions for pedestrian safety in occluded scenarios",VRS - Virtual Reality,B,"Many collisions between pedestrians and cars are caused by poor visibility, such as occlusion by a parked vehicle. Augmented reality (AR) could help to prevent this problem, but it is unknown to what extent the augmented information needs to be embedded into the world. In this virtual reality experiment with a head-mounted display (HMD), 28 participants were exposed to AR designs, in a scenario where a vehicle approached from behind a parked vehicle. The experimental conditions included a head-locked live video feed of the occluded region, meaning it was fixed in a specific location within the view of the HMD (VideoHead), a world-locked video feed displayed across the street (VideoStreet), and two conformal diminished reality designs: a see-through display on the occluding vehicle (VideoSeeThrough) and a solution where the occluding vehicle has been made semi-transparent (TransparentVehicle). A Baseline condition without augmented information served as a reference. Additionally, the VideoHead and VideoStreet conditions were each tested with and without the addition of a guiding arrow indicating the location of the approaching vehicle. Participants performed 42 trials, 6 per condition, during which they had to hold a key when they felt safe to cross. The keypress percentages and responses from additional questionnaires showed that the diminished-reality TransparentVehicle and VideoSeeThrough designs came out most favourably, while the VideoHead solution caused some discomfort and dissatisfaction. An analysis of head yaw angle showed that VideoHead and VideoStreet caused divided attention between the screen and the approaching vehicle. The use of guiding arrows did not contribute demonstrable added value. AR designs with a high level of local embeddedness are beneficial for addressing occlusion problems when crossing. However, the head-locked solutions should not be immediately dismissed because, according to the literature, such solutions can serve tasks where a salient warning or instruction is beneficial. © The Author(s) 2024.",Assisted reality; Augmented reality; Diminished reality; Local presence; Pedestrian safety; Virtual reality experiment,Abstract_Keywords,True,
Scopus,journalPaper,2024,The presence of an avatar can reduce cybersickness in Virtual Reality,VRS - Virtual Reality,B,"Virtual Reality (VR) applications are increasingly being utilized for research, healthcare, and education. Despite their benefits, many VR users report motion sickness-like sensations (cybersickness), such as headache, disorientation, or nausea. Previous studies suggest that the sense of presence (“being there”) in the virtual world may contribute to the severity of cybersickness; however, results have been contradictory, with some studies reporting a negative and some reporting a positive relationship between the two. The goal of the current study was to further investigate how presence and cybersickness are related. Participants (N = 54) were exposed to a VR scene presented on a head-mounted display showing a 15-minute-long passive movement through space. The level of presence was manipulated by including an avatar (astronaut suit with hand-tracking) or no avatar in the virtual environment. Results showed that the avatar group reported significantly less severe cybersickness compared to the no-avatar group. We also found significant, negative correlations between some of the presence metrics (immersion, sensory fidelity) and cybersickness, indicating that cybersickness severity decreased as the level of presence increased. These findings suggest that more immersive VR experiences using an avatar may potentially reduce the risk of experiencing cybersickness. © The Author(s) 2024.",Avatar; Cybersickness; Field dependence; Motion sickness; Presence; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Immersive analytics with augmented reality in meteorology: an exploratory study on ontology and linked data,VRS - Virtual Reality,B,"Although Augmented Reality (AR) has been extensively studied in supporting Immersive Analytics (IA), there are still many challenges in visualising and interacting with big and complex datasets. To deal with these datasets, most AR applications utilise NoSQL databases for storing and querying data, especially for managing large volumes of unstructured or semi-structured data. However, NoSQL databases have limitations in their reasoning and inference capabilities, which can result in insufficient support for certain types of queries. To fill this gap, we aim to explore and evaluate whether an intelligent approach based on ontology and linked data can facilitate visual analytics tasks with big datasets on AR interface. We designed and implemented a prototype of this method for meteorological data analytics. An experiment was conducted to evaluate the use of a semantic database with linked data compared to a conventional approach in an AR-based immersive analytics system. The results significantly highlight the performance of semantic approach in helping the users analysing meteorological datasets and their subjective appreciation in working with the AR interface, which is enhanced with ontology and linked data. © The Author(s) 2024.",Augmented Reality; Immersive analytics; Linked data; Meteorology; Ontology,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Effects of vection type and postural instability on cybersickness,VRS - Virtual Reality,B,"This study directly compared the novel unexpected vection hypothesis and postural instability-based explanations of cybersickness in virtual reality (VR) using head-mounted displays (HMD) for the first time within a commercial VR game. A total of 40 participants (19 males and 21 females) played an HMD-VR game (Aircar) for up to 14 min, or until their first experience of cybersickness. Based on their self-reports, 24 of these participants were classified as being ‘sick’ during the experiment, with the remainder being classified as ‘well’. Consistent with the unexpected vection hypothesis, we found that: (1) ‘sick’ participants were significantly more likely to report unexpected vection (i.e., an experience of self-motion that was different to what they had been expecting), and (2) sickness severity increased (exponentially) with the strength of any unexpected (but not expected) vection. Our results also supported the predictions of postural instability theory, finding that the onset of cybersickness was typically preceded by an increase in participants’ postural instability. However, when both sway and vection measures were combined, only unexpected vection was found to significantly predict the occurrence of sickness. These findings highlight the importance of unusual vection experiences and postural instability in understanding cybersickness. However, they suggest that developers should be able to make use of expected experiences of vection to safely enhance HMD-VR. © The Author(s) 2024.",Cybersickness; Head-mounted display; Motion sickness; Postural instability; Vection; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Continuance intention toward VR games of intangible cultural heritage: A stimulus-organism-response perspective,VRS - Virtual Reality,B,"Virtual reality (VR) games have become a popular method to preserve and transmit intangible cultural heritage in recent years. However, empirical studies pertaining to motivations behind the continuance intention to play VR games featuring intangible cultural heritage have been limited. The objective of this study focuses on answering an essential question: what factors influence user’s continuance intention to play intangible cultural heritage VR games? Both Stimulus-Organism-Response Theory and Technology Acceptance Model (TAM) are considered to develop twelve hypotheses and build the research framework. A survey of 190 respondents was conducted, and the results were analyzed by using PLS-SEM. The results show that visual attractiveness, interactivity, and immersion are significant indicators in measuring users’ continuance intention to play. Additionally, perceived usefulness, perceived ease of use, and perceived enjoyment of VR games positively influence their continuance intention. This study enriches the research of intangible cultural heritage VR games. It also provides theoretical implications for scholars and design strategies for VR developers and designers. © The Author(s) 2024.",Continuance intention; Intangible cultural heritage; S-O-R theory; Structural equation model; VR games,Abstract,True,
Scopus,journalPaper,2024,Testing memory of a VR environment: comparison with the real environment and 2D pictures,VRS - Virtual Reality,B,"In recent years, there has been a growing trend in cognitive psychology research towards recreating experimental situations in virtual reality (VR). VR settings are thought to have higher ecological validity than laboratory settings using digital, two-dimensional (2D) pictures. Some studies have shown cognitive performance in VR settings to follow that of the real world. However, other studies obtained controversial results. The present study tested the memory performance of three groups of participants who were exposed to the same environment (a room) through different modalities: in real life, in VR, and through 2D pictures. The results highlighted that participants who were exposed to the target room in real life had an overall better memory performance, compared to participants who saw the room in VR or through 2D pictures. On the other hand, no differences in memory performance emerged between the VR and 2D picture groups, except for the non-suggestive verbal task. The results suggest that future research should be careful in assuming that performance in VR settings is comparable to real life and that VR is more ecological than traditional 2D media. © The Author(s) 2024.",Suggestibility; Virtual environments; Virtual reality; Visual memory,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Correction to: Cybersickness with passenger VR in the aircraft: Influence of turbulence and VR content (Virtual Reality, (2024), 28, 2, (112), 10.1007/s10055-024-01008-w)",VRS - Virtual Reality,B,"In the Original publication, Figs. 1, 2, 3, 4 are wrong. The correct figures are replaced, and the original publication has been corrected. © The Author(s) 2024.",,Title,True,Duplicate
Scopus,journalPaper,2024,Social cognition training using virtual reality for people with schizophrenia: a scoping review,VRS - Virtual Reality,B,"To date, many interventions for social cognition have been developed. Nevertheless, the use of social cognition training with virtual reality (SCT-VR) in schizophrenia is a recent field of study. Therefore, a scoping review is a suitable method to examine the extent of existing literature, the characteristics of the studies, and the SCT-VR. Additionally, it allows us to summarize findings from a heterogeneous body of knowledge and identify gaps in the literature favoring the planning and conduct of future research. The aim of this review was to explore and describe the characteristics of SCT-VR in schizophrenia. The searched databases were MEDLINE, PsycInfo, Web of Science, and CINAHL. This scoping review considered experimental, quasi-experimental, analytical observational and descriptive observational study designs. The full text of selected citations was assessed by two independent reviewers. Data were extracted from papers included in the scoping review by two independent reviewers. We identified 1,407 records. A total of twelve studies were included for analyses. Study designs were variable, most research was proof-of-concept or pilot studies. Most SCT-VR were immersive and targeted interventions. Number of sessions ranged from 9 to 16, and the duration of each session ranged from 45 to 120 min. Some studies reported a significant improvement in emotion recognition and/or theory of mind. However, SCT-VR is a recent research field in which the heterogeneity in methodological approaches is evident and has prevented the reaching of robust conclusions. Preliminary evidence has shown that SCT-VR could represent a feasible and promising approach for improving SC deficits in schizophrenia. © The Author(s) 2024.",Cognitive remediation; Schizophrenia; Social cognition; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Spatial resolution measurement method for 3D displays from contrast modulation,VRS - Virtual Reality,B,"Augmented Reality 3D head-up displays use a autostereoscopic 3D display as a panel. The 3D optical unit of autostereoscopic 3D displays controls the direction of the light rays in each pixel, allowing the users enjoy 3D world without glasses. However, these 3D optics cause image quality degradation. Deterioration of resolution has a serious impact on 3D image quality. Therefore, it is important to properly measure the 3D resolution according to 3D optics and analyze its impact. In this study, a method for measuring spatial resolution in 3D displays using contrast modulation is proposed. We describe a conventional 2D resolution measurement methods that are standardized. Based on the existing 2D resolution methods, we propose a 3D resolution method. The spatial and frequency signal responses of 3D displays were investigated. The first method is determined by the predominant frequency series. The second method is conducted by contrast modulation. Through experiments with 3D displays, 3D resolution was measured using the proposed method, and the relationship between the parameters and resolution of 3D optics was examined. © The Author(s) 2024.",3D display; Augmented reality display; Contrast modulation; Light field display; Resolution,Abstract_Keywords,True,
Scopus,journalPaper,2024,Designing and evaluation of a mixed reality system for crime scene investigation training: a hybrid approach,VRS - Virtual Reality,B,"Police investigation in real-life crime scenes is an essential aspect of forensic science education. However, the practicality of bringing young investigators to actual crime scenes is often hindered by the costs and challenges involved. In order to overcome these obstacles, new technologies such as mixed reality (MR) are being explored as potential solutions. MR technology offers an interactive and cost-effective way to simulate real-life crime scenes, providing a valuable training experience for young investigators. This paper presents a novel design of a MR system using Microsoft HoloLens 2.0, which is tailored to work in a spatial 3D scanned and reconstructed crime scene using FARO point cloud 3D scanner X130 blended with photogrammetry techniques. The system was developed through the lens of Experiential Learning Theory and designed using a participatory approach, providing a cost-effective solution to help trained Kuwaiti police officers enhance their investigative skills. In order to evaluate the system’s user experience and user interaction, the Questionnaire of User Interaction Satisfaction and User Experience Questionnaire were utilised. Forty-four young police officers evaluated the system. Police students showed positive levels of satisfaction with user interaction and overall user experience with minimal negative feedback. Female students showed higher satisfaction with the overall impression compared to male students. Based on the positive feedback regarding the system expansion, the system will be taken into the commercialisation stage in the future to be provided as an essential tool for crime scene education and investigation practices. © The Author(s) 2024.",3D scanning; Crime scene; Investigation training; Mixed reality; Photogrammetry; User experience; User interaction,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Haptics in social interaction with agents and avatars in virtual reality: a systematic review,VRS - Virtual Reality,B,"Incorporating the sense of touch through haptics in virtual spaces enables people to communicate emotions and engage in more naturalistic and meaningful social interactions. Advances in haptics and virtual reality technologies and applications have been essential to support researchers in the exploration of mediated social touch in virtual environments. The aim of this work is to review the last decade of research on haptics and virtual reality technologies investigating social touch behavior between human avatars as well as between humans and non-human virtual agents. Our systematic review organizes the variety of the conducted research in three dimensions: the context against which mediated social touch is studied, the types of haptics and virtual reality technology used, and empirical studies including data collected and outcome measures. We discuss the results of the analysis of the three dimensions and present implications for future research. We pinpoint the importance of considering in-the-wild studies and emerging issues on social virtual reality; understanding human touch perception for people with different physical and cognitive abilities, and; creating development tools to broaden the exploration of advanced technological setups. © The Author(s) 2024.",Agent; Avatar; Haptics; Mediated social touch; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Bringing the field into the lab: a novel virtual reality outdoor march simulator for evaluating cognitive and physical performance,VRS - Virtual Reality,B,"Soldiers, athletes, and rescue personnel must often maintain cognitive focus while performing intense, prolonged, and physically demanding activities. The simultaneous activation of cognitive and physical functions can disrupt their performance reciprocally. In the current study, we developed and demonstrated the feasibility of a virtual reality (VR)-based experimental protocol that enables rigorous exploration of the effects of prolonged physical and cognitive efforts. A battery of established neurocognitive tests was used to compare novel cognitive tasks to simulated loaded marches. We simulated a 10-km loaded march in our virtual reality environment, with or without integrated cognitive tasks (VR-COG). During three experimental visits, participants were evaluated pre- and post-activity, including the Color Trail Test (CTT), the Synthetic Work Environment (SYNWIN) battery for assessing multitasking, and physical tests (i.e., time to exhaustion). Results show that Strong or moderate correlations (r ≥ 0.58, p ≤ 0.05) were found between VR-COG scores and scores on the cognitive tests. Both the SYNWIN and CTT showed no condition effects but significant time effects, indicating better performance in the post-activity assessment than in the pre-activity assessment. This novel protocol can contribute to our understanding of physical-cognitive interactions, since virtual environments are ideal for studying high performance professional activity in realistic but controlled settings. © The Author(s) 2024.",Cognitive load; Load carriage; Military; Physical effort; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The way of water: exploring the role of interaction elements in usability challenges with in-car VR experience,VRS - Virtual Reality,B,"With advancements in autonomous driving technology, the variety of activities that can be performed in a vehicle has increased. This improves the possibility of watching virtual reality (VR) content on a head-mounted display (HMD). However, unlike VR used in stationary environments, in-car VR can lead to discomfort and motion sickness due to the vehicle movements. Additionally, the obstruction of the outside view during driving may cause user anxiety. In this study, we investigated, for the first time, the effect of dynamic road environments, such as turns, stops, and speed bumps, on the in-car VR experience. Based on our findings, we included situational awareness (SA) cues in the in-car VR content to help users perceive their surroundings and improve the user experience. We conducted a user study with thirty participants to validate the impact of these cues. Consequently, we discovered that the Dynamics cue, which provides SA information while maintaining the context of the VR content, improves user immersion and trust while easing VR motion sickness. © The Author(s) 2024.",Automotive; Autonomous driving; HMD; In-car; User experience; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,A systematic review of techniques and clinical evidence to adopt virtual reality in post-stroke upper limb rehabilitation,VRS - Virtual Reality,B,"Recognizing the limitations of traditional therapy can be tedious and demotivating, we explore VR’s dynamic and immersive environment to potentially improve patient engagement and motivation. This approach promises accelerated recovery by integrating real-time feedback and progress monitoring. This study aims to compare various VR training techniques employed for upper limb rehabilitation in stroke survivors. We have followed the PRISMA guidelines for systematic reviews. Articles were filtered with title words such as “virtual reality rehabilitation”, “rehabilitation”, “upper limb”, “lower limb”, “interactive gaming system”, and “VR based games” were searched in databases (LILACS, PUBMED, IEEE, WoS, and Scopus). Articles published between 2005 and 2021 were analyzed. There were 820 articles found, but only the most relevant 96 papers were analyzed. Most of the studies were randomised controlled trials (RCTs) that were submitted in 2014 or beyond. The sample size ranged from 5 to 96 persons with chronic stroke, or adults and seniors. There were no samples analyzed for those under the age of 18. Nintendo Wii® and Microsoft’s Kinect were the most popular video gaming systems. In most of the publications, the intervention took place 2–3 sessions per week, for about 2–12 weeks, with each session lasting 30 to 60 min. The most assessed outcomes were body steadiness, upper extremity motor capabilities, daily tasks, and quality of life. The Fugl–Meyer Assessment was one the commonly used tool for measuring outcomes. After VR therapy, the research found that quality of life, dynamic steadiness, and upper extremity movement function improved. To achieve dynamic equilibrium, VR proved more beneficial than traditional treatments. The most important outcomes, the researchers focused, were day-to-day activity and physical movements of the patients. Some studies investigated the early consequences of VR on daily activities and social involvement. © The Author(s) 2024.",Gaming; Randomized controlled trials; Rehabilitation; Stroke; Upper limb; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Usability of visualizing position and orientation deviations for manual precise manipulation of objects in augmented reality,VRS - Virtual Reality,B,"Manual precise manipulation of objects is an essential skill in everyday life, and Augmented Reality (AR) is increasingly being used to support such operations. In this study, we investigate whether detailed visualizations of position and orientation deviations are helpful for AR-assisted manual precise manipulation of objects. We developed three AR instructions with different visualizations of deviations: the logical deviation baseline instruction, the precise numerical deviations-based instruction, and the intuitive color-mapped deviations-based instruction. All three instructions visualized the required directions for manipulation and the logical values of whether the object met the accuracy requirements. Additionally, the latter two instructions provided detailed visualizations of deviations through numerical text and color-mapping respectively. A user study was conducted with 18 participants to compare the three AR instructions. The results showed that there were no significant differences found in speed, accuracy, perceived ease-of-use, and perceived workload between the three AR instructions. We found that the visualizations of the required directions for manipulation and the logical values of whether the object met the accuracy requirements were sufficient to guide manual precise manipulation. The detailed visualizations of the real-time deviations could not improve the speed and accuracy of manual precise manipulation, and although they could improve the perceived ease-of-use and user experience, the effects were not significant. Based on the results, several recommendations were provided for designing AR instructions to support precise manual manipulation. © The Author(s) 2024.",Augmented reality; Manual manipulation; Precise manipulation; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Investigating the influence of neck muscle vibration on illusory self-motion in virtual reality,VRS - Virtual Reality,B,"The illusory experience of self-motion known as vection, is a multisensory phenomenon relevant to self-motion processes. While some studies have shown that neck muscle vibrations can improve self-motion parameter estimation, the influence on vection remains unknown. Further, few studies measured cybersickness (CS), presence, and vection concurrently and have shown conflicting results. It was hypothesized that 1) neck vibrations would enhance vection and presence, and 2) CS to negatively relate to presence and vection, whereas presence and vection to positively relate to one another. Thirty-two participants were visually and audibly immersed in a virtual reality flight simulator and occasional neck muscle vibrations were presented. Vection onset and duration were reported through button presses. Turning angle estimations and ratings of vection quality, presence, and CS were obtained after completion of the flights. Results showed no influence of vibrations on turning angle estimation errors, but a medium positive effect of vibrations on vection quality was found. Presence and vection quality were positively related, and no strong association between CS and presence or vection was found. It is concluded that neck vibrations may enhance vection and presence, however, from the current study it is unclear whether this is due to proprioceptive or tactile stimulation. © The Author(s) 2024.",Cybersickness; Flight simulator; Multisensory integration; Vection intensity,Title_Abstract,True,
Scopus,journalPaper,2024,Effectiveness of immersive virtual reality in teaching empathy to medical students: a mixed methods study,VRS - Virtual Reality,B,"Empathy in healthcare has been associated with positive outcomes such as increased patient satisfaction and reduced medical errors. However, research has indicated a decline in empathy among medical professionals. This study examined the effectiveness of Immersive Virtual Reality (IVR) for empathy training in medical education. A convergent mixed methods pretest posttest design was utilized. Participants were 1st-year medical students who engaged in an empathy training IVR educational intervention around a scenario depicting older adults struggling with social isolation. Jefferson Scale of Empathy (JSE) questionnaire was administered before and after the intervention to measure the change in empathy levels. Data were analyzed using a paired sample t-test on the pre-/post-test JSE empathy scores to assess the change in empathy scores. Nineteen qualitative semi structured interviews were conducted immediately after the IVR experience and follow-up interviews were conducted six months later. Qualitative data collected from the interviews’ transcripts were analyzed using a thematic and content analysis approach to capture individual experiences. Students (n = 19) scored 5.94 points higher on the posttest JSE questionnaire compared to pretest (p < 0.01) indicating an improvement in empathy levels. Qualitative analysis showed that the IVR training was well received by the students as a valuable empathy-teaching tool. Immersion, presence, and embodiment were identified as the main features of IVR technology that enhanced empathy and understanding of patients’ experiences. The debriefing sessions were identified as a key element of the training. IVR-based training could be an effective teaching tool for empathy training in medical education and one that is well received by learners. Results from the study offer preliminary evidence that using IVR to evoke empathy is achievable. © The Author(s) 2024.",Empathy training; Medical education; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Assessment of gamified mixed reality environments for upper limb robotic rehabilitation: pilot study on healthy adults,VRS - Virtual Reality,B,"Exoskeletons for rehabilitation have the potential to aid movement and promote intensive, task-oriented, and personalized motor training. However, robotic-assisted training and user experience could be further improve by including Mixed Reality Environments (MREs) and using a gamification approach. This paper presents the design and evaluation of a MR solution based on Microsoft Hololens 2 for robot-assisted bilateral shoulder training. It is based on gamified MREs designed to guide and encourage bilateral synchronous and asynchronous movements of the upper extremities while a 6 Degrees of Freedom (DOF) exoskeleton (FLOAT) provides motor assistance only to the impared limb. The robotic-assisted bimanual tasks involve the use of rehabilitative tools (such as a dowel rod), as a means to interact with the virtual world and to enable the intact limb guide and control the motions of the disable limb. The gamified MR training solution generates meaningful performance metrics from the kinematic analysis of hands movement, captured by Hololens. The subjective evaluation of the gamified MR solution focused on usability, cognitive load, and user experience. Meanwhile, the objective evaluation encompassed the analysis of the robot-assisted movements with and without gamified MREs, a comparative analysis between metrics obtained from Hololens and Vicon data, and the collection of reference data and trajectories. There were twenty-one healthy adults involved in the evaluation of the system. The results with the gamified MREs highlight excellent system usability, low cognitive load, and high user experience. Additionally, integrating gamified MREs into robot-assisted movements enhances shoulder movements. Data collection with Hololens demonstrated to be reliable and consistent. Furthermore, the normal reference values, paths, and velocity profiles obtained from healthy individuals offer a foundation for assessing the performance of individuals with disabilities. In summary, the introduction of gamified MREs for robot-assisted bilateral shoulder movements marks a significant and promising advancement in rehabilitation technology. © The Author(s) 2024.",Exoskeleton; Mixed reality; Robot-assisted; Shoulder rehabilitation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,ParaVerse: co-design of a parachute rehearsal and training virtual-reality enhanced simulator for the Australian Defence Force: combining a generative co-design framework and an agile approach to development,VRS - Virtual Reality,B,"While co-design methods are crucial for developing digital educational interventions that are user-centred, contextually relevant, inclusive, and effective in meeting the diverse needs of learners and educators, little attention has been paid to the potential value of co-design processes for digital application development in the Defence context. This research gauged the efficacy of combining a generative co-design framework making use of agile and iterative co-design principles in an applied research and development project. The project produced an immersive virtual reality based digital solution in collaboration with the Australian Defence Force Special Operations Command (SOCOMD) Army. Specifically, the ParaVerse project sought to develop a solution considering the advanced Tactics, Techniques and Procedures (TTPs) relevant to special operations soldiers for advanced parachute training. A Defence advisory group consisting of a series of subject matter experts was formulated to consult with the research and development team over the course of the co-design process. End-user testing with 35 SOCOMD personnel demonstrated the value of the ParaVerse application for SOCOMD personnel, speaking to the success of the leveraged generative co-design model. End-users rated ParaVerse as having greater capacity to influence education and training practices for SOCOMD and Defence generally in comparison to a pre-existing virtual parachute simulator. ParaVerse was also rated higher for satisfaction and useability and was associated with fewer instances of motion sickness. The Generative Co-Design Framework leveraged for this research provides one roadmap on how to integrate end-users in innovation design, particularly for projects working across the nexus of Defence and academia. © The Author(s) 2024.",Co-design; Defence; Parachute training; Simulation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Intersecting realms: a cross-disciplinary examination of VR quality of experience research,VRS - Virtual Reality,B,"The advent of virtual reality (VR) technology has necessitated a reevaluation of quality of experience (QoE) models. While numerous recent efforts have been dedicated to creating comprehensive QoE frameworks it seems that the majority of the factors studied as potential influencers of QoE are often limited to single disciplinary viewpoints or specific user-related aspects. Furthermore, the majority of literature reviews in this domain seem to have predominantly focused on academic sources, overlooking industry insights. To address these points, the current research took an interdisciplinary literature review approach to examine QoE literature covering both academic and industry sources from diverse fields (i.e., psychology, ergonomics, user experience, communication science, and engineering). Based on this rich dataset, we created a QoE model that illustrated 252 factors grouped into four branches - user, system, context, and content. The main finding of this review emphasized the substantial gap in the current research landscape, where complex interactions among user, system, context, and content factors in VR are overlooked. The current research not only identified this crucial disparity in existing QoE studies but also provided a substantial online repository of over 200 QoE-related factors. The repository serves as an indispensable tool for future researchers aiming to construct a more holistic understanding of QoE. © The Author(s) 2024.",Critical analysis; Literature review; Quality of experience; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,How to trigger user’s willingness to participate in the metaverse? An exploration of the significant factors of the metaverse,VRS - Virtual Reality,B,"The issue of the metaverse has been widely discussed. The purpose of this research is to investigate users’ willingness to participate in the metaverse. This study used the self-efficacy theory and Theory of Reasoned Action (TRA) to explore their willingness to attend the metaverse. Furthermore, the study explored how the basic concepts of the metaverse (Avatars, Decentralized Value Exchange, and Immersive Experience) influence the users’ attitudes (Presence in Second-Life, 3D Interactivity, and Play-to-Earn) toward and willingness with respect to participating in the metaverse. A total of 150 valid experts’ responses were collected through an online questionnaire and analyzed through structural equation modeling. The results revealed that Presence in Second-Life and Play-to-Earn significantly impact the respondents’ willingness to participate in the metaverse. Moreover, 3D Interactivity affected their participation to willingness through Presence in Second-Life and Play-to-Earn. © The Author(s) 2024.",3D interactivity; Avatars; Decentralized value exchange; Immersive experience; Metaverse; Self-efficacy,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Examining the feasibility of immersive virtual reality to measure upper limb motor performance in typically developing children and adolescents,VRS - Virtual Reality,B,"Over the last five years, virtual reality (VR) has become more popular in pediatric physiotherapy. In this study, we assessed the feasibility and acceptability of measuring upper-limb movements in typically-developing children and adolescents using an immersive virtual reality (iVR) headset. Thirty-six typically-developing children (age: 12 ± 2.1 y) were recruited and required to draw circles in a custom-built virtual environment using a Meta-Quest-2 headset. Outcomes were the System Usability Scale (SUS), Developmental Coordination Disorder-Questionnaire (DCD-Q), and three metrics of circle drawing performance (movement time, mean velocity and circle roundness). The mean score for the SUS was 74 ± 11, indicating good levels of acceptability and usability when the participants used the headset. No strong relationships were observed between the circle drawing metrics and DCD-Q scores (rho = < 0.3, p = > 0.05), but circle roundness ratios were positively and significantly correlated with SUS scores (rho = 0.5, p = 0.003). No adverse effects associated with iVR use were reported for any participants. This study showed that iVR is a viable method to measure upper-limb motor performance in children and adolescents, highlighting the potential value of this tool in pediatric physiotherapy practice. © The Author(s) 2024.",Circle drawing task; Head-mounted displays; Pediatric rehabilitation; Technology-based assessment; Virtual environment,Title_Abstract,True,
Scopus,journalPaper,2024,Using in situ research-based design to explore learning module effectiveness and usability in a virtual reality system for workforce training,VRS - Virtual Reality,B,"This study examines the implementation and effectiveness of Virtual Reality (VR) into employee communication training for Texas Department of Transportation employees. The study also explores the impact of iterative, in situ research-based design on VR system usability, and potential relationships between the usability and ergonomics of VR devices, and user receptiveness to VR content. For this work we adapt the definition of in situ to mean a learning activity that takes place in the location or environment where the participants will actually be trained. Following the pandemic, many office-based workers adopted hybrid-remote work formats (Yang, Kim, & Hong, 2023), necessitating updated approaches to employee training and instruction which VR may provide. Further, VR may offer opportunities for private role-play for employee communication practice, as well as an overall more high-fidelity learning opportunity. Findings indicate that VR is an effective and engaging solution for workforce and management communication training, and that hardware configurations influence overall employee enthusiasm for VR-based training. © The Author(s) 2024.",Hybrid-remote work; Management Training; Virtual Humans; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Toward next generation mixed reality games: a research through design approach,VRS - Virtual Reality,B,"Mixed reality (MR) games refer to games that integrate physical entities with digitally mediated contents. Currently, it entails game creators to integrate heterogeneous virtual and physical components, which is often time-consuming and labor-intensive, without the support of a coherent technology stack. The underlying technodiversity manifested by the research corpus suggests a complicated, multi-dimensional design space that goes beyond merely technical concerns. In this research, we adopted a research-through-design approach and proposed an MR game technology stack that facilitates flexible, low-code game development. As design grounding, we first surveyed 34 state-of-the-art studies, and results were synergized into three different spectra of technological affordances, respectively activity range, user interface and feedback control, to inform our next design process. We then went through an iterative prototyping phase and implemented an MR game development toolset. A co-design workshop was conducted, where we invited 15 participants to try the prototype tools and co-ideate the potential use scenarios for the proposed technology stack. First-hand user feedback was collected via questionnaires and semi-structured interviews. As a result, four conceptual game designs with three major design implications were generated, which conjointly reflect a broader understanding on MR gameful experience and contribute fresh insights to this emerging research domain. © The Author(s) 2024.",End user development; Extended reality; Game; Gamification; Mixed reality; Research through design; RFID,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Exploring perspectives on engagement in interactive virtual reality for chronic pain management: insights from a content analysis study,VRS - Virtual Reality,B,"To investigate perspectives of people with chronic pain regarding aspects of immersive virtual reality (IVR) that enhance and reduce engagement and the outcomes of engagement in IVR. This content analysis was performed on data obtained through open-ended interview questions from a study aiming to understand the influence of IVR on chronic pain study at a research lab at a university. Participants included a sample of 20 adults who completed the parent study. Results highlight that presence, agency, customization, and novelty are important aspects that enhance engagement in IVR, with agency and presence being mentioned most frequently. Meanwhile, secondary effects of IVR and usability were said to reduce engagement with the IVR. Outcomes of engagement with IVR include enjoyment, mood elevation, relaxation/calming, a distraction from pain, and a loss of reality. This study provides an initial understanding of individuals’ perspectives of engagement with IVR in relation to chronic pain management. Health professionals using IVR to treat people with chronic pain can use these elements to facilitate engagement in their clients. Further research should be done to study the association between engagement in IVR and pain reduction to improve the development of IVR programs for chronic pain management. © The Author(s) 2024.",Chronic pain; Engagement; Immersive virtual reality; Pain intervention; Virtual reality analgesia,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The effect of VR on fine motor performance by older adults: a comparison between real and virtual tasks,VRS - Virtual Reality,B,"Virtual Reality (VR) technology has the potential to support the aging population and improve testing of daily abilities to detect functional decline. In multiple research studies, VR performance of participants has been assessed by measuring time to complete test, but the effect of learning how to use the VR system and differences between real and virtual environments have been understudied, especially for fine motor tasks. In this study, 20 older adults ages 65–84 performed a task that required fine motor skills in real-life and then in a VR replica of the same task. All participants completed the task in each setting with no difficulties. A clear learning effect was observed in VR, which was attributed to learning how to use the device itself. Still, participants could not reach the same level of performance (time) in VR as in real-life. Participants rated the VR task more mentally and physically demanding than in real-life, as well as more stressful, but with an overall low cognitive demand. In an exploratory cluster analysis, participants with an average age of 69 years old had more technological devices, found the VR system more usable and realistic than participants in the group with an average of 76 years old. This study demonstrated that VR influences time to complete a fine motor task, and that learning effects related to the system could be confounded with actual task performance if not properly considered in VR studies with older adults. © The Author(s) 2024.",Aging; Ergonomic fidelity; Fine motor tasks; Learning; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Viewpoint-sharing method with reduced motion sickness in object-based VR/AR collaborative virtual environment,VRS - Virtual Reality,B,"We propose a viewpoint-sharing method with reduced motion sickness in an object-based remote collaborative virtual environment (CVE). The method is designed with an assumption of asymmetric, object-based CVE where collaborators use non-homogeneous devices, such as immersive virtual reality head-mounted display (VR HMD) and tablet-based augmented reality (AR), and simultaneously interact with 3D virtual objects. Therefore, collaborators interact with different interfaces such as virtual reality (VR) users relying on controllers for virtual locomotion and object manipulation, while AR users perform physical locomotion and multi-touch input for object manipulation. The proposed viewpoint-sharing method allows both users to observe and manipulate the objects in interest from the shared point of view, enabling participants to interact with the objects without the need for virtual/physical locomotion. While viewpoint-sharing, instead of changing point of view, the proposed method performs seamless object transformation to provide a shared point of view, reducing motion sickness and associated discomfort. From our user experiment, the viewpoint-share condition resulted in a 35.47% faster task completion time than the baseline condition which is without proposed viewpoint-sharing. The advantage of viewpoint-sharing regarding system usability was significant, while task workloads were similar in the baseline and viewpoint-sharing conditions. We expect that the proposed viewpoint-sharing method allows users to quickly, efficiently, and collaboratively communicate in an object-based CVE, and represents a step forward in the development of effective remote, asymmetric CVE. © The Author(s) 2024.",Augmented reality; Remote collaboration; Virtual environment; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Temporally enhanced graph convolutional network for hand tracking from an egocentric camera,VRS - Virtual Reality,B,"We propose a robust 3D hand tracking system in various hand action environments, including hand-object interaction, which utilizes a single color image and a previous pose prediction as input. We observe that existing methods deterministically exploit temporal information in motion space, failing to address realistic diverse hand motions. Also, prior methods paid less attention to efficiency as well as robust performance, i.e., the balance issues between time and accuracy. The Temporally Enhanced Graph Convolutional Network (TE-GCN) utilizes a 2-stage framework to encode temporal information adaptively. The system establishes balance by adopting an adaptive GCN, which effectively learns the spatial dependency between hand mesh vertices. Furthermore, the system leverages the previous prediction by estimating the relevance across image features through the attention mechanism. The proposed method achieves state-of-the-art balanced performance on challenging benchmarks and demonstrates robust results on various hand motions in real scenes. Moreover, the hand tracking system is integrated into a recent HMD with an off-loading framework, achieving a real-time framerate while maintaining high performance. Our study improves the usability of a high-performance hand-tracking method, which can be generalized to other algorithms and contributes to the usage of HMD in everyday life. Our code with the HMD project will be available at https://github.com/UVR-WJCHO/TEGCN_on_Hololens2. © The Author(s) 2024.",Augmented reality; Computer vision; Deep learning; Head mounted displays; Tracking,Keywords,True,
Scopus,journalPaper,2024,Unconstrained lightweight control interface for robot-assisted minimally invasive surgery using MediaPipe framework and head-mounted display,VRS - Virtual Reality,B,"Robotic surgery is preferred over open or laparoscopic surgeries due to its intuitiveness and convenience. However, prolonged use of surgical robots can cause neck pain and joint fatigue in wrist and fingers. Also, input systems are bulky and difficult to maintain. To resolve these issues, we propose a novel input module based on real-time 3D hand tracking driven by RGB images and MediaPipe framework to control surgical robots such as patient side manipulator (PSM) and endoscopic camera manipulator (ECM) of da Vinci research kit. In this paper, we explore the mathematical basis of the proposed 3D hand tracking module and provide a proof-of-concept through user experience (UX) studies conducted in a virtual environment. End-to-end latencies for controlling PSM and ECM were 170 ± 10 ms and 270 ± 10 ms, respectively. Of fifteen novice participants recruited for the UX study, thirteen managed to reach a qualifiable level of proficiency after 50 min of practice and fatigue of hand and wrist were imperceivable. Therefore, we concluded that we have successfully developed a robust 3D hand tracking module for surgical robot control and in the future, it would hopefully reduce hardware cost and volume as well as resolve ergonomic problems. Furthermore, RGB image driven 3D hand tracking module developed in our study can be widely applicable to diverse fields such as extended reality (XR) development and remote robot control. In addition, we provide a new standard for evaluating novel input modalities of XR environments from a UX perspective. © The Author(s) 2024.",Computer vision; Ergonomics; Real-time hand tracking; Robotic surgery; User experience study; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Perspective matters: a systematic review of immersive virtual reality to reduce racial prejudice,VRS - Virtual Reality,B,"In the wake of the COVID-19 pandemic and the rise of social justice movements, increased attention has been directed to levels of intergroup tension worldwide. Racial prejudice is one such tension that permeates societies and creates distinct inequalities at all levels of our social ecosystem. Whether these prejudices are present explicitly (directly or consciously) or implicitly (unconsciously or automatically), manipulating body ownership by embodying an avatar of another race using immersive virtual reality (IVR) presents a promising approach to reducing racial bias. Nevertheless, research findings are contradictory, which is possibly attributed to variances in methodological factors across studies. This systematic review, therefore, aimed to identify variables and methodological variations that may underlie the observed discrepancies in study outcomes. Adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, this systematic review encompassed 12 studies that employed IVR and embodiment techniques to investigate racial attitudes. Subsequently, two mini meta-analyses were performed on four and five of these studies, respectively — both of which utilised the Implicit Association Test (IAT) as a metric to gauge these biases. This review demonstrated that IVR allows not only the manipulation of a sense of body ownership but also the investigation of wider social identities. Despite the novelty of IVR as a tool to help understand and possibly reduce racial bias, our review has identified key limitations in the existing literature. Specifically, we found inconsistencies in the measures and IVR equipment and software employed, as well as diversity limitations in demographic characteristics within both the sampled population and the embodiment of avatars. Future studies are needed to address these critical shortcomings. Specific recommendations are suggested, these include: (1) enhancing participant diversity in terms of the sample representation and by integrating ethnically diverse avatars; (2) employing multi-modal methods in assessing embodiment; (3) increasing consistency in the use and administration of implicit and explicit measures of racial prejudice; and (4) implementing consistent approaches in using IVR hardware and software to enhance the realism of the IVR experience. © The Author(s) 2024.",Embodiment; Immersive virtual reality; Implicit bias; Perspective-taking; Race; Racial prejudice,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Improving balance using augmented visual orientation cues: a proof of concept,VRS - Virtual Reality,B,"Falls are a major health concern. Existing augmented reality (AR) and virtual reality solutions for fall prevention aim to improve balance in dedicated training sessions. We propose a novel AR prototype as an assistive wearable device to improve balance and prevent falls in daily life. We use a custom head-mounted display toolkit to present augmented visual orientation cues in the peripheral field of view. The cues provide a continuous space-stationary visual reference frame for balance control using the user’s tracked head position. In a proof of concept study, users performed a series of balance trials to test the effect of the displayed visual cues on body sway. Our results showed that body sway can be reduced with our device, indicating improved balance. We also showed that superimposed movements of the visual reference in forward-backward or sideways directions induce respective sway responses. This indicates a direction-specific balance integration of the displayed cues. Based on our findings, we conclude that artificially generated visual orientation cues using AR can improve balance and could possibly reduce fall risk. © The Author(s) 2024.",Assistive device; Augmented reality; Balance; Fall prevention; Peripheral vision; Walking aid,Abstract_Keywords,True,
Scopus,journalPaper,2024,Multimodal emotion classification using machine learning in immersive and non-immersive virtual reality,VRS - Virtual Reality,B,"Affective computing has been widely used to detect and recognize emotional states. The main goal of this study was to detect emotional states using machine learning algorithms automatically. The experimental procedure involved eliciting emotional states using film clips in an immersive and non-immersive virtual reality setup. The participants’ physiological signals were recorded and analyzed to train machine learning models to recognize users’ emotional states. Furthermore, two subjective ratings emotional scales were provided to rate each emotional film clip. Results showed no significant differences between presenting the stimuli in the two degrees of immersion. Regarding emotion classification, it emerged that for both physiological signals and subjective ratings, user-dependent models have a better performance when compared to user-independent models. We obtained an average accuracy of 69.29 ± 11.41% and 71.00 ± 7.95% for the subjective ratings and physiological signals, respectively. On the other hand, using user-independent models, the accuracy we obtained was 54.0 ± 17.2% and 24.9 ± 4.0%, respectively. We interpreted these data as the result of high inter-subject variability among participants, suggesting the need for user-dependent classification models. In future works, we intend to develop new classification algorithms and transfer them to real-time implementation. This will make it possible to adapt to a virtual reality environment in real-time, according to the user’s emotional state. © The Author(s) 2024.",Affective computing; Emotions; Machine learning; Physiological signals; Virtual reality; Wearables,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Cybersickness with passenger VR in the aircraft: Influence of turbulence and VR content,VRS - Virtual Reality,B,"Using VR in the airplane cabin is appealing, primarily because of the enhanced entertainment value, increased privacy, and improved recreational opportunities provided by higher levels of immersion. However, VR applications in aircrafts contain the risk of passengers developing cybersickness. The particular environment of a moving aircraft in interaction with visual representation of movements in VR could lead to severe cybersickness, especially during turbulence. We had 129 participants experience VR in a full flight simulator with different content (static or dynamic VR clips) and during varying phases of flight including turbulence. The employed simulator is equipped with a cabin module, creating an economically valid environment. VR induced significant but mild symptoms of cybersickness. Nausea and dizziness symptoms were most severe during turbulence and especially with dynamic VR content being presented. More anxious participants tended to report more symptoms. In addition, there was an association with video game use and attitudes toward new technologies. While mild content and short exposure times only led to fairly low expressions of cybersickness, a long-term use of VR under turbulence could possibly become a concern. Airlines should especially address passengers’ negative attitudes toward new technologies, and VR in particular, to reduce fears and the risk of low tolerability. © The Author(s) 2024.",Aircraft cabins; Airplanes; Cybersickness; Moving vehicles; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,The use of CNNs in VR/AR/MR/XR: a systematic literature review,VRS - Virtual Reality,B,"This study offers a systematic literature review on the application of Convolutional Neural Networks in Virtual Reality, Augmented Reality, Mixed Reality, and Extended Reality technologies. We categorise these applications into three primary classifications: interaction, where the networks amplify user engagements with virtual and augmented settings; creation, showcasing the networks’ ability to assist in producing high-quality visual representations; and execution, emphasising the optimisation and adaptability of apps across diverse devices and situations. This research serves as a comprehensive guide for academics, researchers, and professionals in immersive technologies, offering profound insights into the cross-disciplinary realm of network applications in these realities. Additionally, we underscore the notable contributions concerning these realities and their intersection with neural networks. © The Author(s) 2024.",Augmented reality; Convolutional neural network; Extended reality; Mixed reality; Systematic literature review; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Immersive virtual reality applied to the rehabilitation of patients with lower limb amputation: a small randomized controlled trial for feasibility study,VRS - Virtual Reality,B,"Limb amputation significantly impacts the socioeconomic and health aspects of affected individuals, with clinical issues such as phantom limb pain (PLP), phantom limb telescopy (PLT), residual limb pain (RLP), and decreased balance necessitating improved treatments. Although interventions utilizing Immersive Virtual Reality (IVR) have been explored, conducting Randomized Clinical Trials (RCT) within this population presents challenges. This study serves as a feasibility study derived from a small RCT, aiming to investigate the effects of an IVR intervention protocol on individuals with lower limb amputation (LLA) while addressing methodological challenges and exploring alternative study designs. Participants were randomly assigned to either the Control Group (CG), receiving no intervention, or the Intervention Group (IG), undergoing 16 IVR sessions over 8 weeks, with twenty-one participants completing the protocol. Sessions involved observing physical exercises via a head-mounted display. All participants were assessed for pain and balance pre- and post-intervention. IG participants were also evaluated for pain, sense of presence in the virtual environment, and cybersickness on intervention days. Results indicated a significant negative correlation between RLP and time since amputation in the Intervention Group. Analysis of results between IG and CG post-assessment suggests potential benefits of IVR in improving balance and reducing PLT. Despite challenges related to sample size and participant retention, multicenter collaborations and home-based interventions are proposed to mitigate these limitations. This feasibility study lays a foundation for future research aiming to optimize VR interventions for improved outcomes in patients with LLA. © The Author(s) 2024.",Amputation; Phantom limb pain; Postural balance; Rehabilitation research; Residual limb pain; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Key determinants of intention to use virtual reality in medical training,VRS - Virtual Reality,B,"Experts have called for virtual reality (VR) training and learning applications that can facilitate the changes needed in training programmes for years to come. To help expedite the adoption process, this study used a mixed-methods approach to identify the key factors that promote intentions to use VR technology in medical training. The qualitative research was based on interviews with five doctors and medical students, which focused on identifying the most significant determinants. Next, a survey was conducted to collect data from 154 medical interns and students in Spanish universities and hospitals, whose responses were processed using partial least squares-structural equation analysis. The limited sample size means this study is exploratory. The results indicate that perceived entertainment significantly strengthens behavioural intention to use VR technology in medical courses. The findings also underline the potential uses of VR learning tools in healthcare contexts and the need to incorporate this technology into medical training. © The Author(s) 2024.",Key factor; Medical learning; Technology acceptance; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Tracking and co-location of global point clouds for large-area indoor environments,VRS - Virtual Reality,B,"Extended reality (XR) experiences are on the verge of becoming widely adopted in diverse application domains. An essential part of the technology is accurate tracking and localization of the headset to create an immersive experience. A subset of the applications require perfect co-location between the real and the virtual world, where virtual objects are aligned with real-world counterparts. Current headsets support co-location for small areas, but suffer from drift when scaling up to larger ones such as buildings or factories. This paper proposes tools and solutions for this challenge by splitting up the simultaneous localization and mapping (SLAM) into separate mapping and localization stages. In the pre-processing stage, a feature map is built for the entire tracking area. A global optimizer is applied to correct the deformations caused by drift, guided by a sparse set of ground truth markers in the point cloud of a laser scan. Optionally, further refinement is applied by matching features between the ground truth keyframe images and their rendered-out SLAM estimates of the point cloud. In the second, real-time stage, the rectified feature map is used to perform localization and sensor fusion between the global tracking and the headset. The results show that the approach achieves robust co-location between the virtual and the real 3D environment for large and complex tracking environments. © The Author(s) 2024.",Colocation; Large-area; Point cloud; Registration; Tracking,Abstract,True,
Scopus,journalPaper,2024,Augmented reality presentation system of skeleton image based on biomedical features,VRS - Virtual Reality,B,"Aimed at limitations in the description and expression of three-dimensional (3D) physical information in two-dimentsional (2D) medical images, feature extraction and matching method based on the biomedical characteristics of skeletons is employed in this paper to map the 2D images of skeletons into a 3D digital model. Augmented reality technique is used to realize the interactive presentation of skeleton models. Main contents of this paper include: Firstly, a three-step reconstruction method is used to process the bone CT image data to obtain its three-dimensional surface model, and the corresponding 2D–3D bone library is established based on the identification index of the 2D image and the 3D model; then, a fast and accurate feature extraction and matching algorithm is developed to realize the recognition, extraction, and matching of 2D skeletal features, and determine the corresponding 3D skeleton model according to the matching result. Finally, based on the augmented reality technique, an interactive immersive presentation system is designed to achieve visual effects of the virtual human bone model superimposed and rendered in the world scenes, which improves the effectiveness of information expression and transmission, as well as the user's immersion and embodied experience. © The Author(s) 2024.",2D–3D bone mapping; Augmented reality presentation; Bone feature recognition and matching; Construction of human skeleton library,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Exploring sex differences in collaborative virtual environments for participation equality and user experience,VRS - Virtual Reality,B,"Communication technology plays a crucial role in facilitating remote collaborative work. This study investigated sex differences in Perceived Participation Equality and User Experience across different communication formats, i.e., face-to-face communication, conventional video conferences, and Virtual Reality (VR). An empirical study was conducted involving 15 groups, each comprising three participants, who engaged in a decision-making task. A research model was developed to evaluate the interplay between perceived participation equality, empathy, and immersion. This model was employed across three communication conditions and included both male and female participants. These findings on sex differences in user experience could help create a connected, cohesive, and productive remote collaborative work environment. © The Author(s) 2024.",Collaborative work; Immersion; Participation equality; Sex differences; User experience; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"The impact of virtual and augmented reality on presence, user experience and performance of Information Visualisation",VRS - Virtual Reality,B,"The fast growth of virtual reality (VR) and augmented reality (AR) head-mounted displays provides a new medium for interactive visualisations and visual analytics. Presence is the experience of consciousness within extended reality, and it has the potential to increase task performance. This project studies the impact that a sense of presence has on data visualisation performance and user experience under AR and VR conditions. A within-subjects design recruited 38 participants to complete interactive visualisation tasks within the novel immersive data analytics system for genomic data in AR and VR, and measured speed, accuracy, preference, presence, and user satisfaction. Open-ended user experience responses were also collected. The results implied that VR was more conducive to efficiency, effectiveness, and user experience as well as offering insight into possible cognitive load benefits for VR users. © The Author(s) 2024.",Augmented reality; Biomedical data; Immersive visualisation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Testing the ‘differences in virtual and physical head pose’ and ‘subjective vertical conflict’ accounts of cybersickness,VRS - Virtual Reality,B,"When we move our head while in virtual reality, display lag will generate differences in our virtual and physical head pose (known as DVP). While DVP are a major trigger for cybersickness, theories differ as to exactly how they constitute a provocative sensory conflict. Here, we test two competing theories: the subjective vertical conflict theory and the DVP hypothesis. Thirty-two HMD users made continuous, oscillatory head rotations in either pitch or yaw while viewing a large virtual room. Additional display lag was applied selectively to the simulation about the same, or an orthogonal, axis to the instructed head rotation (generating Yaw-Lag + Yaw-Move, Yaw-Lag + Pitch-Move, Pitch-Lag + Yaw-Move, and Pitch-Lag + Pitch-Move conditions). At the end of each trial: (1) participants rated their sickness severity and scene instability; and (2) their head tracking data were used to estimate DVP throughout the trial. Consistent with our DVP hypothesis, but contrary to subjective vertical conflict theory, Yaw-Lag + Yaw-Move conditions induced significant cybersickness, which was similar in magnitude to that in the Pitch-Lag + Pitch-Move conditions. When extra lag was added along the same axis as the instructed head movement, DVP was found to predict 73–76% of the variance in sickness severity (with measures of the spatial magnitude and the temporal dynamics of the DVP both contributing significantly). Ratings of scene instability were also found to predict sickness severity. Taken together, these findings suggest that: (1) cybersickness can be predicted from objective estimates of the DVP; and (2) provocative stimuli for this sickness can be identified from subjective reports of scene instability. © 2024, The Author(s).",Cybersickness; Head-mounted display; Motion sickness; Motion-to-photon latency; Sensory conflict; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,A virtual gym in your pocket: the influence of augmented reality exercise app characteristics on user’s continuance intention,VRS - Virtual Reality,B,"In recent years, with the development of augmented reality (AR) technology and the prevalence of COVID-19, augmented reality exercise applications (AREAs) have entered people’s lives and changed people’s exercise ways. However, there is still little research on how AREAs affect users’ continuance intention, which limits our deeper understanding of the further use of AREAs by users for the potential benefits they provide. This study investigates the role of AREAs by proposing the original proposition of AREAs characteristic classification based on previous AR literature and dividing their characteristics into two categories: service and system characteristics. Through an online empirical study, 398 valid questionnaires were collected to test the hypotheses using the structural equation model. The results showed that hedonic and utilitarian value and presence are vital inner states which mediate the influence of service and system characteristics on user’s satisfaction and continuance intention. The results further show that only hedonic value is found to have a positive and significant relationship with continuance intention. This study contributes to the literature in the AR field by examining how different AR application characteristics affect user continuance intention in the context of sports and fitness. It also suggests that practitioners should identify the impact of different characteristics on user value and focus on the hedonic aspects of the application. © The Author(s) 2024.",AR exercise applications; Continuance intention; Service characteristics; Stimulus-organism-response model; System characteristics,Title_Abstract,True,
Scopus,journalPaper,2024,A comparative study of CT-based volumetric assessment methods for total lung capacity with the development of an adjustment factor: incorporating VR imaging for improved accuracy,VRS - Virtual Reality,B,"Physiological methods for measuring total lung capacity (TLC), including body-box plethysmography (BBP), are costly and require specialized expertise. Computed tomography (CT)-based TLC assessment is essential in clinical practice for candidates of lung transplantation and those unable to undergo standard lung function testing. While CT-based algorithms were studied to estimate TLC, their accuracy should be further evaluated. This study aimed to compare the BBP measurement of TLC (TLCpleth) with three CT-based methods for measuring TLC, one of them is an innovative virtual reality (VR)-based method. Additionally, we aimed to develop an adjustment factor that will allow a new, non-invasive, cost-effective estimation of the TLCpleth. TLC was calculated for 24 adult patients using three different CT-based volumetric assessment methods: an older region-growing algorithm (TLCrg), a more recent convolutional neural network-based algorithm (TLCcnn), and a VR-based method (TLCvr). Agreement between each method and TLCpleth was evaluated, and an adjustment factor was developed using linear regression. The correlation between the three CT-based methods and TLCpleth ranged from 0.91 to 0.92 (p < 0.001). TLCvr measurements were 80.13% (CI:75.08–85.18%, P < 0.001) of TLCpleth measures, whereas TLCcnn and TLCrg estimates were 71.3% and 77.1% of TLCpleth, respectively. An adjustment factor is proposed to estimate TLCpleth based on the three CT-based methods. This study is the first to evaluate the correlation between BBP, VR volumetric analysis, and two iterations of CT volumetric software for measuring total lung capacity (TLC). After being corrected by an adjustment factor, VR- and CT-based assessments provide accurate estimates of TLCpleth. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Computed tomography; Polygon summation; Static lung volumes; Total lung capacity; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,The impact of misaligned idiotropic and visual axes on spatial ability under altered visuospatial conditions,VRS - Virtual Reality,B,"Spatial ability, a critical dimension of human cognition, represents the ability to gather, perceive, and manipulate spatial information to create an accurate and complete mental representation of spatial environments. Previous studies have examined spatial ability in normal spatial conditions of the earth. However, emerging technologies and increasing exploration of hard-to-reach locations are transforming future workplaces into environments with altered visuospatial conditions, which may pose serious challenges to workers’ productivity and safety. One such condition is the misalignment of idiotropic and visual axes that may exist in microgravity during space explorations or underwater during deep-sea explorations. In this study, we investigate whether and to what extent misaligned idiotropic and visual axes influence spatial ability. The misalignment was simulated in Virtual Reality (VR) with three conditions: aligned (control group), misaligned (experiment group I), and dynamically misaligned (experiment group II) idiotropic and visual axes. The spatial ability of 99 participants was measured through spatial visualization, relations, and orientation abilities using the Purdue Spatial Visualization Test: Rotations (PSVTR), Mental Cutting Test (MCT), and Perspective-Taking Ability (PTA) test, respectively. For the MCT and PTA tests, the results show no significant differences in response accuracy among the three conditions. The PSVTR test results reflect a statistically significant difference in accuracy among the groups. The three groups did not have significantly different response times for the three tests. The results suggest that the misalignment of the body and visual axes may influence spatial visualization, but may not impact spatial relations or orientation. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented and virtual reality; Interdisciplinary projects; Spatial ability; Spatial cognition,Abstract_Keywords,True,
Scopus,journalPaper,2024,Eye-tracking on virtual reality: a survey,VRS - Virtual Reality,B,"Virtual reality (VR) has evolved substantially beyond its initial remit of gaming and entertainment, catalyzed by advancements such as improved screen resolutions and more accessible devices. Among various interaction techniques introduced to VR, eye-tracking stands out as a pivotal development. It not only augments immersion but offers a nuanced insight into user behavior and attention. This precision in capturing gaze direction has made eye-tracking instrumental for applications far beyond mere interaction, influencing areas like medical diagnostics, neuroscientific research, educational interventions, and architectural design, to name a few. Though eye-tracking’s integration into VR has been acknowledged in prior reviews, its true depth, spanning the intricacies of its deployment to its broader ramifications across diverse sectors, has been sparsely explored. This survey undertakes that endeavor, offering a comprehensive overview of eye-tracking’s state of the art within the VR landscape. We delve into its technological nuances, its pivotal role in modern VR applications, and its transformative impact on domains ranging from medicine and neuroscience to marketing and education. Through this exploration, we aim to present a cohesive understanding of the current capabilities, challenges, and future potential of eye-tracking in VR, underscoring its significance and the novelty of our contribution. © The Author(s) 2024.",Attention; Eye-tracking; Perception; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Evaluating the viewer experience of interactive virtual reality movies,VRS - Virtual Reality,B,"Significant advances in virtual reality (VR) technology have called into question the traditional methods of cinema storytelling and dissemination. New VR devices, such as the Meta (Oculus) Quest, have expanded the possibilities for viewing movies. The purpose of this study is to compare the emotional and cognitive impacts of VR and traditional 2D movies. In this study, sixty volunteers were divided into two groups and presented a movie (Gala) in 2D or VR format. We employed a multimodal method to assess the cognitive and emotional effects of the film both during and after watching. Our technique combined self-reports, interviews, questionnaires, and objective heart rate and EEG brain activity data. After quantitative and qualitative evaluation, it was discovered, that regardless of media, there was a substantial influence of the movie on the emotional state of the participant’s mood. Moreover, compared to the traditional 2D-movie, the VR movie led to more consistent and robust positive effect on all aspects of self-rated affect. The difference in self-reported mood was corroborated by reduced EEG amplitudes in the beta frequency band, indicating higher levels of positive affectivity, which was only observed for the VR movie. Lastly, the VR movie also leads to overall higher self-rated immersion and engagement than the 2D version. Our results highlight the potential of VR movies to engage and emotionally affect audiences beyond traditional cinema. Moreover, our study highlights the value of using a multidisciplinary method for analysing audience impacts. © 2023, The Author(s).",Emotion; Film analysis; Filmmaking; Objective measures; Self-report; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A mixed reality application for total hip arthroplasty,VRS - Virtual Reality,B,"Total hip arthroplasty (or total hip replacement) is the current surgical solution for the treatment of advanced coxarthrosis, with the objective of providing mobility and pain relief to patients. For this purpose, surgery can be planned using preoperative images acquired from the patient and navigation systems can also be used during the intervention. Robots have also been used to assist in interventions. In this work, we propose a new mixed reality application for total hip arthroplasty. The surgeon only has to wear HoloLens 2. The application does not require acquiring preoperative or intraoperative images of the patient and uses hand interaction. Interaction is natural and intuitive. The application helps the surgeon place a virtual acetabular cup onto the patient's acetabulum as well as define its diameter. Similarly, a guide for drilling and implant placement is defined, establishing the abduction and anteversion angles. The surgeon has a direct view of the operating field at all times. For validation, the values of the abduction and anteversion angles offered by the application in 20 acetabular cup placements have been compared with real values (ground-truth). From the results, the mean (standard deviation) is 0.375 (0.483) degrees for the error in the anteversion angle and 0.1 (0.308) degrees for the abduction angle, with maximum discrepancies of 1 degree. A study was also carried out on a cadaver, in which a surgeon verified that the application is suitable to be transferred to routine clinical practice, helping in the guidance process for the implantation of a total hip prosthesis. © The Author(s) 2024.",Arthroplasty; Hip; HoloLens; Mixed reality; Surgery; Total hip arthroplasty,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Driving emotions: using virtual reality to explore the effect of low and high arousal on driver’s attention,VRS - Virtual Reality,B,"The role played by emotions and attention is crucial for the development of advanced driver assistance systems that improve safety by flexibly adapting to the current state of the driver. In the present study, we used immersive virtual reality as a testing tool to investigate how different emotional states affect drivers’ attention in a divided attention task. Two different emotional states, diversified by valence and arousal, were induced before performing a divided attention task in a driving simulation. The experimental task developed for this study allowed us to explore if and how two different emotional states can affect the way drivers divide their attention between a central driving-related task and a peripheral visual task. Our results showed that scared drivers presented lower reaction times at the central task compared to relaxed drivers. On the contrary, the emotional state did not affect the performance at the peripheral task, which revealed instead a significant effect of the eccentricity at which the visual stimuli were presented, influencing both the accuracy of targets’ perception and participants’ reaction times. © The Author(s) 2024.",ADAS; Attention; Emotions; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Hand-as-a-prop: using the hand as a haptic proxy for manipulation in virtual reality,VRS - Virtual Reality,B,"Haptic feedback can be almost as important as visual information in virtual reality environments. On the one hand, in Active Haptic Feedback, specialized devices such as vibrotactile gloves are employed; however, these solutions can be expensive, vendor-specific or cumbersome to setup. On the other hand, Passive Haptic Feedback approaches use inexpensive objects as proxies for the virtual entities; but mapping virtual objects to real props is not scalable nor flexible. We propose the Hand-as-a-Prop technique, which consists in using human hands as object props. We implemented two modalities: Self, where the user’s non-dominant hand act as the virtual object while the dominant hand grabs, translates and releases it; and External, where the hand of another person is used. Hand-as-a-Prop can represent multiple shapes with a single prop and does not require extra hardware. We performed an evaluation comparing both Self and External Hand-as-a-Prop with traditional Object Props in terms of user experience (goodness, ease, realism, fatigue, and preference) and performance (task completion time and translation time). Results showed that Hand-as-a-Prop was rated as neutral tending to positive, and in some cases, the performance was similar to Object Props. Users preferred Self Hand-as-a-Prop over External Hand-as-a-Prop and also obtained better results. © 2023, The Author(s).",Controller-free interaction; Human actuation; Manipulation; Self-haptics; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A real-time wearable AR system for egocentric vision on the edge,VRS - Virtual Reality,B,"Real-time performance is critical for Augmented Reality (AR) systems as it directly affects responsiveness and enables the timely rendering of virtual content superimposed on real scenes. In this context, we present the DARLENE wearable AR system, analysing its specifications, overall architecture and core algorithmic components. DARLENE comprises AR glasses and a wearable computing node responsible for several time-critical computation tasks. These include computer vision modules developed for the real-time analysis of dynamic scenes supporting functionalities for instance segmentation, tracking and pose estimation. To meet real-time requirements in limited resources, concrete algorithmic adaptations and design choices are introduced. The proposed system further supports real-time video streaming and interconnection with external IoT nodes. To improve user experience, a novel approach is proposed for the adaptive rendering of AR content by considering the user’s stress level, the context of use and the environmental conditions for adjusting the level of presented information towards enhancing their situational awareness. Through extensive experiments, we evaluate the performance of individual components and end-to-end pipelines. As the proposed system targets time-critical security applications where it can be used to enhance police officers’ situational awareness, further experimental results involving end users are reported with respect to overall user experience, workload and evaluation of situational awareness. © The Author(s) 2024.",Artificial intelligence; Augmented reality; Intelligent user interfaces; Situational awareness,Abstract_Keywords,True,
Scopus,journalPaper,2024,Holograms for seamless integration of remote students in the classroom,VRS - Virtual Reality,B,"The new global scenario imposed by the SARS-CoV-2 virus has given rise to an atypical and problematic situation in multiple spheres. In very little time, the abrupt change from face-to-face to remote has not only required a rapid widespread use of digital technology, but also a change in methodology and communicative interactions. In the field of education, teachers have had to interact in new environments, with the combined use of face-to-face and non-face-to-face teaching being a major challenge. This paper presents the design and implementation of a cyber presence system for educational environments using Microsoft’s HoloLens 2 Mixed Reality (MR) headset. A software tool is developed that improves teaching scenarios through communication in mixed environments. The tool enables teachers to integrate the students in the classroom in a common space with remote students connected by videoconference. Our system is not limited to education, however, as it can also be deployed in any setting that requires remote communication, such as companies and governmental institutions. © 2024, The Author(s).",Education; Holograms; HoloLens 2; Mixed reality; Remote communication,Abstract_Keywords,True,
Scopus,journalPaper,2024,The impact of fantasy on young children’s recall: a virtual reality approach,VRS - Virtual Reality,B,"Educational materials for preschool children are often embedded with fantastical elements. Nevertheless, there is still little empirical evidence on their effectiveness, especially as concerns long-term retention. Virtual reality offers new ecological possibilities for investigating this type of learning, especially through the impact of immersion. In a between-design study, 168 children aged four to six years followed a virtual reality presentation on China presented by either a realistic young girl or an anthropomorphic animal. The level of immersion was manipulated, with half of the children following the presentation in immersive virtual reality (IVR) and the other half in desktop virtual reality (D-VR). Participants were asked to complete a new/old recognition task and a quiz task immediately after the presentation and once again one week later, with an additional transfer task being added for the second series. One week later, children performed significantly better on the new/old recognition task in the realistic condition when compared to the anthropomorphic condition. However, there were no differences observed in the quiz task and in the transfer task. It therefore seems that, under certain conditions, children remember a cultural presentation better when it is presented by a realistic avatar. The results further showed that the children performed significantly worse in the IVR conditions on all tasks. A possible explanation for this result is that IVR demands excessive cognitive resources from preschool children. Further studies should explore this unexpected result, as well as what could be done to make IVR effective for learning in preschool children. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Fantasy; Preschool children; Recall; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Fully immersive learning with virtual reality for assessing students in art history,VRS - Virtual Reality,B,"Immersive learning systems using consumer grade headsets for virtual reality (VR) software can now reach the classroom. VR represents an important step toward situated learning for bringing realistic experiences to show to students different types of content the way it appears in real life. With the three-dimensional effect, it adds an immersive dimension that can bring the student into a unique environment that is directly connected to the learning content. While VR experiences can be helpful in educational settings, they must satisfy the learning objectives of the course and the specific needs of the students. In this paper, we propose a customizable VR application for displaying paintings for their analysis, and their associated questions designed for instructors in art history. To improve the accessibility and adaptivity to instructors and specific learning materials, we propose the definition of the paintings’ characteristics and the questionnaires associated with the paintings in the JSON open-standard file format, facilitating application changes without any prior programming knowledge. We compare the proposed VR modality with a web-based application on a computer desktop with 35 undergraduate students with art history experience. In both modalities, we assess the workload and usability; the VR sickness symptoms and the motivation in the VR condition. The results indicate better usability and lower workload with the VR condition. While there are no differences in terms of students’ performance for answering the questions, 77% of students prefer the VR condition. The Reduced Instructional Materials Motivation Survey shows a high motivation in the student population. Finally, the system evaluation supports the conclusion that the proposed VR system can be deployed in the art history classroom as the system has a high usability and medium workload. © 2024, The Author(s).",Art history; Education; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"PanoEmo, a set of affective 360-degree panoramas: a psychophysiological study",VRS - Virtual Reality,B,"There is a significant increase in the use of virtual reality in scientific experiments in the fields of ergonomics, education, and psychology among others. Many researchers successfully provoked different affective states in participants in order to capture physiological correlates or apply psychotherapeutic techniques. All these studies employed different stimuli, like 3D pictures, computer-built graphics and 180- or 360-degree panoramic photographs. In an attempt to begin the standardization of the measurements, we propose PanoEmo, a set of affective 360-degree photographic panoramas. Our aim was to explore the emotional reactions in response to PanoEmo, based on self-report scales, somatic, and vegetative affective indices. Fifty-five participants watched 45 photographic panoramas of different valence during 20 s without a special task. Self-reported valence correlated positively to zygomaticus major and negatively to corrugator supercilii electromyographic activity. Zygomaticus major also correlated positively to arousal. Respiratory rate correlated negatively to valence. Pleasant panoramas provoked a slower respiratory rate, while unpleasant ones increased it. Skin conductance response was positively related to self-reported arousal. Unexpectedly, heart rate did not correlate to self-report measures during the whole epoch, but it correlated positively around 5 s after the panorama onset. As a limitation, we should mention that our database contains a much higher number of positive panoramas. Although we expected the equal number of negative, neutral, and positive panoramas, we found a prevalence of positive ones. Nonetheless, subsequent studies should enrich the set with more negative panoramas to get a homogeneously distributed database. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Arousal; Database; Emotion; Psychophysiology; Valence; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Assessment of music performance anxiety in a virtual auditorium through the study of ambient lighting and audience distance,VRS - Virtual Reality,B,"Performance anxiety is a common problem affecting musicians’ concentration and well-being. Musicians frequently encounter greater challenges and emotional discomfort when performing in front of an audience. Recent research suggests an important relationship between the characteristics of the built environment and people’s well-being. In this study, we explore modifying the built environment to create spaces where musicians are less aware of the presence of the audience and can express themselves more comfortably. An experiment was conducted with 61 conservatory musicians playing their instrument in a virtual auditorium in front of an audience of hundreds of virtual humans. They performed at different distances from the audience and under different levels of ambient lighting, while their eye movements were recorded. These data, together with questionnaires, were used to analyse the way the environment is perceived. The results showed that reducing the light intensity above the audience made the view of the auditorium more calming, and the same effect was observed when the distance between the audience and the musician was increased. Eye-tracking data showed a significant reduction in saccadic eye movements as the distance from the audience increased. This work provides a novel approach to architecture influence on musicians’ experience during solo performances. The findings are useful to designers and researchers. © The Author(s) 2024.",Eye tracking; Music; Neuroarchitecture; Performance anxiety; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,Using a virtual reality interview simulator to explore factors influencing people’s behavior,VRS - Virtual Reality,B,"Virtual reality interview simulator (VRIS) is an effective and valid tool that uses virtual reality technology to train people’s interview skills. Typically, it offers candidates prone to being very nervous during interviews the opportunity to practice interviews in a safe and manageable virtual environment and realistic settings, providing real-time feedback from a virtual interviewer on their performance. It helps interviewees improve their skills, reduce their fears, gain confidence, and minimize the cost and time associated with traditional interview preparation. Yet, the major anxiety-inducing elements remain unknown. During an interview, the anxiety levels, overall experience, and performance of interviewees might be affected by various circumstances. By analyzing electrodermal activity and questionnaire, we investigated the influence of five variables: (I) Realism; (II) Question type; (III) Interviewer attitude; (IV) Timing; and (V) Preparation. As such, an orthogonal design L8(41×24) with eight experiments (OA8 matrix) was implemented, in which 19 college students took part in the experiments. Considering the anxiety, overall experience, and performance of the interviewees, we found that Question type plays a major role; secondly, Realism, Preparation, and Interviewer attitude all have middle influence; lastly, Timing has little to no impact. Specifically, professional interview questions elicited a greater degree of anxiety than personal ones among the categories of interview questions. This work contributes to our understanding of anxiety-stimulating factors during job interviews in virtual reality and provides cues for designing future VRIS. © The Author(s) 2024.",Anxiety; Interview; User interfaces; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Analysis of the user experience (On site vs. Virtual Reality) through biological markers and cognitive tests in museums: the case of Museo Cristo de la Sangre (Murcia, Spain)",VRS - Virtual Reality,B,"Digital technologies have changed almost every aspect of our lives, including the way we access heritage. Following the pandemic caused by COVID-19 and the technological evolution of recent years, museums and institutions, among others, have changed the way they display their collections, taking a greater interest in new technologies, platforms and digital software. This technological boom finds its greatest transformation with the implementation of Virtual Reality (VR) and Metaverse in the museum sector. This article shows the concrete influence of VR/Metaverse in a museum room previously digitised through different techniques. Subsequently, the impact over user experience in the VR scenario versus on-site visit has been measured. In parallel, to measure the enzyme alpha-amylase in saliva, a cognitive test and usability test (SUS) were carried out to determine the learning capacity and degree of satisfaction obtained with experience alongside the room of the Museo de la Sangre in Murcia (Spain). © The Author(s) 2024.",Alpha-amylase; Digitisation; Metaverse; Spatial; System usability scale; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Assessing the perceptual equivalence of a firefighting training exercise across virtual and real environments,VRS - Virtual Reality,B,"The advantages of Virtual Reality (VR) over traditional training, together with the development of VR technology, have contributed to an increase in the body of literature on training professionals with VR. However, there is a gap in the literature concerning the comparison of training in a Virtual Environment (VE) with the same training in a Real Environment (RE), which would contribute to a better understanding of the capabilities of VR in training. This paper presents a study with firefighters (N = 12) where the effect of a firefighter training exercise in a VE was evaluated and compared to that of the same exercise in a RE. The effect of environments was evaluated using psychophysiological measures by evaluating the perception of stress and fatigue, transfer of knowledge, sense of presence, cybersickness, and the actual stress measured through participants’ Heart Rate Variability (HRV). The results showed a similar perception of stress and fatigue between the two environments; a positive, although not significant, effect of the VE on the transfer of knowledge; the display of moderately high presence values in the VE; the ability of the VE not to cause symptoms of cybersickness; and finally, obtaining signs of stress in participants’ HRV in the RE and, to a lesser extent, signs of stress in the VE. Although the effect of the VE was shown to be non-comparable to that of the RE, the authors consider the results encouraging and discuss some key factors that should be addressed in the future to improve the results of the training VE. © 2024, The Author(s).",Biofeedback; Computer graphics; Professional training; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Overground walking while using a virtual reality head mounted display increases variability in trunk kinematics and reduces dynamic balance in young adults,VRS - Virtual Reality,B,"This study analyzed the effects of walking freely in virtual reality (VR) compared to walking in the real-world on dynamic balance and postural control. For this purpose, nine male and twelve female healthy participants underwent standard 3D gait analysis while walking randomly in a real laboratory and in a room-scale overground VR environment resembling the real laboratory. The VR was delivered to participants by a head-mounted-display which was operated wirelessly and calibrated to the real-world. Dynamic balance and postural control were assessed with (1) the margin of stability (MOS) in the anteroposterior (AP-MOS) and mediolateral (ML-MOS) directions at initial-contact, (2) the relationship between the mediolateral center of mass (COM) position and acceleration at mid-stance with subsequent step width, (3) and trunk kinematics during the entire gait cycle. We observed increased mediolateral (ML) trunk linear velocity variability, an increased coupling of the COM position and acceleration with subsequent step width, and a decrease in AP-MOS while walking in VR but no change in ML-MOS when walking in VR. Our findings suggest that walking in VR may result in a less reliable optical flow, indicated by increased mediolateral trunk kinematic variability, which seems to be compensated by the participants by slightly reweighing sensorimotor input and thereby consciously tightening the coupling between the COM and foot placement to avoid a loss of balance. Our results are particularly valuable for future developers who want to use VR to support gait analysis and rehabilitation. © 2023, The Author(s).",Dynamic stability; Gait analysis; Immersive virtual reality; Motion capturing; Postural control,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Hands-free multi-type character text entry in virtual reality,VRS - Virtual Reality,B,"Multi-type characters, including uppercase and lowercase letters, symbols, and numbers, are essential in text entry activities. Although multi-type characters are used in passwords, instant messages, and document composition, there has been limited exploration of multi-character text entry for virtual reality head-mounted displays (VR HMDs). Typically, multi-type character entry requires four kinds of keyboards between which users need to switch. In this research, we explore hands-free approaches for rapid multi-type character entry. Our work explores two efficient and usable hands-free approaches for character selection: eye blinks and dwell. To enable quick switching between keyboards, we leverage the usability and efficiency of continuous head motions in the form of cross-based activation. In a pilot study, we explored the usability and efficiency of four locations of the switch keys, the two hands-free selection mechanisms, and crossing-based switching. In the main experiment, we evaluated four user-inspired layouts designed according to the findings from the pilot study. Results show that both blinking and dwell can work well with crossing-based switching and could lead to a relatively fast text entry rate (5.64 words-per-minute (WPM) with blinking and 5.42 WPM with dwell) with low errors (lower than 3% not corrected error rate (NCER)) for complex 8-digit passwords with upper/lowercase letters, symbols, and numbers. For sentences derived from the Brown Corpus, participants can reach 8.48 WPM with blinking and 7.78 WPM with dwell. Overall, as a first exploration, our results show that it is usable and efficient to perform hands-free text entry in VR using either eye blinks or dwell for character selection and crossing for mode switching. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Hands-free interaction; Keyboard layout; Mode switching; Multi-type character entry; Text entry; User study; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"3D model-based tracking combining edges, keypoints and fiducial markers",VRS - Virtual Reality,B,"Model-based tracking is an essential task in fields such as Augmented Reality. State-of-the-art approaches rely on the model’s edges, sometimes combined with image keypoints and color. Nevertheless, these image features are not considered part of the model but as temporary information discarded every time the tracking process is restarted. This paper proposes a novel approach that employs an enhanced model that combines edges, keypoints, and fiducial markers for robust and real-time tracking. The experiments conducted show that our method outperforms state-of-the-art model-based approaches and suggest that fiducial markers are a good choice for texturing models. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",3D model-based tracking; Fiducial markers; Keypoints,Abstract,True,
Scopus,journalPaper,2024,Quasi-3D: reducing convergence effort improves visual comfort of head-mounted stereoscopic displays,VRS - Virtual Reality,B,"The diffusion of virtual reality urges to solve the problem of vergence-accommodation conflict arising when viewing stereoscopic displays, which causes visual stress. We addressed this issue with an approach based on reducing ocular convergence effort. In virtual environments, vergence can be controlled by manipulating the binocular separation of the virtual cameras. Using this technique, we implemented two quasi-3D conditions characterized by binocular image separations intermediate between 3D (stereoscopic) and 2D (monoscopic). In a first experiment, focused on perceptual aspects, ten participants performed a visuo-manual pursuit task while wearing a head-mounted display (HMD) in head-constrained (non-immersive) condition for an overall exposure time of ~ 7 min. Passing from 3D to quasi-3D and 2D conditions, progressively resulted in a decrease of vergence eye movements—both mean convergence angle (static vergence) and vergence excursion (dynamic vergence)—and an increase of hand pursuit spatial error, with the target perceived further from the observer and larger. Decreased static and dynamic vergence predicted decreases in asthenopia trial-wise. In a second experiment, focused on tolerance aspects, fourteen participants performed a detection task in near-vision while wearing an HMD in head-free (immersive) condition for an overall exposure time of ~ 20 min. Passing from 3D to quasi-3D and 2D conditions, there was a general decrease of both subjective and objective visual stress indicators (ocular convergence discomfort ratings, cyber-sickness symptoms and skin conductance level). Decreased static and dynamic vergence predicted the decrease in these indicators. Remarkably, skin conductance level predicted all subjective symptoms, both trial-wise and session-wise, suggesting that it could become an objective replacement of visual stress self-reports. We conclude that relieving convergence effort by reducing binocular image separation in virtual environments can be a simple and effective way to decrease visual stress caused by stereoscopic HMDs. The negative side-effect—worsening of spatial vision—arguably would become unnoticed or compensated over time. This initial proof-of-concept study should be extended by future large-scale studies testing additional environments, tasks, displays, users, and exposure times. © The Author(s) 2024.",Asthenopia; Cyber-sickness; Depth perception; Head-mounted display; Near vision; Physiological signals; Sensory-motor adaptation; Skin conductance; Spatial vision; Stereoscopic display; Vergence eye movements; Virtual reality; Visual stress,Abstract_Keywords,True,
Scopus,journalPaper,2024,Examination of fire scene reconstructions using virtual reality to enhance forensic decision-making. A case study in Scotland.,VRS - Virtual Reality,B,"When attending a crime scene, first responders are responsible for identifying areas of potential interest for subsequent forensic examination. This information is shared with the police, forensic practitioners, and legal authorities during an initial meeting of all interested parties, which in Scotland is known as a forensic strategy meeting. Swift documentation is fundamental to allow practitioners to learn about the scene(s) and to plan investigative strategies, traditionally relying on word-of-mouth briefings using digital photographs, videos, diagrams, and verbal reports. We suggest that these early and critical briefings can be augmented positively by implementing an end-to-end methodology for indoor 3D reconstruction and successive visualisation through immersive Virtual Reality (VR). The main objective of this paper is to provide an integrative documentation tool to enhance the decision-making processes in the early stages of the investigation. Taking a fire scene as an example, we illustrate a framework for rapid spatial data acquisition of the scene that leverages structure-from-motion photogrammetry. We developed a VR framework that enables the exploration of virtual environments on a standalone, low-cost immersive head-mounted display. The system was tested in a two-phased inter-agency fire investigation exercise, where practitioners were asked to produce hypotheses suitable for forensic strategy meetings by (1) examining traditional documentation and then (2) using a VR walkthrough of the same premises. The integration of VR increased the practitioners’ scene comprehension, improved hypotheses formulation with fewer caveats, and enabled participants to sketch the scene, in contrast to the orientation challenges encountered using conventional documentation. © The Author(s) 2024.",Decision-making; Fire; Forensic science; Reconstruction; Scene investigation; Training,Title_Abstract,True,
Scopus,journalPaper,2024,Therapist perspectives on telehealth-based virtual reality exposure therapy,VRS - Virtual Reality,B,"Virtual reality (VR) can enhance mental health care. In particular, the effectiveness of VR-based exposure therapy (VRET) has been well-demonstrated for treatment of anxiety disorders. However, most applications of VRET remain localized to clinic spaces. We aimed to explore mental health therapists’ perceptions of telehealth-based VRET (tele-VRET) by conducting semi-structured, qualitative interviews with 18 telemental health therapists between October and December 2022. Interview topics included telehealth experiences, exposure therapy over telehealth, previous experiences with VR, and perspectives on tele-VRET. Therapists described how telehealth reduced barriers (88.9%, 16/18), enhanced therapy (61.1%, 11/18), and improved access to clients (38.9%, 7/18), but entailed problems with technology (61.1%, 11/18), uncontrolled settings (55.6%, 10/18), and communication difficulties (50%, 9/18). Therapists adapted exposure therapy to telehealth by using online resources (66.7%, 12/18), preparing client expectations (55.6%, 10/18), and adjusting workflows (27.8%, 5/18). Most therapists had used VR before (72.2%, 13/18) and had positive impressions of VR (55.6%, 10/18), but none had used VR clinically. In response to tele-VRET, therapists requested interactive session activities (77.8%, 14/18) and customizable interventions components (55.6%, 10/18). Concerns about tele-VRET included risks with certain clients (77.8%, 14/18), costs (50%, 9/18), side effects and privacy (22.2%, 4/18), and inappropriateness for specific forms of exposure therapy (16.7%, 3/18). These results reveal how combining telehealth and VRET may expand therapeutic options for mental healthcare providers and can help inform collaborative development of immersive health technologies. © The Author(s) 2024.",Clinical practice; Exposure therapy; Mental health; Telehealth; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,HoloGCS: mixed reality-based ground control station for unmanned aerial vehicle,VRS - Virtual Reality,B,"Human–robot interaction (HRI), which studies the interaction between robots and humans, appears as a promising research idea for the future of smart factories. In this study, HoloLens as ground control station (HoloGCS) is implemented, and its performance is discussed. HoloGCS is a mixed reality-based system for controlling and monitoring unmanned aerial vehicles (UAV). The system incorporates HRI through speech commands and video streaming, enabling UAV teleoperation. HoloGCS provides a user interface that allows operators to monitor and control the UAV easily. To demonstrate the feasibility of the proposed systems, a user case study (user testing and SUS-based questionnaire) was performed to gather qualitative results. In addition, throughput, RTT, latency, and speech accuracy were also gathered and analyzed to evaluate quantitative results. © The Author(s) 2024.",Ground control station; Human–robot interactions; Microsoft HoloLens; Mixed reality; Speech control; Unmanned aerial vehicle; Video streaming,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Long short-term memory prediction of user’s locomotion in virtual reality,VRS - Virtual Reality,B,"Nowadays, there is still a challenge in virtual reality to obtain an accurate displacement prediction of the user. This could be a future key element to apply in the so-called redirected walking methods. Meanwhile, deep learning provides us with new tools to reach greater achievements in this type of prediction. Specifically, long short-term memory recurrent neural networks obtained promising results recently. This gives us clues to continue researching in this line to predict virtual reality user’s displacement. This manuscript focuses on the collection of positional data and a subsequent new way to train a deep learning model to obtain more accurate predictions. The data were collected with 44 participants and it has been analyzed with different existing prediction algorithms. The best results were obtained with a new idea, the use of rotation quaternions and the three dimensions to train the previously existing models. The authors strongly believe that there is still much room for improvement in this research area by means of the usage of new deep learning models. © The Author(s) 2024.",Deep learning; Locomotion; User prediction; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Evaluation of the novelty effect in immersive Virtual Reality learning experiences,VRS - Virtual Reality,B,"In this study, the novelty effect or initial fascination with new technology is addressed in the context of an immersive Virtual Reality (iVR) experience. The novelty effect is a significant factor contributing to low learning outcomes during initial VR learning experiences. The aim of this research is to measure the effectiveness of a tutorial at mitigating the novelty effect of iVR learning environments among first-year undergraduate students. The iVR tutorial forms part of the iVR learning experience that involves the assembly of a personal computer, while learning the functions of the main components. 86 students participated in the study, divided into a Control group (without access to the tutorial) and a Treatment group (completing the tutorial). Both groups showed a clear bimodal distribution in previous knowledge, due to previous experience with learning topics, giving us an opportunity to compare tutorial effects with students of different backgrounds. Pre- and post-test questionnaires were used to evaluate the experience. The analysis included such factors as previous knowledge, usability, satisfaction, and learning outcomes categorized into remembering, understanding, and evaluation. The results demonstrated that the tutorial significantly increased overall satisfaction, reduced the learning time required for iVR mechanics, and improved levels of student understanding, and evaluation knowledge. Furthermore, the tutorial helped to homogenize group behavior, particularly benefiting students with less previous experience in the learning topic. However, it was noted that a small number of students still received low marks after the iVR experience, suggesting potential avenues for future research. © 2024, The Author(s).",Head-mounted display; Learning; Novelty effect; Serious games; Tutorial; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Spatially dependent veiling glare degrades image quality in medical extended reality,VRS - Virtual Reality,B,"Spatially dependent veiling glare in medical extended reality (MXR) degrades image quality and needs to be characterized across technologies. Measurement methods of veiling glare on virtual and augmented reality (VR and AR) head-mounted displays (HMDs) have not been established in regulatory evaluation of MXR devices or display measurement standards. We describe an experimental bench setup enabling highly flexible translations and rotations of the light measuring device in five degrees of freedom within the eye box. Glare point spread functions (PSFs) of multiple VR and AR headsets are extracted and compared across the display field of view (FOV) in dark environment. At the center, the evaluated VR headsets (HTC VIVE Pro and VIVE Pro 2) demonstrate reduced long-range glare compared to the tested AR HMDs (Microsoft HoloLens 2 and Epson Moverio BT-300). The measured PSFs at multiple locations are spatially invariant for the HoloLens 2. However, veiling glare on the evaluated VR HMDs substantially increases toward the periphery of the FOV primarily due to optical aberration. For VR devices in medical use, increased peripheral glare can lead to image quality degradation and poor visualization of anatomical details. © 2024, This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply.",Augmented reality; Head-mounted display; Medical extended reality; Medical image quality; Veiling glare; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Design principles for social exchange in social virtual reality-enabled virtual teams,VRS - Virtual Reality,B,"Social virtual reality (SVR) is a novel technology that can simulate and potentially enhance our face-to-face interactions. However, our understanding of interpersonal communication in SVR is still limited. To address this research gap, we describe how SVR enables social exchange (i.e., fundamental communication patterns of trust and reciprocity between individuals), which is closely related to virtual team performance. We present an information systems design theory for social exchange in SVR-enabled virtual teams (SE-SVR). Drawing from affordance theory and social exchange theory, we describe how SVR material properties (i.e., avatars, virtual objects, virtual space, and verbal and nonverbal communication features) enable and foster social exchange in SVR. As a theoretical contribution, we propose design principles for social exchange in SVR and connect them with testable theoretical propositions. Furthermore, we present the concept of interacting with presence, which facilitates users’ affordance perceptions in SVR. We conceptually validate our design principles and illustrate our design through an artifact instantiation: XR Campus, which is a minimum viable product of a collaborative platform for the ECIU University. Our SE-SVR theory has important research and practice implications because it explains how critical aspects of organizational remote communication can be considered in SVR design. © 2023, The Author(s).",Affordance theory; Design theory; Reciprocity; Social exchange theory; Social virtual reality; Trust,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Using narrative 360° video as a tool to promote breast self-examination,VRS - Virtual Reality,B,"This experiment examined the feasibility of 360° video as a tool for public health messaging by investigating the effect that viewing the 360° documentary The Waiting Room VR had on female viewers’ sense of identification, attitudes to breast cancer screening and mortality salience. A key part of the documentary places participants in a viewpoint ambiguously aligned to that of the film’s director and subject, Victoria Mapplebeck (VM), in a scene that recreates her radiotherapy treatment for breast cancer. Eighty female participants watched the documentary either sitting upright with the chair back set at a 90° angle or reclining with the chair back set at a 140° angle (consistent with VMs posture) under conditions of either high or low cognitive load. The effect of posture type was measured explicitly using questionnaires on presence, identification and breast self-examination (BSE) intention as well as implicitly using a lexical decision task to measure death-thought awareness (DTA). Reclined posture led to a higher sense of spatial presence but no increase in identification with VM. Significantly increased identification with VM led to greater intention to conduct BSE. There were no effects of posture, cognitive load or identification on DTA. The implications of these results for using 360° video as a behaviour change tool, the effects of the COVID-19 pandemic on the terror management manipulation and the relevance of spatial viewpoint in 360° video are discussed. © 2024, The Author(s).",360° Video; Breast self-examination; Identity; Posture; Presence; Public health communication; Terror management theory; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,Augmented reality in delivering experiential values: moderating role of task complexity,VRS - Virtual Reality,B,"This study examined if greater experiential values are provided when shopping via an AR mobile app compared to shopping via a general mobile website without AR. The mediating role of experiential values between the two shopping methods and customer loyalty as well as the moderating effect of task complexity between the two shopping methods and experiential values were further investigated. An exciting eyewear retailer’s mobile site and mobile app embedded with AR features were used. A total of 302 usable respondents participated in the study. Shoppers exposed to an AR function perceived greater aesthetics, escapism, enjoyment, and efficiency than those exposed to a non-AR mobile site. Also, compared to shoppers exposed to a general non-AR mobile site, shoppers exposed to an AR mobile app showed greater customer loyalty through the four experiential values. Task complexity modified the effects of AR on consumers’ perceived escapism and efficiency experiential values. This research fills the gap in the literature by investigating AR’s experiential values in connection with customer loyalty by comparing an AR-embedded mobile app with a general mobile site without an AR feature. The additional examination of task complexity also contributes to a complete understanding of AR experiential benefits considering consumers’ perceptions about AR operation task complexity. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Customer loyalty; Experiential value; Task complexity,Title_Keywords,True,
Scopus,journalPaper,2024,Automatic portals layout for VR navigation,VRS - Virtual Reality,B,"Portals layout in a large virtual scene can help users improve navigation efficiency, but determining the number and the positions of the portals has some challenges. In this paper, we propose two automatic virtual portals layout methods for efficient VR navigation. We first introduced a visibility importance-based method to determine the portals’ positions and numbers for a given scene. To improve the walkability of the VR environment, based on the visibility importance-based method, we propose a simulated annealing-based portal layout method to optimize the portals’ positions further. To reduce the number of reverse redirections in the navigation, we also proposed a real-time portal orientation determination algorithm to determine the orientations of the portals. We designed a user study to test the two methods we propose. The results showed that our methods made the VR navigation more efficient than the portals random layout and non-portal methods. Our methods achieved a significant reduction of task completion time, total viewpoint translation, and the number of reverse path redirections without increasing the scores of SSQ, IPQ, and task load. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Navigation; Portal; Teleportation; Virtual reality,Keywords,True,
Scopus,journalPaper,2023,Utilization of virtual reality for operating room fire safety training: a randomized trial,VRS - Virtual Reality,B,"Operating room fires are catastrophic and often preventable. The optimal means to prepare healthcare workers for a fire is unclear. Virtual reality allows for hands-on practice in scenarios that are difficult to replicate in real life. Therefore, we designed an examination of the impact of virtual reality fire safety training. Sixty anesthesiology residents were randomized into three groups. One group underwent standard fire safety training, one group watched a fire safety video, and one group completed a virtual reality-based fire safety module. After an 8-month washout, residents were asked to manage a simulated case of an operating room fire. Participants were graded on their performance, completed a knowledge assessment, and provided feedback about their experiences. In total, 47 residents completed the follow-up assessment. No knowledge gains were seen in any group. Those in the VR group exhibited enhanced skills with fire management as they were more likely to douse the flame within 10 s as compared to both the control (12 (63.2) vs. 2 (13.0), adj p = 0.012) and the video group (12(63.2) vs. 3 (23.0), adj p = 0.043). Self-rated performance in the simulated fire was no different between the groups; however, the self-rated “effectiveness” of training was superior for VR. Utilizing virtual reality-based training for OR fire safety demonstrated enhanced management skills when managing a simulated operating room fire. More work is needed to further elucidate the optimal timing and frequency of training as well as examine the optimal modality for fire safety training. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Fire safety; High-fidelity simulation; Operating room fire; Patient safety; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A systematic review of wearable biosensor usage in immersive virtual reality experiences,VRS - Virtual Reality,B,"Wearable biosensors are increasingly incorporated in immersive Virtual Reality (iVR) applications. A trend that is attributed to the availability of better quality, less costly, and easier-to-use devices. However, consensus is yet to emerge over the most optimal combinations. In this review, the aim is to clarify the best examples of biosensor usage in combination with iVR applications. The high number of papers in the review (560) were classified into the following seven fields of application: psychology, medicine, sports, education, ergonomics, military, and tourism and marketing. The use of each type of wearable biosensor and Head-Mounted Display was analyzed for each field of application. Then, the development of the iVR application is analyzed according to its goals, user interaction levels, and the possibility of adapting the iVR environment to biosensor feedback. Finally, the evaluation of the iVR experience was studied, considering such issues as sample size, the presence of a control group, and post-assessment routines. A working method through which the most common solutions, the best practices, and the most promising trends in biofeedback-based iVR applications were identified for each field of application. Besides, guidelines oriented towards good practice are proposed for the development of future iVR with biofeedback applications. The results of this review suggest that the use of biosensors within iVR environments need to be standardized in some fields of application, especially when considering the adaptation of the iVR experience to real-time biosignals to improve user performance. © The Author(s) 2024.",Biofeedback; Biosensors; Head-mounted displays; Heart rate; Physiology; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Shaping the auditory peripersonal space with motor planning in immersive virtual reality,VRS - Virtual Reality,B,"Immersive audio technologies require personalized binaural synthesis through headphones to provide perceptually plausible virtual and augmented reality (VR/AR) simulations. We introduce and apply for the first time in VR contexts the quantitative measure called premotor reaction time (pmRT) for characterizing sonic interactions between humans and the technology through motor planning. In the proposed basic virtual acoustic scenario, listeners are asked to react to a virtual sound approaching from different directions and stopping at different distances within their peripersonal space (PPS). PPS is highly sensitive to embodied and environmentally situated interactions, anticipating the motor system activation for a prompt preparation for action. Since immersive VR applications benefit from spatial interactions, modeling the PPS around the listeners is crucial to reveal individual behaviors and performances. Our methodology centered around the pmRT is able to provide a compact description and approximation of the spatiotemporal PPS processing and boundaries around the head by replicating several well-known neurophysiological phenomena related to PPS, such as auditory asymmetry, front/back calibration and confusion, and ellipsoidal action fields. © 2023, The Author(s).",Immersive audio; Motor planning; Peripersonal space; Spatial audio rendering; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Augmented reality headsets for surgical guidance: the impact of holographic model positions on user localisation accuracy,VRS - Virtual Reality,B,"Novel augmented reality headsets such as HoloLens can be used to overlay patient-specific virtual models of resection margins on the patient’s skin, providing surgeons with information not normally available in the operating room. For this to be useful, surgeons wearing the headset must be able to localise virtual models accurately. We measured the error with which users localise virtual models at different positions and distances from their eyes. Healthy volunteers aged 20–59 years (n = 54) performed 81 exercises involving the localisation of a virtual hexagon’s vertices overlaid on a monitor surface. Nine predefined positions and three distances between the virtual hexagon and the users’ eyes (65, 85 and 105 cm) were set. We found that, some model positions and the shortest distance (65 cm) led to larger localisation errors than other positions and larger distances (85 and 105 cm). Positional errors of more than 5 mm and 1–5 mm margin errors were found in 29.8% and over 40% of cases, respectively. Strong outliers were also found (e.g. margin shrinkage of up to 17.4 mm in 4.3% of cases). The measured errors may result in poor outcomes of surgeries: e.g. incomplete tumour excision or inaccurate flap design, which can potentially lead to tumour recurrence and flap failure, respectively. Reducing localisation errors associated with arm reach distances between the virtual models and users’ eyes is necessary for augmented reality headsets to be suitable for surgical purposes. In addition, training surgeons on the use of these headsets may help to minimise localisation errors. © The Author(s) 2024.",Augmented reality; Augmented reality headsets; Image marker; Skin tumour removal; Surgery; Surgical navigation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A method of comparing virtual reality orthognathic surgical predictions and postsurgical treatment outcomes,VRS - Virtual Reality,B,"Background: Orthognathic surgical predictions using 2D cephalometric films, 2D computer-assisted planning, and 3D computer-assisted surgical simulation are currently accepted methods to evaluate postsurgical outcomes. Recent advancements have paved the way for a new method of surgical prediction using immersive virtual reality. The objective of the study was to evaluate a method of comparing virtual reality (VR) orthognathic surgical predictions with postsurgical treatment outcomes of patients. Methods: Pre- and postsurgical cone-beam computed tomography (CBCT) data of 14 patients who underwent one-jaw mandibular advancement surgery were collected. The presurgical CBCTs were rendered for the VR surgical manipulation, and the prediction was exported as a STL file. Skeletal and dental landmarks were placed, and differences were calculated in all dimensions (Right/Left, Anterior/Posterior, Superior/Inferior, 3D distance, pitch, roll, yaw). A one-sample t-test was conducted to determine clinical significance (p < 0.05). Results: The differences in the linear measurements of component dimensions for VR surgical prediction and actual postsurgical results were less than 2 mm (p < 0.05), with the exceptions of left gonial angle in the right/left dimension; right and left gonial angles, left mental foramen, and gnathion in the anterior/posterior dimension; and all 3D distance measurements. The difference in all angular measurements (pitch, roll, yaw) were less than 4° (p < 0.05). Conclusions: The described method was successful in evaluating clinically acceptable limits in 3D distance measurements, component (x, y, z) dimensions and for pitch, yaw and roll of virtual reality surgical predictions and the actual surgical outcome. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Jaw surgery; Orthognathic surgical prediction; Postsurgical treatment outcomes assessment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Text readability in augmented reality: a multivocal literature review,VRS - Virtual Reality,B,"Augmented reality (AR) is making its way into many sectors. Its rapid evolution in recent years has led to the development of prototypes demonstrating its effectiveness. However, to be able to push these prototypes to the scale of fully usable applications, it is important to ensure the readability of the texts they include. To this end, we conducted a multivocal literature review (MLR) to determine the text parameters a designer can tune, as well as the contextual constraints they need to pay attention to, in relation to Optical See-Through (OST) and Video See-Through (VST) displays. We also included guidelines from device manufacturing and game engines sites to compare the current state of research in the academic and industrial worlds. The results show that parameters pertaining more to letter legibility have been extensively studied (e.g., color and size), while those pertaining to the whole text still require further research (e.g., alignment or space between lines). The former group of parameters, and their associated constraints, were assembled in the form of two decision trees to facilitate implementation of AR applications. Finally, we also concluded that there was a lack of alignment between academic and industrial recommendations. © The Author(s) 2024.",Augmented Reality; Legibility; Mixed Reality; Readability; Text,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A scope of presence-related feelings in AR studies,VRS - Virtual Reality,B,"The idiosyncrasies of augmented reality bring us advantages, as shown in literature, and a great assortment of options. On the other hand, due to its relationship with the real surroundings, it may be a challenge to deal with when evaluating these systems, especially if the intention is to understand feelings of presence-like: There are a lot of variables in the equation. This study aims to analyse a state of the art of AR evaluations that conducted presence-related feelings and discusses limitations and remarks for further research. The current research is able to state that questionnaires are the most used tool to ascertain presence-like feelings, and that mobile devices have been the preferred device to implement AR applications. The studies are fairly divided between 1) analysing a single scenario to ascertain the variable at study, and 2) creating two or more scenarios to make a comparison. When comparing two or more scenarios, a between-subjects design is preferred among researchers. Additionally, it has been identified as of paramount importance the need to study and objectively measuring the ratio between the virtual content and the real scenario in the experience, as well as to deeply research the interaction between the real scenario and the virtual elements. The importance of the types of interaction in AR applications is also highlighted. © 2024, The Author(s).",Evaluating presence; Mobile augmented reality; Presence in AR,Abstract_Keywords,True,
Scopus,journalPaper,2024,An augmented reality application and experiment for understanding and learning spatial transformation matrices,VRS - Virtual Reality,B,"Understanding spatial transformations and their mathematical representations is essential in computer-aided design, computer graphics, robotics, etc. This research has developed and tested an augmented reality (AR) application (BRICKxAR/T) to enhance students’ learning of spatial transformation matrices. BRICKxAR/T leverages AR features, including information augmentation, physical–virtual object interplay, and embodied learning, to create a novel and effective visualization experience for learning. In this paper, we evaluated the BRICKxAR/T as a learning intervention using LEGO models for physical and virtual manipulatives in an experiment. The experiment compared AR (N = 29) vs. non-AR (N = 30) learning workshops with pre- and post-tests on Purdue Visualization of Rotations Test and math questions to assess students’ learning gains. All participants math scores significantly improved with the AR workshop tending to show greater improvements. The post-workshop survey showed students were inclined to think BRICKxAR/T an interesting and useful application, and they spent more time learning in AR than non-AR. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Embodied learning; Spatial transformations; Transformation matrices; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Revisiting multimedia learning design principles in virtual reality-based learning environments for autistic individuals,VRS - Virtual Reality,B,"Virtual reality (VR) offers promising opportunities for supporting autistic learners in developing social and cognitive skills. However, designing VR-based learning environments optimized for these learners requires a nuanced understanding that addresses their unique needs. This review article reconsiders the key theories of multimedia learning, including cognitive load theory and cognitive theory of multimedia learning, with the aim of illuminating how these theories can inform the development of effective VR-based learning environments for autistic learners. We propose four design goals for a VR-based learning environment optimized for autistic learners: (1) minimizing learners’ extraneous load via attention guiding, (2) managing intrinsic cognitive load in problem-solving, (3) fostering germane processing through multiple representations, and (4) assessing cognitive load and implementing adaptive learning support design. In this exploration, we bring to demonstrate prevalent design challenges of existing VR-based learning environments for autistic individuals and offer prospective research trajectories for their enhancement. By incorporating a strengths-based approach, accommodating the diverse sensory needs, and recognizing the cognitive differences among autistic individuals, we aspire to advance a more inclusive VR design practice. This review presents crucial insights and direction for researchers and designers aiming to create effective, accessible, and inclusive VR-based learning environments for autistic learners and beyond. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Autism; Cognitive load; Multimedia learning design; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Effect of optical flow and user VR familiarity on curvature gain thresholds for redirected walking,VRS - Virtual Reality,B,"Virtual reality (VR) locomotion should allow users to move freely in the virtual space while staying within the tracking area in the real space. The redirected walking (RDW) technique enables users to walk naturally in an unlimited virtual space within a limited tracking area by rotating the virtual scene view. However, conflicting visual and vestibular signals during RDW can lead to user discomfort and decreased immersion. To avoid user discomfort, an RDW gain should be within the detection threshold (DT) range. However, a large angle of walking redirection is required when physically avoiding obstacles or escaping from a narrow space, so DT expansion is necessary. In this study, to change the curvature DT range and enhance RDW performance, we proposed an optical flow (OF)-generating vection in a virtual environment. Further, we investigate methods to reduce user discomfort and increase RDW efficiency considering familiar and unfamiliar VR users. The findings showed that the introduction of OF led to a reduction in the DT range for all users, irrespective of the OF’s direction. However, conditions with OF resulted in an extended DT range for users familiar with VR while concurrently diminishing the DT range for those who were VR unfamiliar. To delve further, our analysis indicated that when both the OF and redirecting directions were identical, the RDW performance was robust to VR familiarity, whereas in opposing directions, the DT range increased for VR-familiar users. Our study findings suggested using OF for the RDW technique and extending its applicability in virtual environments. © 2024, The Author(s).",Detection thresholds; Optical flow; Redirected walking; Virtual reality; VR familiarity,Abstract_Keywords,True,
Scopus,journalPaper,2024,Training using a commercial immersive virtual reality system on hand–eye coordination and reaction time in students: a randomized controlled trial,VRS - Virtual Reality,B,"Abstract: The implementation of VR games opens up a wide range of opportunities for the development of dexterity, speed and precision of movements among various professional groups. The aim of this study was to investigate the effects of a commercial immersive VR music game on hand–eye coordination and reaction time speed in students. This study enrolled 32 individuals, randomly assigned to the experimental or control group. The intervention consisted of a 15-min training session of the immersive music game “Beat Saber”, once a day for 5 consecutive days. The primary outcomes included reaction time measurements: the plate tapping test and the ruler-drop test (Ditrich's test), trial making test (TMT) A and TMT B to assess coordination and visual attention, likewise VR sickness assessment by Virtual Reality Sickness Questionnaire (VRSQ). The secondary outcome included an energy expenditure assessment (SenseWear Armband). The data analysis revealed a statistically significant improvement in hand–eye coordination in the experimental group, with no improvement in the control group. The results were similar in measurements of reaction time. Analysis of the VRSQ questionnaire results showed a statistically significant reduction in oculomotor domain symptoms and total score during successive training days. The immersive VR music game has the potential to improve reaction time and hand–eye coordination in students. Graphical abstract: [Figure not available: see fulltext.] © 2024, The Author(s).",Beat Saber; Hand–eye coordination; Immersion; Reaction time; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Enhancing weight perception in virtual reality: an analysis of kinematic features,VRS - Virtual Reality,B,"This study investigates weight perception in virtual reality without kinesthetic feedback from the real world, by means of an illusory method called pseudo-haptic. This illusory model focuses on the dissociation of visual input and somatosensory feedback and tries to induce the sensation of virtual objects' loads in VR users by manipulating visual input. For that, modifications on the control-display ratio, i.e., between the real and virtual motions of the arm, can be used to produce a visual illusionary effect on the virtual objects' positions as well. Therefore, VR users perceive it as velocity variations in the objects' displacements, helping them achieve a better sensation of virtual weight. A primary contribution of this paper is the development of a novel, holistic assessment methodology that measures the sense of the presence in virtual reality contexts, particularly when participants are lifting virtual objects and experiencing their weight. Our study examined the effect of virtual object weight on the kinematic parameters and velocity profiles of participants' upward arm motions, along with a parallel experiment conducted using real weights. By comparing the lifting of real objects with that of virtual objects, it was possible to gain insight into the variations in kinematic features observed in participants' arm motions. Additionally, subjective measurements, utilizing the Borg CR10 questionnaire, were conducted to assess participants' perceptions of hand fatigue. The analysis of collected data, encompassing both subjective and objective measurements, concluded that participants experienced similar sensations of fatigue and changes in hand kinematics during both virtual object tasks, resulting from pseudo-haptic feedback, and real weight lifting tasks. This consistency in findings underscores the efficacy of pseudo-haptic feedback in simulating realistic weight sensations in virtual environments. © The Author(s) 2024.",Kinematic features; Multi-sensory conflict; Pseudo-haptic feedback; Virtual weight; Weight perception,Title_Abstract,True,
Scopus,journalPaper,2023,"Inclusive Immersion: a review of efforts to improve accessibility in virtual reality, augmented reality and the metaverse",VRS - Virtual Reality,B,"Virtual Reality (VR) and Augmented Reality (AR) afford new forms of work and leisure. While affordable and effective VR and AR headsets are now available, neither technology has achieved widespread user adoption. However, we predict continual technological advances and cost reductions are likely to lead to wider diffusion in society. Bridging the chasm from the early adopters to the early majority will require careful consideration of the needs of a more casual and diverse user population. In particular, it is desirable to minimise the exclusion of potential users based on their unique needs and maximise the inclusion of users in these novel immersive experiences. Ensuring equitable access to the emerging metaverse further reinforces the need to consider the diverse needs of users. We refer to this objective of maximising the accessibility and enjoyment potential of users of VR, AR and the metaverse as Inclusive Immersion. This paper reviews the research and commercial landscape seeking to address the accessibility needs of users in VR and AR. The survey provides the basis for a synthesis of the emerging strategies for maximising the inclusiveness of VR and AR applications. Finally, we identify several unaddressed accessibility challenges requiring further research attention. Our paper consolidates disparate efforts related to promoting accessible VR and AR and delivers directions for advancing research in this area. © 2023, The Author(s).",Accessibility; Augmented Reality; Disability; Mixed Reality; The metaverse; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Impact of immersive virtual reality games or traditional physical exercise on cardiovascular and autonomic responses, enjoyment and sleep quality: a randomized crossover study",VRS - Virtual Reality,B,"Objectives: To assess the potential of immersive virtual reality (IVR) in achieving moderate exercise intensity, and 2) to examine the acute effects of two IVR exergame sessions (BOXVR and Beat Saber), comparing them with the impact of traditional exercise on heart rate variability (HRV), perceived effort, delayed onset muscle soreness, motivation, and sleep. Materials and methods: A crossover design was used. The participants (n = 22) randomly performed two sessions of IVR and one session of moderate intensity physical activity, each session lasting 30 min. Heart Rate (HR) and HRV, Perceived Exertion Scale, Intrinsic Motivation Inventory, sleep quality, and perceived pain, were evaluated. Results: The cardiac response to the activities was significantly higher when participants performed traditional physical activity as compared to the BOXVR and Beat Saber games. Traditional training provided a different HRV response as compared to Beat Saber (LnRMSSD, p = 0.025; SDNN, p = 0.031). Although the sessions were planned for moderate intensity, BOXVR generated a moderate intensity (49.3% HRreserve), Beat Saber (29.6% HRreserve) a light one, and the Circuit session, a vigorous one (62.9% HRreserve). In addition, traditional training reported higher perceived exertion and pain with less enjoyment. Differences were observed between the exergames. BOXVR resulted in a lower cardiac response (HRmax and HRmean), and a higher perception of exertion and pain at 72 h. The sleep variables analyzed were not altered by any of the sessions. Conclusions: BOXVR and traditional training can lead to moderate intensity physical activity. However, traditional training could result in lower adherence to physical exercise programs, as it was perceived as more intense and less enjoyable. © The Author(s) 2024.",Adherence; Exercise; Exergames; Intrinsic motivation; Physical activity; Sedentary,Title_Abstract,True,
Scopus,journalPaper,2024,Augmented reality for sailing: a comparative study of head stabilized vs boat stabilized visualization data for wind and bearing angle,VRS - Virtual Reality,B,"The wind has been a natural and renewable resource used for professional and recreational maritime transportation of small and large vessels since human history. Sailing is making a comeback due to the growing focus on sustainability, accelerated by the recent global energy crisis. Seafarers rely on wind and bearing angle visualization to navigate efficiently and safely, thanks to the use of sensors and compasses. This paper focuses on Augmented Reality in Head-Mounted Displays visualization of wind and bearing angle data. We analyzed the literature and generated a heatmap of the used areas in the user’s field of view. Second, we designed and implemented two interfaces that use two different visualization techniques: Boat Stabilized (BS) and Head Stabilized (HS). We compared them in between the subject experiment (N = 44), using a simulated Virtual Reality simulator of the sailing scenario. The user’s primary task is wind events recognition, while obstacles (buoys) detection is secondary. We measured both task errors and reaction time, and submit NASA RTLX, SUS, UEQ, and visive auditive and kinesthetic (VAK) questionnaires. We found that BS has a significantly lower reaction time and better usability in the primary and secondary tasks. Both visualization techniques have similar users perceived cognitive load and user experience evaluation. VAK test showed that BS is better for kinaesthetic types and HS is better for visual types. © 2024, The Author(s).",Augmented reality; Human–computer interaction; Nautical; Navigation; Sailing; User study,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A systematic evaluation of an RTK-GPS device for wearable augmented reality,VRS - Virtual Reality,B,"Global Positioning Satellite (GPS) systems sample points on the Earth’s surface with meter accuracy. Real-Time Kinematic (RTK) devices improve GPS performances by providing real-time correction data from ground stations, achieving centimeter accuracy. Reliable tracking approaches are essential for Augmented Reality (AR) applications, especially for outdoor scenarios, which still present unsolved challenges. AR handheld tracking capabilities have been greatly improved by integrating visual tracking approaches with RTK devices, whereas little is known about combining wearable AR interfaces with RTK systems. Although wearable AR devices are intrinsically designed for AR applications, their performance dramatically reduces in large outdoor areas, comprising the user experience. Hence, this paper provides a rigorous evaluation of a small-size RTK device that does not need any additional software integration to collect positional data. The main goal of the assessment is to verify whether its integration with a wearable AR device is advantageous or not. The evaluation has been performed considering both static and dynamic scenarios in open-sky and urban areas. The results show that the RTK device can achieve 1 cm accuracy when used in open-sky areas. In contrast, its accuracy dramatically reduces in the proximity of buildings and obstacles, showing average errors ranging from 76 to 2561%. Since wearable AR devices have an average accuracy of 2 cm, the outcomes indicate that RTK devices should be combined with wearable AR devices only when the RTK device is far from obstacles. On the contrary, the positional data should be completely avoided when barriers surround the RTK device. © 2023, The Author(s).",Augmented reality; GPS; Outdoor tracking; Real-time kinematic,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Individualized foveated rendering with eye-tracking head-mounted display,VRS - Virtual Reality,B,"Foveated rendering (FR) technology is designed to improve the efficiency of graphical rendering processes. In rendering, individualized approaches can help to balance users’ experiences of visual quality and saving computational resource. However, previous studies have not rigorously examined it related with the FR techniques. To address this issue, we developed an individualized FR (IFR) method using different central vision sizes and peripheral vision resolutions across individuals in virtual reality. In three user studies with 88 participants who were divided into groups designated as “large central area (LCA)” and “small central area (SCA),” the effects of IFR were compared with those of using the full-resolution condition and the average FR condition. The results indicate that the LCA group experienced higher visual quality under the IFR and full-resolution conditions than under the average FR condition. In contrast, the SCA group exhibited comparable levels of dependent measures between the IFR and average FR conditions, but both were lower than those of the full-resolution condition. We also evaluated the computational benefits of the proposed IFR method, and the results demonstrated the effectiveness of our approach in saving resources across the two groups. Although lower-bound adjustments may be required for some users, our overall results suggest that IFR is a malleable technology for enhancing rendering efficiency in virtual reality. © 2024, The Author(s).",Eye-tracking; Foveated rendering; Head-mounted display; Individualization; User study; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,A collaborative AR application for education: from architecture design to user evaluation,VRS - Virtual Reality,B,"Augmented reality applications can be used in an educational context to facilitate learning. In particular, augmented reality has been successfully used as a tool to boost students’ engagement and to improve their understanding of complex topics. Despite this, augmented reality usage is still not common in schools and it still offers mostly individual experiences, lacking collaboration capabilities which are of paramount importance in a learning environment. This work presents an application called ARoundTheWorld, a multiplatform augmented reality application for education. It is based on a software architecture, designed with the help of secondary school teachers, that provides interoperability, multi-user support, integration with learning management systems and data analytics capabilities, thus simplifying the development of collaborative augmented reality learning experiences. The application has been tested by 44 students and 3 teachers from 3 different educational institutions to evaluate the usability as well as the impact of collaboration functionalities in the students’ engagement. Qualitative and quantitative results show that the application fulfils all the design objectives identified by teachers as key elements for augmented reality educational applications. Furthermore, the application was positively evaluated by the students and it succeeded in promoting collaborative behaviour. These results show that ARoundTheWorld, and other applications built using the same architecture, could be easily developed and successfully integrated into existing schools curricula. © The Author(s) 2024.",Augmented reality; Collaborative learning; Data analysis; Multi-user interactions; User evaluation,Abstract_Keywords,True,
Scopus,journalPaper,2023,Lightweight real-time hand segmentation leveraging MediaPipe landmark detection,VRS - Virtual Reality,B,"Real-time hand segmentation is a key process in applications that require human–computer interaction, such as gesture recognition or augmented reality systems. However, the infinite shapes and orientations that hands can adopt, their variability in skin pigmentation and the self-occlusions that continuously appear in images make hand segmentation a truly complex problem, especially with uncontrolled lighting conditions and backgrounds. The development of robust, real-time hand segmentation algorithms is essential to achieve immersive augmented reality and mixed reality experiences by correctly interpreting collisions and occlusions. In this paper, we present a simple but powerful algorithm based on the MediaPipe Hands solution, a highly optimized neural network. The algorithm processes the landmarks provided by MediaPipe using morphological and logical operators to obtain the masks that allow dynamic updating of the skin color model. Different experiments were carried out comparing the influence of the color space on skin segmentation, with the CIELab color space chosen as the best option. An average intersection over union of 0.869 was achieved on the demanding Ego2Hands dataset running at 90 frames per second on a conventional computer without any hardware acceleration. Finally, the proposed segmentation procedure was implemented in an augmented reality application to add hand occlusion for improved user immersion. An open-source implementation of the algorithm is publicly available at https://github.com/itap-robotica-medica/lightweight-hand-segmentation . © 2023, The Author(s).",Augmented reality; Hand segmentation; MediaPipe; Online processing; Semantic segmentation,Abstract_Keywords,True,
Scopus,journalPaper,2024,The value of collision feedback in robotic surgical skills training,VRS - Virtual Reality,B,"Collision feedback about instrument and environment interaction is often lacking in robotic surgery training devices. The PoLaRS virtual reality simulator is a newly developed desk trainer that overcomes drawbacks of existing robot trainers for advanced laparoscopy. This study aimed to assess the effect of haptic and visual feedback during training on the performance of a robotic surgical task. Robotic surgery-naïve participants were randomized and equally divided into two training groups: Haptic and Visual Feedback (HVF) and No Haptic and Visual Feedback. Participants performed two basic virtual reality training tasks on the PoLaRS system as a pre- and post-test. The measurement parameters Time, Tip-to-tip distance, Path length Left/Right and Collisions Left/Right were used to analyze the learning curves and statistically compare the pre- and post-tests performances. In total, 198 trials performed by 22 participants were included. The visual and haptic feedback did not negatively influence the time to complete the tasks. Although no improvement in skill was observed between pre- and post-tests, the mean rank of the number of collisions of the right grasper (dominant hand) was significantly lower in the HVF feedback group during the second post-test (Mean Rank = 8.73 versus Mean Rank = 14.27, U = 30.00, p = 0.045). Haptic and visual feedback during the training on the PoLaRS system resulted in fewer instrument collisions. These results warrant the introduction of haptic feedback in subjects with no experience in robotic surgery. The PoLaRS system can be utilized to remotely optimize instrument handling before commencing robotic surgery in the operating room. © The Author(s) 2024.",Haptic feedback; Patient safety; Robotic surgery; Simulation training; Skills acquisition; Visual feedback,Abstract,True,
Scopus,journalPaper,2024,Virtual reality environments for stress reduction and management: a scoping review,VRS - Virtual Reality,B,"Virtual reality, a cutting-edge innovation in the realm of digital experiences, though more frequently employed for entertainment and education, can also serve as a tool for immersing users in therapeutic settings that promote relaxation and mindfulness. An increasing number of research attempts investigate its usability and impact on stress evaluation, management and reduction. This scoping review aims to depict the current role of virtual reality in stress reduction and identify common methods and practice, technology patterns as well as gaps. Results depict the emerging research interest in the domain of VR-based stress reduction systems. The developed systems included in this review were basically addressed to the general public (59%) for daily life stress reduction utilizing a commercial VR headset often combined with supportive sensors. Guided imagery emerged as the most implemented method, but it is also noteworthy that almost all studies implicitly used this method. According to the analysis, most studies performed evaluation of the proposed VR system including both subjective and objective measurements to provide evidence on its efficiency and its actual impact on stress levels. Finally, validation methodologies attempt to point out the potential of VR technology in the direction of providing an efficient solution for the alleviation of stress burdens. Even though numerous studies report the usefulness and efficiency of VR technology regarding stress reduction, several challenges still need to be addressed, mainly because of the difficult definition, detection and evaluation of stress. An approach integrating the existing knowledge regarding signals that can act as biomarkers of stress and qualitative measurements could open new pathways toward the development of more impactful VR-based stress reduction systems. © The Author(s) 2024.",Stress reduction; Virtual environments; Virtual reality; VR,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Older adults’ user experience of virtual tourism: exploring presence and experiential value with respect to age difference,VRS - Virtual Reality,B,"Virtual tourism or VR tourism has attracted great attention for its ability to simulate real-world experiences at a much lower cost and safer than actual travel. However, previous studies have mainly targeted young people or studies on older adults did not compare different age groups with both quantitative and qualitative methods. This study analyzed empirical user experiences of VR tourism via head-mounted displays (HMDs) by age group, with particular focus on experiential values, presence, and cybersickness. It was found that older adults perceived higher experiential values and presence than younger people and experienced less cybersickness, which is in contrast to previous research suggesting that susceptibility to VR sickness may increase with age. Furthermore, many older adults considered that virtual tourism can be an alternative to actual travel. This study addresses the effectiveness and applicability of VR tourism in older demographics and its potential contribution to their welfare. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Human-centered computing; Human–computer interaction (HCI); Interaction paradigms; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,HapMotion: motion-to-tactile framework with wearable haptic devices for immersive VR performance experience,VRS - Virtual Reality,B,"We present a novel haptic rendering framework that translates the performer’s motions into wearable vibrotactile feedback for an immersive virtual reality (VR) performance experience. Here, we employ a rendering pipeline that extracts meaningful vibrotactile parameters including intensity and location. We compute these parameters from the performer’s upper-body movements which play a significant role in a dance performance. Therefore, we customize a haptic vest and sleeves to support vibrotactile feedback on the frontal and back parts of the torso and shoulders as well. To capture essential movements from the VR performance, we propose a method called motion salient triangle (MST). MST utilizes key skeleton joints’ movements to compute the associated haptic parameters. Our method supports translating both choreographic and communicative motions into vibrotactile feedback. Through a series of user studies, we validate the user preference for our method compared to the conventional motion-to-tactile and audio-to-tactile methods. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Haptics; Performance; Virtual reality; Wearables,Abstract_Keywords,True,
Scopus,journalPaper,2024,Mixed reality holograms for percutaneous lead extraction of cardiac implantable electronic devices: Mixed reality image-guided interventions,VRS - Virtual Reality,B,"To assess the potential of mixed reality holograms (MixR) based on CT images to improve percutaneous lead extraction (PLE) planning and intraoperative assistance. This was a prospective, controlled, single-centre study. Five patients with CIED infection for PLE were included in the study. Conventional imaging (chest radiograph and CT) and MixR holograms were evaluated for preoperative planning to identify common complications such as vascular thrombosis, broken leads, loops, kinking, fibrosis along the wires, and perforation of cardiovascular structures. The degree of difficulty of the procedure was estimated based on potential complications. After the PLE procedure, the level of concordance between conventional imaging and MixR holograms with intraoperative findings was evaluated. The utility of MixR intraoperative guidance was also assessed. MixR holograms demonstrated a very high correlation in predicting the presence of loops, kinking, and fibrosis compared to conventional imaging, which showed a low-to-high correlation. MixR also showed a high correlation in estimating the degree of difficulty of the procedure compared to conventional imaging, which tended to underestimate it. The surgeon who performed the PLE agreed that MixR was helpful during intraoperative assistance. MixR holograms based on CT images are an effective tool for understanding cardiovascular anatomy and detecting potential areas of complications. MixR may be used as a complementary tool for both preoperative planning and intraoperative assistance in PLE procedures. Graphical abstract: Mixed reality holograms for intraprocedural intervention assistance.[Figure not available: see fulltext.]. © 2024, The Author(s).",Augmented reality; Holograms; Mixed reality; Percutaneous lead extraction,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Sensorimotor adaptation in virtual reality: Do instructions and body representation influence aftereffects?,VRS - Virtual Reality,B,"Perturbations in virtual reality (VR) lead to sensorimotor adaptation during exposure, but also to aftereffects once the perturbation is no longer present. An experiment was conducted to investigate the impact of different task instructions and body representation on the magnitude and the persistence of these aftereffects. Participants completed the paradigm of sensorimotor adaptation in VR. They were assigned to one of three groups: control group, misinformation group or arrow group. The misinformation group and the arrow group were each compared to the control group to examine the effects of instruction and body representation. The misinformation group was given the incorrect instruction that in addition to the perturbation, a random error component was also built into the movement. The arrow group was presented a virtual arrow instead of a virtual hand. It was hypothesised that both would lead to a lower magnitude and persistence of the aftereffect because the object identity between hand and virtual representation would be reduced, and errors would be more strongly attributed to external causes. Misinformation led to lower persistence, while the arrow group showed no significant differences compared to the control group. The results suggest that information about the accuracy of the VR system can influence the aftereffects, which should be considered when developing VR instructions. No effects of body representation were found. One possible explanation is that the manipulated difference between abstract and realistic body representation was too small in terms of object identity. © The Author(s) 2024.",(4–6) Error attribution; Aftereffects; Object identity; Sensorimotor adaptation,Title_Abstract,True,
Scopus,journalPaper,2024,Virtual reality natural experiences for mental health: comparing the effects between different immersion levels,VRS - Virtual Reality,B,"Virtual nature is an innovative approach for promoting mental health. The purpose of this study was to compare the effects on mental health outcomes between two immersion levels of virtual reality natural experiences. The study design was a cluster trial. Healthy adults were allocated to two experimental groups. Identical pre-recorded 360° videos of natural scenes and sounds were played on the two virtual reality devices, one with a higher immersive level via a head-mounted display and the other one with a lower immersive level via a smartphone. The intervention was conducted for 30 min per session, once a week for 12 weeks. Data were collected by self-reported questionnaires at the baseline and post-intervention. In total, 54 participants completed the interventions. A significantly greater effect was revealed on improving happiness, self-rated health, and physical, mental, social, and environmental quality of life, and ameliorating distress, depression, and somatization in participants who experienced the higher immersive level compared to participants who experienced the lower immersive level. Virtual reality natural experiences with high immersion are recommended to promote mental health. © The Author(s) 2024.",Anxiety; Depression; Green space; Head-mounted display; Presence; Smartphone,Title_Abstract,True,
Scopus,journalPaper,2024,Not just a game: the effect of active versus passive virtual reality experiences on anxiety and sadness,VRS - Virtual Reality,B,"The use of virtual reality (VR) technology is becoming more common and can be harnessed as a tool to improve various emotional and psychological aspects. The present research explored whether different kinds of VR experience (i.e., active versus passive) would differently affect people’s mood, anxiety and sadness. Undergraduate students (n = 133) were randomly assigned to three study conditions: active game VR experience, passive VR experience and control 2D passive viewing and filled out a battery of questionnaires before and after manipulation. The results show that following both VR exposures (but not following the control condition), participants’ moods improved, and the degree of anxiety was reduced. The degree of sadness was reduced only following the active game VR experience. Regarding self-efficacy, it was higher in the passive VR experience but lower following the active game VR experience (and not affected by the control condition). In conclusion, the results indicate that short VR experiences could provide a suitable alternative for the lack of accessible treatments to improve mood and to alleviate levels of anxiety and sadness, although further research is needed to tailor and refine the exact VR experience that would best improve each specific psychological aspect. © 2024, The Author(s).",Active VR experience; Anxiety; Depression; Passive VR experience; Self-efficacy; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Designing for collaborative learning in immersive virtual reality: a systematic literature review,VRS - Virtual Reality,B,"Immersive learning technologies such as virtual reality have long been deemed as the next generation of digital learning environments. There is a limited number of studies addressing how immersive technologies can be designed, applied, and studied in collaborative learning settings. This paper presents a systematic review of empirical studies reporting on use of immersive virtual reality in collaborative learning within educational and professional learning settings. 11 studies have been grouped and coded in a textual narrative synthesis, outlining the pedagogical concepts behind the learning design, as well as the design of virtual reality environments and the collaborative learning activities in which the technology is employed. The results suggest that collaborative learning in virtual reality can currently be conceptualised as a shared experience in an immersive, virtually mediated space, where there is a shared goal/problem which learners must attend to collaboratively. This conceptualisation implies a need to design technologies, environments, and activities that support participation and social interaction, fostering collaborative learning processes. Based on the outlined conceptualisation, we present a series of recommendations for designing for collaborative learning in immersive virtual reality. The paper concludes that collaborative learning in virtual reality creates a practice- and reflection space, where learning is perceived as engaging, without the risk of interfering with actual practices. Current designs however struggle with usability, realism, and facilitating social interaction. The paper further identifies a need for future research into what happens within virtual reality, rather than only looking at post-virtual reality evaluations. © The Author(s) 2024.",360VR; Collaborative learning; Collaborative training; Pedagogy; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Evaluating the effectiveness of virtual reality for safety-relevant training: a systematic review,VRS - Virtual Reality,B,"The commercial release of affordable, low-cost, and consumer-ready virtual reality (VR) devices has increased the accessibility for researchers to investigate the benefits of VR technology including those aimed at education and training. VR technology provides several opportunities that may provide benefits over traditional training methods, this is particularly relevant for safety training due to its ability to safely simulate dangerous scenarios that would otherwise be difficult to access. When implementing a new technology, it is important to evaluate and validate its effectiveness. This paper presents a systematic review of VR safety-relevant training studies that perform an evaluation of their effectiveness. This comprehensive review includes 136 studies published between 2016 and August 2021. Results presented in this paper include application domains, study objectives, study designs, and evaluation measures. Results show that the majority of studies were applicable to health services with the majority focusing on effectiveness evaluation using true- or quasi-experimental design. This study then categorizes each reported evaluation measure into one of the four levels in Kirkpatrick’s model for training evaluation, results showed that the majority of studies evaluated learning (72.06%) and reaction (66.18%) levels with very few studies evaluating behavior and results levels. This study concludes by providing insights and recommendations to help future researchers make informed decisions when designing an effectiveness evaluation study for VR safety-relevant training applications. © 2023, The Author(s).",Evaluation methodologies; Safety training; Training delivery method; Training effectiveness; Training evaluation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A comprehensive survey on AR-enabled local collaboration,VRS - Virtual Reality,B,"With the rapid development of augmented reality (AR) technology and devices, it is widely used in education, design, industry, game, medicine and other fields. It brings new development opportunities for computer-supported cooperative work. In recent years, there has been an increasing number of studies on AR collaboration. Many professional researchers have also summarized and commented on these local and remote applications. However, to the best of our knowledge, there is no comprehensive review specifically on AR-enabled local collaboration (AR-LoCol). Therefore, this paper presents a comprehensive survey of research between 2012 and 2022 in this domain. We surveyed 133 papers on AR-LoCol in Web of Science, 75% of which were published between 2018 and 2022. Next, we provide an in-depth review of papers in seven areas, including time (synchronous and asynchronous), device (hand-held display, desktop, spatial AR, head-mounted display), participants (double and multiple), place (standing, indoor and outdoor), content (virtual objects, annotations, awareness cues and multi-perspective views), and area (education, industry, medicine, architecture, exhibition, game, exterior design, visualization, interaction, basic tools). We discuss the characteristics and specific work in each category, especially the advantages and disadvantages of different devices and the necessity for shared contents. Following this, we summarize the current state of development of AR-LoCol and discuss possible future research directions. This work will be useful for current and future researchers interested in AR-LoCol systems. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",AR-enabled local collaboration; Augmented reality (AR); Computer-supported cooperative work; Literature review; Survey,Abstract_Keywords,True,
Scopus,journalPaper,2024,Feasibility study to identify machine learning predictors for a Virtual Environment Grocery Store,VRS - Virtual Reality,B,"Virtual reality-based assessment and training platforms proffer the potential for higher-dimensional stimulus presentations (dynamic; three dimensional) than those found with many low-dimensional stimulus presentations (static; two-dimensional) found in pen-and-paper measures of cognition. Studies have investigated the psychometric validity and reliability of a virtual reality-based multiple errands task called the Virtual Environment Grocery Store (VEGS). While advances in virtual reality-based assessments provide potential for increasing evaluation of cognitive processes, less has been done to develop these simulations into adaptive virtual environments for improved cognitive assessment. Adaptive assessments offer the potential for dynamically adjusting the difficulty level of tasks specific to the user’s knowledge or ability. Former iterations of the VEGS did not adapt to user performance. Therefore, this study aimed to develop performance classifiers from participants (N = 75) using three classification techniques: Support Vector Machines (SVM), Naive Bayes (NB), and k-Nearest Neighbors (kNN). Participants were categorized as either high performing or low performing based upon the number items they were able to successfully find and add to their grocery cart. The predictors utilized for the classification focused on the times to complete tasks in the virtual environment. Results revealed that the SVM (88% correct classification) classifier was the most robust classifier for identifying cognitive performance followed closely by kNN (86.7%); however, NB tended to perform poorly (76%). Results suggest that participants’ task completion times in conjunction with SVM or kNN can be used to adjust the difficult level to best suit the user in the environment. © 2024, The Author(s).",Adaptive virtual environments; Cognitive; Machine learning; Psychological assessment,Abstract,True,
Scopus,journalPaper,2024,Local geometric edge features based registration for textureless object in augmented reality assisted assembly,VRS - Virtual Reality,B,"Image-based methods have been widely used in augmented reality (AR) assistant assembly systems. However, due to the lack of sufficient texture information on the surface of assembly part, traditional image feature matching methods still face challenges. This paper proposes a coarse-to-fine AR registration method for textureless assembly part. In the first stage, a new feature matching method which is called line neighborhood edge descriptor (LNED) is presented to find the coarse camera pose from textureless image. The LNED take the contour line of assembly part as the description object, and use local geometric edge of assembly part to describe the contour line. During the image matching, the binary encoding is used to reduce the computational consumption for LNED. In the second stage, spatial points in the CAD model of assembly part are reverse projected to the textureless image based on the coarse camera pose. And the bundle adjustment method based on the edge distance of the textureless image is adopted to iteratively calculate the precise camera pose. In the experimental evaluation, the proposed registration method shows high accuracy and fast speed in comparison with conventional registration methods, which demonstrates that our method can effectively solve the problem of AR registration for textureless assembly part. © 2024, The Author(s).",Augmented assembly; Bundle adjustment; Feature description; Registration,Title_Abstract,True,
Scopus,journalPaper,2024,Developing a framework for heterotopias as discursive playgrounds: a comparative analysis of non-immersive and immersive technologies,VRS - Virtual Reality,B,"The discursive space represents the reordering of knowledge gained through accumulation. In the digital age, multimedia has become the language of information, and the space for archival practices is provided by non-immersive technologies, resulting in the disappearance of several layers from discursive activities. Heterotopias are unique, multilayered epistemic contexts that connect other systems through the exchange of information. This paper describes a process to create a framework for Virtual Reality, Mixed Reality, and personal computer environments based on heterotopias to provide absent layers. This study provides virtual museum space as an informational terrain that contains a “world within worlds” and presents place production as a layer of heterotopia and the subject of discourse. Automation for the individual multimedia content is provided via various sorting and grouping algorithms, and procedural content generation algorithms such as Binary Space Partitioning, Cellular Automata, Growth Algorithm, and Procedural Room Generation. Versions of the framework were comparatively evaluated through a user study involving 30 participants, considering factors such as usability, technology acceptance, and presence. The results of the study show that the framework can serve diverse contexts to construct multilayered digital habitats and is flexible for integration into professional and daily life practices. © 2024, The Author(s).",Design research; Design tools; Multimedia; User interface; Virtual museums; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,"Reaching interactions in virtual reality: the effect of movement direction, hand dominance, and hemispace on the kinematic properties of inward and outward reaches",VRS - Virtual Reality,B,"Recent literature has revealed that when users reach to select objects in VR, they can adapt how they move (i.e., the kinematic properties of their reaches) depending on the: (1) direction they move, (2) hand they use, and (3) side of the body where the movement occurs. In the present work, we took a more detailed look at how kinematic properties of reaching movements performed in VR change as a function of movement direction for reaches performed on each side of the body using each hand. We focused on reaches in 12 different directions that either involved moving inward (toward the body midline) or outward (away from the body midline). Twenty users reached in each direction on both left and right sides of their body, using both their dominant and non-dominant hands. The results provided a fine-grained account of how kinematic properties of virtual hand reaches change as a function of movement direction when users reach on either side of their body using either hand. The findings provide practitioners insights on how to interpret the kinematic properties of reaching behaviors in VR, which has applicability in emerging contexts that include detecting VR usability issues and using VR for stroke rehabilitation. © The Author(s) 2024.",Goal-directed reaching; Motor control; Movement kinematics; Virtual hand interaction; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2024,Real-time safe validation of autonomous landing in populated areas: from virtual environments to Robot-In-The-Loop,VRS - Virtual Reality,B,"Safe autonomous landing for Unmanned Aerial Vehicles (UAVs) in populated areas is a crucial aspect for successful integration of UAVs in populated environments. Nonetheless, validating autonomous landing in real scenarios is a challenging task with a high risk of injuring people. In this work, we propose a framework for safe real-time and thorough evaluation of vision-based autonomous landing in populated scenarios, using photo-realistic virtual environments and physics-based simulation. The proposed evaluation pipeline includes the use of Unreal graphics engine coupled with AirSim for realistic drone simulation to evaluate landing strategies. Then, Software-/Hardware-In-The-Loop can be used to test beforehand the performance of the algorithms. The final validation stage consists in a Robot-In-The-Loop evaluation strategy where a real drone must perform autonomous landing maneuvers in real-time, with an avatar drone in a virtual environment mimicking its behavior, while the detection algorithms run in the virtual environment (virtual reality to the robot). This method determines the safe landing areas based on computer vision and convolutional neural networks to avoid colliding with people in static and dynamic scenarios. To test the robustness of the algorithms in adversary conditions, different urban-like environments were implemented, including moving agents and different weather conditions. We also propose different metrics to quantify the performance of the landing strategies, establishing a baseline for comparison with future works on this challenging task, and analyze them through several randomized iterations. The proposed approach allowed us to safely validate the autonomous landing strategies, providing an evaluation pipeline, and a benchmark for comparison. An extensive evaluation showed a 99% success rate in static scenarios and 87% in dynamic cases, demonstrating that the use of autonomous landing algorithms considerably prevents accidents involving humans, facilitating the integration of drones in human-populated spaces, which may help to unleash the full potential of drones in urban environments. Besides, this type of development helps to increase the safety of drone operations, which would advance drone flight regulations and allow their use in closer proximity to humans. © The Author(s) 2024.",Robot-In-The-Loop; Unmanned aerial vehicles; Validation; Virtual environments; Virtual reality; Visual-based landing,Abstract_Keywords,True,
Scopus,journalPaper,2024,Assessing the validity of VR as a training tool for medical students,VRS - Virtual Reality,B,"The advances in Virtual Reality technologies, increased availability and reducing hardware costs have diminished many of the early challenges in the adoption of VR. However, a commonly identified gap in immersive Virtual Reality-Head Mounded Display (VR-HMD) training for medical education is the confidence in the long-term validity of the applications, in particular, the acceleration of the learning curve efficacy of learning outcomes over time and actual skills translation into real environments. Research shows a wide range of ad hoc applications, with superficial evaluations often conducted by technology vendors, based on assumed environments and tasks, envisaged (as opposed to actual) users and effectiveness of learning outcomes underpinned with little or no research focusing on a requirements-driven validation approach. This presents decision-making challenges for those seeking to adopt, implement and embed such systems in teaching practice. The current paper aims to (i) determine whether medical VR training improves the skill acquisition of training candidates, (ii) determine the factors affecting the acquisition of skills and (iii) validate the VR-based training using requirement-driven approach. In this paper, we used within- and between-subject design approaches to assess the validity of VR-based surgical training platform developed by Vantari VR against requirements which have been identified to have impact on learning processes and outcomes in VR-based training. First, study and control groups were compared based on their level of skill acquisitions. Then, by tailoring a requirements framework, the system was validated against the appropriate requirements. In total, 74 out of 109 requirements were investigated and evaluated against survey, observer and stakeholder workshop data. The training scenario covered the topic of Arterial Blood Gas (ABG) collection for second-year university medical students. In total 44 students volunteered to participate in this study, having been randomly assigned to either the study or control group. Students exposed to VR training (the study group) outperformed the control group in practical clinical skills training tasks and also adhered to better safety and hygiene practices. The study group also had a greater procedural completion rate over the control group. Students showed increased self-efficacy and knowledge scores immediately post-VR training. Prior ABG training did not impact on VR training outcomes. Low levels of simulation sickness, physical strain and stress, coupled with high levels of enjoyability, engagement, presence and fidelity were identified as factors affecting the overall training experience. In terms of learning, high scores were recorded for active learning, cognitive benefit and reflective thinking. Lastly, by validating the system against 74 system requirements, the study found a user acceptance level of 75%. This enabled the identification of weaknesses of the current system and possible future directions. © 2024, The Author(s).",Education; HMD-VR; Immersive; Surgical; Training; Validation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Social VR design features and experiential outcomes: narrative review and relationship map for dyadic agent conversations,VRS - Virtual Reality,B,"The application of virtual reality to the study of conversation and social interaction is a relatively new field of study. While the affordances of VR in the domain compared to traditional methods are promising, the current state of the field is plagued by a lack of methodological standards and shared understanding of how design features of the immersive experience impact participants. In order to address this, this paper develops a relationship map between design features and experiential outcomes, along with expectations for how those features interact with each other. Based on the results of a narrative review drawing from diverse fields, this relationship map focuses on dyadic conversations with agents. The experiential outcomes chosen include presence & engagement, psychological discomfort, and simulator sickness. The relevant design features contained in the framework include scenario agency, visual fidelity, agent automation, environmental context, and audio features. We conclude by discussing the findings of the review and framework, such as the multimodal nature of social VR being highlighted, and the importance of environmental context, and lastly provide recommendations for future research in social VR. © The Author(s) 2024.",Agent; Conversation study; Narrative review; Social VR; Virtual reality; VR,Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality in academic English writing: exploring factors influencing abstract knowledge learning,VRS - Virtual Reality,B,"Virtual reality technology has been increasingly used in language education. Its immersive nature can help learners to better focus and understand subjects that can be visualized, such as mechanical design, architectural design, and molecular chemical construction. The novelty of the environment can also stimulate learners’ enthusiasm for learning. Nonetheless, little research has focused on how learners’ prior knowledge affects their learning experience and learning outcomes in VR environments, especially when they learn abstract conceptual knowledge. In this work, we developed a VR learning system to help learners acquire abstract knowledge of academic English writing. We used both control and experimental groups to evaluate this system and did a comparative analysis between learners with different prior knowledge. Our results show that compared with learning in a traditional way, learning abstract knowledge in a VR environment can provide learners with a better experience. We also found that learners with poor prior knowledge learned more efficiently in a VR environment when compared to learning in a non-VR environment. Our work sheds light on how to design a VR abstract knowledge learning system for learners with/without previous knowledge. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Abstract knowledge learning; Academic English writing; Prior knowledge; Technology-enhanced learning; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Designing a 3D gestural interface to support user interaction with time-oriented data as immersive 3D radar charts,VRS - Virtual Reality,B,"The design of intuitive three-dimensional user interfaces is vital for interaction in virtual reality, allowing to effectively close the loop between a human user and the virtual environment. The utilization of 3D gestural input allows for useful hand interaction with virtual content by directly grasping visible objects, or through invisible gestural commands that are associated with corresponding features in the immersive 3D space. The design of such interfaces remains complex and challenging. In this article, we present a design approach for a three-dimensional user interface using 3D gestural input with the aim to facilitate user interaction within the context of Immersive Analytics. Based on a scenario of exploring time-oriented data in immersive virtual reality using 3D Radar Charts, we implemented a rich set of features that is closely aligned with relevant 3D interaction techniques, data analysis tasks, and aspects of hand posture comfort. We conducted an empirical evaluation (n= 12) , featuring a series of representative tasks to evaluate the developed user interface design prototype. The results, based on questionnaires, observations, and interviews, indicate good usability and an engaging user experience. We are able to reflect on the implemented hand-based grasping and gestural command techniques, identifying aspects for improvement in regard to hand detection and precision as well as emphasizing a prototype’s ability to infer user intent for better prevention of unintentional gestures. © 2024, The Author(s).",3D gestural input; 3D radar chart; Empirical study; Immersive analytics; User interface design; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,VR Designer: enhancing fashion showcases through immersive virtual garment fitting,VRS - Virtual Reality,B,"This paper introduces a Virtual Reality (VR) application tailored for fashion designers and retailers, transcending traditional garment design and demonstration boundaries by presenting an immersive digital garment showcase within a captivating VR environment. Simulating a virtual retail store, designers navigate freely, selecting from an array of avatar-garment combinations and exploring garments from diverse perspectives. This immersive experience offers designers a precise representation of the final product’s aesthetics, fit, and functionality on the human body. Our application can be considered as a pre-manufacturing layer, that empowers designers and retailers with a precise understanding of how the actual garment will look and behave. Evaluation involved comprehensive feedback from both professional and undergraduate fashion designers, gathered through usability testing sessions. © The Author(s) 2024.",3D fashion design; Avatar-based garment simulation; Avatar-garment fitting; Digital garment design; Garment visualization; Unity; Virtual garment fitting; Virtual reality; XR,Abstract_Keywords,True,
Scopus,journalPaper,2023,An innovative mixed reality approach for maxillofacial osteotomies and repositioning,VRS - Virtual Reality,B,"Craniomaxillofacial surgeries are performed using custom-made physical cutting guides and resin dental splints that present several drawbacks (e.g. time and cost required for their design and production). The literature commonly provides augmented/mixed reality (AR/MR) solutions for assisting maxillofacial osteotomies and repositioning without any interactive guide. This work proposes a new MR application, useful for osteotomy and repositioning, providing interactive, fast, and intuitive feedback to the surgeon, who is then supported in performing the bone fragment resection and replacement frame by frame. The proposed application speeds up the surgery and reduces under/overshooting errors. Moreover, the idea of integrating osteotomy and repositioning assistance in the same MR application is rarely found in the literature. It is an entirely novel approach to craniomaxillofacial surgery. The MR application has been designed with a three-button menu. The “App Start” calibrates the app, the “Osteotomy Mode” visualises the holograms of the cutting lines and drilling points, and the “Repositioning Mode” visualises the step-by-step real-time feedback to precisely support the surgeon placing the osteotomised bone fragment towards the final pre-planned position. The MR app has been developed in Unity and deployed on Microsoft HoloLens V2. A laboratory test bench was realised to validate the accuracy of the proposed MR-based approach. The validation protocol consists of two tasks to test the osteotomy and repositioning modes using a 3D-printed skull phantom. For osteotomy, the accuracy is 0.89 mm (genioplasty), 1.24 mm (maxillary osteotomy), 1.33 mm (orthognathic surgery), and 2.89 mm (mandibular angle osteotomy). For repositioning, the accuracy is 0.6 mm (anteroposterior deviation), 0.7 mm (mediolateral deviation), and 0.6° (angular deviation). © 2023, The Author(s).",Augmented reality; Hololens; Maxillofacial surgery; Mixed reality; Surgical guide,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The impact of visual and auditory distractions on the performance of neurodiverse students in virtual reality (VR) environments,VRS - Virtual Reality,B,"Ambient environmental stimuli may impact how a student is or is not able to apply themselves in cognitive and educational tasks. For neurodivergent learners, these barriers can be compounded as they may be more likely to attend to task-irrelevant ambient noise. The affordances of new systems, such as virtual reality (VR), could be useful for allowing neurodivergent students more deliberate control over what information they experience and what information they do not. This study seeks to explore the dynamics of attention in VR environments. To address this, participants were asked to perform a number of visual search tasks in VR to assess the impact of both visual and auditory distractions on speed and accuracy markers. Results indicate a differential impact of background noise on the performance of neurotypical and neurodivergent participants. Potential benefits to neurodiverse populations and design recommendations in this emerging space are discussed. © 2024, The Author(s).",Autism; Cognitive processing; Neurodiverse; Virtual reality; Visual search,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Correction to: Using interpretative phenomenological analysis to gain a qualitative understanding of presence in virtual reality (Virtual Reality, (2023), 27, 2, (1173-1185), 10.1007/s10055-022-00719-2)",VRS - Virtual Reality,B,"In the Original publication, two authors, Dr. Jenny Hallam and Dr. Simon Bignell who contributed to the research were not included in the author group. This has now been corrected in the original publication. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.",,Title,True,
Scopus,journalPaper,2024,Navigation in immersive virtual reality: a comparison of 1:1 walking to 1:1 wheeling,VRS - Virtual Reality,B,"In most virtual reality (VR) experiences, locomotion is not the primary objective. Yet how someone moves through a virtual environment (VE) has a profound impact on their experience. Extant research considers 1:1 walking, where the same movement a user makes in the real world is translated identically into movement in the virtual world, as the paragon of comfortable VR locomotion. However, this form of locomotion is not practical in most applications and remains inaccessible to non-ambulant persons using VR. In the current study, 1:1 walking was compared to 1:1 wheeling, whereby pushes made on a physical wheelchair translated directly into movement in the VE. User experience in 1:1 walking was compared to 1:1 wheeling through changes in self-reported positive and negative affect, simulator sickness, system usability, and presence. Participants’ ability to learn a complex VE in the two conditions was also assessed with a spatial updating task. These comparisons revealed no statistically significant differences. This finding challenges the prevailing assumption that 1:1 walking in VR is superior to all other locomotion techniques and serves as an appeal for VR developers to consider designing applications for a seated experience that is safe, comfortable, and accessible to a broader population of people with diverse needs. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Locomotion; Navigation; Virtual reality; Wheelchair,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Non-photorealistic rendering as a feedback strategy in virtual reality for rehabilitation,VRS - Virtual Reality,B,"Virtual reality (VR) rehabilitation has been proven to be a very promising method to increase the focus and attention of patients by immersing them in a virtual world, and through that, improve the effectiveness of the rehabilitation. One of the biggest challenges in designing VR Rehabilitation exercises is in choosing feedback strategies that guide the patient and give the appropriate success/failure indicators, without breaking their sense of immersion. A new strategy for feedback is proposed, using non-photorealistic rendering (NPR) to highlight important parts of the exercise the patient needs to focus on and fade out parts of the scene that are not relevant. This strategy is implemented into an authoring tool that allows rehabilitators specifying feedback strategies while creating exercise profiles. The NPR feedback can be configured in many ways, using different NPR schemes for different layers of the exercise environment such as the background environment, the non-interactive exercise objects, and the interactive exercise objects. The main features of the system including the support for universal render pipeline, camera stacking, and stereoscopic rendering are evaluated in a testing scenario. Performance tests regarding memory usage and supported frames per second are also considered. In addition, a group of rehabilitators evaluated the system usability. The proposed system meets all the requirements to apply NPR effect in VR scenarios and solves all the limitations with regard to technical function and image quality. In addition, the system performance has been shown to meet the targets for low-cost hardware. Regarding authoring tool usability rehabilitators agree that is easy to use and a valuable tool for rehabilitation scenarios. NPR schemes can be integrated into VR rehabilitation scenarios achieving the same image quality as non-VR visualizations with only a small impact on the frame rate. NPR schemes are a good visual feedback alternative. © The Author(s) 2024.",Feedback strategies; Non-photorealistic rendering; Rehabilitation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The restorative and state enhancing potential of abstract fractal-like imagery and interactive mindfulness interventions in virtual reality,VRS - Virtual Reality,B,"The restorative and mental state enhancing effects of brief mindfulness-based interventions (MBIs) and restorative environments such as nature has been supported in the research literature. However, regular adoption of these practices is limited by practical constraints and motivational barriers. The current study addressed these challenges by introducing two novel approaches which utilise the immersive and interactive qualities of virtual reality (VR). This included an interactive MBI and an abstract restorative environment using fractal-like imagery. These approaches were explored using a comparative evaluation of two short (6 min) VR interventions: Passive VR (applying principles from restorative interventions) and Interactive VR (implementing a focused attention form of mindfulness meditation). A mixed methods approach revealed increased state mindfulness, reduced mental fatigue, and enhanced aspects of mood (calm/relaxation, anxiety) consistently between conditions. Between group differences revealed additional benefits for cognition (focus), mood (happiness and sadness), and motivational value with the interactive intervention. The abstract environment, used in both interventions, maintained comparable levels of perceived restoration with a nature VR control condition. The results provide preliminary evidence supporting the use of interactive approaches for mindfulness interventions and abstract versions of restorative environments. © The Author(s) 2024.",Abstract environment; Attention restoration; Cognitive enhancement; Fractal; Guided breathing; Interactive meditation; Mindfulness meditation; Mood; Natural environment; Virtual reality; VR; Well-being,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,The distinction between first-person perspective and third-person perspective in virtual bodily self-consciousness,VRS - Virtual Reality,B,"The distinction between the first-person perspective (1PP) and the third-person perspective (3PP) has been widely regarded as fundamental and rigid, and many researchers hold that genuine bodily illusions can only be experienced from the 1PP. We applied VR technology to investigate whether this mainstream view is correct. In our experiments, the participants were immersed in a VR environment in which they saw a life-sized virtual body either from the 1PP or from the 3PP. They either passively received tactile stimulations and/or actively interacted with a virtual soccer ball. Our VR system created novel visuo-motor-tactile correlations between the real and the virtual world: when the participant interacted with a real plastic soccer ball, he/she would feel corresponding tactile sensations and see the avatar performing the exact same movements. We found that a clear sense of ownership over the avatar was induced not only in the 1PP condition but also in the Passive-3PP and the Active-3PP conditions. We also observed evidence suggesting that it is possible to experience one’s body-location, 1PP-location, as well as self-location, both from the 1PP and from the 3PP. Together, we demonstrate that there is in fact no fundamental gap between embodied 1PP and embodied 3PP in the virtual world. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",1PP-location; Body ownership; Body-location; First-person perspective; Self-location; Virtual reality,Keywords,True,
Scopus,journalPaper,2024,Virtual reality (VR) as a testing bench for consumer optical solutions: a machine learning approach (GBR) to visual comfort under simulated progressive addition lenses (PALs) distortions,VRS - Virtual Reality,B,"For decades, manufacturers have attempted to reduce or eliminate the optical aberrations that appear on the progressive addition lens’ surfaces during manufacturing. Besides every effort made, some of these distortions are inevitable given how lenses are fabricated, where in fact, astigmatism appears on the surface and cannot be entirely removed, or where non-uniform magnification becomes inherent to the power change across the lens. Some presbyopes may refer to certain discomfort when wearing these lenses for the first time, and a subset of them might never adapt. Developing, prototyping, testing and purveying those lenses into the market come at a cost, which is usually reflected in the retail price. This study aims to test the feasibility of virtual reality (VR) for testing customers’ satisfaction with these lenses, even before getting them onto production. VR offers a controlled environment where different parameters affecting progressive lens comforts, such as distortions, image displacement or optical blurring, can be inspected separately. In this study, the focus was set on the distortions and image displacement, not taking blur into account. Behavioural changes (head and eye movements) were recorded using the built-in eye tracker. We found participants were significantly more displeased in the presence of highly distorted lens simulations. In addition, a gradient boosting regressor was fitted to the data, so predictors of discomfort could be unveiled, and ratings could be predicted without performing additional measurements. © The Author(s) 2023.",Addition; Comfort; Distortions; Eye-tracking; Lenses; Progressive; Reality; Virtual,Title_Abstract,True,
Scopus,journalPaper,2024,Mixed reality prototyping for usability evaluation in product design: a case study of a handheld printer,VRS - Virtual Reality,B,"Prototyping is a critical step in the usability evaluation for product design. The maturity and affordability of mixed reality technology provide an opportunity to explore its application in prototyping. This study explored a flexible solution to create the mixed reality prototype for a handheld product by employing 3D printing, interactive 3D simulation, electronic prototyping platform, and Microsoft HoloLens. A comparative experiment was conducted to validate the effectiveness of the proposed prototype solution for usability evaluation. The results demonstrated that usability testing using the mixed prototype can accurately reveal changes in user performance across different task complexities, functional attributes, and physical contexts. The subjective assessments of product usability using the mixed prototype were highly consistent with the actual product. However, the absolute value of performance obtained from usability testing with the mixed prototype may deviate from the true value. In conclusion, mixed prototypes are more suitable for comparing the usability of different design alternatives under different conditions rather than obtaining an absolute measure of usability. This study establishes a significant theoretical foundation for product design assessment utilizing mixed prototypes, while providing practical guidance to designers and developers regarding the evaluation of product usability using mixed prototypes. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Mixed reality; Product design; Prototyping; Usability,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Ray tracing-based construction of 3D background model for real-time stereoscopic rendering of live immersive video,VRS - Virtual Reality,B,"Immersive video stored in multiview video-plus-depth format can provide viewers with vivid immersive experiences. However, rendering such video in real time in immersive environments remains a challenging task due to the high resolution and refresh rate demanded by recent extended reality displays. An essential issue in this immersive rendering is the disocclusion problem that inevitably occurs when virtual views are synthesized via the de facto standard 3D warping technique. In this paper, we present a novel virtual view synthesis framework that, from a live immersive video stream, renders stereoscopic images in real time for a freely moving virtual viewer. The main difference from previous approaches is that the surrounding background environment of the immersive video’s virtual scene is progressively reproduced on the fly directly in the 3D space while the input stream is being rendered. To allow this, we propose a new 3D background modeling scheme that, based on GPU-accelerated real-time ray tracing, efficiently and incrementally builds the background model in compact 3D triangular mesh. Then, we demonstrate that the 3D background environment can effectively alleviate the critical disocclusion problem in the immersive rendering, eventually reducing spatial and temporal aliasing artifacts. It is also suggested that the 3D representation of background environment enables extension of the virtual environment of immersive video by interactively adding 3D visual effects during rendering. © 2024, The Author(s).",3D background model; Disocclusion; Immersive video; Ray tracing; Real-time stereoscopic rendering; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2024,Immersive virtual reality for complex skills training: content analysis of experienced challenges,VRS - Virtual Reality,B,"This study aimed to examine the challenges that adult participants experienced in immersive virtual reality (I-VR). Practitioners have indicated that some challenges persist from trainee to trainee and scholars have called for the design and development of virtual reality (VR) applications based on learning theories. Thus, we examined challenges immersed learners experienced during self-discovery of game mechanics and assembly task within an early-development I-VR program. We clarified the immersive learning phenomenon by studying the self-reported problem statements from 168 university students and staff. They used an HTC Vive Pro Eye device and a custom-built software. Through an iterative content analysis of post-survey and video-stimulated recall interviews, we retrieved 481 problem statements from the participants. As a result, we derived and detailed 89 challenges, 22 component features, 11 components, and 5 principal factors of immersive learning. The most cited components that the participants found challenging were the use of controllers and functions, reciprocal software interaction, spatial and navigational constraints, relevance realisation, and learner capabilities. Closer inspection of the quantified data revealed that the participants without digital gaming experience reported relatively more hardware-related problem statements. The findings regarding the constraints of immersive learning helped clarify the various actants involved in immersive learning. In this paper, we provide a design implication summary for VR application developers. Further research on theory-based development and design implications in various immersive training settings is needed. © The Author(s) 2024.",Autonomous training; Challenge; Complex skill; Design implication; Immersive learning theory; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,ARLexic game: an augmented reality-based serious game for training of dyslexic and dysgraphic children,VRS - Virtual Reality,B,"Over the years, researchers have discovered increased problems among children related to reading and writing. Dyslexia and Dysgraphia are the most common problems they try to solve with various paper-based activities and gaming interventions. But they either lack interactivity, and children get bored of the games after some time, or therapists and parents cannot learn about their children's performance. Augmented Reality is nowadays evolving technology in the field of education. Although some Augmented Reality applications have developed in the market, there is no feasible solution for children suffering from Dyslexia and Dysgraphia. In this paper, we developed an Augmented Reality-based Serious Game named ARLexic game to train children with Dyslexia and Dysgraphia. We performed an experiment with dyslexic and dysgraphic children aged from 7 to 14 years, along with their teachers, to evaluate their performance. Our study results show that ARLexic Game is an entertaining and easy-to-use game for children. Children also engage with the application for a longer time due to Augmented Reality. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Dysgraphia; Dyslexia; Self-determination theory; Serious game,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Evaluating the impact of passive physical everyday tools on interacting with virtual reality museum objects,VRS - Virtual Reality,B,"Museums are increasingly embracing new methods and technologies to enhance the visitor experience. Virtual Reality (VR) provides the opportunity to experience objects and situations that are not readily available or don’t otherwise exist making it well suited to museum applications. Museum visitors represent an ultra-diverse cohort with technology experience levels ranging from first-time users through to experts, and typically needing to interact with the exhibit with little to no induction and training, and in many instances as a once off encounter. To support such users, this paper evaluates the impact of passive physical everyday tools to provide passive haptic feedback and enhance user interaction with desk-top sized museum objects. Museums face challenges in exhibiting larger objects and in this work the cargo area of a utility vehicle (i.e. ute) was selected as contextually suitable larger object. Three different interaction techniques are used with and without everyday physical tools and experiments undertaken to investigate the impact of the physical tools on the usability and user experience with free-hand interaction techniques. A comparison between using the passive physical tool for the interaction technique and without showed improved efficiency for two of the techniques and positive impact on the user experience with the mechanically more complex of the interaction techniques. These insights may prove useful in the design of interaction techniques for enhanced free-hand interaction with museum objects in VR. © 2024, Crown.",Gesture-based interaction techniques; Interaction design; Museum objects; Natural user interfaces; User experience; Virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,User experience model and design strategies for virtual reality-based cultural heritage exhibition,VRS - Virtual Reality,B,"A virtual reality (VR) based cultural heritage exhibition (VRCHE) is an important type of VR-based museum exhibition. The user experience (UX) design of VRCHE has encountered opportunities and due to the differences in human–computer interaction between VR-based and conventional interaction interfaces, so proposing the UX model of VRCHE is crucial. Although there are some existing works that study the UX models of VRCHEs, they are not complete enough to describe the UX of VRCHEs or offer any design strategies due to the methodologies and experimental materials that they currently use. This study creates experiments utilizing grounded theory that combine qualitative and quantitative approaches. Then, the study synthesizes three-level coding and quantitative analysis findings from grounded theory, builds a detailed model of the VRCHE UX using theoretical coding, and proposes design strategies. © The Author(s) 2024.",Cultural heritage; Grounded theory; Museum exhibition; User experience; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A color Passthrough mixed reality application for learning piano,VRS - Virtual Reality,B,"This work presents the development of a mixed reality (MR) application that uses color Passthrough for learning to play the piano. A study was carried out to compare the interpretation outcomes of the participants and their subjective experience when using the MR application developed to learn to play the piano with a system that used Synthesia (N = 33). The results show that the MR application and Synthesia were effective in learning piano. However, the students played the pieces significantly better when using the MR application. The two applications both provided a satisfying user experience. However, the subjective experience of the students was better when they used the MR application. Other conclusions derived from the study include the following: (1) The outcomes of the students and their subjective opinion about the experience when using the MR application were independent of age and gender; (2) the sense of presence offered by the MR application was high (above 6 on a scale of 1 to 7); (3) the adverse effects induced by wearing the Meta Quest Pro and using our MR application were negligible; and (4) the students showed their preference for the MR application. As a conclusion, the advantage of our MR application compared to other types of applications (e.g., non-projected piano roll notation) is that the user has a direct view of the piano and the help elements appear integrated in the user’s view. The user does not have to take their eyes off the keyboard and is focused on playing the piano. © The Author(s) 2024.",Color Passthrough; Learning; Meta Quest Pro; Mixed reality; Passthrough; Piano; Synthesia,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A Meta-analysis of augmented reality programs for education and training,VRS - Virtual Reality,B,"The application of augmented reality (AR) for education and training has grown dramatically in recent years, resulting in an expansive research domain within a relatively short amount of time. Two primary goals of the current article are to (a) summarize this literature by determining the overall effectiveness of AR programs relative to alternative comparisons and (b) assess the extent that AR program effectiveness is influenced by aspects of hardware, software, outcome, context, and methodology. A meta-analysis of over 250 studies supports that AR programs produce learning outcomes that are, on average, three-fifths of a standard deviation larger than alternative comparisons. Our results surprisingly show that AR programs using head-mounted displays produce significantly smaller effects than those using other output hardware (e.g., smartphones and tablets), and programs using image recognition are no more effective than those using alternative input methods (e.g., QR codes). We further find that most other aspects do not significantly influence observed program effectiveness; however, studies with younger participants produced significantly larger effects, and naturalistic studies produced significantly larger effects than laboratory studies. In our discussion, we utilize these findings to suggest promising theoretical perspectives for the study of AR, and we highlight methodological practices that can produce more accurate research moving forward. Thus, the current article summarizes research on AR education and training programs, identifies aspects that do and do not influence program efficacy, and provides several avenues for future research and practice. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Education; Learning; Meta-analysis; Mixed reality; Training,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Constructing hermeneutical relations: a postphenomenological inquiry into immersive VR memory palaces,VRS - Virtual Reality,B,"While VR adaptations of the mnemonic Method of Loci (or the ‘Memory Palace’ technique) show promising results in increasing mnemonic capabilities, little to no research has explored the use and integration of Virtual Memory Palaces over time in a context of self-initiated studying. To explore the use of Virtual Memory Palaces (VMPs) contextually, we conducted an in-the-wild study where we gave ten participants a VR Head-Mounted Display through which they could access and furnish their VMP over eight weeks. We conducted go-along interviews in our participants’ VMPs at various intervals throughout the eight-week study, exploring their creations and querying them about their experience. Based on our findings, this article discusses individual and contextual factors that come into play when a VMP is approached as a personal project in the midst of an already-established study routine. We frame our study as a postphenomenological inquiry into the mediating effects of VMPs, where our primary interest lies in what relationship the students developed to the VMP. © 2023, The Author(s).",Immersive virtual reality; Method of loci; Postphenomenology; Virtual memory palace,Keywords,True,
Scopus,journalPaper,2024,ARGo: augmented reality-based mobile Go stone collision game,VRS - Virtual Reality,B,"In this study, we present a mobile Go stone collision game based on augmented reality, which we call ARGo, inspired by the traditional Korean board game, Alkkagi. ARGo aims to resolve two main issues: (1) the portability and space constraints of the original Alkkagi and (2) the limited sense of reality due to the touchscreen-based interface of the existing mobile Alkkagi games. To improve a sense of the reality of the game, ARGo provides a gameplay interface similar to the original Alkkagi by recognizing the user‘s hand motion based on AR. Additionally, it provides a customization mechanism for each user to improve the recognition of the hand motion and the strength of the attack considering each user‘s characteristics. Finally, we make the following three main contributions. First, we employ the automata theory to design the game and collision scenarios between stones. Consequently, we can clearly define the complicated states incurred by AR-based motion recognition and collisions between virtual objects. Second, we propose a collision equation based on Continuous Collision Detection tailored to ARGo, i.e., Go stones and their collisions. Through experimental studies, we demonstrate that the collision equation enables the simulation of the exact collision effects. Third, through user experience studies, we verify the effectiveness of ARGo by showing the effects of the functions implemented in ARGo and its superiority over the existing mobile game Alkkagi Mania. © 2024, The Author(s).",Augmented reality; Automata theory-based game design; Collision effects; Customization; Mobile games; Motion recognition,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Physiological responses to the virtual reality-based Trier social stress test in patients with psychosis,VRS - Virtual Reality,B,"Objectives: The Trier social stress test (TSST) is one of the most reliable and widely used laboratory tests for evaluating the physiological stress response. We developed a virtual reality-based TSST (VR-TSST) and investigated the physiological responses to this test in patients with psychosis and healthy controls (HCs). Methods: The participants comprised 60 patients with psychosis and 66 HCs. The VR-TSST consisted of three scenarios: a resting phase (baseline; 2 min), a job interview (5 min), and a mental arithmetic task (5 min). Blood cortisol levels were measured at baseline, during the test, and at 5–10 min and 30 min after the test. The skin conductance level, heart rate, and RR intervals were measured at baseline, during the job interview, and during the arithmetic task. Results: The VR-TSST produced no discernible cortisol response in patients with psychosis compared to the HCs. However, a higher skin conductance level and heart rate and shorter RR intervals were found in the patients than in the HCs at baseline, during the job interview, and during the arithmetic task. Conclusion: These findings suggest that the current version of the VR-TSST induces stronger autonomic and cardiovascular, but not endocrine, responses in patients with psychosis than in HCs. The VR-TSST could be a valuable tool to evaluate or train the stress response in patients with psychosis. © 2023, The Author(s).",Cortisol; Heart rate; RR interval; Skin conductance level; Trier social stress test; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Robust vision-based glove pose estimation for both hands in virtual reality,VRS - Virtual Reality,B,"In virtual reality (VR) applications, haptic gloves provide feedback and more direct control than bare hands do. Most VR gloves contain flex and inertial measurement sensors for tracking the finger joints of a single hand; however, they lack a mechanism for tracking two-hand interactions. In this paper, a vision-based method is proposed for improved two-handed glove tracking. The proposed method requires only one camera attached to a VR headset. A photorealistic glove data generation framework was established to synthesize large quantities of training data for identifying the left, right, or both gloves in images with complex backgrounds. We also incorporated the “glove pose hypothesis” in the training stage, in which spatial cues regarding relative joint positions were exploited for accurately predict glove positions under severe self-occlusion or motion blur. In our experiments, a system based on the proposed method achieved an accuracy of 94.06% on a validation set and achieved high-speed tracking at 65 fps on a consumer graphics processing unit. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Glove dataset; Glove tracking; Hand pose estimation; Hand tracking; Haptic glove; Vision-based tracking,Title_Abstract,True,
Scopus,journalPaper,2024,A virtual reality data visualization tool for dimensionality reduction methods,VRS - Virtual Reality,B,"In this paper, we present a virtual reality interactive tool for generating and manipulating visualizations for high-dimensional data in a natural and intuitive stereoscopic way. Our tool offers support for a diverse range of dimensionality reduction (DR) algorithms, enabling the transformation of complex data into insightful 2D or 3D representations within an immersive VR environment. The tool also allows users to include annotations with a virtual pen using hand tracking, to assign class labels to the data observations, and to perform simultaneous visualization with other users within the 3D environment to facilitate collaboration. © The Author(s) 2024.",Information visualization; Natural interface; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A novel low cybersickness dynamic rotation gain enhancer based on spatial position and orientation in virtual environments,VRS - Virtual Reality,B,"Hardware and software resources nowadays make possible new Virtual Reality (VR) interaction methods. Numerous challenges have been involved over the years, and one of the most interesting is locomotion in virtual environments. In particular, Real Walking (RW) is one of the most effective Virtual Locomotion Techniques (VLT). It causes only reduced cybersickness, but it also requires proportional real walkable space to the virtual one, often requiring wide areas. In this context, optimization techniques have been proposed in the literature, e.g., reorientation or relocation. In this work, a novel method for improving reorientation in a virtual environment, exploiting a dynamic Rotation Gain Multiplication Factor (RGMF) based on the competence of the user in VR systems usage is proposed. The results highlight the effectiveness of the system and show the specific target of users that mainly appreciated it. © 2023, The Author(s).",Dynamic rotation gain; Human-computer interaction; Reorientation; User experience; Virtual locomotion technique,Abstract,True,
Scopus,journalPaper,2023,User behavior modeling for AR personalized recommendations in spatial transitions,VRS - Virtual Reality,B,"There have been studies on personalized augmented reality (AR) systems taking users’ contexts and histories into account. However, there is insufficient research on incorporating real-time user behavior and interactions into the personalized recommendations in spatial transitions, which can be used for new users without user history data. The spatial transitions, distances between two Point of Interests (POIs) of an AR tour trajectory through which users should pass to visit AR contents, need to be filled using the personalized AR contents to reduce the discontinuity experience. This paper aims to propose a user behavior model to recommend personalized contents in the AR tour trajectory to create a personalized experience. First, we model three interactions, including staring, skipping, and liking, using mathematical methods; and two spatial behaviors, including standing and moving, using a rule engine. Second, a content filtering-based recommendation method is presented to apply the proposed user model to recommend personalized AR contents in the spatial transitions. The experiment results showed that the proposed real-time user behavior model brought notable improvements in creating a personal experience of the AR tour system. The proposed method outperformed the conventional method in terms of recommendation performance metrics, so that it achieved an average of 15% more precision and recall scores and an average of 32% more personalization scores than those of the conventional method. Furthermore, the level of personalization perceived by the proposed user model demonstrated significant positive relationships with the level of decision effectiveness, system trustworthiness, and perceived recommendation transparency. The findings showed the effectiveness of the proposed framework in solving the cold-start recommendation problem. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Personalized recommendation; Spatial transitions; Tour trajectory; User behavior model,Abstract_Keywords,True,
Scopus,journalPaper,2024,Fast and robust virtual try-on based on parser-free generative adversarial network,VRS - Virtual Reality,B,"Image-based virtual try-on models have recently become popular leading to many new developments, especially in the past three years. The problem of virtual try-on requires trying on a cloth image on a target person’s image. Implementing the same turns out to be a complicated task. It involves calculating the position, angle, and texture for the cloth to be placed on the target that could be in varying orientations. Also, texture may change as a result of any change in orientation. Therefore, generating textures for the cloth also poses a major challenge. In this article, we propose a generative adversarial network-based virtual try-on network that is robust, fast, and parser-free. We dive into some of the latest developments in the field of virtual try-on models and discuss their market feasibility as well as techniques. It is observed that the performance of our proposed network is comparable to the state-of-the-art models, and it outperforms the latter in terms of execution speed owing to its low time complexity. Moreover, it uses a parser-free architecture. It does not require any external input or processing while testing or applying a trained model. It uses a “teacher-student” approach to learn from existing models. The loss function is based on final output of the model. Therefore, it can also learn its shortcomings from the output of the model, unlike other architectures where much of the training is done in a self-supervised manner from the real person’s image. © 2024, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; GAN; U-Net; Virtual try-on; VITON,Keywords,True,
Scopus,journalPaper,2024,A Multi-user Cross-platform hands-on virtual lab within the Metaverse – the case of machining training,VRS - Virtual Reality,B,"Distance learning has become a popular learning channel today. However, while various distance learning tools are available, most of them only support a single platform, offer only the trainer’s perspective, and do not facilitate student-instructor interaction. As a result, distance learning systems tend to be inflexible and less effective. To address the limitations of existing distance learning systems, this study developed a cross-platform hands-on virtual lab within the Metaverse that enables multi-user participation and interaction for distance education. Four platforms, HTC VIVE Pro, Microsoft HoloLens 2, PC, and Android smartphone, are supported. The virtual lab allows trainers to demonstrate operation steps and engage with multiple trainees simultaneously. Meanwhile, trainees have the opportunity to practice their operational skills on their virtual machines within the Metaverse, utilizing their preferred platforms. Additionally, participants can explore the virtual environment and interact with each other by moving around within the virtual space, similar to a physical lab setting. The user test compares the levels of presence and usability in the hands-on virtual lab across different platforms, providing insights into the challenges associated with each platform within the Metaverse for training purposes. Furthermore, the results of the user test highlight the promising potential of the architecture due to its flexibility and adaptability. © The Author(s) 2024.",Cross-platform; Distance learning; Hands-on; Metaverse; Multi-user,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Training street-level “sense of scale” as a spatial perception ability in recorded virtual reality: A Solomon four-group design approach,VRS - Virtual Reality,B,"Although spatial perception abilities are essential for improving architectural and urban design competence, the lack of appropriate educational tools has prevented them from being taught in higher education settings; accordingly, many students struggle with identifying such spatial characteristics. Using a Solomon four-group design approach, this study aimed to verify the effectiveness of virtual reality (VR)-aided training for “sense of scale,” which despite being one of the most critical abilities in design competence, is difficult to teach both in the classroom and on-site. We recruited 80 first- and second-year university students majoring in physical planning, and conducted two street-level sense of scale training sessions, as well as pre- and post-tests. We measured the sense of scale accuracy (i.e., the difference between the actual and reported values) using mean absolute percentage error, and the effect of experimental treatment was analyzed. The results demonstrated that the training was significantly effective at estimating dimensions for street width and building height but not D/H ratio. This suggests that a sense of “absolute” scale can be acquired through two VR-aided training sessions. Additional post hoc questionnaire surveys also demonstrated that the participants felt that VR-aided training was more effective for scale practice than traditional learning methods. The findings demonstrate that for spatial perception abilities that are difficult to teach with conventional teaching methods, VR-aided training could be utilized as an effective education technique. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Design education; Scale practice Virtual reality; Sense of scale; Solomon four-group design; Spatial perception ability,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A preliminary investigation into the efficacy of training soccer heading in immersive virtual reality,VRS - Virtual Reality,B,"Recent research has suggested a link between repetitive soccer heading and the increased incidence of neurodegenerative disease in retired players. In response, restrictions have been introduced to limit the amount of soccer heading in training and competitive matches. Therefore, while heading remains an integral part of the game, players are restricted in the amount of training that they can gain on this important skill without potentially harming their long-term wellbeing. The aim of this study was to provide a preliminary investigation into the efficacy of training soccer heading in immersive virtual reality (VR) which allows the practice of the skill without the risk of repetitive head impacts. Thirty-six recreational soccer players were divided into a VR group (n = 18) who trained soccer heading on three occasions over a 7–10-day period in VR and a control group (n = 18) who received no training in soccer heading. Measures of real-world heading performance (i.e. the number of goals scored and shot accuracy), perceived confidence and perceived self-efficacy were assessed pre- and post-training. The results showed that the VR group experienced significant improvements in the number of goals scored and increased their perceptions of confidence and self-efficacy. These results show preliminary support for the inclusion of VR-based training in soccer heading where players can hone their heading skills without exposure to repeated head impacts. Implications and practical applications are discussed. © 2023, The Author(s).",Concussion: sub-concussion; Football; Intervention; Sport,Title_Abstract,True,
Scopus,journalPaper,2023,Marketing in the metaverse era: toward an integrative channel approach,VRS - Virtual Reality,B,"The development pace of digital socialization has accelerated drastically in the past decade, especially with the COVID-19 pandemic. Through that continuing digital shift, the idea of the metaverse, a virtual parallel world that can digitally replicate people’s lives, is developing fast through Meta’s (previously known as Facebook) announcement in October 2021 that it will dedicate sizeable investments in it. While the metaverse provides immense opportunities to brands, the primary concern will be on how integrate it with current media and retail channels, whether they are offline or online. Accordingly, using an exploratory qualitative approach, this study examined the potential strategic channel-based marketing routes that companies would face in the presence of the metaverse. The findings show that the route to market will become much more complex given the metaverse’s own platform setup. Strategic multichannel and omnichannel routes are examined through a proposed framework that takes into consideration the expected evolution of the metaverse platform. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Metaverse; Multichannel; Omnichannel; Platform; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Home-based immersive virtual reality physical rehabilitation in paediatric patients for upper limb motor impairment: a feasibility study,VRS - Virtual Reality,B,"Upper limb motor impairment (ULMI) rehabilitation is a long-term, demanding and challenging process to recover motor functionality. Children and adolescents may be limited in daily life activities due to reduced functions such as decreased joint movement or muscle weakness. Home-based therapy with Immersive Virtual Reality can offer greater accessibility, delivery and early rehabilitation to significantly optimise functional outcomes and quality of life. This feasibility study aimed to explore the perceptions and impacts of an immersive and interactive VR scenario suitable for ULMI rehabilitation for children at home. It was analysed using mixed methods (quantitative and qualitative) and from a multidirectional perspective (patients, clinicians and family members). Amongst the main results, it was found that IVR for ULMI home rehabilitation (1) is easy to learn and acceptable; (2) improves motor function; (3) reduces the difficulty in the reproduction of therapeutic movements; (4) is motivating and enjoyable and (5) improves quality of life. This study is the first study on the use of IVR applied to home rehabilitation of ULMI in children. These results suggested that similar outcomes may be possible with self-directed IVR home rehabilitation compared to face to face conventional rehabilitation, which can be costly to both the patient and the healthcare system, decreasing the length of stay at the hospital and treatment duration. It has also presented an innovative solution to the Covid-19 emergency where children could not receive their clinic therapy. Further research is recommended to understand better the mechanisms involved in physiotherapeutic recovery and how IVR rehabilitation helps to improve conventional treatments. Trial Registration Protocol ID NCT05272436. Release Date: 9th March 2022. © 2023, The Author(s).",Children’s rehabilitation; Pain management; Patient-centred design; Upper limb motor impairment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Developing a virtual reality environment for educational and therapeutic application to investigate psychological reactivity to bullying,VRS - Virtual Reality,B,"Understanding how bullying victimisation influences cognitive and emotional processes may help to direct early intervention to prevent the development of psychopathology. In a convenience sample of 67 female adolescents, we assessed the potential of a newly developed classroom-set bullying experience in virtual reality (VR) to evoke psychological reactions. Two VR experiences were co-developed with young people, one neutral and one hostile (bullying). Participants were matched and assigned to a condition based on measures of anxiety, depression, paranoia, and previous bullying, before experiencing either the neutral or hostile scenario. Before and after the VR session, participants completed measures of negative affect and levels of distress. All participants remained immersed for the whole duration, which supports the acceptability of using these VR experiences with more vulnerable participants. Those experiencing the hostile version reported greater negative affect post-immersion compared to those experiencing the neutral version (p =.018; d = 0.61). Although non-significant, a similar outcome was found regarding distress (p =.071; d = 0.37). Whilst we did not find a significant relationship between pre-existing internalisation on negative affect and distress, our sample was limited by containing adolescents with relatively low levels of previous bullying experience. Yet we still found evidence that the VR scenario evoked bullying-related psychological reactions. Further testing with a more representative groups of adolescents, especially those with more experience of bullying, would be advised. The VR scenario could potentially be used in educational and therapeutic settings to enhance empathy towards victimised children or enhance resilience following victimisation. © 2023, The Author(s).",Adolescents; Anxiety; Bullying; Depression; Psychological reactivity; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,How to shoot yourself right with a smartphone?,VRS - Virtual Reality,B,"This article presents a new use case of using handheld augmented reality to set up a smartphone camera for taking a self-photograph needed for evaluating the user’s sports exercise. The problem addressed is that of settings up the rear smartphone camera so that users can take photographs of themselves performing sports poses without the need for assistance from another person. The proposed solution uses an augmented reality avatar with the same body properties as the user, which is placed into the scene so that the user can set up the camera to best capture the avatar. We also propose a straightforward mode of interaction for placing the avatar into the scene and setting up the camera for the desired purpose. We conducted a user study to explore the usability and user experience of the proposed solution. The results showed that the augmented reality visual aids, especially the avatar, enhanced the effectiveness of taking the self-photograph and that users perceived augmented reality elements favorably. © 2023, The Author(s).",Augmented reality; Self-capture; Sports training app; Virtual avatar; Visual aid,Abstract_Keywords,True,
Scopus,journalPaper,2023,Using immersive virtual reality to remotely examine performance differences between dominant and non-dominant hands,VRS - Virtual Reality,B,"Circle drawing may be a useful task to study upper-limb function in patient populations. However, previous studies rely on expensive and bulky robotics to measure performance. For clinics or hospitals with limited budgets and space, this may be unfeasible. Virtual reality (VR) provides a portable and low-cost tool with integrated motion capture. It offers potentially a more feasible medium by which to assess upper-limb motor function. Prior to use with patient populations, it is important to validate and test the capabilities of VR with healthy users. This study examined whether a VR-based circle drawing task, completed remotely using participant’s own devices, could capture differences between movement kinematics of the dominant and non-dominant hands in healthy individuals. Participants (n = 47) traced the outline of a circle presented on their VR head-mounted displays with each hand, while the positions of the hand-held controllers were continuously recorded. Although there were no differences observed in the size or roundness of circles drawn with each hand, consistent with prior literature our results did show that the circles drawn with the dominant hand were completed faster than those with the non-dominant hand. This provides preliminary evidence that a VR-based circle drawing task may be a feasible method for detecting subtle differences in function in clinical populations. © 2023, The Author(s).",Assessment; Dominant hand; Meta Quest; Non-dominant hand; Stroke; Upper limb,Title_Abstract,True,
Scopus,journalPaper,2023,Full body video-based self-avatars for mixed reality: from E2E system to user study,VRS - Virtual Reality,B,"In this work, we explore the creation of self-avatars through video pass-through in mixed reality (MR) applications. We present our end-to-end system, including custom MR video pass-through implementation on a commercial head-mounted display (HMD), our deep learning-based real-time egocentric body segmentation algorithm, and our optimized offloading architecture, to communicate the segmentation server with the HMD. To validate this technology, we designed an immersive VR experience where the user has to walk through a narrow tile path over an active volcano crater. The study was performed under three-body representation conditions: virtual hands, video pass-through with color-based full-body segmentation, and video pass-through with deep learning full-body segmentation. This immersive experience was carried out by 30 women and 28 men. To the best of our knowledge, this is the first user study focused on evaluating video-based self-avatars to represent the user in a MR scene. Results showed no significant differences between the different body representations in terms of presence, with moderate improvements in some Embodiment components between the virtual hands and full-body representations. Visual Quality results showed better results from the deep-learning algorithms in terms of the whole body perception and overall segmentation quality. In this study, we provide some discussion regarding the use of video-based self-avatars and some reflections on the evaluation methodology. The proposed E2E solution is in the boundary of the state-of-the-art, so there is still room for improvement before it reaches maturity. However, this solution serves as a crucial starting point for MR applications where users can feel immersed and interact with their own bodies. © 2023, The Author(s).",Egocentric vision; Embodiment; Mixed reality; Offloading; Video-based self-avatars,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"The role of digital literacy in augmented, virtual, and mixed reality in popular science education: a review study and an educational framework development",VRS - Virtual Reality,B,"This study aims to bridge the gap between extended reality (XR) and digital literacy (DL) in popular science education and further develop a DL–XR framework. XR includes augmented, virtual, and mixed reality (AR, VR, and MR), which has received increased attention and has been used for educational purposes in recent years. However, the studies of XR in popular science education and its impact on students are scant. It is also challenging to find studies entailing XR and DL in education. This study not only offers an overview of the status quo of XR education but also is the first research presenting a referential framework that systematically integrates the many dimensions of XR and DL for future research and educational practices. XR has been extensively used in museums, benefiting users with immersive, authentic, hands-on, and interactive experiences. In the DL–XR framework, based on the variations of “individual-group” and “passive consumption-active creation”, eight dimensions of DL linked to XR are proposed, including “access and understanding”, “evaluation”, “ethics and well-being”, “interaction”, “collaboration”, “creation”, “problem-solving”, and “civic engagement and responsibility”. In the nurturing of DL, evidence revealed that XR is mostly used for learners to access knowledge/information and interact with virtual items; nonetheless, its applications for active creation, problem-solving, and collaboration are seldom prioritised. This study further proposes integrating project-based learning into XR pedagogical practices, which can maximise its impact on learning and empower the learners to achieve advanced levels of DL. © 2023, The Author(s).",Augmented reality; Digital literacy; Mixed reality; Popular science education; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Incorporating AR/VR-assisted learning into informal science institutions: A systematic review,VRS - Virtual Reality,B,"Virtual reality (VR) and augmented reality (AR) technologies have been used in informal science institutions such as science centres, science museums, zoos, botanical gardens, and aquariums to provide visitors with engaging and appealing learning experiences. However, there is a lack of systematic reviews to synthesise the contexts in which such technologies have been applied, how AR/VR-assisted learning is designed, and what learning outcomes have commonly been reported in such learning contexts. A total of 22 studies were identified for this review. We find, first, that AR and VR have been primarily used in science museums and biology learning, mainly for learning content knowledge. Learning activities supported by AR typically involve the scientific observation of phenomena or objects. Second, AR and VR are often used to superimpose supplementary materials onto exhibits and simulate scientific phenomena or visually present abstract concepts. Mobile devices are more prevalent than head-mounted displays or other techniques. Third, perceptions and knowledge achievement are typically measured outcomes, and incorporating AR and VR has the potential to promote academic achievement and perceptions. Several implications are provided for future research. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented and virtual reality; Informal science institutions; Learning outcomes; Science centres; Science museums,Abstract_Keywords,True,
Scopus,journalPaper,2023,"Correction to: Acceptance and use of virtual reality games: an extension of HMSAM (Virtual Reality, (2023), 27, 3, (1585-1605), 10.1007/s10055-023-00749-4)",VRS - Virtual Reality,B,"In the original publication, Fig. 3 was published wrongly. The corrected Fig. 3 is as follows: (Figure presented.) The structural model (significant paths only) © 2023, The Author(s).",,Title,True,
Scopus,journalPaper,2023,"MAC: multimodal, attention-based cybersickness prediction modeling in virtual reality",VRS - Virtual Reality,B,"Cybersickness is one of the greatest barriers to the adoption of virtual reality. A growing body of research has focused on identifying the characteristics of cybersickness and finding ways to mitigate it through the utilization of data from VR content, physiological signals, and body movement, along with artificial intelligence techniques. In this work, we extend prior research on cybersickness prediction by considering the role of different data modalities. We propose a novel deep learning model named multimodal, attention-based cybersickness (MAC), which learns temporal sequences and characteristics of video flows, eye movement, head movement, and electrodermal activity. Based on data collected from 27 participants, we demonstrate the effectiveness of MAC, showing an F1-score of 0.87. Our experimental results further show not only the influences of gender and prior VR experience but also the effectiveness of the attention mechanism on model performance, emphasizing the importance of considering the characteristics of data types and users in cybersickness modeling. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Deep learning; User characteristics; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Dropout rate in randomised controlled trials of balance and gait rehabilitation in multiple sclerosis: is it expected to be different for virtual reality-based interventions? A systematic review with meta-analysis and meta-regression,VRS - Virtual Reality,B,"To assess and meta-analyse the pooled dropout rate from the randomised control trilas that use virtual reality for balance or gait rehabilitation in people with multiple sclerosis. A systematic review of randomised control trials with meta-analysis and meta-regressions was performed. A search was conducted in PubMed, Scopus, Web of Science, the Physiotherapy Evidence Database, the Cochrane Database, CINHAL, LILACS, ScienceDirect, and ProQuest. It was last updated in July 2022. After the selection of studies, a quality appraisal was carried out using the PEDro Scale and the Revised Cochrane risk-of-bias tool for randomised trials. A descriptive analysis of main characteristics and dropout information was performed. An overall proportion meta-analysis calculated the pooled dropout rate. Odds ratio meta-analysis compared the dropout likelihood between interventions. The meta-regression evaluated the influence of moderators related to dropout. Sixteen studies with 656 participants were included. The overall pooled dropout rate was 6.6% and 5.7% for virtual reality and 9.7% in control groups. The odds ratio (0.89, p = 0.46) indicated no differences in the probability of dropouts between the interventions. The number, duration, frequency, and weeks of sessions, intervention, sex, multiple sclerosis phenotype, Expanded Disability Status Scale score, and PEDro score were not moderators (p > 0.05). Adverse events were not reported and could not be analysed as moderators. Dropouts across the virtual reality and control comparators were similar without significant differences. Nonetheless, there is a slight trend that could favour virtual reality. Standardisation in reporting dropouts and adverse events is recommended for future trials. PROSPERO database, registration number ID CRD42021284989. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Adherence; Attrition; Dropout rate; Multiple sclerosis; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Effects of personalized movement models in virtual reality on pain expectancy and motor behavior in patients with chronic back pain: a feasibility study,VRS - Virtual Reality,B,"Cognitive-behavioral therapy (CBT) of chronic pain focuses on behavioral, cognitive, affective and social factors that play a role in the transition from acute to chronic pain, which often is initially caused by a specific event but then takes on “a life of its own”. CBT models assume that fear of pain and subsequent avoidance behavior contribute to pain chronicity and the maintenance of chronic pain. In chronic back pain (CBP), avoidance is often addressed by teaching patients to reduce pain behaviors (such as guarding and bracing that may become dysfunctional over time) and increase healthy behaviors (such as physical exercise and meaningful social activities). The current study explored if personalized virtual movement models (doppelganger avatars), who maximize model-observer similarity in virtual reality (VR), can influence fear of pain, motor avoidance and movement-related pain and function. In a randomized controlled trial, participants with CBP observed and imitated an avatar (AVA, N = 17) or a videotaped model (VID, N = 16) over three sessions, where moving a beverage crate, bending sideward (BS), and rotation in the horizontal plane (RH) were shown. Self-reported pain expectancy, as well as engagement, functional capacity and pain during movements, were analyzed along with range of motion (ROM). The AVA group reported higher engagement with no significant group differences observed in ROM. Pain expectancy increased in AVA but not VID over the sessions. Pain and limitations did not significantly differ. However, we observed a significant moderation effect of group, with prior pain expectancy predicting pain and avoidance in the VID but not in the AVA group. This can be interpreted as an effect of personalized movement models decoupling pain behavior from movement-related fear and pain expectancy. Thus, personalized virtual movement models may provide an additional tool for exposure and exercise treatments in cognitive-behavioral treatment approaches to CBP. © 2023, The Author(s).",Avatars; Chronic back pain; Cognitive-behavioral therapy; Fear avoidance; Mixed reality; Model-observer similarity; Movement behavior; Observational modelling; Observational placebo; Pain expectancy; Range of motion; Third-person perspective; Virtual doppelgangers; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Altered functional connectivity of the hippocampus with the sensorimotor cortex induced by long-term experience of virtual hand illusion,VRS - Virtual Reality,B,"Synchronizing the movement of a virtual hand with an unseen real hand in a virtual environment is an effective method to induce a sense of ownership of the virtual hand. Although several neuroimaging studies have revealed neural mechanisms related to the ongoing process of ownership illusion, the effect of the long-term experience of ownership illusion on brain activity remains to be investigated. Here, we developed an apparatus based on real-time image matting and virtual reality technology which allowed us to use the image of participants’ real hands as their virtual hands and synchronize the movement of the virtual hands with the unseen real hands in a virtual scene. Resting-state functional imaging data were acquired immediately after participants completed several light office tasks with the virtual hands in either the virtual environment (virtual hand condition) or real environment (real hand condition). Significant positive functional connectivity of the hippocampus with the primary somatosensory and motor cortex was only observed in the virtual hand condition. The results provided novel evidence for the involvement of hippocampal-sensorimotor connection in the long-term experience of virtual hand ownership. The functional connection reorganization of the hippocampus might promote multi-sensory information integration into memory and updated the sense of body ownership, which offered important insights into the neural network underlying the availability of long-term use of VR technology in healthcare and rehabilitation. © 2023, The Author(s).",Brain plasticity; Hippocampus; Resting-state fMRI; Virtual hand ownership illusion; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,SAPIENS in XR: operationalizing interaction-attention in extended reality,VRS - Virtual Reality,B,"We examine peripheral interactions in XR environments, for which we propose a conceptual space with two specialized dimensions, interaction-attention and reality-virtuality. We also formalize the notion of an “XR display” to expand the application range of ambient displays from physical environments to XR. To operationalize these conceptual contributions for researchers and practitioners, we capitalize on Sapiens, an open-source event-based software architecture for peripheral interactions in smart environments, to propose Sapiens-in-XR, an extended architecture that also covers XR displays. In a simulation study based on a Poisson probabilistic model of notification delivery, we demonstrate the efficiency of the event processing pipeline of Sapiens-in-XR with an average processing time of just 18ms from event creation to delivery. We present simulations of peripheral interaction scenarios enabled by our conceptual space and Sapiens-in-XR, and report empirical results from a controlled experiment implementing one scenario, where users were asked to maintain their focus of attention in the central field of view while notifications were displayed at the attention periphery. Our results show similar user perception and the same level of user performance for understanding and recalling content of notifications in either the virtual and physical environments. Our conceptual space, software architecture, and simulator constitute tools meant to assist researchers and practitioners to explore, design, and implement peripheral interactions in XR. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Conceptual space; Event-based software architecture; Extended reality; Head-mounted displays; Peripheral interaction,Title_Keywords,True,
Scopus,journalPaper,2023,A pilot randomized controlled trial of virtual reality delivered relaxation for chronic low back pain,VRS - Virtual Reality,B,"Chronic lower back pain (CLBP) is a widespread health problem with lifetime incidence up to 80% in the U.S. Conventional treatments, such as surgery and pharmacotherapy have limitations in that they primarily target physical aspects of pain, and certain medications run the risk of abuse, tolerance, sedation, and possible overdose. Progressive muscle relaxation (PMR) is a validated technique that is neither invasive nor with impairing side effects. The effects and reach of PMR may be enhanced using technological advances, such as virtual reality (VR), which is piloted for feasibility in the current project. This study presents a randomized controlled trial investigating the usability and efficacy of a VR-based PMR program. Participants (n = 18) were randomly assigned to the VR treatment or waitlist control group. Treatment participants completed five VR-PMR sessions. Results indicated the novel VR program was highly usable and immersive. Comparison of pre- and post-treatment measures indicated that VR participants reported significantly lower pain levels and improvements in pain-related beliefs compared to controls. Additionally, those who received VR-PMR reported significantly lower state anxiety at the conclusion of the study. Improvements in medication-related beliefs were also found post-treatment. This controlled trial provides preliminary support for a novel, immersive VR relaxation modality as a promising new adjunctive or alternative approach for CLBP management. Future studies can further validate the use of VR, specifically VR-based PMR, for management and treatment of chronic pain. With increased accessibility of consumer VR headsets, a program such as this may improve pain management outside of the medical setting. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Back pain; Chronic pain; Cognitive behavior therapy; Progressive muscle relaxation; Relaxation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Nature in virtual reality improves mood and reduces stress: evidence from young adults and senior citizens,VRS - Virtual Reality,B,"Large populations worldwide have been deprived from nature experiences due to mass quarantines and lockdowns during the COVID-19 pandemic, and face a looming mental health crisis. Virtual reality offers a safe and practical solution to increase nature exposure. This research examined the effects of virtual nature using a within-subject design with young adults (Study 1) and senior citizens (Study 2). Results from the young adult sample showed that walking in a virtual forest reduced negative affect due to enhanced nature connectedness, and reduced stress measured by heart rate. Consistently, the senior citizen sample reported improved positive affect due to enhanced nature connectedness after the virtual nature walk. Our findings unveil the underlying mechanism of how virtual nature may improve psychological well-being and demonstrated how virtual nature can be used as an intervention to promote mental health. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Emotion; Mental health; Nature; Stress; Virtual reality; Well-being,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality interaction based on visual attention and kinesthetic information,VRS - Virtual Reality,B,"Emerging multimedia technologies significantly enhance the naturalness and immersion of human computer interaction. Currently, research on kinesthetic information has gained increasing attentions of multimedia community. However, effective interaction between kinesthetic and other multimedia signals remains a challenging task. In this paper, we propose a visual-kinesthetic interaction in Virtual Reality (VR) and real-world control tasks. First, we model the correlation between user’s visual attention and kinesthetic positions under different tasks. Second, we utilize an attention-based Long Short-Term Memory network to predict the kinesthetic positions. Third, we build a VR system with robotic car control, which validates our model in VR interaction and control tasks. With a high task achievement rate, we envision the implementation of kinesthetic information in a more natural interaction system. The VR interaction system based on the proposed model can also provide guidance for the design of immersive robot teleoperation systems. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Haptics; Human–Computer Interaction (HCI); Multimedia; Virtual Reality (VR); Visual attention,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Rendering massive indoor point clouds in virtual reality,VRS - Virtual Reality,B,"This paper addresses the challenges of rendering massive indoor point clouds in Virtual Reality. In these kind of visualizations the point of view is never static, imposing the need of a one-shot (i.e. non-iterative) rendering strategy, in contrast with progressive refinement approaches that assume that the camera position does not change between most consecutive frames. Our approach benefits from the static nature of indoor environments to pre-compute a visibility map that enables us to boost real-time rendering performance. The key idea behind our visibility map is to exploit the cluttered topology of buildings in order to effectively cull the regions of the space that are occluded by structural elements such as walls. This does not only improve performance but also the visual quality of the final render, allowing us to display in full detail the space and preventing the user to see the contiguous spaces through the walls. Additionally, we introduce a novel hierarchical data structure that enables us to display the point cloud with a continuous level of detail with a minimal impact on performance. Experimental results show that our approach outperforms state-of-the-art techniques in complex indoor environments and achieves comparable results in outdoor ones, proving the generality of our method. © 2023, The Author(s).",Laser scanning; Lidar; Point cloud; Pre-computed visibility; Rendering; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Concert experiences in virtual reality environments,VRS - Virtual Reality,B,"Spurred by recent advances in digital technologies, virtual concerts have become established modes for event attendance and represent a rapidly growing segment of the music industry. Yet, up to now, general experience of virtual concert attendees remains largely underexplored. Here, we focus on a subcategory in this domain: music concerts in virtual reality (VR). Our approach is situated within the theoretical framework of embodied music cognition and entailed investigation through a survey study. Responses of seventy-four VR concert attendees were collected, consisting of demographics, motivations, experiences, and future perspectives. In contrast to previous research, which generally identified social connectedness as a main motivator for concert attendance, our sample regarded it as one of the least important incentives. On the other hand, in line with previous studies, ‘seeing specific artists perform’ and ‘uniqueness of the experience’, were pivotal. The latter was mostly fueled by the possibility to experience/interact with visuals and environments considered as unconceivable in the real world. Furthermore, 70% of our sample regarded VR concerts as ‘the future of the music industry’, mainly relating to the accessibility of such events. Positive evaluations of VR concert experiences, as well as future perspectives regarding the medium, were significantly influenced by the level of experienced immersivity. To our knowledge, this is the first study to provide such an account. © 2023, The Author(s).",Livestream; Music concerts; Presence; Social connection; Uniqueness; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Learning game innovations in immersive game environments: a factor analytic study of students’ learning inventory in virtual reality,VRS - Virtual Reality,B,"The research is designed to trigger students’ game innovations and facilitate their creative learning process by providing them with a “stimulated” immersive game environment to achieve self-regulated learning practice. This research has tailor-made three situated immersive games for data collection. The Self-regulated Learning model is applied to investigate students’ cognitive, motivational, and emotional aspects of their learning process in these immersive game environments. This study involved a sample of 64 undergraduate students in interactive media design. The three factors and foci in this research are, (1) Cognitive Processing for Creative Thinking, (2) Personal Learning Motivation (PLM), and (3) Environmental Stimulation in Game. The positive results of this research indicated the use of immersive games in game innovation education is promising. The use of situated learning pedagogy provides students with an effective learning process for discovery learning. Students are experiencing “learn-by-doing” and exploring diverse solutions through the experiences gathered from the stimulated immersive environments. The result also provides educators with an alternative pedagogical approach to refining their future curriculums. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Digital gaming; Game innovations; Immersive environment; Learning inventory; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2023,Drivers of immersive virtual reality adoption intention: a multi-group analysis in chemical industry settings,VRS - Virtual Reality,B,"The present study uses the modified Unified Theory of Acceptance and Use of Technology 2 to examine the effect of factors such as performance expectancy (PE), effort expectancy (EE), social influence (SI), and hedonic motivation (HM) that may motivate operators and employees to adopt IVR-based technology into their training. Results of a multi-group analysis based on nationality, prior IVR experience, and/or length of work experience, to analyse the potential similarities and/or differences in perception and acceptance towards using IVR-based technology are also presented. The quantitative research data were gathered using an online questionnaire from 438 chemical operators and/or employees who either speak German, French, or English. Partial least squares structural equation modelling and multi-group analysis based on SmartPLS™ version 3 were used to carry out the path and multi-group analyses. The results show that the behavioural intention (BI) towards adoption of IVR was influenced by PE, EE, and HM for all abovementioned subpopulation. However, the relationship of SI to BI was not supported for respondents with prior IVR experience and for respondents coming from Western region. Although Henseler’s-based multi-group PLS analysis reveals that there was no significant difference between the group comparisons, it is still important to take into account these socio-demographic factors as there are definite group differences in terms of the ranking order of each construct for the IVR adoption intentions among each subpopulation. The implications and future directions were discussed. © 2021, The Author(s).",Adoption intention; Chemical industry; PLS-SEM; Training; UTAUT 2 multi-group analysis; Virtual reality adoption,Title_Keywords,True,
Scopus,journalPaper,2023,Suitability test of virtual reality applications for older people considering the spatial orientation ability,VRS - Virtual Reality,B,"Previous studies showed similar spatial orientation ability (SO) between real world (RW) and virtual reality (VR). As the SO deteriorates with age, it is crucial to investigate whether the degradation is similar in VR, as it may affect the use of VR tools for older people, such as in physical therapy. Therefore, we extended our previous study, in which similar SO between RW and VR was measured for younger participants (18–35 years) with a higher age group (> 55 years) to examine the VR's influence on their SO. Two main tests were conducted. In the first test, the participants were blindfolded, asked to rotate (0°, 45°, 180°, 225°) on a fixed starting position, and walk straight to different objects they had memorized before. This test was conducted twice. An ANOVA only revealed a significant interaction between the factors Age (young/old) and Condition (VR/RW) for the 45°-rotation in the second run. Here, both age groups performed similarly in RW, while in VR, greater deviations in the older participants appeared. Nevertheless, the overall Age*Condition-interaction in the first test was not significant. In the second test, subjects were required to walk blindfolded to two objects starting from different positions. The starting position and objects changed three times in each condition but were equal between RW and VR. No interactions between the factors Age and Condition were found (p > 0.05). Both tests showed a similar influence of VR on the SO of both age groups, supporting the usage of VR, regardless of age. © 2023, The Author(s).",Age groups; Head-mounted display; Spatial orientation; Sports; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Identifying presence of cybersickness symptoms using AI-based predictive learning algorithms,VRS - Virtual Reality,B,"Cybersickness (CS) affects a large proportion of virtual reality (VR) users causing a combination of nausea, headaches and dizziness which would create barriers to the users, VR designers/developers and the stakeholders in the production industry. Although design principles suggest methods to avoid CS, challenges remain as new demands and systems continue to penetrate the competitive market. The dilemma is whether to use VR technology by experiencing the ultimate virtual world using a head-mounted display (HMD) with possible CS triggers or to avoid the triggers by avoiding using VR. With the huge success and potential in the entertainment industry, it is very important to focus on the solutions to handling CS dilemmas. Therefore, the main observation for the developers is to have a guide around the set of established design principles aiming to broadly reduce CS. In this paper, we provide a method to apply artificial intelligence (AI) techniques and use machine learning (ML) algorithms including support vector machines (SVMs), decision trees (DTs) and K-nearest neighbours (KNNs) to predict CS outcomes. Based on our findings, we have observed that DT and SVM surpassed KNN in test accuracy. Additionally, DT exhibited better results than both SVM and KNN in train accuracy. By exploiting the power of ML, developers will be able to predict the potential occurrence of CS while developing VR projects to find ways to alleviate CS more effectively. © 2023, The Author(s).",Artificial intelligence (AI); Cybersickness (CS); Decision trees (DTs); Head-mounted displays (HMDs); K-nearest neighbours (KNNs); Machine learning (ML); Support vector machines (SVMs); Virtual reality (VR),Abstract_Keywords,True,
Scopus,journalPaper,2023,Transferring the approach avoidance task into virtual reality: a study in patients with alcohol use disorder versus healthy controls,VRS - Virtual Reality,B,"Study aims were to (I) transfer the measurement of the approach bias (Apb) related to alcoholic stimuli via the Approach Avoidance Task (AAT) into Virtual Reality (VR), (II) check whether measuring Apb in VR leads to similar or different results compared to the classical PC-based version, (III) check the validity of VR versus PC-based bias scores in terms of relatedness to clinical variables. Different ‘grasping-conditions’ were tested and contrasted in VR concerning (Ia) feasibility (performance): (1) never grasp, (2) always grasp, (3) grasp when PULLing stimuli towards oneself. (Ib) Differences in the bias scores between patients with alcohol use disorder (AUD) and healthy controls (HC) were examined for each grasping-condition. (II) PC-based bias scores were computed and contrasted for AUD versus HC. (III) Correlations of the different VR- versus PC-based bias scores with AUD symptom severity and impulsivity were checked to evaluate validity. (Ia) Grasping-condition 1, followed by 3, showed acceptable (> 50%) and good (> 80%) rates of correct performances allowing for robust median estimation. (Ib) Significant differences in the resulting bias scores emerged between AUD and HC only for grasping-condition 1 (p = 0.034) and 3 at trend-level (p = 0.093). For grasping-condition 1 the Apb Median for AUD was different from zero at a non-significant trend-level (p = 0.064). (II) The PC-based bias scores did not discriminate between AUD versus HC groups. (III) Grasping-condition 1 and 3 VR-based bias scores correlated significantly with impulsivity. In sum, transferring the AAT into VR is feasible, valid, and best implemented without an additional grasping-component when using the VR-controller. This way of Apb assessment represents a viable, perhaps even superior, alternative to PC-based assessments. Trial registration The trial was pre-registered at AsPredicted #76854: ‘Transferring the approach avoidance task into virtual reality’, 10/13/2021; prior to any analyses being undertaken. © 2023, The Author(s).",AAT; Alcohol use disorder; Approach bias; Validation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Correction: cleAR: an interoperable architecture for multi-user AR-based school curricula (Virtual Reality, (2023), 27, 3, (1813-1825), 10.1007/s10055-023-00764-5)",VRS - Virtual Reality,B,"In the original publication, the affiliation of second author, Dr. Ana Dominguez, was wrongly published. It should be as given below. “Fundación Vicomtech, Basque Research and Technology Alliance (BRTA), San Sebastian, Spain” The original publication has been updated of the same. © 2023, The Author(s).",,Title,True,
Scopus,journalPaper,2023,Is social presence (alone) a general predictor for good remote collaboration? comparing video and augmented reality guidance in maintenance procedures,VRS - Virtual Reality,B,"A common practice in scenarios of remote collaboration is to provide a representation from distributed team members, aiming to positively influence the level of social presence and in turn the work effort. Nevertheless, these stimuli can lead to fractured learning experiences, since collaborators need to split attention among the task, the shared information, and the counterpart representation. This paper explored how the last affects social presence, and other dimensions of collaboration, as well as task resolution in scenarios of remote guidance. A user study was conducted, comparing two distinct conditions: traditional video chat (team members representation always visible) and Augmented Reality (AR) annotations (collaborators representation never available). These were selected due to ongoing research with partners from the industry sector, following the insights of a participatory design process. A real-life use-case was considered, i.e., synchronous maintenance task with 4 completion stages that required a remote expert using a computer to guide 37 on-site participants wielding a handheld device. The results of the study are described and discussed based on data analysis, showing that the majority of participants preferred the AR-based condition, despite the absence of the expert representation. © 2023, The Author(s).",Augmented reality annotations; Collaborative process; Remote collaboration; Social presence; Task resolution; User study; Video stream,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,DexHand: dexterous hand manipulation motion synthesis for virtual reality,VRS - Virtual Reality,B,"Natural object manipulation is one of the important human skills. However, generating natural hand manipulation motions that are adaptive to object shapes and the tasks at hand in virtual reality is still a challenge. In this paper, we propose a neural network-based finger movement generation approach, enabling the generation of plausible hand motions interacting with objects. Given the object shape and movement features in the wrist coordinate system, the first network Generator infers the hand pose at the current frame that matches the object motion and shape. The second network Optimizer then fine-tunes the pose to improve the plausibility of hand-object interaction. Notably, a differentiable optimization module is proposed to generate the training dataset for Optimizer. Experimental results show that our approach can generate plausible dexterous hand manipulation motions for hand-object interaction without obvious delay. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Deep learning; Hand motion; Hand synthesis; Neural network; Object manipulation; Plausible hand interaction; Virtual hand,Title_Abstract,True,
Scopus,journalPaper,2023,CAVE and HMD: distance perception comparative study,VRS - Virtual Reality,B,"This paper proposes to analyse user experience using two different immersive device categories: a cave automatic virtual environment (CAVE) and a head-mounted display (HMD). While most past studies focused on one of these devices to characterize user experience, we propose to fill the gap in comparative studies by conducting investigations using both devices, considering the same application, method and analysis. Through this study, we want to highlight the differences in user experience induced when using either one of these technologies in terms of visualization and interaction. We performed two experiments, each focusing on a specific aspect of the devices employed. The first one is related to distance perception when walking and the possible influence of the HMD’s weight, which does not occur with CAVE systems as they do not require wearing any heavy equipment. Past studies found that weight may impact distance perception. Several walking distances were considered. Results revealed that the HMD’s weight does not induce significant differences over short distances (above three meters). In the second experiment, we focused on distance perception over short distances. We considered that the HMD’s screen being closer to the user’s eyes than in CAVE systems might induce substantial distance perception differences, especially for short-distance interaction. We designed a task in which users had to move an object from one place to another at several distances using the CAVE and an HMD. Results revealed significant underestimation compared to reality as in past work, but no significant differences between the immersive devices. These results provide a better understanding of the differences between the two emblematic virtual reality displays. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",CAVE; Distance perception; HMD; Interaction paradigms; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Passive identification of subjective preferences toward individual items using eye-tracking in a virtual reality environment,VRS - Virtual Reality,B,"The usage of virtual reality (VR) has been growing in many fields of research and therapy thanks to its immersive and gamified nature. Detection of the users’ subjective experience is thus essential for the effective personalization of content. Eye-tracking (ET) data and specifically gaze, in two-dimensional tasks, has been linked to value-based choices and emotional states. Therefore, here we aimed to develop a method for passive identification of subjective preferences based on ET data collected during a VR experience. For this purpose, we developed a naturalistic dynamic VR task where participants searched and looked at complex objects of pets and their control shapes that appeared in pre-defined locations in random order. At the end of the task, participants ranked their preference, valence, and arousal of the items they saw during the task. ET data was recorded using a built-in binocular eye-tracker within the VR headset. We found that the gaze behavior features of the median distance of gaze from the center of objects and the median gaze scan speed showed a significant interaction with object type (pets/shapes), as well as a significant positive relation to preference and valence rankings of pets. Our results suggest that these gaze behavior features could be used as passive biomarkers for detecting individual preferences and pleasantness, and in the future may enable successful personalization of VR content in real-time for various applications such as optimization of psychiatric diagnosis and treatment sessions. © 2023, The Author(s).",Eye-tracking; Head-mounted display (HMD); Internal state; Personalization; Preferences; Virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Improving student learning outcomes using narrative virtual reality as pre-training,VRS - Virtual Reality,B,"Our research focuses on incorporating narrative immersive virtual reality (IVR) into a classroom setting and assessing its impact when used as pre-training material before a multimedia lesson. Narrative experiences in IVR can be highly informative and affecting; however, our understanding of the educational impact of narratives in IVR is rudimentary. To address this, our study examines the cognitive and affective benefits of utilising a narrative-based IVR experience titled Thin Ice VR, as pre-training for students studying polar history and climate change. To further enhance our understanding of the relationships between IVR design and learning outcomes, key design elements of the narrative IVR experience used in the study are described. A between-groups experiments was conducted with 139 high school students to determine if those that viewed narrative IVR before continuing their learning with multimedia materials would show increased knowledge transfer (achieving a better understanding of the material presented), knowledge acquisition (new knowledge added to memory), engagement, motivation and emotional reaction. Results showed a significant increase in knowledge transfer when narrative IVR was used as pre-training material. However, using narrative IVR as pre-training had minimal impact on knowledge acquisition, engagement and motivation. Afforded by the sense of presence in IVR, the immersive narrative experience was heightened and able to elicit an emotional reaction. Utilising the sense of presence in IVR to place students in the narrative positions IVR as an effective medium for telling stories in classroom settings. When students use narrative IVR before further study, the experience significantly benefits both cognitive and affective learning outcomes. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Education; Narrative; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Action recognition based on multimode fusion for VR online platform,VRS - Virtual Reality,B,"The current popular online communication platforms can convey information only in the form of text, voice, pictures, and other electronic means. The richness and reliability of information is not comparable to traditional face-to-face communication. The use of virtual reality (VR) technology for online communication is a viable alternative to face-to-face communication. In the current VR online communication platform, users are in a virtual world in the form of avatars, which can achieve “face-to-face” communication to a certain extent. However, the actions of the avatar do not follow the user, which makes the communication process less realistic. Decision-makers need to make decisions based on the behavior of VR users, but there are no effective methods for action data collection in VR environments. In our work, three modalities of nine actions from VR users are collected using a virtual reality head-mounted display (VR HMD) built-in sensors, RGB cameras and human pose estimation. Using these data and advanced multimodal fusion action recognition networks, we obtained a high accuracy action recognition model. In addition, we take advantage of the VR HMD to collect 3D position data and design a 2D key point augmentation scheme for VR users. Using the augmented 2D key point data and VR HMD sensor data, we can train action recognition models with high accuracy and strong stability. In data collection and experimental work, we focus our research on classroom scenes, and the results can be extended to other scenes. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Action recognition; Data augmentation; Remote education; Virtual reality online platform,Abstract_Keywords,True,
Scopus,journalPaper,2023,Body ownership illusion through virtual reality as modulator variable for limbs rehabilitation after stroke: a systematic review,VRS - Virtual Reality,B,"Stroke is the leading cause of motor impairments and generates distortion of body representation. Hence, stroke can modulate the sense of embodiment, namely the feeling of being inside the body (ownership), in the place where the body is located (location), and moving the body according to its own intentions (agency). A growing number of studies have adopted virtual reality (VR) to train motor abilities. However, the impact of the body illusion on the rehabilitation outcome is not fully understood. The present systematic review investigates the modulating role of the body illusion elicited by VR on motor rehabilitation in post-stroke patients after embodying a virtual avatar. The research was led in the main databases—PubMed, Scopus, PsychINFO, and Web of Science—and four studies matched the inclusion criteria (e.g., to have a sample of adult post-stroke patients, to use VR as an instrument for motor rehabilitation, to adopt the paradigm of the body illusion as a modulator for motor rehabilitation, to test the sense of body illusion outcome). Research outcomes demonstrated that two studies adopted the immersive and two the non-immersive embodied VR; three studies focused on the upper limb, and one on lower limb rehabilitation. Two studies compare VR training with traditional therapy, and two are pilot studies with only one experimental group. The studies demonstrated the feasibility of the body illusion as an accelerator for motor rehabilitation compared to the non-embodied condition, and as a positive correlator of the rehabilitation outcome. The finding should be taken with caution due to the limited studies included; however, they are encouraging to justify further research efforts in this area. © 2023, The Author(s).",Body ownership illusion; Motor rehabilitation; Stroke; Systematic review; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Fitted avatars: automatic skeleton adjustment for self-avatars in virtual reality,VRS - Virtual Reality,B,"In the era of the metaverse, self-avatars are gaining popularity, as they can enhance presence and provide embodiment when a user is immersed in Virtual Reality. They are also very important in collaborative Virtual Reality to improve communication through gestures. Whether we are using a complex motion capture solution or a few trackers with inverse kinematics (IK), it is essential to have a good match in size between the avatar and the user, as otherwise mismatches in self-avatar posture could be noticeable for the user. To achieve such a correct match in dimensions, a manual process is often required, with the need for a second person to take measurements of body limbs and introduce them into the system. This process can be time-consuming, and prone to errors. In this paper, we propose an automatic measuring method that simply requires the user to do a small set of exercises while wearing a Head-Mounted Display (HMD), two hand controllers, and three trackers. Our work provides an affordable and quick method to automatically extract user measurements and adjust the virtual humanoid skeleton to the exact dimensions. Our results show that our method can reduce the misalignment produced by the IK system when compared to other solutions that simply apply a uniform scaling to an avatar based on the height of the HMD, and make assumptions about the locations of joints with respect to the trackers. © 2023, The Author(s).",Avatar; Embodiment; Full-body tracking; Inverse kinematics; User representation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Mild simulator sickness can alter heart rate variability, mental workload, and learning outcomes in a 360° virtual reality application for medical education: a post hoc analysis of a randomized controlled trial",VRS - Virtual Reality,B,"Virtual reality (VR) applications could be beneficial for education, training, and treatment. However, VR may induce symptoms of simulator sickness (SS) such as difficulty focusing, difficulty concentrating, or dizziness that could impair autonomic nervous system function, affect mental workload, and worsen interventional outcomes. In the original randomized controlled trial, which explored the effectiveness of using a 360° VR video versus a two-dimensional VR video to learn history taking and physical examination skills, only the former group participants had SS. Therefore, 28 undergraduate medical students who participated in a 360° VR learning module were included in this post hoc study using a repeated measures design. Data of the Simulator Sickness Questionnaire (SSQ), heart rate variability (HRV) analysis, Task Load Index, and Mini-Clinical Evaluation Exercise were retrospectively reviewed and statistically analyzed. Ten (36%) participants had mild SS (total score > 0 and ≤ 20), and 18 (64%) had no SS symptom. Total SSQ score was positively related to the very low frequency (VLF) band power, physical demand subscale, and frustration subscale, and inversely related to physical examination score. Using multilevel modeling, the VLF power mediated the relationship between total SSQ score and physical examination score. Furthermore, frustration subscale moderated the mediating effects of the VLF power. Our results highlight the importance of documenting SS to evaluate a 360° VR training program. Furthermore, the combination of HRV analysis with mental workload measurement and outcome assessments provided the important clinical value in evaluating the effects of SS in VR applications in medical education. © 2022, The Author(s).",360° video; Heart rate variability; Mini-clinical evaluation exercise; Simulator sickness; Task load index; Visual reality,Title_Abstract,True,
Scopus,journalPaper,2023,Demographic differences in presence across seven studies,VRS - Virtual Reality,B,"It is often necessary for virtual reality (VR) users to experience a sense of presence for the benefits of VR applications to be realized. However, feelings of presence are subjective and depend not only on the nature of the VR environment but also on the users’ unique characteristics. To maximize the likelihood of achieving desired VR outcomes, it is important to understand the user characteristics that impact the likelihood of users’ feelings of social and environmental presence. Addressing this knowledge gap is an important first step toward verifying whether all user populations have access to equally efficacious VR experiences. To this end, we report data from seven independent samples collected within one laboratory group (total N = 1145). In these studies, participants were asked to perform tasks in VR such as traversing environments, pointing at and selecting objects, and interacting with virtual humans. Meta-analyses revealed that, on average, feelings of presence were not significantly related to age or gender, but differed by racial group membership. Significant racial differences in presence were found for both environmental and social presence. Black participants reported approximately half a standard deviation more presence than White participants. No overall differences between Asian and White participants’ reported presence were found. These findings provide a context for future studies that may explore demographic differences in presence directly. © 2023, This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply.",Age differences; Gender differences; Presence; Racial differences; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Multisensory processing of emotional cues predicts intrusive memories after virtual reality trauma,VRS - Virtual Reality,B,"Research has shown that high trait anxiety can alter multisensory processing of threat cues (by amplifying integration of angry faces and voices); however, it remains unknown whether differences in multisensory processing play a role in the psychological response to trauma. This study examined the relationship between multisensory emotion processing and intrusive memories over seven days following exposure to an analogue trauma in a sample of 55 healthy young adults. We used an adapted version of the trauma film paradigm, where scenes showing a car accident trauma were presented using virtual reality, rather than a conventional 2D film. Multisensory processing was assessed prior to the trauma simulation using a forced choice emotion recognition paradigm with happy, sad and angry voice-only, face-only, audiovisual congruent (face and voice expressed matching emotions) and audiovisual incongruent expressions (face and voice expressed different emotions). We found that increased accuracy in recognising anger (but not happiness and sadness) in the audiovisual condition relative to the voice- and face-only conditions was associated with more intrusions following VR trauma. Despite previous results linking trait anxiety and intrusion development, no significant influence of trait anxiety on intrusion frequency was observed. Enhanced integration of threat-related information (i.e. angry faces and voices) could lead to overly threatening appraisals of stressful life events and result in greater intrusion development after trauma. © 2023, The Author(s).",Emotion recognition; Intrusive memories; Multisensory processing; Trauma film; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Perceived usefulness of, engagement with, and effectiveness of virtual reality environments in learning industrial operations: the moderating role of openness to experience",VRS - Virtual Reality,B,"The development of virtual reality (VR) in enhancing the effectiveness of the learning process, with its interactive, immersive, and intuitive pedagogical environment, has become a necessity for corporations with increasingly complex operations. However, VR users’ perceptions, openness and learning effectiveness are seldom comprehensively evaluated, particularly in learning complex industrial operations. In this study, grounded in the technology acceptance model, a moderated mediation model of perceived usefulness, ease of use, openness to experience, and engagement in VR-based learning was developed. The model was empirically validated using responses collected from 321 users who were trained on aircraft and cargo terminal operations powered by a novel VR-based learning platform. A survey to measure openness to experience and a pre-training performance test were carried out, followed by a post-training survey of learners’ intrinsic factors, including the influence of perceived usefulness, openness to experience, and attitude towards learning. The study revealed that learners with an open attitude towards experiencing new technology tend to perceive VR technology as a useful platform for training. In addition, the learners with more positive views of VR technology-supported training were more engaged in learning. © 2023, The Author(s).",Attitude towards learning; Openness to experience; Pedagogical development; Perceived usefulness of VR; Technology acceptance model; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Using 360-degree immersive videos to assess multiple transdiagnostic symptoms: A study focusing on fear of negative evaluation, paranoid thoughts, negative automatic thoughts, and craving",VRS - Virtual Reality,B,"Over the last 20 years, virtual reality (VR) has gained a great interest for both assessment and treatment of various psychopathologies. However, due to high costs and material specificity, VR remains disadvantageous for clinicians. Adopting a multiple transdiagnostic approach, this study aims at testing the validity of a 360-degree immersive video (360IV) for the assessment of five common psychological symptoms (fear of negative evaluation, paranoid thoughts, negative automatic thoughts, craving for alcohol and for nicotine). A 360IV was constructed in the Darius Café and included actors behaving naturally. One hundred and fifty-eight adults from the general population were assessed in terms of their proneness towards the five symptoms, were then exposed to the 360IV and completed measures for the five state symptoms, four dimensions of presence (place, plausibility, copresence and social presence illusions) and cybersickness. Results revealed that the five symptoms occurred during the immersion and were predicted by the participants’ proneness towards these symptoms. The 360IV was also able to elicit various levels of the four dimensions of presence while producing few cybersickness. The present study provides evidence supporting the use of the 360IV as a new accessible, ecological, and standardized tool to assess multiple transdiagnostic symptoms. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",360-degree immersive video; Assessment; Transdiagnostic; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Intelligence at play: game-based assessment using a virtual-reality application,VRS - Virtual Reality,B,"Several studies have shown that video games may indicate or even develop intellectual and cognitive abilities. As intelligence is one of the most widely used predictors of job performance, video games could thus have potential for personnel assessment. However, few studies have investigated whether and how virtual reality (VR) games can be used to make inferences about intelligence, even though companies increasingly use VR technology to recruit candidates. This proof-of-concept study contributes to bridging this gap between research and practice. Under controlled laboratory conditions, 103 participants played the commercial VR game Job Simulator and took the short version of the intelligence test BIS-4. Correlation and regression analysis reveal that, on average, participants who completed the game more quickly than others had higher levels of general intelligence and processing capacity, suggesting that VR games may provide useful supplementary tools in the prediction of job performance. Still, our results also indicate that game-based assessments have limitations that deserve researchers’ attention, which lead us to discuss directions for future research. © 2023, The Author(s).",Assessment; Cognition; Intelligence; Recruitment; Video games; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,"Towards an objective measurement of presence, place illusion, and plausibility illusion in virtual reality using electroencephalography",VRS - Virtual Reality,B,"Presence is a mental state when a user in virtual reality (VR) reacts to events in it as if it is real. Place illusion (PI) and plausibility illusion (PSI) are components of Presence and depend on the Immersion and Coherence of a VR system and the experience. Measuring Presence (PI and PSI) is critical when designing new systems and experiences. However, the traditional questionnaire-based methods of measuring Presence have limitations. Therefore, we propose a method for augmented measurement of Presence, PI, and PSI of VR users with biosignals. For this, we designed a within-subjects experiment that presented VR scenarios with varying levels of Immersion and Coherence while measuring electroencephalography (EEG) of users concurrently. For validation, we took feedback from post-scenario questionnaires presented within the VR environment. The study was conducted with 20 participants. Analyzing the Questionnaire responses revealed that an increase in plausibility illusion or place illusion led to an increase in Presence, confirming the fundamental theory. Analyzing the EEG results revealed that the Envelope Amplitude Correlation features and Spectral Coherence are able to discriminate between the Presence, PI, and PSI of the user. In contrast, there are many entropy features that increase for high PSI. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",EEG; Measurement; Physiological sensing; Place illusion; Plausibility illusion; Presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A cross-platform application for the ecological and remote assessment of memory impairment in aging: ECO-MEMORY,VRS - Virtual Reality,B,"This work aims to present the first step of a creation of an instrument to assess memory deficits responding to the needs imposed by the inability to access clinical care, such as physical or geographical constraints or still limitations imposed during the pandemic era. The older population, who would benefit from these services, may be at risk as access to services that support psychological and neuropsychological needs, which are not considered essential, has frequently been restricted in recent years. Moreover, because deficits are commonly mistaken for the effects of physiological aging, the early signs of cognitive decline might be ignored. On these bases, we used the potential of 360-degree media to create an application for memory assessment without the physical presence of clinicians: ECO-MEMORY. Firstly, we developed the application and evaluated its usability. ECO-MEMORY is divided into four sections, each addressing a different memory task: recognizing objects and faces, learning a path, and creating an allocentric map. Thirteen older adults who used the tablet application provided usability data as well as qualitative feedback on their experience. After the performance, the System Usability Scale, the Senior Technology Acceptance Model, and the Independent Television Commission Sense of Presence were administered. We performed a qualitative analysis and descriptive statistics, which showed that ECO-MEMORY is a usable instrument. Also, it was enjoyable for users who generally accepted technology in their life. ECO-MEMORY may therefore offer a promising approach to memory evaluation by including real-world scenarios. © 2023, The Author(s).",360° video; Aging; Assessment; Memory; Virtual reality,Keywords,True,
Scopus,journalPaper,2023,A visuo-haptic mixed reality manual milling training simulator,VRS - Virtual Reality,B,"With the advancements in technology, mixed reality (MR) has found numerous applications in industries and job training. However, most MR applications only provide visual feedback, lacking realistic haptic feedback. Nevertheless, certain jobs require haptic sensations in the hands. To address this issue, this study integrates realistic haptic feedback into an MR environment and develops a visuo-haptic MR milling simulator for conventional manual milling operation training using Microsoft HoloLens 2. This simulator comprises a virtual spindle, a virtual milling tool, a virtual workpiece, and a physical mockup of the milling table. It provides realistic cutting force feedback when the virtual milling tool collides with the virtual workpiece. Users can operate the milling simulator with their bare hands and feel cutting force as if they were operating a real milling machine. The developed training system allows novices to intuitively understand the relationship between milling parameters and milling conditions. The usability and immersiveness of the visuo-haptic MR milling simulator are studied, and the results show that realistic haptic feedback increases usability, realism, immersiveness, and presence. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Manual milling operation; Microsoft HoloLens 2; Mixed reality; Visuo-haptic,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,The effects of virtual reality glasses and external cold and vibration on procedural pain and anxiety in children during venous phlebotomy: randomized controlled trial,VRS - Virtual Reality,B,"Needle-related procedures are among the most feared and painful experiences reported by children and their parent. For this reason, use of effective methods of pain relief is very important during phlebotomy procedures in children. The aim of this study is to research two different distraction methods (external cold and vibration-Buzzy + virtual reality) on relief of procedural pain and anxiety in children during phlebotomy. This study is a prospective, randomized and controlled trial. Sample of the study consisted of a total of 119 children who met the sample selection criteria. Children aged 7 to 12 years who required phlebotomy were divided into three groups: buzzy (n = 40), virtual reality (n = 40), and control (n = 39). Data were collected using the information form, Wong–Baker FACES Pain Rating Scale, and Children’s Fear Scale. In the study, 119 children [girls n = 59 (49.6%), boys n = 60 (50.4%)] were included. The children’s pain levels were assessed and reported by the parents and observers and the children themselves who self-reported using Wong–Baker FACES. The children’s anxiety levels were also assessed using the Children’s Fear Scale. A significant difference was found between the groups in terms of the parent-reported and observer-reported assessments (p < 0.05). In the self-reported assessment, the pain levels of the VR and Buzzy group were lower than the control group, but were not statistically significant (p > 0.05). According to the parent-reported and observer-reported assessments, a significant difference was found between procedural anxiety levels. VR is more effective than external cold and vibration-Buzzy in reducing pain during phlebotomy and should be preferred as the first choice. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Children; Distraction; Needle procedures; Pain,Title_Abstract,True,
Scopus,journalPaper,2023,A scoping review of the use of lab streaming layer framework in virtual and augmented reality research,VRS - Virtual Reality,B,"The use of multimodal data allows excellent opportunities for human–computer interaction research and novel techniques regarding virtual and augmented reality (VR/AR) experiences. Collecting, coordinating, and synchronizing a large amount of data from multiple VR/AR hardware while maintaining a high framerate can be a daunting task, despite the compelling nature of multimodal data. The Lab Streaming Layer (LSL) is an open-source framework that enables the synchronous collection of various types of multimodal data, unlike existing expensive alternatives. However, despite its potential, this framework has not been fully adopted by the VR/AR research community. In this paper, we present a guideline of the LSL framework’s use in VR/AR research as well as report current trends by performing a comprehensive literature review on the subject. We extract 549 publications using LSL from January 2015 to March 2022. We analyze types of data, displays, and targeted application areas. We describe in-depth reviews of 38 selected papers and provide use of LSL in the VR/AR research community while highlighting benefits, challenges, and future opportunities. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Lab Streaming layer; Literature review; Multimodal data collection; Open-source data collection; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Brain activity during cybersickness: a scoping review,VRS - Virtual Reality,B,"Virtual reality (VR) experiences can cause a range of negative symptoms such as nausea, disorientation, and oculomotor discomfort, which is collectively called cybersickness. Previous studies have attempted to develop a reliable measure for detecting cybersickness instead of using questionnaires, and electroencephalogram (EEG) has been regarded as one of the possible alternatives. However, despite the increasing interest, little is known about which brain activities are consistently associated with cybersickness and what types of methods should be adopted for measuring discomfort through brain activity. We conducted a scoping review of 33 experimental studies in cybersickness and EEG found through database searches and screening. To understand these studies, we organized the pipeline of EEG analysis into four steps (preprocessing, feature extraction, feature selection, classification) and surveyed the characteristics of each step. The results showed that most studies performed frequency or time-frequency analysis for EEG feature extraction. A part of the studies applied a classification model to predict cybersickness indicating an accuracy between 79 and 100%. These studies tended to use HMD-based VR with a portable EEG headset for measuring brain activity. Most VR content shown was scenic views such as driving or navigating a road, and the age of participants was limited to people in their 20 s. This scoping review contributes to presenting an overview of cybersickness-related EEG research and establishing directions for future work. © 2023, The Author(s).",Cybersickness; Electroencephalogram; Virtual reality; VR sickness,Abstract_Keywords,True,
Scopus,journalPaper,2023,cleAR: an interoperable architecture for multi-user AR-based school curricula,VRS - Virtual Reality,B,"Although there are some experiences that demonstrate the validity of the use of augmented reality in schools to help students understand and retain complex concepts, augmented reality has not been widely adopted in the education sector yet. This is in part because it is hard to use augmented reality applications in collaborative learning scenarios and to integrate them in the existing school curricula. In this work, we present an interoperable architecture that simplifies the creation of augmented reality applications, enables multi-user student collaboration and provides advanced mechanisms for data analysis and visualization. A review of the literature together with a survey answered by 47 primary and secondary school teachers allowed us to identify the design objectives of cleAR, an architecture for augmented reality-based collaborative educational applications. cleAR has been validated through the development of three proofs of concept. cleAR provides a more mature technological ecosystem that will foster the emergence of augmented reality applications for education and their inclusion in existing school programs. © 2023, The Author(s).",Artificial intelligence; Augmented reality; Collaborative learning; Multi-user interactions; Visual learning analytics,Abstract_Keywords,True,
Scopus,journalPaper,2023,"Correction: Mixed reality and sensor real-time feedback to increase muscle engagement during deep core exercising (Virtual Reality, (2023), 27, 4, (3435-3449), 10.1007/s10055-022-00726-3)",VRS - Virtual Reality,B,"The article “Mixed reality and sensor real-time feedback to increase muscle engagement during deep core exercising”, written by L. Lancere, M. Jürgen and H. Gapeyeva, was originally published electronically on the publisher’s internet portal (currently SpringerLink) on January 4, 2023, without open access. With the author(s)’ decision to opt for Open Choice, the copyright of the article changed on [Date] to © The Author(s) [2023] and the article is forthwith distributed under the terms of the Creative Commons Attribution 4.0 International License (), which permits use, duplication, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (), which permits use, duplication, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",,Title_Abstract,True,
Scopus,journalPaper,2023,Toward the validation of VR-HMDs for medical education: a systematic literature review,VRS - Virtual Reality,B,"The latest technological advancements in the domain of virtual reality (VR) have created new opportunities to use VR as a training platform for medical students and practitioners more broadly. Despite the growing interest in the use of VR as a training tool, a commonly identified gap in VR-training for medical education is the confidence in the long-term validity of the applications. A systematic literature review was undertaken to explore the extent of VR (in particular head-mounted displays) applications for medical training with an additional focus on validation measures. The papers included in this review discussed empirical case studies of specific applications; however, these were mostly concerned with human–computer interaction and were polarized between demonstrating that a conceptual technology solution was feasible for simulation or looked at specific areas of VR usability with little discussion on validation measures for long-term training effectiveness and outcomes. The review uncovered a wide range of ad hoc applications and studies in terms of technology vendors, environments, tasks, envisaged users and effectiveness of learning outcomes. This presents decision-making challenges for those seeking to adopt, implement and embed such systems in teaching practice. The authors of this paper then take a wider socio-technical systems perspective to understand how the holistic training system can be engineered and validated effectively as fit for purpose, through distillation of a generic set of requirements from the literature review to aid design specification and implementation, and to drive more informed and traceable validation of these types of systems. In this review, we have identified 92 requirement statements in 11 key areas against which a VR-HMD training system could be validated; these were grouped into design considerations, learning mechanisms and implementation considerations. © 2023, The Author(s).",HMD-VR; Immersive technology; Medical training; Requirement; Surgical training; Systematic review; Validation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Harmonize: a shared environment for extended immersive entertainment,VRS - Virtual Reality,B,"Virtual reality (VR) and augmented reality (AR) applications are very diffuse nowadays. Moreover, recent technology innovations led to the diffusion of commercial head-mounted displays for immersive VR: users can enjoy entertainment activities that fill their visual fields, experiencing the sensation of physical presence in these virtual immersive environments. Even if AR and VR are mostly used separately, they can be effectively combined to provide a multi-user shared environment (SE), where two or more users perform some specific tasks in a cooperative or competitive way, providing a wider set of interactions and use cases compared to immersive VR alone. However, due to the differences between the two technologies, it is difficult to develop SEs offering a similar experience for both AR and VR users. This paper presents Harmonize, a novel framework to deploy applications based on SEs with a comparable experience for both AR and VR users. Moreover, the framework is hardware-independent, and it has been designed to be as much extendable to novel hardware as possible. An immersive game has been designed to test and to evaluate the validity of the proposed framework. The assessment of the system through the System Usability Scale questionnaire and the Game Experience Questionnaire shows a positive evaluation. © 2021, The Author(s).",Augmented reality; Collaborative environments; Immersive entertainment; Immersive environments; Shared environments; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,An educational virtual reality game for learning historical events,VRS - Virtual Reality,B,"This work presents the design, development, and evaluation of an exploration-based educational VR game for learning about historical Apollo lunar roving missions. The design and development are conducted with a three-phase progressive methodology that covers Modeling & Simulation, Gamification & Edufication, and VRization. The immersive experience presents the gamified content in an engaging way and enables learning activities through exploratory play. Players are given the choices of active and passive learning modes to explore the virtual lunar environment and learn spatial, factual, and operating knowledge. We run a user study to understand the learning effectiveness and engagement with this VR game and the influence of different learning modes on the learning outcomes. We discuss the findings and pedagogical implications that can be applied to the development of educational VR games for learning about similar historical events. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Gaming; VR gamification; VR-based learning,Title,True,
Scopus,journalPaper,2023,The potential of using virtual reality-based self-paced treadmill to assess road-crossing safety and self-evaluation with traumatic brain injuries: a series case study,VRS - Virtual Reality,B,"Impaired self-awareness (ISA) is common following traumatic brain injury (TBI) and can significantly impact safe road-crossing. Road-crossing interventions are variable and involve high-risk real-world situations. Virtual reality (VR)-based road-crossing can elicit changes in real-world functioning but has not been trialled in the TBI population. The primary objective of this research was to explore whether VR-based self-paced treadmill technology offers a safe road-crossing assessment mechanism for people with TBI. Three participants with TBI completed two road-crossing pilot-trials using a VR-based self-paced treadmill. Avatar feedback and verbal feedback were provided between trials. Participants were provided with a safe road-crossing strategy for the second pilot-trial. The Researcher and Participant evaluated road-crossing following each trial using the Mayo-Portland Adaptability Inventory and the number of safe road-crossings to assess changes in self-evaluation and performance between trials. One of the participants perceived improvements in self-evaluation and performance in the second pilot-trial. All participants attempted to apply the safe road-crossing strategy advised. No safety issues were identified using the VR-based self-paced treadmill within this study’s protocol thereby supporting the primary objective of the work. Future research is warranted to strengthen the evidence-base for using VR to elicit improvements in ISA in road-crossing and in generalising findings to the wider TBI population. © 2023, The Author(s).",Feedback; Impaired self-awareness; Road-crossing; Self-evaluation; Traumatic brain injury; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Wordsphere: virtual reality text input interface,VRS - Virtual Reality,B,"In recent years, virtual reality has moved from a fantasy or science fiction theme to a reality increasingly closer to our homes and computers. As is the case of various technological advances, from military and scientific use to casual and routine use, which are increasingly adapted to day-to-day use cases, as is observed with the announcement of a new fashionable term, the metaverse. In the same way, this possibility today in the sight of innovators, entrepreneurs, merchants, and businesspeople, among others, begins to generate applications that show benefits in different branches such as education, medicine, psychology, human resources, real estate, tourism. The detail is that within the innovation, very little is being done for an improvement in text input, which generates a stagnation for continuing with a method of capturing text, even already known with lack of intuition and not optimal, such as the provided by the QWERTY keyboard. This article presents a proposal for a new text input method, a three-dimensional (3D) text input prototype focused on immersion in virtual reality, Wordsphere, which takes advantage of the need for technological adoption for novice users in virtual reality and, in turn, leaves on the table future lines of work and research. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Natural language processing; Qwerty paradigm; Text entry; User input; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A mapping-based redirected walking algorithm for large-scale VR,VRS - Virtual Reality,B,"In VR, the size mismatch between virtual and real space is one of the difficulties, so walking through a large-scale VR scene in a real small area (tracking space) is a challenging problem. We design a novel redirected walking (RDW) algorithm based on a mapping approach to direct users away from the boundary of the tracking area with low rotational distortion. First, the virtual path is decomposed into a set of segments. Then, each segment is mapped into curves and stitched together by minimizing the internal energy with smoothness constraints between adjacent curves. Ultimately, we obtain continuous and smooth curves in the real space. We conduct both simulated and live-user studies to validate the algorithm. Experimental results show that our algorithm has no reset compared with other RDW methods, can significantly speed up and smooth the navigation, reduce perceptual distortion, and show the potential to steer multi-user simultaneously in realtime. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Internal Energy; Real Walking; Redirected Walking; Virtual Reality,Keywords,True,
Scopus,journalPaper,2023,To social with social distance: a case study on a VR-enabled graduation celebration amidst the pandemic,VRS - Virtual Reality,B,"This paper introduces a timely case study on a 3D virtual reality-enabled graduation celebration project that provided an alternative approach to celebrating university graduation amidst the COVID-19 pandemic. The project was carefully designed with fun, engaging, and interactive activities to compensate for the cancelation of the conventional commencement in spring 2020 due to the pandemic. More than 20 graduating students, faculty and staff members participated in a 2-h virtual reality live event appearing as 3D avatars on their own computers. Quantitative and qualitative data collected from 10 participants were analyzed for user experience, cognitive contribution, technology acceptance, and the feasibility. Results revealed that such a project was technologically sound, functionally acceptable, user-friendly, and practically implementable. The results also informed the lessons learned from the current design and the places to improve. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Graduation celebration; Pandemic; Social distance; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,A virtual positioning system for external beam radiotherapy,VRS - Virtual Reality,B,"In radiation therapy, positioning patients to ensure the accuracy of the setup of a procedure is a routine and labour-intensive process that substantially determines the efficacy of treatment. In this study, we propose a virtual positioning system that can simulate the positioning process with a visible beam path under the broad view of a life-like patient-positioning platform to obviate problems with excessively narrow view. This system integrates image processing, computer graphics, and virtual reality to encompass a 3D treatment target reconstructed from medical images of different modalities in a virtual scene. An innovative evaluation method is further proposed to verify the efficacy of the system using a full-scale 3D solid anthropometric model, and a treatment target showed an accuracy of 99% in calibration and a mean compatibility of 95% with an actual measurement. The proposed virtual positioning system provides a training and education platform for radiation therapy. The methodology through which the system was developed is also disclosed as a promising approach to improve the efficiency and safety of the position verification process for existing systems. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Image registration; Positioning verification; Radiation treatment planning; Virtual simulation,Abstract,True,
Scopus,journalPaper,2023,"A social VR-based collaborative exergame for rehabilitation: codesign, development and user study",VRS - Virtual Reality,B,"Immersive virtual reality (VR)-based exercise video games (exergames) are increasingly being employed as a supportive intervention in rehabilitation programs to promote engagement in physical activity, especially for elderly users. A multifaceted and iterative codesign process is essential to develop sustainable exergaming solutions. The social aspect is considered one of the key motivating factors in exergames; however, research on the social aspect of VR exergames has been limited. Previous studies have relied on competitiveness in exergames, but research has shown that competition can lead to adverse effects on users. With the aim of motivating elderly individuals to participate in physical exercise and improving social connectedness during rehabilitation, this work presents a social VR-based collaborative exergame codesigned with elderly participants and therapists. This exergame stimulates full-body exercise and supports social collaboration among users through a collaborative game task. Furthermore, this article presents a user study based on a mixed-methods approach to gather user feedback on exergame design and the effect of social collaboration versus playing alone in a VR exergame in terms of physical exertion and motivation. This study spanned five weeks (99 exergaming sessions) with 14 elderly participants divided into two groups, one playing collaboratively and the other playing individually. Between-group comparisons were performed at baseline (first week) and in the fourth week, and within-group comparisons were performed in the fifth week, when the participants played the exergame in counterbalanced order. In contrast to the first week, the participants exergaming collaboratively in the fourth week reported significantly higher intrinsic motivation on all subscales (enjoyment: p < 0.02, effort: p < 0.002, usefulness: p < 0.01) and physical exertion (p < 0.001) than those playing alone. Thereafter, exergaming in counterbalanced order during the fifth week resulted in significant differences (medium to large effect size) within groups. The participants found the social VR gameplay enjoyable and agreed that collaboration played a vital role in their motivation. They reported various health benefits, a minimal increase in symptoms of simulator sickness, and excellent usability scores (83.75±13.3). In this work, we also identify various key design principles to support healthcare professionals, researchers and industrial experts in developing ergonomic and sustainable VR-based exergames for senior citizens. © 2022, The Author(s).",Eldercare; Exergaming; Immersive healthcare; Metaverse; Motivation; Physical rehabilitation; Social interaction; User-centered design; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality technology using a 360° video: development and evaluation of an educational tool for intraoral radiography using the bisecting angle technique,VRS - Virtual Reality,B,"Intraoral radiography (IOR) practice education is essential for dental students. However, the risk of radiation exposure has resulted in the use of textbooks to learn IOR. Thus, a new educational tool that can effectively use fewer shots or provide indirect experience when practice is not feasible is needed. In this study, we developed a new educational tool called “educational media for the bisecting angle technique” using virtual reality (EMBAT-VR) and evaluated the user experience among students. IOR was divided into 12 steps for 14 teeth, and a scenario was prepared from the perspectives of the operator and patient. On the basis of this scenario, the IOR was reenacted and recorded using a 360° camera. The tool was built on a head-mounted display using the Unity Engine. Eighty-four students were enrolled to evaluate the task performance, browsing search, and satisfaction on a 5-point Likert scale; the corresponding values for the tests were 3. 78 ± 0.70, 3.88 ± 0.76, and 4.01 ± 0.71, respectively. EMBAT-VR was used to investigate the satisfaction (user experience). Responses to 21 questions from 24 students who used traditional textbooks (control group) and 22 students who used the VR educational tool (experimental group) were statistically analyzed using the Mann–Whitney U test. A statistically significant difference was observed between the experimental (4.16 ± 0.64) and control (2.69 ± 0.54) groups. In the usability evaluation, EMBAT-VR presented with a higher score than traditional textbooks. Nonetheless, its effect when performing actual IOR imaging needs follow-up research. © 2023, The Author(s).",360° video; Bisecting angle technique; Dental education; Head-mounted display; Intraoral radiography; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Wearable augmentative and alternative communication (wAAC): a novel solution for people with complex communication needs,VRS - Virtual Reality,B,"Communication is a vital skill of a human’s life. People with different types of disabilities may have Complex Communication Needs and may need a wearable device to help them to communicate. Augmentative and Alternative Communication (AAC) is a term which refers to the methods of facilitating or replacing people’s communication abilities. Brain–computer interfaces (BCI) and Eye-Gaze Technology (EGT) are two widely used access technologies in AAC devices. However, there are only a few studies that have investigated the utilisation of these technologies in a Virtual Reality (VR) or Augmented Reality (AR) environment. VR and AR are both modern technologies which provide immersive environments. In addition, the Mixed Reality (MR) environment combines virtual reality with real life and may offer extra benefits such as better immersion, better interaction, and more information. This paper proposed an MR-based wearable AAC device and compared the usability and acceptability between its Eye-Gaze (EG) and BCI interaction options. Eight neurotypical participants and two participants with cerebral palsy participated. The result showed high usability (accuracy = 93.30%, the information transfer rate was 8.55 selections per minutes) and acceptability (QUEST 2.0 = 4.30, NASA-TLX = 2.14) in the EG session. In contrast, the usability of the BCI system in the current design was questionable. This novel interaction method using Electroencephalogram signals is not sufficiently exploited at the moment, and more research is suggested in the future. © 2023, The Author(s).",Brain–computer interfaces (BCI); Cerebral palsy (CP); Complex communication needs (CCN); Electroencephalograph (EEG); Eye-Gaze technology (EGT); Mixed reality (MR),Abstract_Keywords,True,
Scopus,journalPaper,2023,Visual working memory in immersive visualization: a change detection experiment and an image-computable model,VRS - Virtual Reality,B,"Visual working memory (VWM) is a cognitive mechanism essential for interacting with the environment and accomplishing ongoing tasks, as it allows fast processing of visual inputs at the expense of the amount of information that can be stored. A better understanding of its functioning would be beneficial to research fields such as simulation and training in immersive Virtual Reality or information visualization and computer graphics. The current work focuses on the design and implementation of a paradigm for evaluating VWM in immersive visualization and of a novel image-based computational model for mimicking the human behavioral data of VWM. We evaluated the VWM at the variation of four conditions: set size, spatial layout, visual angle (VA) subtending stimuli presentation space, and observation time. We adopted a full factorial design and analysed participants’ performances in the change detection experiment. The analysis of hit rates and false alarm rates confirms the existence of a limit of VWM capacity of around 7 ± 2 items, as found in the literature based on the use of 2D videos and images. Only VA and observation time influence performances (p<0.0001). Indeed, with VA enlargement, participants need more time to have a complete overview of the presented stimuli. Moreover, we show that our model has a high level of agreement with the human data, r>0.88 (p<0.05). © 2023, The Author(s).",3D spatial arrangement; Change blindness; Depth; Immersive virtual reality; Saliency; Visual angle,Abstract_Keywords,True,
Scopus,journalPaper,2023,Cheer for me: effect of non-player character audience feedback on older adult users of virtual reality exergames,VRS - Virtual Reality,B,"The presence of an audience and its feedback could affect people’s performance and experience during an event, especially related to sports such as tennis or boxing. Similarly, in videogames, players’ gameplay could be affected if there is an audience and its feedback in response to players’ performance in the environment. The inclusion of an audience with non-player characters (NPC) is common in videogames in general. However, there is a limited exploration of the use of an NPC audience in virtual reality (VR) exergames, especially focusing on elderly players. To fill this gap, this work examines the effect of an NPC audience and its associated feedback (with/without) on elderly users of VR exergames. In a user study, we used 120 NPC in a virtual audience. Results showed that the presence of the NPC audience with responsive feedback led to higher performance (with a higher success rate of performing gesture actions, more successful combinations of actions (or combos for short) performed, and more opponent’s combos prevented) and better gameplay experience (with higher levels of competence, autonomy, relatedness, immersion, and intuitive controls) of elderly players. Our results can help frame the design and engineering of VR exergames that are targeted at elderly users to help them have an enhanced gameplay experience and improve their health. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Audience feedback; Elderly users; Exergames; Non-player characters; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Development of educational virtual reality attitude scale: A validity and reliability study,VRS - Virtual Reality,B,"The aim of the study is to address a gap in the literature by developing an educational virtual reality (edVR) attitude measurement instrument, which determines college students’ attitudes towards using VR technology for educational purposes. A sequential exploratory mixed method was employed to develop the measurement instrument. Initially, a qualitative approach was used to establish the face and content validity of the instrument and subsequently a quantitative approach was used to test the construct validity and reliability of attitude statement items. Critical reviews and constructive feedback were gathered from a range of parties, including target users (i.e., college students), learning technology experts, assessment and evaluation authority, and linguists of English and Turkish. The psychometric properties of edVR attitude measurement instrument were tested with a total sample of 305 sophomore, junior and senior students studying at different faculties. The exploratory factor analysis (EFA) results confirmed the single-factor structure with nine items, explaining 63.46% of the total variance and the confirmatory factor analysis (CFA) results indicated a sufficient fit of this single-factor model. The Cronbach’s alpha coefficient for the edVR attitude measurement instrument was 0.92 and the test–retest reliability of the instrument was 0.94. The t-values were significant for all items for 27% of the participants to compare the top and bottom. As a result, the edVR attitude measurement instrument was valid and reliable in measuring students’ attitudes towards educational VR. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Attitude; Attitude towards virtual reality; Virtual reality; Virtual reality attitude instrument,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Immersive virtual reality for improving cognitive deficits in children with ADHD: a systematic review and meta-analysis,VRS - Virtual Reality,B,"Virtual reality (VR) shows great potential in treating and managing various mental health conditions. This includes using VR for training or rehabilitation purposes. For example, VR is being used to improve cognitive functioning (e.g. attention) among children with attention/deficit-hyperactivity disorder (ADHD). The aim of the current review and meta-analysis is to evaluate the effectiveness of immersive VR-based interventions for improving cognitive deficits in children with ADHD, to investigate potential moderators of the effect size and assess treatment adherence and safety. The meta-analysis included seven randomised controlled trials (RCTs) of children with ADHD comparing immersive VR-based interventions with controls (e.g. waiting list, medication, psychotherapy, cognitive training, neurofeedback and hemoencephalographic biofeedback) on measures of cognition. Results indicated large effect sizes in favour of VR-based interventions on outcomes of global cognitive functioning, attention, and memory. Neither intervention length nor participant age moderated the effect size of global cognitive functioning. Control group type (active vs passive control group), ADHD diagnostic status (formal vs. informal) and novelty of VR technology were not significant moderators of the effect size of global cognitive functioning. Treatment adherence was similar across groups and there were no adverse effects. Results should be cautiously interpreted given the poor quality of included studies and small sample. © 2023, The Author(s).",Attention; Attention deficit-hyperactivity disorder (ADHD); Children; Cognition; Memory; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality simulation-based training in otolaryngology,VRS - Virtual Reality,B,"VR simulators will gain wider place in medical education in order to ensure high quality surgical training. The integration of VR simulators into residency programs is actually required more than ever in the era after the pandemic. In this review, the literature is reviewed for articles that reported validation results of different VR simulators designed for the field of otolaryngology. A total of 213 articles searched from Pubmed and Web of Science databases with the key words “virtual reality simulation” and “otolaryngology” on January 2022 are retrieved. After removal of duplicates, 190 articles were reviewed by two independent authors. All the accessible articles in english and which report on validation studies of virtual reality systems are included in this review. There were 33 articles reporting validation studies of otolaryngology simulators. Twenty one articles reported on otology simulator validation studies, eight articles reported rhinology simulator validation studies and four articles reported on pharyngeal and laryngeal surgery simulators. Otology simulators are shown to increase the performance of the trainees. In some studies, efficacy of simulators has been found comparable to cadaveric bone dissections and trainees reported that VR simulators was very useful in facilitating the learning process and improved the learning curves. Rhinology simulators designed for endoscopic sinus surgery are shown to have the construct validity to differentiate the surgeons of different level of expertise. Simulators in temporal bone surgery and endoscopic sinus surgery can mimic the surgical environment and anatomy along with different surgical scenarios, thus can be more implemented in surgical training and evaluation of the trainees in the future. Currently there are no validated surgical simulators for pharyngeal and laryngeal surgery. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Otolaryngology; Surgical simulator; Surgical training; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A user experience analysis for a mobile Mixed Reality application for cultural heritage,VRS - Virtual Reality,B,"Mixed Reality has emerged as a valuable tool for the promotion of cultural heritage. In this context, in particular, the metaphor of virtual portals allows the virtual visit of monuments that are inaccessible or no longer exist in their original form, integrating them into the real environment. This paper presents the development of a Mixed Reality mobile application that proposes a virtual reconstruction of the church of Sant’Elia in Ruggiano, in the southern province of Lecce (Italy). By placing the virtual portal in the same place where the entrance of the church was located, the user can cross this threshold to enter inside and make a virtual journey into the past. The user experience was evaluated by administering a questionnaire to 60 users who tried the application. From the data collected, four user experience factors were identified (interest, focus of attention, presence and usability), which were compared between young and old, male and female users, and between users who had already visited the church in person and all other users. In general, the scores reveal a total independence of the other three factors from usability and a very high level of interest. © 2023, The Author(s).",Extended Reality; Mixed Reality; Mobile applications; Presence; User experience; Virtual portals,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Wand: 360 ∘ video projection mapping using a 360 ∘ camera,VRS - Virtual Reality,B,"In a surrounding projection-based display environment (e.g., a dome theater), the viewers can enjoy a 360 ∘ video with a strong sense of immersion. Building a thriving immersive environment requires two sophisticated steps. First, to generate a single seamless screen, multiple projectors constituting the surrounding display should be carefully registered to the surface. Second, a 360 ∘ video should be mapped to the projection area by considering the display surface geometry and a sweet spot (i.e., a reference viewing position) to allow viewers to perceive the correct perspectives. In this study, Wand, a novel system utilizing a consumer 360 ∘ spherical camera as a calibration device, is proposed to efficiently solve these two issues. Wand first establishes correspondences between the 360 ∘ camera and projectors using structured light patterns, and then filters out any outliers using heuristic criteria. Next, by assuming that the camera is positioned in a sweet spot, Wand solves the geometric registration of the projectors by formulating it as a simple 2D grid mesh parameterization with the correspondence constraints. Consequently, each projector mesh is directly registered into the spherical coordinates, allowing each projector to easily render a perspective-correct view from a 360 ∘ video. We applied Wand to various environments of different dimensions and shapes. The results demonstrate that our method can be used to successfully build seamless and immersive displays and provide correct perspectives at a sweet spot. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",360-degree camera; Automatic geometric registration; Multi-projector display; Projection mapping; Video projection; Virtual reality,Keywords,True,
Scopus,journalPaper,2023,Mixed reality and sensor real-time feedback to increase muscle engagement during deep core exercising,VRS - Virtual Reality,B,"In lower extremity amputee rehabilitation programs, difficult-to-master targeted activation of deep core muscles and pursed-lip breathing training are prescribed to treat poor movement quality and to improve recovery after amputation. Non-invasive wireless sensors and mixed reality (MR) technologies are proposed as a solution. The main aim was to validate a novel rehabilitation technology by exploring whether a combined verbal and visual mixed reality feedback (VF + MR) will initiate a greater change in muscle electrical activation magnitude compared to verbal feedback only (VF) during exercising. The second objective was to evaluate the effectiveness of specific exercise program targeted to engage specifically deep core muscles. Pre-post-test cross-over study involved electromyographic activity (EMG) analysis from Transversus Abdominis (TA) and Multifidus (MF) muscles and self-reported questionnaires to evaluate the efficiency of MR feedback. Anthropometric data, state of health, subjective low back pain (Oswestry Disability Index), and physical activity level (IPAQ) estimation were analysed. The data from 13 patients following unilateral transtibial and transfemoral amputation showed a significant EMG increase in (VF + MR) for Chair Lean (p = 0.03) and Bent Leg Raise (p = 0.0005) exercises for TA muscle. Even though there was no significant difference in Back Bridge and Side Plank exercises, 6 to 10 participants depending on the exercise, had an increase of EMG in the range of 50–400% for both – TA and MF muscles. The proposed solution has a high potential for increasing motivation, self-awareness, and muscle engagement during exercises, based on EMG and self-reported questionnaire data. © 2023, The Author(s).",Mixed reality; Multifidus; Pursed lip breathing; Real-time feedback; Surface electromyography; Transversus abdominis,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,ARbility: re-inviting older wheelchair users to in-store shopping via wearable augmented reality,VRS - Virtual Reality,B,"Engaging in outdoor daily activities such as shopping is an essential, yet challenging activity for older wheelchair users (OWU). However, little is investigated on how to augment the OWU’s independence during their in-person shopping experiences, specifically by addressing their physical conditions. We first conducted semi-structured interviews and a large-scale survey with 77 people in total to discover OWU’s needs and pain points in comparison with those of general older adults or wheelchair population. Based on these findings, we propose ARbility, a wearable AR-based shopping system for OWU which supports product recognition from seated positions on wheelchairs and one-stop shopping functionality for minimizing physical loads. In our user evaluation with 13 OWU in a real-world environment, ARbility demonstrated 33% decrease in arm movement, with the participants validating its efficacy and usability in qualitative interviews. We conclude with implications on how a wearable AR-based shopping system supports the active aging and inclusion of OWU. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Accessibility; Augmented reality; Object detection; Wheelchair users,Title_Keywords,True,
Scopus,journalPaper,2023,Planar fiducial markers: a comparative study,VRS - Virtual Reality,B,"Fiducial markers are a cost-effective solution for solving labeling and monocular localization problems, making them valuable tools for augmented reality (AR), robot navigation, and 3D modeling applications. However, with the development of many marker detection systems in the last decade, it has become challenging for new users to determine which is best suited for their needs. This paper presents a qualitative and quantitative evaluation of the most relevant marker systems. We analyze the available alternatives in the literature, describe their differences and limitations, and conduct detailed experiments to compare them in terms of sensitivity, specificity, accuracy, computational cost, and performance under occlusion. To our knowledge, this study provides the most comprehensive and updated comparison of fiducial markers. In the Conclusion section, we offer recommendations on which method to use based on the application requirements. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Fiducial planar fiducial markers; Marker system comparison; Pose estimation,Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual fieldwork on a ship’s bridge: virtual reality-reconstructed operation scenarios as contextual substitutes for fieldwork in design education,VRS - Virtual Reality,B,"Designing for professional, high-risk user contexts often implies limited accessibility for interaction designers to conduct field research and field testing, and the measures taken by most universities in Norway in 2020 to prevent COVID-19 spread have further contributed to the problem of achieving the contextual insight needed throughout the design process by severely restricting travel for research purposes. In this paper, we describe the use of virtual reality-reconstructed operation scenarios (VRROS) for Arctic-going vessels implemented in support of and as a substitute for the contextual aspects of fieldwork in the education of master’s students studying interaction design. The virtual reality rig contains three scenarios contextualizing ships’ bridges and their surroundings originally developed for research on designing navigation and operation applications using augmented reality technology. We evaluate whether aspects of the VRROS can substitute for real fieldwork by evaluating students’ use of the VRROS using a student questionnaire. Finally, we discuss the value and potential of using VRROS as a supplement and support when studying how to design for hard-to-reach contexts in the future. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Contextual support; Fieldwork; Interaction design education; Virtual reality-reconstructed operation scenarios; VR simulator,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Immersive and desktop virtual reality in virtual fashion stores: a comparison between shopping experiences,VRS - Virtual Reality,B,"With the high growth and prosperity of e-commerce, the retail industry needs to explore new technologies that improve digital shopping experiences. In the current technological scenario, Virtual Reality (VR) emerges as a tool and an opportunity for enhancing shopping activities, especially for the fashion industry. This study explores whether using Immersive Virtual Reality (IVR) technologies enhances the shopping experience in the fashion industry compared to Desktop Virtual Reality (DVR). A within-subject experiment was carried out involving a sample of 60 participants who completed a simulated shopping experience. In the first mode (DVR), a desktop computer setup was used to test the shopping experience using a mouse and keyboard for navigation. The second mode (IVR) exploited a Head-Mounted Display (HMD), and controllers, that allowed navigation while seated on a workstation to avoid sickness. Participants had to find a bag in the virtual shop and explore its features until they were ready to purchase it. Post-hoc measures of time duration of the shopping experience, hedonic and utilitarian values, user experience, and cognitive load were compared. Results showed that participants experienced higher hedonism and utilitarianism in the IVR shop compared to DVR. The cognitive load was comparable in both modes, while user experience was higher in IVR. In addition, the time duration of the shopping experience was higher in IVR, where users stayed immersed and enjoyed it for longer. This study has implications for fashion industry research, as the use of IVR can potentially lead to novel shopping patterns by enhancing the shopping experience. © 2023, The Author(s).",Fashion industry; Retailing; Shopping experience; User study; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality environment for exposure therapy in obsessive–compulsive disorder: a validation study,VRS - Virtual Reality,B,"Introduction: Obsessive–compulsive disorder (OCD) is characterised by recurrent, repetitive, and unwanted thoughts or impulses triggering significant anxiety. Exposure and response prevention is currently the first-line therapy for OCD. The goal of this validation study was to confirm the potential of the VR house environment that incorporates OCD-specific items that cluster around major symptom dimensions: ‘contamination’, ‘symmetry’, ‘checking’ and ‘hoarding’ to induce anxiety and compulsive behaviour in patients with OCD. Method: We assessed a sample of OCD patients (n = 44) that was compared to a group of healthy controls (n = 31). The severity of OCD symptoms was assessed in all subjects. During a single session, participants were asked to approach a set of 10 stimuli (covering four OCD dimensions) and rate their current intensity of distress/anxiety and compulsive tendencies (scales 0–5) provoked by observing each stimulus. Before and after the VR exposure, participants completed questionnaires assessing subjective levels of anxiety (before/after VR exposure), their sense of presence in VR and experienced simulator sickness. Results: The results show that the OCD group reports elevated levels of distress and compulsive behaviour when confronted with VR exposure stimuli compared to the control group, but no increase in anxiety levels has been observed after the VR exposure. The subjective ratings of provoked distress and compulsive behaviour are not associated with severity of OCD symptoms, perceived sense of presence, association with cybersickness symptoms is weak. Conclusion: Our data suggest that the VR house environment is a suitable tool for VR exposure therapy in OCD patients as it demonstrates OCD symptom provocation relevant for individual patients. © 2023, The Author(s).",Anxiety; Medical applications; Obsessive–compulsive disorder; Presence; Virtual reality exposure therapy,Title_Keywords,True,
Scopus,journalPaper,2023,Recognizing shopper demographics from behavioral responses in a virtual reality store,VRS - Virtual Reality,B,"The use of virtual reality (VR) technology in the context of retail is a significant trend in current consumer research, as it offers market researchers a unique opportunity to measure purchase behavior more realistically. Yet, effective methods for assessing the virtual shopping experience based on consumer’s demographic characteristics are still lacking. In this study, we examine the validity of behavioral biometrics for recognizing the gender and age of customers in an immersive VR environment. We used behavior measures collected from eye-tracking, body posture (head and hand), and spatial navigation sources. Participants (n = 57) performed three tasks involving two different purchase situations. Specifically, one task focused on free browsing through the virtual store, and two other tasks focused on product search. A set of behavioral features categorized as kinematic, temporal, and spatial domains was processed based on two strategies. First, the relevance of such features in recognizing age and gender with and without including the spatial segmentation of the virtual space was statistically analyzed. Second, a set of implicit behavioral features was processed and demographic characteristics were recognized using a statistical supervised machine learning classifier algorithm via a support vector machine. The results confirmed that both approaches were significantly insightful for determining the gender and age of buyers. Also, the accuracy achieved when applying the machine learning classifier (> 70%) indicated that the combination of all metrics and tasks was the best classification strategy. The contributions of this work include characterizing consumers in v-commerce spaces according to the shopper’s profile. © 2023, The Author(s).",Consumer demographics; Eye-tracking (ET); Machine learning; Navigation; Shopping experience; Virtual reality; Virtual store,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,The affordances of clinical simulation immersive technology within healthcare education: a scoping review,VRS - Virtual Reality,B,"Whilst clinical simulation is established as an effective education tool within the healthcare community, the inability to offer authentic educational learning environments remains problematic. Advances in technology such as immersive virtual reality offer new opportunities to enhance traditional practice to an extent that may transform learning. However, with traditional clinical simulation stress and anxiety can both hinder performance and learning, yet it is unknown what nuances are applicable within a clinical virtual simulation environment. Determining potential benefits, drawbacks (including related stress and anxiety) and affordances of immersive technology clinical simulation designs may help provide an understanding of its usefulness. The aim of this scoping review is to investigate the range and nature of evidence associated with immersive virtual reality clinical simulation and education design. In addition, the review will describe authentic immersive technology clinical simulation use and reported stress response measurements. A search of seven electronic database and grey literature was performed in accordance with the Joanna Briggs Institute methodology. A key term search strategy was employed with five themes identified and investigated: (1) Healthcare professionals, (2) Clinical simulation, (3) Immersive virtual reality, (4) Stress/anxiety and (5) Authentic learning design. Application of the search strategy resulted in a hit total of 212 articles. Twelve articles met inclusion criteria. With most literature focusing on procedural performance and non-transferable education needs, there was a paucity of research that specifically investigated immersive virtual reality clinical simulation education and related stress. Therefore, this scoping review contributes new understandings by providing valuable insight and potential research gaps into current immersive virtual reality clinical simulation, its relationship to stress and the education design models currently being utilised to develop these concepts. © 2023, The Author(s).",Clinical simulation; Education design; Healthcare; Immersive reality; Stress,Abstract,True,
Scopus,journalPaper,2023,Designing augmented reality for future commercial aviation: a user-requirement analysis with commercial aviation pilots,VRS - Virtual Reality,B,"Augmented reality (AR) capable head-mounted displays (HMDs) have been proposed as technological enablers of several complex future flight concepts, which will bring accompanying pilot situation awareness (SA) and operational safety enhancements. However, relevant aviation design guidance concerning the implementation of modern HMD technologies and AR symbology is sparse. Consequently, the current study describes an SA grounded user-requirements analysis of operational applications for HMD technologies and AR symbology, with the intention of providing inputs for future designs of commercial aviation systems. In addition, insights from the study are relevant for AR design more generally. Endsley’s three-level SA model (1988) was applied as a framework to focus group discussions with eleven aviation subject matter experts. Thematic analysis highlighted multiple operational scenarios where HMD technology and AR may enhance SA, along with the requirements of the technologies to provide these relevant advantages. In future, more detailed user-centred design recommendations should be sought for the specific applications identified within the current study. © 2023, The Author(s).",Augmented reality; Design requirements; Head-mounted displays; Situation awareness; User-centred design,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Assessment of a mixed reality smart home controller: HoloHome pilot study on healthy adults,VRS - Virtual Reality,B,"This work presents HoloHome, a Mixed Reality application that aims to provide a new means of interaction to control the Smart Home components while providing continuous support for the inhabitants, including older adults and people with limited mobility and cognitive skills. HoloHome integrates several technological paradigms such as Mixed Reality, Internet of Things, and Ambient Assisted Living to maximize the tailored domestic comfort for the Smart Home’s inhabitants while promoting their comfort, safety, and independence in performing daily activities. It is designed and implemented on Microsoft HoloLens, ensuring the connection of the Mixed Reality environment with the distributed network of domestic sensors through the WiFi-enabled microcontrollers to present an innovative method of interaction with the Smart Home and IoT network. Additionally, HoloHome provides unique services and assistive technologies mainly designed for older adults and people with short-term memory deficits to provide continuous assistance, support, and instruction for household activities. This paper conducts the assessment of HoloHome, where the proposed system's usability, cybersickness, and effectiveness are investigated. The usability of HoloHome is validated on a sample of healthy adults to highlight desirable modifications before further assessments of older adults and people with disabilities. This pilot study demonstrated a good level of perceived usability (71.5%) among healthy adults, leading toward the best possible adjustments prior to proposing the system to the more fragile users. © 2023, The Author(s).",Aging; Assistive technology; Internet of things; Microsoft HoloLens; Mixed reality; Smart home,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"A mixed reality system combining augmented reality, 3D bio-printed physical environments and inertial measurement unit sensors for task planning",VRS - Virtual Reality,B,"Successful surgical operations are characterized by preplanning routines to be executed during actual surgical operations. To achieve this, surgeons rely on the experience acquired from the use of cadavers, enabling technologies like virtual reality (VR) and clinical years of practice. However, cadavers, having no dynamism and realism as they lack blood, can exhibit limited tissue degradation and shrinkage, while current VR systems do not provide amplified haptic feedback. This can impact surgical training increasing the likelihood of medical errors. This work proposes a novel Mixed Reality Combination System (MRCS) that pairs Augmented Reality (AR) technology and an inertial measurement unit (IMU) sensor with 3D printed, collagen-based specimens that can enhance task performance like planning and execution. To achieve this, the MRCS charts out a path prior to a user task execution based on a visual, physical, and dynamic environment on the state of a target object by utilizing surgeon-created virtual imagery that, when projected onto a 3D printed biospecimen as AR, reacts visually to user input on its actual physical state. This allows a real-time user reaction of the MRCS by displaying new multi-sensory virtual states of an object prior to performing on the actual physical state of that same object enabling effective task planning. Tracked user actions using an integrated 9-Degree of Freedom IMU demonstrate task execution This demonstrates that a user, with limited knowledge of specific anatomy, can, under guidance, execute a preplanned task. In addition, to surgical planning, this system can be generally applied in areas such as construction, maintenance, and education. © 2023, The Author(s).",Augmented reality; Inertial measurement unit; Task planning; Three-dimensional biological printing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,The application of markerless motion capture (MMC) technology in rehabilitation programs: a systematic review and meta-analysis,VRS - Virtual Reality,B,"This review explores the effects of markerless motion capture technology-based rehabilitation programs targeting clinical populations and identifies the types of MMC systems used. A systematic search was conducted in the PubMed, Medline, CINAHL, CENTRAL, EMBASE, and IEEE databases. All eligible studies—single-group or controlled trial studies investigating the effectiveness of MMC technology-based rehabilitation programs—were selected. Single-group studies were qualitatively described; only controlled trial studies were included in the meta-analysis. Effects regarding the application of MMC technology for different types of patients and training body parts are summarized. Five single-group studies and 18 controlled trial studies were included. All studies applied MMC technology as a form of virtual reality training to provide rehabilitation programs. Most of the studies were conducted in regard to upper extremity training in stroke populations. Our meta-analysis revealed that there is no significant difference in the upper limb rehabilitation effects between VR training and control interventions. There is potential to apply MMC technology as an alternative way of providing rehabilitation to increase patients’ motivation and adherence. Future studies on the design of training programs and MMC systems in home settings, which are affordable and accessible for patients, are warranted. (This review is registered in PROSPERO, registration ID: CRD42022298189). © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Markerless motion capture; Rehabilitation; Stroke; Upper extremity; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Immersive virtual-reality computer-assembly serious game to enhance autonomous learning,VRS - Virtual Reality,B,"Immersive virtual reality (VR) environments create a very strong sense of presence and immersion. Nowadays, especially when student isolation and online autonomous learning is required, such sensations can provide higher satisfaction and learning rates than conventional teaching. However, up until the present, learning outcomes with VR tools have yet to prove their advantageous aspects over conventional teaching. The project presents a VR serious game for teaching concepts associated with computer hardware assembly. These concepts are often included in any undergraduate’s introduction to Computer Science. The learning outcomes are evaluated using a pre-test of previous knowledge, a satisfaction/usability test, and a post-test on knowledge acquisition, structured with questions on different knowledge areas. The results of the VR serious game are compared with another two learning methodologies adapted to online learning: (1) an online conventional lecture; and (2) playing the same serious game on a desktop PC. An extensive sample of students (n = 77) was formed for this purpose. The results showed the strong potential of VR serious games to improve student well-being during spells of confinement, due to higher learning satisfaction. Besides, ease of usability and the use of in-game tutorials are directly related with game-user satisfaction and performance. The main novelty of this research is related to academic performance. Although a very limited effect was noted for learning theoretical knowledge with the VR application in comparison with the other methodologies, this effect was significantly improved through visual knowledge, understanding and making connections between different concepts. It can therefore be concluded that the proposed VR serious game has the potential to increase student learning and therefore student satisfaction, by imparting a deeper understanding of the subject matter to students. © 2021, The Author(s).",Active learning; Computer science; e-Learning; Educational game; Game engine; Head mounted display; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Toward inertial position tracking for head-mounted displays: a dataset and a deep learning approach evaluation,VRS - Virtual Reality,B,"This work addresses the problem of correcting the drift error produced by inertial sensors for tracking the position of a Head-Mounted Display in virtual reality applications. Unlike state-of-the-art works, which use exteroceptive sensors to address the problem, this work introduces a novel approach to virtual reality based on deep learning and using data exclusively from proprioceptive sensors. The main contributions of this work are: (1) generating a database with readings taken from an inertial measurement unit located on a virtual reality headset while the users perform different activities and (2) testing deep neural networks based on bidirectional LSTM layers to predict trajectories of the user’s head in virtual environments. Additionally, the results show that, compared to previous approaches based only on inertial readings, the proposed approach improves the estimation of the coordinate of the virtual reality headset corresponding to the user’s height. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Deep learning; Drift error; Position estimation; Tracking; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Design and evaluation of an adaptive virtual reality training system,VRS - Virtual Reality,B,"Successful operation of military aviation depends on effective pilot training. The current training capabilities of the United States Air Force might not be sufficient to meet the demand for new pilots. To help resolve this issue, this study focused on developing a prototype of an adaptive virtual reality (VR) training system. The system was built leveraging the three key elements of an adaptive training system including the trainee’s performance measures, adaptive logic, and adaptive variables. The prototype was based on a procedure for an F-16 cockpit and included adaptive feedback, temporal display features, and various difficulty levels to help trainees maintain an optimal level of cognitive workload while completing their training. An experiment with 20 human participants was conducted, and a trend favoring the use of adaptive training was identified. Results suggested that adaptive training could improve performance and reduce workload as compared to the traditional non-adaptive VR-based training. Implementation of adaptive VR training has the potential to reduce training time and cost. The results from this study can assist in developing future adaptive VR-training systems. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Adaptive; Cognitive workload; Training; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Presence and simulator sickness predict the usability of a virtual reality attention task,VRS - Virtual Reality,B,"Attention is the ability to actively process specific information within one’s environment over longer periods of time while disregarding other details. Attention is an important process that contributes to overall cognitive performance from performing every day basic tasks to complex work activities. The use of virtual reality (VR) allows study of the attention processes in realistic environments using ecological tasks. To date, research has focused on the efficacy of VR attention tasks in detecting attention impairment, while the impact of the combination of variables such as mental workload, presence and simulator sickness on both self-reported usability and objective attention task performance in immersive VR has not been examined. The current study tested 87 participants on an attention task in a virtual aquarium using a cross-sectional design. The VR task followed the continuous performance test paradigm where participants had to respond to correct targets and ignore non-targets over 18 min. Performance was measured using three outcomes: omission (failing to respond to correct targets), commission errors (incorrect responses to targets) and reaction time to correct targets. Measures of self-reported usability, mental workload, presence and simulator sickness were collected. The results showed that only presence and simulator sickness had a significant impact on usability. For performance outcomes, simulator sickness was significantly and weakly associated with omission errors, but not with reaction time and commission errors. Mental workload and presence did not significantly predict performance. Our results suggest that usability is more likely to be negatively impacted by simulator sickness and lack of presence than performance and that usability and attention performance are linked. They highlight the importance of considering factors such as presence and simulator sickness in attention tasks as these variables can impact usability. © 2023, The Author(s).",Attention; Mental workload; Performance; Presence; Simulator sickness; Usability; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality for fire safety training: study of factors involved in immersive learning,VRS - Virtual Reality,B,"For several years, virtual reality (VR) has been increasingly used in many fields, including learning. It has shown many benefits such as increased learner’s safety and engagement. However, the effects of factors that can influence the enhancement of knowledge and the development of skills within immersive virtual devices are still debated. This paper explores the impact of the level of immersion on conceptual and procedural learning outcomes within a virtual environment (VE) for fire safety training. Using a moderated mediation model, we investigated the impact that sense of presence, motivation, cognitive load and emotions had on the relationship between immersion and learning. We then identified that immersion exerts a direct positive effect on procedural learning, but does not exert a direct effect on conceptual learning. None of the relationships between immersion and both types of learning by the sense of presence, the cognitive load, the motivation, and the emotions were significant. Finally, we found that immersion affects the sense of presence through motivation. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented and virtual reality; Factor involved in immersive learning; Human–computer interface; Media in education,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"ORUN-VR2: a VR serious game on the projectile kinematics: design, evaluation, and learning outcomes",VRS - Virtual Reality,B,"Virtual reality (VR) offers possibilities for Science, Technology, Engineering, and Mathematics education by facilitating the understanding of abstract concepts. In this work, we designed, developed, and tested an immersive VR serious game—ORUN-VR—to engage students and to improve their understanding of the Physics concepts related to the projectile kinematics. User-based experiments, knowledge and usability tests, engagement and presence evaluations, screen capturing, and statistical analysis were performed to obtain and analyse data from over 130 high school students that have played ORUN-VR. Knowledge tests indicated an overall learning gain of 52% for the students who played in comparison with those who did not, with differences between men and women. Game engagement and presence VR were positively evaluated. The effects of game engagement, presence in VR, and gender on the learning outcomes are discussed. Our results showed that ORUN-VR can be a valuable immersive VR environment to support learning in projectile kinematics. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Kinematics; Physics teaching; Serious games; STEM; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Validation of a surgical navigation system for hypertensive intracerebral hemorrhage based on mixed reality using an automatic registration method,VRS - Virtual Reality,B,"Hypertensive intracerebral hemorrhage (HICH) is a kind of intracerebral bleeding disease that affects 2.5 per 10,000 people world wide each year. An effective way of curing this disease is to perform a puncture procedure through the dura with a brain puncture drill and tube. The insertion accuracy determines the quality of surgery. Currently, augmented reality—and mixed reality (MR)-based surgical navigation is a promising new technology for surgical navigation in the clinic, aiming to improve the safety and accuracy of surgery. In this study, we present a novel multimodel MR navigation system for HICH surgery. With multi-information fusion technology, organs and hematoma data extracted from CT images can be fused with real patient, which allows surgeons to perform punctures easily. An automatic registration method with a 3D-printed fiducial marker was performed to significantly decrease the time required for surgery preparation.Phantom experiments and user tests were performed in this study, the results of these phantom experiments demonstrated that the mean registration error was 1.18 mm, the insertion error was 1.74 and the average time consumption was 15.9 min, which indicating that this approach was sufficient for clinical application. All the experimental results indicated that this system shows particular promise for use in training inexperienced surgeons, and the next steps would be to refine the system based on the findings with more experienced surgeons. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Automatically registration; Hypertensive intracerebral hemorrhage; Mixed reality; Surgical navigation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Sensory reweighting: a common mechanism for subjective visual vertical and cybersickness susceptibility,VRS - Virtual Reality,B,"The malaise symptoms of cybersickness are thought to be related to the sensory conflict present in the exposure to virtual reality (VR) content. When there is a sensory mismatch in the process of sensory perception, the perceptual estimate has been shown to change based on a reweighting mechanism between the relative contributions of the individual sensory signals involved. In this study, the reweighting of vestibular and body signals was assessed before and after exposure to different typical VR experiences and sickness severity was measured to investigate the relationship between susceptibility to cybersickness and sensory reweighting. Participants reported whether a visually presented line was rotated clockwise or counterclockwise from vertical while laying on their side in a subjective visual vertical (SVV) task. Task performance was recorded prior to VR exposure and after a low- and high-intensity VR game. The results show that the SVV was significantly shifted away from the body representation of upright and towards the vestibular signal after exposure to the high-intensity VR game. Cybersickness measured using the fast motion sickness (FMS) scale found that sickness severity ratings were higher in the high intensity compared to the low-intensity experience. The change in SVV from baseline after each VR exposure modelled using a simple 3-parameter Gaussian regression fit was found to explain 49.5% of the variance in the FMS ratings. These results highlight the aftereffects of VR for sensory perception and suggest a potential relationship between the susceptibility to cybersickness and sensory reweighting. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Multisensory integration; Sensory conflict; Sensory reweighting; Subjective vertical; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Improving real-world skills in people with intellectual disabilities: an immersive virtual reality intervention,VRS - Virtual Reality,B,"Virtual reality (VR) is a promising tool for training life skills in people with intellectual disabilities. However, there is a lack of evidence surrounding the implementation, suitability, and effectiveness of VR training in this population. The present study investigated the effectiveness of VR training for people with intellectual disabilities by assessing (1) their ability to complete basic tasks in VR, (2) real-world transfer and skill generalisation, and (3) the individual characteristics of participants able to benefit from VR training. Thirty-two participants with an intellectual disability of varying severity completed a waste management training intervention in VR that involved sorting 18 items into three bins. Real-world performance was measured at pre-test, post-test, and delayed time points. The number of VR training sessions varied as training ceased when participants met the learning target (≈ 90% correct). A survival analysis assessed training success probability as a function of the number of training sessions with participants split by their level of adaptive functioning (as measured on the Adaptive Behaviour Assessment System Third Edition). The learning target was met by 19 participants (59.4%) within ten sessions (Mdn = 8.5, IQR 4–10). Real-world performance significantly improved from pre- to post-test and pre- to delayed test. There was no significant difference from post- to delayed test. Further, there was a significant positive relationship between adaptive functioning and change in the real-world assessment from the pre-test to the post- and delayed tests. VR facilitated the learning of most participants, which led to demonstrations of real-world transfer and skill generalisation. The present study identified a relationship between adaptive functioning and success in VR training. The survival curve may assist in planning future studies and training programs. © 2023, The Author(s).",Adaptive functioning; Cybersickness; Intellectual disability; Learning; Skill generalisation; Training; Transfer; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A low-cost AR application to control arm prosthesis,VRS - Virtual Reality,B,"This paper presents an augmented reality application to assist with myoelectric prostheses control for people with limb amputations. For that, we use the low-cost Myo armband coupled with low-level signal processing methods specifically built to control filters’ levels and processing chain. In particular, we use deep learning techniques to process the signals and to accurately identify seven different hand gestures. From that, we have built an augmented reality projection of a hand based on AprilTag markers that displays the gesture identified by the deep learning techniques. With the aim to properly train the gesture recognition system, we have built our own dataset with nine subjects. This dataset was combined with one publicly available to work with the data of 24 subjects in total. Finally, three different deep learning architectures have been comparatively studied, achieving high accuracy values (being 95.56% the best one). This validates our hypothesis that it is possible to have an adaptive platform able to fast learn personalized hand/arm gestures while projecting a virtual hand in real-time. This can reduce the adaptation time to myoelectric prostheses and improve the acceptance levels. © 2022, The Author(s).",Augmented reality; Deep learning; Electromyography signal processing; Virtual rehabilitation,Abstract_Keywords,True,
Scopus,journalPaper,2023,Perception-based high quality distributed virtual reality,VRS - Virtual Reality,B,"Virtual reality has great potential to enable remote collaborative work from anywhere in the world. Developing virtual reality into a platform suitable for natural interaction and immersive collaboration requires the experience to be reliably stable. For a networked collaborative environment, perceived smoothness of motion is limited by the tick rate, that is, the frequency at which information is distributed. As tick rate increases, motion will appear increasingly smooth; however, excessive tick rates may introduce additional load on a network without any perceptible benefit to a user. This paper details two visual psychophysics experiments (N1= 16 , N2= 11) carried out to evaluate participant sensitivity to tick rate in virtual reality. The influence of three variables, velocity, complexity, and digital medium were investigated. Both velocity and digital medium displayed a significant effect, whilst complexity did not show significance. A model was then built and validated from the results of these experiments. The model predicts for average walking speed within the desktop condition, that 90% of the population will perceive motion to be smooth at 56 Hz, whilst this 90% threshold lies at 113 Hz for the VR condition. This model can predict participant perception of tick rate under given conditions, enabling networks to intelligently optimise participant experience without adding unnecessary further load on the network. © 2023, The Author(s).",Networks; Psychophysics; Tick rate; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Augmented grocery shopping: fostering healthier food purchases through AR,VRS - Virtual Reality,B,"Food choices are intimately related to individual health. Therefore, the food we buy should be carefully chosen. However, grocery shopping is typically done in noisy environments, and food products usually present cluttered labels with dense texts that make it hard to properly evaluate relevant nutritional data. Augmented reality (AR) allows a shopper to visualize digitally generated contents onto real objects and to interact with them. In this experiment, we investigated the effects of delivering nutritional information using AR technology on food choices. To this end, we ran a between-participants laboratory experiment in which participants were asked to choose among the products available. The experimental group received the food-related information via AR, while the control group had ordinary access to food packaging. We found that AR technology facilitated the choice of healthier food items. Additionally, participants in the experimental group reported that they based their decisions on nutritional information rather than on the appearance of the package. The present work highlights how AR can be exploited to bring to the foreground information that would otherwise be hard to spot, thereby increasing the consumer’s awareness of the overall characteristics of the product. © 2023, The Author(s).",Augmented reality; Food choices; Food facts; Human–computer interaction; Nutritional label,Abstract_Keywords,True,
Scopus,journalPaper,2023,Pointing in the third-person: an exploration of human motion and visual pointing aids for 3D virtual mirrors,VRS - Virtual Reality,B,"A “virtual mirror” is a promising interface for virtual or augmented reality applications in which users benefit from seeing themselves within the environment, such as serious games for rehabilitation exercise or biological education. While there is extensive work analyzing pointing and providing assistance for first-person perspectives, mirrored third-person perspectives have been minimally considered, limiting the quality of user interactions in current virtual mirror applications. We address this gap with two user studies aimed at understanding pointing motions with a mirror view and assessing visual cues that assist pointing. An initial two-phase preliminary study had users tune and test nine different visual aids. This was followed by in-depth testing of the best four of those visual aids compared with unaided pointing. Results give insight into both aided and unaided pointing with this mirrored third-person view, and compare visual cues. We note a pattern of consistently pointing far in front of targets when first introduced to the pointing task, but that initial unaided motion improves after practice with visual aids. We found that the presence of stereoscopy is not sufficient for enhancing accuracy, supporting the use of other visual cues that we developed. We show that users perform pointing differently when pointing behind and in front of themselves. We finally suggest which visual aids are most promising for 3D pointing in virtual mirror interfaces. © 2023, The Author(s).",Human factors; Pointing and selection; Virtual mirrors,Abstract,True,
Scopus,journalPaper,2023,Construction and effect of relationships with agents in a virtual reality environment,VRS - Virtual Reality,B,"Recent studies have been investigating behavior (of approach and avoidance) toward the interaction with agents in virtual reality. This paper aims to explore the cognitive construction of a complementary relationship with agents in virtual reality through a brief interaction in a virtual task. A sample of 53 adult participants with or attending higher education was studied, divided into a control group of 20, and two experimental groups, one of 20 and another of 13 participants. The experimental groups interacted with a virtual agent, one group knowing that it was a non-human character (NPC) that communicated verbally, and the second group being informed that it was an avatar controlled in real-time by another person. The results confirmed an effect of cognitive flexibility on the anthropomorphic and complementary perception of NPCs and the application of relational models toward NPCs. An effect of the nature of the agents (NPC vs. Avatar) in the application of relational models was mediated by the complementary anthropomorphic projection of the agents. The previous interaction with the virtual agent, perceived as an NPC, provided faster skin conductance responses with images of the NPC. These findings contribute to: (1) understanding the impact of cognitive flexibility, a neuropsychological construct, in the construction of projections and relationships with agents in virtual reality; (2) creation and use of virtual reality tools based on communication in a function-led approach; (3) critical view of social models, for the development of a scale that allows assessing anthropomorphism and complementarity in virtual reality. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Anthropomorphism; Cognitive flexibility; Complementarity; Relation models; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Immersive virtual reality as support for the mental health of elderly women: a randomized controlled trial,VRS - Virtual Reality,B,"Several forms of virtual reality (VR) have shown promise in treating mental disorders. However, there is a lack of research investigating the use of multicomponent immersive VR. Therefore, this study aimed to evaluate the effectiveness of an immersive virtual reality (IVR) intervention that incorporated Japanese garden aesthetics, relaxation, and elements of Erickson’s psychotherapy in alleviating depression and anxiety symptoms among elderly women. Sixty women with depressive symptoms were randomly assigned to one of two treatment groups. Both groups received eight (twice a week for four weeks) low-intensity general fitness training sessions. The IVR group (n = 30) received eight additional VR-based relaxation sessions, whereas the control group (n = 30) received eight group relaxation. As outcome measures, the geriatric depression scale (GDS; primary) and Hospital Anxiety and Depression Scale (HADS; secondary) were administered before and after the interventions. The protocol was registered in the ClinicalTrials.gov PRS database (Registration number: NCT05285501). Patients receiving IVR therapy exhibited a greater significant reduction in the GDS (adjusted mean post-difference of 4.10; 95% CI = 2.27–5.93) and HADS (2.95; 95% CI = 0.98–4.92) scores than those receiving the control intervention. In conclusion, IVR with elements of psychotherapy, relaxation, and garden aesthetics may alleviate the severity of depression and anxiety symptoms in elderly women. © 2023, The Author(s).",Exposure therapy; Head-mounted display; Hypnosis; Mood disorders; Virtual therapeutic garden; VRET,Title_Abstract,True,
Scopus,journalPaper,2023,Examining the potential of VR program Tilt Brush in reducing anxiety,VRS - Virtual Reality,B,"Recent advancement in technology has made virtual reality (VR) more accessible and immersive than ever before, resulting in its increasing utility in various industries. Despite this, VR has remained an underutilised tool within clinical psychology. This study aimed to explore the potential of using VR for therapeutic benefits through examining the level of flow and anxiety-reducing effects of freeform drawing in real life (on paper) versus drawing in VR (using Tilt Brush) via a randomised-controlled trial with 40 participants. State and trait anxiety was measured using the State-Trait Anxiety Inventory, level of flow was measured using the Long Flow State Scale, and level of presence was measured using the iGroup Presence Questionnaire. Overall level of flow was not significantly different between both groups, implying drawing in VR induces as much flow as drawing in real life. Level of flow was positively correlated to level of presence experienced in the VR group (p <.01). Although there was no significant interaction effect, both groups experienced an overall decrease in state anxiety, with the VR group experiencing a significant reduction of state anxiety from pre- to post-test (p <.01). © 2022, The Author(s).",Anxiety; Drawing; Flow; Presence; Tilt Brush; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Real and perceived feet orientation under fatiguing and non-fatiguing conditions in an immersive virtual reality environment,VRS - Virtual Reality,B,"Lower limbs position sense is a complex yet poorly understood mechanism, influenced by many factors. Hence, we investigated the position sense of lower limbs through feet orientation with the use of Immersive Virtual Reality (IVR). Participants had to indicate how they perceived the real orientation of their feet by orientating a virtual representation of the feet that was shown in an IVR scenario. We calculated the angle between the two virtual feet (α-VR) after a high-knee step-in-place task. Simultaneously, we recorded the real angle between the two feet (α-R) (T1). Hence, we assessed whether the acute fatigue impacted the position sense. The same procedure was repeated after inducing muscle fatigue (T2) and after 10 min from T2 (T3). Finally, we also recorded the time needed to confirm the perceived position before and after the acute fatigue protocol. Thirty healthy adults (27.5 ± 3.8: 57% women, 43% men) were immersed in an IVR scenario with a representation of two feet. We found a mean difference between α-VR and α-R of 20.89° [95% CI: 14.67°, 27.10°] in T1, 16.76° [9.57°, 23.94°] in T2, and 16.34° [10.00°, 22.68°] in T3. Participants spent 12.59, 17.50 and 17.95 s confirming the perceived position of their feet at T1, T2, T3, respectively. Participants indicated their feet as forwarding parallel though divergent, showing a mismatch in the perceived position of feet. Fatigue seemed not to have an impact on position sense but delayed the time to accomplish this task. © 2023, The Author(s).",Fatigue; Lower limbs; Neuroscience; Position sense; Proprioception; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Assessment of user-interaction strategies for neurosurgical data navigation and annotation in virtual reality,VRS - Virtual Reality,B,"While virtual-reality (VR) has shown great promise in radiological tasks, effective user-interaction strategies that can improve efficiency and ergonomics are still under-explored and systematic evaluations of VR interaction techniques in the context of complex anatomical models are rare. Therefore, our study aims to identify the most effective interaction techniques for two common neurosurgical planning tasks in VR (point annotation and note-taking) from the state-of-the-arts, and propose a novel technique for efficient sub-volume selection necessary in neuroanatomical navigation. We assessed seven user-interaction methods with multiple input modalities (gaze, head motion, controller, and voice) for point placement and note-taking in the context of annotating brain aneurysms for cerebrovascular surgery. Furthermore, we proposed and evaluated a novel technique, called magnified selection diorama (Maserama) for easy navigation and selection of complex 3D anatomies in VR. Both quantitative and semi-quantitative (i.e., NASA Task Load Index) metrics were employed through user studies to reveal the performance of each interaction scheme in terms of accuracy, efficiency, and usability. Our evaluations demonstrated that controller-based interaction is preferred over eye-tracking-based methods for point placement while voice recording and virtual keyboard typing are better than freehand writing for note-taking. Furthermore, our new Maserama sub-volume selection technique was proven to be highly efficient and easy-to-use. Our study is the first to provide a systematic assessment of existing and new VR interaction schemes for neurosurgical data navigation and annotation. It offers valuable insights and tools to guide the design of future VR systems for radiological and surgical applications. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Eye-tracking; Human–computer interaction; MRI; Neurosurgery; Surgical navigation; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2023,Predicting motor behavior: an efficient EEG signal processing pipeline to detect brain states with potential therapeutic relevance for VR-based neurorehabilitation,VRS - Virtual Reality,B,"Virtual reality (VR)-based motor therapy is an emerging approach in neurorehabilitation. The combination of VR with electroencephalography (EEG) presents further opportunities to improve therapeutic efficacy by personalizing the paradigm. Specifically, the idea is to synchronize the choice and timing of stimuli in the perceived virtual world with fluctuating brain states relevant to motor behavior. Here, we present an open source EEG single-trial based classification pipeline that is designed to identify ongoing brain states predictive of the planning and execution of movements. 9 healthy volunteers each performed 1080 trials of a repetitive reaching task with an implicit two-alternative forced choice, i.e., use of the right or left hand, in response to the appearance of a visual target. The performance of the EEG decoding pipeline was assessed with respect to classification accuracy of right vs. left arm use, based on the EEG signal at the time of the stimulus. Different features, feature extraction methods, and classifiers were compared at different time windows; the number and location of informative EEG channels and the number of calibration trials needed were also quantified, as well as any benefits from individual-level optimization of pipeline parameters. This resulted in a set of recommended parameters that achieved an average 83.3% correct prediction on never-before-seen testing data, and a state-of-the-art 77.1% in a real-time simulation. Neurophysiological plausibility of the resulting classifiers was assessed by time–frequency and event-related potential analyses, as well as by Independent Component Analysis topographies and cortical source localization. We expect that this pipeline will facilitate the identification of relevant brain states as prospective therapeutic targets in closed-loop EEG-VR motor neurorehabilitation. © 2021, The Author(s).",Brain-state decoding; Brain–computer interface (BCI); Classification; EEG; EEG-VR; Hand selection; Human-in-the-loop; Machine learning; Motor behavior; Motor intention; Neurorehabilitation; Open source pipeline; Pre-movement; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Acceptance and use of virtual reality games: an extension of HMSAM,VRS - Virtual Reality,B,"Virtual reality (VR) is considered as one of the technological megatrends of 2020s, and today, VR systems are used in various settings, digital gaming being among the most popular ones. However, there has been a dearth of understanding regarding the central factors behind VR gaming acceptance and use. The present study therefore aimed to explain the factors that drive the use and acceptance of VR games. We extended the hedonic-motivation system acceptance model with utilitarian and inconvenience factors to capture the pertinent features of VR systems more holistically. We proposed a theoretical model and analyzed it through covariance-based structural equation modeling using an online survey sample of 473 VR gamers. Our findings help explain the role of different antecedents behind VR gaming acceptance and demonstrate that VR gaming is driven more by the hedonic gaming aspects than by the utilitarian health and well-being aspects of VR games, enjoyment being the strongest driver behind VR gaming intention and immersion. Moreover, findings also suggested that use intentions and immersion levels are not significantly diminished by physical discomfort and VR sickness. The findings, which potentially extend to other VR systems as well, also pose important implications for the providers of VR games. As the main contribution, based on our empirical findings, we provide a greater theoretical understanding on VR gaming acceptance and use. © 2023, The Author(s).",Digital games; Dual-purposed systems; User behavior; Virtual reality; Virtual reality gaming; VR,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Sensory cue integration of visual and vestibular stimuli: a case study for 4D rides,VRS - Virtual Reality,B,"This paper investigated human self-motion perception through the visual and vestibular sensory systems under the context of virtual reality (VR) and 4D. Consistently with general 4D riding applications, we designed and used sinusoidal motions as stimuli, which resembled simple roller coaster rides moving in three directions of pitch, surge, and heave. Based on the Bayesian integration model, we experimented to determine the uncertainty involved in the two sensory systems and their relative contributions. We factored in small sensory discrepancies between the visual and vestibular cues and visually noticeable obstacles that could distract viewers. We found that the vestibular system contributed more dominantly to the perception in the ratio of 7:3 than the vision, demonstrating vestibular capture. We also discovered that the visual scenes that contain eye-catching elements and pure optical flows can hamper self-motion perception while increasing the perceptual uncertainty. Our findings can serve as a basis for designing motion effects for VR and 4D applications, especially in situations where multiple sensory systems are stimulated simultaneously. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Bayesian integration; Differential threshold; Dual-task paradigm; Self-motion perception; Vestibular capture; Visual distractor; Visuo-vestibular perception,Abstract,True,
Scopus,journalPaper,2023,VR unseen gaze: inducing feeling of being stared at in virtual reality,VRS - Virtual Reality,B,"The main aim of this paper is to investigate a method that can induce a VR user’s feeling of being stared at. Contrary to the previous method that directly informs users of unseen gaze with a voice and subtitles, the proposed method provides an indirect subtle stimulus to induce a feeling that they are being watched. Our study began with the observations reported by the previous studies that the feeling of being stared at appears to be highly correlated with hypervigilance, anxiety, and fear of ambient information around people. To clarify this further, we additionally conducted an online survey. Based on the results of the previous studies and our online survey, we defined two types of factors that may effectively induce the user to feel that they are watched: environmental factors (darkness, absence/presence of people, reddish color palette, and suspenseful background music) and stimulative factors (subtle changes in vision, subtle changes in sound, and feeling in the back of the neck). Afterward, two experiments were conducted for in-depth investigation of environmental and stimulative factors, respectively. The purpose was to find out what kinds of factors should be provided at what strength to induce the user’s feeling of being watched among the defined factors. Lastly, an application test was performed to not only clarify the advantages and limitations of the proposed method but also propose design guidelines for future use. We expect that our study will serve as a cornerstone for providing a new type of VR experience that the feeling of being watched. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Feeling of being stared at; Scopaesthesia; Unseen gaze; User interfaces; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2023,The effect of embodied interaction designs on flow experience: examination in VR games,VRS - Virtual Reality,B,"Embodied interaction (EI) is a body-based interactive paradigm that has the potential to enhance the flow experience in virtual reality (VR). To examine this hypothesis, this paper distinguishes three common types of EI in VR, namely body-based, tangible, and avatar-based EI. In empirical studies 1–3, three comparative experiments were carried out to examine the respective effects of these EI modes on flow experience. Subjective and physiological data (e.g., electrodermal activity, etc.) from studies 1–2 show that the use of body-based and tangible EI leads to the enhancement of physiological arousal (an important indicator of concentration) and flow experience. Study 3 reveals the effect of avatar-based EI on flow experience. Using a high-ownership avatar is found to enhance the sense of presence and involvement, which then improves flow. This effect is only for experienced users, showing a moderating effect. The mechanism of these positive effects requires further clarification. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Avatar; Embodied interaction; Flow experience; Tangible interaction; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,BeHere: a VR/SAR remote collaboration system based on virtual replicas sharing gesture and avatar in a procedural task,VRS - Virtual Reality,B,"In this paper, we focus on the significance of remote collaboration using virtual replicas, avatar, and gesture on a procedural task in industry; thus, we present a Virtual Reality (VR)/Spatial Augmented Reality (SAR) remote collaboration system, BeHere, based on 3D virtual replicas and sharing gestures and avatar. BeHere enables a remote expert in VR to guide a local worker in real-time to complete a procedural task in the real-world. For the remote VR site, we construct a 3D virtual environment using virtual replicas, and the user can manipulate them by using gestures in an intuitive interaction and see their partners’ 3D virtual avatar. For the local site, we use SAR to enable the local worker to see instructions projected onto the real-world based on the shared virtual replicas and gestures. We conducted a formal user study to evaluate the prototype system in terms of performance, social presence, workload, and ranking and user preference. We found that the combination of visual cues of gestures, avatar, and virtual replicas plays a positive role in improving user experience, especially for remote VR users. More significantly, our study provides useful information and important design implications for further research on the use of gesture-, gaze- and avatar-based cues as well as virtual replicas in VR/AR remote collaboration on a procedural task in industry. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Hand gesture; Human–computer interaction; Remote collaboration; Spatial augmented reality; Virtual reality; Virtual replicas or avatar,Abstract_Keywords,True,
Scopus,journalPaper,2023,The effects of spatial configuration on relative translation gain thresholds in redirected walking,VRS - Virtual Reality,B,"In this study, we explore how spatial configurations can be reflected in determining the threshold range of Relative Translation Gains (RTGs), a translation gain-based Redirected Walking (RDW) technique that scales the user’s movement in Virtual Reality (VR) in different ratios for width and depth. While previous works have shown that various cognitive factors influence the RDW threshold, studies investigating the impact of environmental aspects on the RDW threshold with regard to the user’s visual perception were lacking. Therefore, we examined the effect of spatial configurations on the RTG threshold by analyzing the participant’s responses and gaze distribution data in two user studies. The first study concerned the size of the virtual room and the existence of objects within it, and the second study focused on the combined impact of room size and spatial layout. Our results show that room size, the existence of objects, and spatial layout all significantly affect the RTG threshold range. Based on our findings, we propose virtual space rescaling guidelines to increase the range of adjustable movable space with RTGs for developers: placing distractors in the room, setting the perceived movable space to be larger than the adjusted movable space for an empty room, and avoiding the placement of object clusters in the center of the room. Our findings can be used to adaptively rescale VR users’ space according to the target virtual space’s configuration with a unified coordinate system that enables the utilization of physical objects in a virtual scene. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Locomotion; Redirected walking; Relative translation gains; Threshold; Virtual reality; Visual cognition,Abstract_Keywords,True,
Scopus,journalPaper,2023,The effect of narrative element incorporation on physical activity and game experience in active and sedentary virtual reality games,VRS - Virtual Reality,B,"Narratives are pervasive in video games and have been found to increase physical activity in active video games. However, the effect of incorporating narrative elements has seldom been examined in fully immersive virtual reality games. We investigated the effect of narrative element incorporation (between-subject: narrative vs. no narrative) in active virtual reality and sedentary virtual reality games (within-subject) and examined between- and within-subject effects on physical activity behavior, game experience, and physical activity engagement. We randomized 36 sedentary college students to either the narrative or the non-narrative group. All participants played an active virtual reality and a sedentary virtual reality game in counter-balanced order. Before each game session, they either watched a 5-min narrative video (narrative) or directly played the original virtual reality games without narratives (non-narrative). We collected participants’ physical activity data using wrist-worn accelerometers; we obtained their game experience and physical activity engagement via questionnaires. The narrative group spent a greater proportion of time in moderate-to-vigorous physical activity (%) and had less non-movement time during the active virtual reality gameplay than the non-narrative group (all p values <.05). The active virtual reality sessions induced a greater positive affect and greater physical activity engagement ratings than the sedentary virtual reality sessions. The incorporation of narrative elements in active virtual reality increased the relative time spent in moderate-to-vigorous physical activity and reduced non-movement time, compared to the non-narrative group. Active virtual reality encouraged more activity by participants and offered them a more enjoyable gaming experience in which they engaged more. Active virtual reality is a feasible physical activity promotion option among sedentary adults; the incorporation of narrative elements in active virtual reality helps increase relative moderate-to-vigorous physical activity and should be further explored for its efficacy. © 2023, The Author(s).",Active video game; Exergame; Game experience; Narrative; Physical activity; Story; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Usability, user experience and mental workload in a mobile Augmented Reality application for digital storytelling in cultural heritage",VRS - Virtual Reality,B,"Augmented Reality (AR) has become an increasingly used technology to support and enhance the enjoyment of cultural heritage. Particularly relevant is its importance for digital storytelling: by framing a portion of a fresco or painting with a smartphone, an AR mobile application can provide contextually relevant information, also in the form of multimedia content, that can help the user to understand the story and meaning behind the images. In this type of application, human factors are of fundamental importance for the effectiveness of the narrative: a mobile AR application must avoid distracting the user’s attention from the content in order to encourage a good level of concentration and immersion. The case study presented in this paper deals with a mobile AR application developed to guide visitors in the interpretation of the frescoes inside the Basilica of Saint Catherina of Alexandria in Galatina. The aim of the study is the analysis of the relations among usability, user experience and mental workload factors in AR-based digital storytelling. © 2023, The Author(s).",Augmented Reality; Cultural heritage; Digital storytelling; Mental workload; Mobile; Usability; User experience,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Eye Tracking in Virtual Reality: a Broad Review of Applications and Challenges,VRS - Virtual Reality,B,"Eye tracking is becoming increasingly available in head-mounted virtual reality displays with various headsets with integrated eye trackers already commercially available. The applications of eye tracking in virtual reality are highly diversified and span multiple disciplines. As a result, the number of peer-reviewed publications that study eye tracking applications has surged in recent years. We performed a broad review to comprehensively search academic literature databases with the aim of assessing the extent of published research dealing with applications of eye tracking in virtual reality, and highlighting challenges, limitations and areas for future research. © 2023, The Author(s).",Eye tracking; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A haptic-feedback virtual reality system to improve the Box and Block Test (BBT) for upper extremity motor function assessment,VRS - Virtual Reality,B,"The Box and Block Test (BBT) has been widely used to assess gross upper extremity (UE) motor function. We designed a haptic-feedback virtual reality (VR) system, named the VBBT, to improve the BBT for more specific assessments. The VBBT task required users to move virtual blocks from one compartment of a virtual box to the other within one minute. The focus of this pilot study was to examine the validity, reliability and motivation of the novel assessment. Totally, 113 healthy subjects and 16 post-stroke patients were recruited for a thorough evaluation. We found that scores of the BBT and VBBT were significantly correlated, both of which declined as participants’ age. The normative ranges of kinematic metrics in different age groups were used to identify deficiencies in UE motor function involving smoothness, hand dexterity and motion efficiency. Also, a significant correlation between the VBBT and Action Research Arm Test (ARAT) (|r|≥ 0.56) indicated concurrent validity of the novel assessment. Test–retest results indicated that the VBBT assessment had high reliability (ICCs = 0.62–0.80). The Intrinsic Motivation Inventory results showed that the VBBT was given higher scores for the enjoyment (p < 0.05) and completion effort (p < 0.05) than that for the BBT, indicating patients have greater motivation in the VBBT assessment. In conclusion, the VBBT can provide validated, reliable and motivative assessment for UE motor function with kinematic metrics. It suggests that the haptic-feedback VR contributes to the BBT in specific assessments of UE motor function. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Haptic device; Motivation; Reliability; Stroke; Validity; Virtual Box and Block Test,Title_Abstract,True,
Scopus,journalPaper,2023,The effect of wearing a head-mounted display on the boundaries of the cervical range of motion based on perceived comfort in a static posture,VRS - Virtual Reality,B,"The head-mounted display (HMD) for virtual reality (VR) systems is becoming more popular due to technological advancements, providing users with a more immersive experience. Since an HMD is a wearable device, head movement may be restricted, compared to situations in which an HMD is not worn, and perceived discomfort during head movement may increase. Thus, this study recruited 30 participants and investigated the effects on perceived comfort from wearing an HMD, considering the cervical range of motion (CROM) in four head movement directions (flexion, extension, lateral bending, and rotation). We propose angle boundaries for joint isocomfort in order to evaluate the appropriateness of working postures in an HMD-based VR environment. Although the maximum CROM for head movement in all directions was not significantly limited by wearing the HMD, the main effects of wearing HMD on perceived discomfort were observed from extension and lateral bending. Based on the perceived CROM comfort score, appropriate ranges for each head movement direction are proposed. This study contributes to the establishment of ergonomic guidelines on the use of HMDs and the development of VR content. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cervical range of motion; Head-mounted display; Perceived comfort; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Rehabilitation of visual functions in adult amblyopic patients with a virtual reality videogame: a case series,VRS - Virtual Reality,B,"Amblyopia or lazy eye is a dysfunction of the visual system that appears during childhood and traditionally has been considered untreatable in adults. Its main consequences are the loss of visual acuity and contrast sensitivity of the amblyopic eye and binocular vision impairments. During the last years videogames have been used as a therapeutic tool for amblyopia with the inconclusive results. The present work has assessed the effectiveness of a virtual reality videogame (AmbliOK®) in the neurorehabilitation of four adult clinical cases with anisometropic amblyopia. Visual acuity, contrast sensitivity, stereopsis and interocular suppression were assessed before, during, immediately after, one month and one year (in one patient) after the training. The intervention was conducted along four weeks (10 h) and yielded the variable results. In general, all patients showed an improvement in visual functions although not all ameliorated in the same way. Visual acuity measures improved in all patients, falling outside the amblyopia criterion at the end of the treatment. However, the improvement was not maintained one month later in two patients. Contrast sensitivity progressively improved for the amblyopic and the fellow eyes with all patients showing better results one month after the treatment. The patient assessed one year after still showed better results than in the baseline. Patients showing bad stereopsis in the baseline reached a performance considered normal one month and even one year after the treatment. The effectiveness of the treatment seems to be related to the characteristics of patients. © 2021, The Author(s).",Amblyopia; Contrast sensitivity; Stereopsis; Videogame; Virtual reality; Visual acuity,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Emotion recognition using facial expressions in an immersive virtual reality application,VRS - Virtual Reality,B,"Facial expression recognition (FER) is an important method to study and distinguish human emotions. In the virtual reality (VR) context, people’s emotions are instantly and naturally triggered and mobilized due to the high immersion and realism of VR. However, when people are wearing head mounted display (HMD) VR equipment, the eye regions will be covered. The FER accuracy will be reduced if the eye region information is discarded. Therefore, it is necessary to obtain the information of eye regions using other methods. The main difficulty in FER in an immersive VR context is that the conventional FER methods depend on public databases. The image facial information in the public databases is complete, so these methods are difficult to directly apply to the VR context. To solve this problem, this paper designs and implements a solution for FER in the VR context as follows. A real facial expression database collection scheme in the VR context is implemented by adding an infrared camera and infrared light source to the HMD. A virtual database construction method is presented for FER in the VR context, which can improve the generalization of models. A deep network named the multi-region facial expression recognition model is designed for FER in the VR context. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Data synthesis; Emotion recognition; Machine learning; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Human–machine integration based augmented reality assisted wire-bending training system for orthodontics,VRS - Virtual Reality,B,"With the increasing demand for orthodontic treatment, the skill of wire bending is more and more important for orthodontists. Traditional wire bending training needs a high cost of time and resources. In this paper, an augmented reality assisted wire-bending training system (ARAWTS) is proposed. ARAWTS provides 4 typical wire bending training tasks for the trainee and can give training feedback and improvement advice to the trainee by gesture recognition during the training. For the elaborate and vague wire bending gesture recognition, we develop a temporal logical relation (TLR) module to sparsely sample dense frames and learn the TLRs between frames of gestures. To reduce the computational cost and time, we introduce a new type of sparse optical flow called Focus Grid Optical Flow (FGOF). From the results of experiments, the proposed algorithm implemented on an AR device (HoloLens) achieves a high recognition rate with low computational complexity and ARAWTS is proved reliable. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Focus Grid Optical Flow; Gesture recognition; Orthodontic treatment; Temporal logical relation; Wire bending,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Educational UTAUT-based virtual reality acceptance scale: a validity and reliability study,VRS - Virtual Reality,B,"This study aims to fill a gap in current research on virtual reality (VR) by developing a valid and reliable educational VR acceptance scale based on the unified theory of acceptance and use of technology (UTAUT) model to measure the level of students’ acceptance and use of VR systems. In three phases, the reliability and validity studies of the scale were performed with a total sample of 440 second, third, and fourth-year undergraduate students studying at various faculties in the 2021–2022 academic year. The face validity and content validity of the scale were examined by obtaining expert opinions. Exploratory factor analysis (EFA) was carried out with the first group of samples (n = 186) and confirmatory factor analysis (CFA) was carried out with the second group of samples (n = 219). After conducting EFA, the scale had four factors with 18 items, explaining 67.62 percent of the total variance. According to CFA, the construct of the 4-factor with 21 items scale had a good fit with the data. Cronbach’s alpha coefficient and test–retest methods reliability coefficient of scale that were calculated to determine the reliability of the measurements were found to be.88 and.89, respectively. The discriminatory power of the items was examined by comparing the participants’ bottom 27 percent and top 27 percent and calculating adjusted item-total correlations. The findings revealed that the educational UTAUT-based virtual reality acceptance scale was a valid and reliable instrument to measure students’ acceptance and use of VR systems. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Technology acceptance; UTAUT; Virtual reality; Virtual reality acceptance; Virtual reality scale,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,The sentiment of a virtual rock concert,VRS - Virtual Reality,B,"We created a virtual reality version of a 1983 performance by Dire Straits, this being a highly complex scenario consisting of both the virtual band performance and the appearance and behaviour of the virtual audience surrounding the participants. Our goal was to understand the responses of participants, and to learn how this type of scenario might be improved for later reconstructions of other concerts. To understand the responses of participants we carried out two studies which used sentiment analysis of texts written by the participants. Study 1 (n = 25) (Beacco et al. in IEEE Virtual Reality: 538–545, 2021) had the unexpected finding that negative sentiment was caused by the virtual audience, where e.g. some participants were fearful of being harassed by audience members. In Study 2 (n = 26) notwithstanding some changes, the audience again led to negative sentiment—e.g. a feeling of being stared at. For Study 2 we compared sentiment with questionnaire scores, finding that the illusion of being at the concert was associated with positive sentiment for males but negative for females. Overall, we found sentiment was dominated by responses to the audience rather than the band. Participants had been placed in an unusual situation, being alone at a concert, surrounded by strangers, who seemed to pose a social threat for some of them. We relate our findings to the concept of Plausibility, the illusion that events and situations in the VR are really happening. The results indicate high Plausibility, since the negative sentiment, for example in response to being started at, only makes sense if the events are experienced as actually happening. We conclude with the need for co-design of VR scenarios, and the use of sentiment analysis in this process, rather than sole reliance on concepts proposed by researchers, typically expressed through questionnaires, which may not reflect the experiences of participants. © 2022, The Author(s).",Concert; Evaluation; Performance; Plausibility; Presence; Sentiment analysis; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Using interpretative phenomenological analysis to gain a qualitative understanding of presence in virtual reality,VRS - Virtual Reality,B,"Quantitative methods have thus far been the predominant methodological stance of virtual presence research, leaving much to be desired in terms of qualitative understanding. Yet, virtual experiences are a highly personal engagement, unique to each individual, and their presence in virtual reality can be viewed in terms of its experiential individuality. This aspect of the virtual experience is overlooked by conventional quantitative methods, which clusters ratings or scores to form group deductions. Therefore, to address the qualitative gap in the literature and provide an appropriate examination of virtual experiences from the perspective of the individual, an Interpretative Phenomenological Approach was undertaken. This alternate methodology sought to reveal which aspects of virtual experiences users identify as enabling feelings of presence. Examination of common themes among accounts of individuals were performed, to investigate the generation of feelings of presence in virtual reality. Online recruitment provided six interviewees who participated in online semi-structured interviews, prior to Interpretive Phenomenological Analysis. Three superordinate themes were identified: visual satisfaction, freedom of interaction and suspension of real life. Expectance, realism and prevention of disbelief are among the sub-themes identified that contributed to the interviewee’s highly present experiences. The identified themes demonstrated the greatest influences of enabling a deeper sense of presence, in turn enhancing their experiences within virtual reality. In acknowledging these mitigating influences, it is hoped this may enable future virtual systems to build upon the research provided and produce consistently high-presence experiences. Consequently, this can aid educational, therapeutic and entertainment applications of virtual reality. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Immersion; Interpretive phenomenological analysis (IPA); Presence; Virtual environment (VE); Virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Evaluation of a VR prototype for neuropsychological rehabilitation of attentional functions,VRS - Virtual Reality,B,"Background: Recent research has established virtual reality (VR) applications as a valid and viable tool for neuropsychological assessment. In contrast, there are only a few studies on its potential as a therapeutic program. To examine the prospects of VR as a tool for functional rehabilitation, a VR training program of attentional functions was conceptualized during a Hackathon in 2018. The prototype of the immersive VR training program takes the patient on a virtual journey around the world (VR Traveller). In different locations around the globe, patients exercise different subcomponents of attention in a visually appealing and ecologically valid environment. Objective: To evaluate the feasibility, acceptability and tolerability of the newly developed VR Traveller prototype for neurorehabilitation training. Method: Thirty-five patients with acquired brain injury and mild to moderate attention deficits were instructed to complete the VR Traveller training program in a 20–30 min session during inpatient neurorehabilitation. Feasibility and acceptability were assessed with the user experience questionnaire (UEQ) and a self-constructed feasibility questionnaire, and tolerability was assessed with the virtual reality sickness questionnaire (VRSQ). Results: Analyses of the UEQ and the feasibility questionnaire yield evidence for a high acceptance among most patients. The VRSQ data suggest that symptoms of VR sickness were hardly experienced. Conclusion: Patients’ ratings of the VR training in terms of acceptability and feasibility were positive, suggesting that VR programs represent an accepted, feasible, and well-received alternative to traditional cognitive rehabilitation approaches. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Attention training; Feasibility; Immersive VR; Neuropsychological rehabilitation,Abstract,True,
Scopus,journalPaper,2023,FreesoundVR: soundscape composition in virtual reality using online sound repositories,VRS - Virtual Reality,B,"The intersection between sound and music computing and Virtual Reality (VR) has grown significantly over the past decades, amounting to an established area of research today. However, still scarce research has been conducted on the development of specific tools for sound design and composition. In this paper, we investigate a new way of exploring online sound repositories to retrieve sounds to be used in soundscape composition, which leverages the VR medium. Specifically, we created a VR system that allows users to search, download, and explore Freesound content in an immersive manner, as well as to use it for soundscape composition practices via a virtual digital audio workstation (DAW). The tags associated to a sound in the repository were converted into virtual objects and environments, which the user could navigate while listening to the sound. We conducted a user study with 16 composers where the developed system was compared against a conventional counterpart comprising the Freesound web version and the Audacity DAW. Overall, quantitative and qualitative results did not indicate a clear and generalized preference for a system over the other. The usability of the two systems along with their offered creativity support, cognitive workload and emotional impact were deemed to be at a comparable level. Nevertheless, the full potential of VR in creating novel compositional experiences also clearly emerged. Our study shows that VR is an effective medium to support users’ creativity during the process of exploring and selecting sounds from an online repository as well as for composing a soundscape. © 2022, The Author(s).",Composition; Internet of audio things; Musical XR; Soundscape,Title_Abstract,True,
Scopus,journalPaper,2023,Comparison of the effect of 360° versus two-dimensional virtual reality video on history taking and physical examination skills learning among undergraduate medical students: a randomized controlled trial,VRS - Virtual Reality,B,"Before caring for patients, video instruction is commonly used for undergraduate medical students, and 360° virtual reality (VR) videos have gained increasing interest in clinical medical education. Therefore, the effect of immersive 360° VR video learning compared with two-dimensional (2D) VR video learning in clinical skills acquisition should be evaluated. This randomized, intervention-controlled clinical trial was aimed to assess whether immersive 360° VR video improves undergraduate medical students' learning effectiveness and reduces the cognitive load in history taking and physical examination (H&P) training. From May 1 2018 to October 30 2018, 64 senior undergraduate medical students in a tertiary academic hospital were randomized to receive a 10-min immersive 360° (360° VR video group; n = 32) or 2D VR instructional video (2D VR video group; n = 32), including essential knowledge and competency of H&P. The demographic characteristics of the two groups were comparable for age, sex, and cognitive style. The total procedure skill score, physical examination score, learner’s satisfaction score, and total cognitive load in the 360° VR video group were significantly higher than those in the 2D VR video group (effect sizes [95% confidence interval]: 0.72 [0.21–1.22], 0.63 [0.12–1.13], 0.56 [0.06–1.06], and 0.53 [0.03–1.03], respectively). This study suggested that a10-minute 360° VR video instruction helped undergraduate medical students perform fundamental H&P skills as effectively as 2D VR video. Furthermore, the 360° VR video might result in significantly better procedural metrics of physical examinations with higher learner satisfaction despite the higher cognitive load. © 2022, The Author(s).",360° video; Cognitive load; History taking; Physical examination; Two-dimensional video; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Designing effective virtual reality environments for pain management in burn-injured patients,VRS - Virtual Reality,B,"Burn patients engage in repetitive painful therapeutic treatments, such as wound debridement, dressing changes, and other medical processes high in procedural pain. Pharmacological analgesics have been used for managing pain, but with ineffective results and negative side effects. Studies on pain management for burn patients suggested that Virtual Reality can treat procedural pain. This paper describes the process of designing, testing, and deploying a Virtual Reality system into a hospital setting. Firstly, a workshop was conducted to identify the most suitable types of Virtual Reality contents for the needs of burn-injured patients. Then, an experimental study, with 15 healthy adults, explored the analgesic impact of the Virtual Reality contents. The pain was induced through a cold pressor. Finally, we deployed the Virtual Reality system into the hospital to examine its efficiency on burn-injured inpatients. This study presents factors for the effective design and deployment of Virtual Reality for burn-injured patients residing in a hospital. Those factors refer to the use of cartoonish features and a choice of content based on each patient’s interests to increase the positive emotions and the use of interactive features, portable equipment to reduce pain and increase the feasibility of the technology in clinical settings. Finally, our results indicated that the extension of the VR use after the therapeutic session could support more effective pain treatment. Trial registration number Protocol ID: AA8434. © 2021, The Author(s).",Anxiety; Burn injuries; Interactivity; Pain; Patient-centred design; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Participatory design and evaluation of virtual reality physical rehabilitation for people living with dementia,VRS - Virtual Reality,B,"Emerging research confirms the need for technologically enhanced solutions to support non-pharmacological interventions which can improve the quality of life, the mental and physical health of demented people. Several types of research examined if virtual reality can be an effective solution. This paper aims to present the cyclic process of prototyping, testing, analysing, and refining the VR system in real-world clinical settings. Seven people with moderate to severe dementia were recruited. The experiment required the patients to attend three virtual reality iterations of rapid prototyping with user testing. All three iterations involved training activities with upper body movements similar to their usual physical training. A mixed-methods design measured affect and emotional behaviour using the Observed Emotion Rating Scale and the Visual Analog Scale. Content analysis was conducted following observations and interviews. During each iteration of rapid prototyping with user testing, quantitative measurements of performance, independence and time were recorded. Eye tracking and movement information were captured by the system. Finally, a simplified version of the presence and usability scales evaluated the system. The results of this study provide further evidence that virtual reality can play a significant role in the improvement of people’s with dementia physical training and emotional health when is appropriately designed. The results present the vital factors which should be incorporated in a virtual reality system which are: 1) a simple interactions modality; 2) visible visual targets and continuous feedback; 3) personalized virtual environments; 4) personalized range of movements. © 2022, The Author(s).",Dementia; Emotional health; Eye tracking; Physical rehabilitation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Do individual constraints induce flexibility of visual field dependence following a virtual immersion? Effects of perceptive style and cybersickness,VRS - Virtual Reality,B,"Accurately perceiving the gravitational direction is key to successful interaction in our terrestrial environment. In this context field dependence (FD), the importance given to static and/or dynamic visual cues has largely been discussed. Although first considered a trait, several studies suggest FD be flexible in response to postural or visual contexts and to poor virtual reality user experience. The aim of this study was therefore to determine the influence of a disruptive virtual immersion on the level of static and dynamic FD. Forty-five participants were exposed to a virtual maritime environment for up to 14 min. Cybersickness and sense of presence were measured. Before and after virtual immersion, the rod and frame test and the rod and disk test were performed to assess static and dynamic FD, respectively. We demonstrated a significant decrease in both levels of FD after immersion in initially more dependent participants. Decrease in static FD was explained by high initial static FD and severe cybersickness, while decrease in dynamic FD was only explained by the initial level of dynamic FD. In this study, we provide evidence confirming FD flexibility, likely reflecting an adaptation process to environmental or individual-related constraints. Yet, static and dynamic FD seems to rely on separate mechanisms, highlighting the necessity to specify which characteristic of visual information (static or dynamic) individuals depend on when assessing their FD. Our results question the reliability of virtual reality for perceptive or motor diagnoses without considering its consequences, specifically in vulnerable populations such as the elderly. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Field dependence; Perceptive style; Virtual immersion; Virtual reality; Visual cues,Abstract_Keywords,True,
Scopus,journalPaper,2023,Mixed reality depth contour occlusion using binocular similarity matching and three-dimensional contour optimisation,VRS - Virtual Reality,B,"Mixed reality applications often require virtual objects that are partly occluded by real objects. However, previous research and commercial products have limitations in terms of performance and efficiency. To address these challenges, we propose a novel depth contour occlusion (DCO) algorithm. The proposed method is based on the sensitivity of contour occlusion and a binocular stereoscopic vision device. In this method, a depth contour map is combined with a sparse depth map obtained from a two-stage adaptive filter area stereo matching algorithm and the depth contour map of the objects extracted by a digital image stabilisation optical flow method. We also propose a quadratic optimisation model with three constraints to generate an accurate dense map of the depth contour for high-quality real-virtual occlusion. The whole process is accelerated by GPU. To evaluate the effectiveness of the algorithm, we demonstrate a time consumption statistical analysis for each stage of the DCO algorithm execution. To verify the reliability of the real-virtual occlusion effect, we conduct an experimental analysis on single-sided, enclosed, and complex occlusions. Subsequently, we compare it with the occlusion method without quadratic optimisation. With our GPU implementation for real-time DCO, the evaluation indicates that applying the presented DCO algorithm enhances the real-time performance and the visual quality of real-virtual occlusion. © 2022, The Author(s).",Binocular camera; Depth contour extraction; Mixed reality; Occlusion handling; Real-virtual occlusion,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Effects of coordinate system and position of AR notification while walking,VRS - Virtual Reality,B,"Augmented reality (AR) head-mounted displays (HMDs) allow users to easily receive notifications while participating other tasks by projecting information directly in their field of view. Although HMDs offer such benefits in displaying notifications, there is insufficient research on the effective placement of AR notifications when the user is walking. For this, we conducted two studies based on different types of AR information to identify how the users perceive and understand the AR notifications according to placement while walking. We compared two different coordinate systems (display-fixed and body-fixed) and three different positions (top, right, and bottom) for icon-type and text-type notifications. The results indicated that using a display-fixed coordinate system for icon-type notifications yields significantly higher noticeability and comprehension. In contrast, using a body-fixed coordinate system for text-type notifications significantly improved comprehension and walking performance. Regarding the position of notifications, the bottom position resulted in a significantly higher noticeability and comprehension for both icon- and text-type notifications compared with the top. Based on these results, we draw some recommendations for the future design of notifications in AR HMDs. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",AR notification; Augmented reality; Coordinate systems; Placement; User interface; Walking,Abstract_Keywords,True,
Scopus,journalPaper,2023,Postural instability-induced compensative movements in virtual reality,VRS - Virtual Reality,B,"During virtual reality usage, two egocentric mental representations are constructed simultaneously. The first representation is rooted in the physical reality in which VR is set up, and the second originates from the mental construction of a computer-generated virtual environment. In both cases, participants configure their posture based on multimodal stimuli while responding to environmental cues. In most cases, the postural cues provided by the digital and real environment may be conflicting. In this study, 50 right-handed volunteers were enrolled. In a pre-test session, attentional focus-related personality bias (perspective-taking) was assessed, and afterward, postural movements and presence experiences were measured while the participants performed a spatial orientation task in VR. Participants were placed in an upright position with their right hands positioned in front of a physically real point on the laboratory wall. Afterward, participants were exposed to a VR environment in which they performed a room-tilting task. Participants with higher hand-related presence scores showed decreased compensatory hand drift in the VR environment. The rate of contralateral hand drift showed a reversed association with the intensity of the perspective-taking trait. VR-induced postural instability can be attenuated by the compensative hand drift that alleviates the conflicts between the two rival inner VR and outer VR environments that compete for attention and provide different reference cues. © 2022, The Author(s).",Attention allocation; Compensative hand drift; Hemispace; Immersion; Posture; Reference taking; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Immersive virtual reality for upper limb rehabilitation: comparing hand and controller interaction,VRS - Virtual Reality,B,"Virtual reality shows great potential as an alternative to traditional therapies for motor rehabilitation given its ability to immerse the user in engaging scenarios that abstract them from medical facilities and tedious rehabilitation exercises. This paper presents a virtual reality application that includes three serious games and that was developed for motor rehabilitation. It uses a standalone headset and the user's hands without the need for any controller for interaction. Interacting with an immersive virtual reality environment using only natural hand gestures involves an interaction that is similar to that of real life, which would be especially desirable for patients with motor problems. A study involving 28 participants (4 with motor problems) was carried out to compare two types of interaction (hands vs. controllers). All of the participants completed the exercises. No significant differences were found in the number of attempts necessary to complete the games using the two types of interaction. The group that used controllers required less time to complete the exercise. The performance outcomes were independent of the gender and age of the participants. The subjective assessment of the participants with motor problems was not significantly different from the rest of the participants. With regard to the interaction type, the participants mostly preferred the interaction using their hands (78.5%). All four participants with motor problems preferred the hand interaction. These results suggest that the interaction with the user’s hands together with standalone headsets could improve motivation, be well accepted by motor rehabilitation patients, and help to complete exercise therapy at home. © 2022, The Author(s).",Hand gestures; Hand tracking; Motor rehabilitation; Serious games; Standalone headsets; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Improvement of robustness against electrode shift for facial electromyogram-based facial expression recognition using domain adaptation in VR-based metaverse applications,VRS - Virtual Reality,B,"Recognition of users’ facial expressions and reflecting them on the face of the user’s virtual avatar is a key technology for realizing immersive virtual reality (VR)-based metaverse applications. As a method to realize this technology, a facial electromyogram (fEMG)-based facial expression recognition (FER) system, with the fEMG electrodes being attached on the pad of a VR headset, has recently been proposed. However, the performance of such FER systems has severely deteriorated when the locations of fEMG electrodes change by the re-wearing of the VR headset, requiring long and tedious calibration sessions every time the user wears the VR headset. In this study, we developed an fEMG-based FER system that is robust against electrode shifts by employing new signal processing techniques: covariate shift adaptation techniques in feature and classifier domains. To verify the feasibility of the proposed method, fEMG data were recorded while participants were making 11 facial expressions repeatedly in four sessions, between which they detached and reattached the fEMG electrodes on their faces. Our experiments showed that classification accuracy dropped from 88 to 79% by the change of the electrode locations when the proposed method was not applied, whereas the accuracy was significantly improved up to 86% when the proposed covariate shift adaptation method was employed. It is expected that the proposed method would contribute to enhancing the practicality of the fEMG-based FER, promoting the practical application of the fEMG-based FER to VR-based metaverse applications. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Covariate shift adaptation; Electrode shift; Electromyogram; Facial expression recognition; Human–machine interface; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Investigating the effectiveness of immersive VR skill training and its link to physiological arousal,VRS - Virtual Reality,B,"This paper details the motivations, design, and analysis of a study using a fine motor skill training task in both VR and physical conditions. The objective of this between-subjects study was to (a) investigate the effectiveness of immersive virtual reality for training participants in the ‘buzz-wire’ fine motor skill task compared to physical training and (b) investigate the link between participants’ arousal with their improvements in task performance. Physiological arousal levels in the form of electro-dermal activity (EDA) and ECG (Electrocardiogram) data were collected from 87 participants, randomly distributed across the two conditions. Results indicated that VR training is as good as, or even slightly better than, training in physical training in improving task performance. Moreover, the participants in the VR condition reported an increase in self-efficacy and immersion, while marginally significant differences were observed in the presence and the temporal demand (retrieved from NASA-TLX measurements). Participants in the VR condition showed on average less arousal than those in the physical condition. Though correlation analyses between performance metrics and arousal levels did not depict any statistically significant results, a closer examination of EDA values revealed that participants with lower arousal levels during training, across conditions, demonstrated better improvements in performance than those with higher arousal. These findings demonstrate the effectiveness of VR in training and the potential of using arousal and training performance data for designing adaptive VR training systems. This paper also discusses implications for researchers who consider using biosensors and VR for motor skill experiments. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Electro-dermal activity; Heart rate variability; Immersive virtual reality; Physiological arousal; Skill training,Abstract_Keywords,True,
Scopus,journalPaper,2023,"VR-based fire evacuation in underground rail station considering staff’s behaviors: model, system development and experiment",VRS - Virtual Reality,B,"As an enclosed and normally crowded environment, underground rail station is a dangerous place when fire emergency happens. Traditional methods usually adopt simulation or drill to evaluate station design or contingency plans, which differ far from real situations in both parameters and people’s complicated behavior in an emergency, especially the staff’s guiding action. Staff’s behaviors are enormously important for evacuation, because evacuation path is complex and smoking diffusing is in the same direction with the path to some extent. In this regard, a fire evacuation system considering staff’s behavior in virtual reality environment was developed. The distinguished features of the system embodied in three aspects. First, it provided a vivid, interactive and immersed platform for staff to practice and optimize evacuation plan. Second, staff’s behaviors and their implication on passengers’ evacuation were modeled. Passengers were treated as multi-agent that can feel the surrounding environment including staff’s guiding, and they can change their paths in real-time which implemented by a dynamic path choice model. Third, input and visualization of the fire data extracted from fire simulation, and the 3D station model, provide quite real evacuation environment. Recruiting staff from operation company as subjects, the platform was applied for fire evacuation test considering staff’s intervention. The results demonstrate that the proposed model and developed system can be effectively used for contingency plan evaluation and improvement. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Evacuation; Fire emergency; Underground rail station; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,"Retraction Note: Subjective visual vertical in virtual reality (Curator SVV): validation and normative data (Virtual Reality, (2018), 22, 4, (315-320), 10.1007/s10055-018-0336-5)",VRS - Virtual Reality,B,"The Editor-in-Chief has retracted this article. After an investigation by the University of Sydney, it was concluded that this article did not receive ethical approval for the research described in this study. David Szmulewicz agrees to this retraction. Leigh A. McGarvie has not responded to any correspondence from the editor/publisher about this retraction. Elodie Chiarovano and Hamish G. MacDougall have not explicitly stated whether they agree to this retraction notice. © 2022, Springer-Verlag London Ltd., part of Springer Nature.",,Title,True,
Scopus,journalPaper,2023,The virtualization of human–robot interactions: a user-centric workload assessment,VRS - Virtual Reality,B,"Interest in the virtualization of human–robot interactions is increasing, yet the impact that collaborating with either virtual or physical robots has on the human operator’s mental state is still insufficiently studied. In the present work, we aimed to fill this gap by conducting a systematic assessment of a human–robot collaborative framework from a user-centric perspective. Mental workload was measured in participants working in synergistic co-operation with a physical and a virtual collaborative robot (cobot) under different levels of task demands. Performance and implicit and explicit workload were assessed as a function of pupil size variation and self-reporting questionnaires. In the face of a similar self-reported mental demand when maneuvering the virtual or physical cobot, operators showed shorter operation times and lower implicit workload when interacting with the virtual cobot compared to its physical counterpart. Furthermore, the benefits of collaborating with a virtual cobot most vividly manifested when the user had to position the robotic arm with higher precision. These results shed light on the feasibility and importance of relying on multidimensional assessments in real-life work settings, including implicit workload predictors such as pupillometric measures. From a broader perspective, our findings suggest that virtual simulations have the potential to bring significant advantages for both the user's mental well-being and industrial production, particularly for highly complex and demanding tasks. © 2022, The Author(s).",Collaborative robotic; Human–robot collaboration; Human–robot interaction; Mental workload; Virtual reality,Keywords,True,
Scopus,journalPaper,2023,The study of virtual reality influence on the process of professional training of miners,VRS - Virtual Reality,B,"Virtual reality technologies are actively applied for the organization of professional training in various industries, as well as in distance learning. However, numerous studies show the presence of a large number of negative factors that limit the effectiveness of using these technologies (united by the concept of ""cybersickness""). The study, identification and reduction in the influence of these negative factors will increase the immersiveness and quality of the professional training process. Within the framework of this study, several hypotheses have been put forward regarding the negative and positive impact of VR technologies on the process of professional training, the coal and mining industry has been chosen as the subject area. Thus, the problem of effective training of miners for activities in regular and emergency situations is considered, in the latter case, VR technologies would allow forming the necessary set of skills and knowledge about actions in emergency situations. To confirm the declared hypotheses, an experimental group of 30 people was formed, corresponding to the trained miners by age characteristics. Based on the analysis, a list of quantitative and qualitative metrics for evaluating interaction with virtual reality was formed, the software of virtual scenes for two tasks (moving simple objects and a set of exercises in a virtual mine) was developed. The experimental group repeatedly performed these exercises, which allowed us to analyze the dynamics of changes in the average values of quantitative and qualitative metrics. The data obtained were processed by statistical tests (Shapiro–Wilk, Kruskal–Wallis, Mann–Whitney), which allowed us to assess the impact of the selected configurations (with and without VR) and the number of attempts on the selected metrics. The obtained results partially or completely confirmed the declared hypotheses and allowed us to form a list of recommendations for the organization of high-quality professional training using virtual reality technologies. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Negative factors of interaction with virtual reality; Professional training; Qualitative metrics; Quantitative metrics; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,New technologies for the classification of proximal humeral fractures: Comparison between Virtual Reality and 3D printed models—a randomised controlled trial,VRS - Virtual Reality,B,"Correct classification of fractures according to their patterns is critical for developing a treatment plan in orthopaedic surgery. Unfortunately, for proximal humeral fractures (PHF), methods for proper classification have remained a jigsaw puzzle that has not yet been fully solved despite numerous proposed classifications and diagnostic methods. Recently, many studies have suggested that three-dimensional printed models (3DPM) can improve the interobserver agreement on PHF classifications. Moreover, Virtual Reality (VR) has not been properly studied for classification of shoulder injuries. The current study investigates the PHF classification accuracy relative to an expert committee when using either 3DPM or equivalent models displayed in VR among 36 orthopaedic surgery residents from different hospitals. We designed a multicentric randomised controlled trial in which we created two groups: a group exposed to a total of 34 3DPM and another exposed to VR equivalents. Association between classification accuracy and group assignment (VR/3DPM) was assessed using mixed effects logistic regression models. The results showed VR can be considered a non-inferior technology for classifying PHF when compared to 3DPM. Moreover, VR may be preferable when considering possible time and resource savings along with potential uses of VR for presurgical planning in orthopaedics. © 2023, The Author(s).",Interobserver agreement; Proximal humeral fracture; Shoulder surgery planning; Three-dimensional printed models; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Visualization in virtual reality: a systematic review,VRS - Virtual Reality,B,"Rapidly growing virtual reality (VR) technologies and techniques have gained importance over the past few years, and academics and practitioners have been searching for efficient visualizations in VR. To date, the emphasis has been on the employment of game technologies. Despite the growing interest and potential, visualization studies have lacked a common baseline in the transition period of 2D visualizations to immersive ones. To this end, the presented study aims to provide a systematic literature review that explains the state-of-the-art research and future trends in visualization in virtual reality. The research framework is grounded in empirical and theoretical works of visualization. We characterize the reviewed literature based on three dimensions: (a) Connection with visualization background and theory, (b) Evaluation and design considerations for virtual reality visualization, and (c) Empirical studies. The results from this systematic review suggest that: (1) There are only a few studies that focus on creating standard guidelines for virtual reality, and each study individually provides a framework or employs previous studies on traditional 2D visualizations; (2) With the myriad of advantages provided for visualization and virtual reality, most of the studies prefer to use game engines; (3) Although game engines are extensively used, they are not convenient for critical scientific studies; and (4) 3D versions of traditional statistical visualization techniques, such as bar plots and scatter plots, are still commonly used in the data visualization context. This systematic review attempts to add a clear picture of the emerging contexts, different elements, and interdependencies to the literature. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Game technologies; Systematic review; Virtual reality; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A virtual reality platform to simulate orientation and mobility training for the visually impaired,VRS - Virtual Reality,B,"Blindness and low vision are an urgent, steadily increasing public health concern. One of the most dramatic consequences of the debilitating conditions that cause visual impairment (VI) is the loss of mobility. Immobility is a grave impediment to quality of life. Orientation and mobility (O&M) training is a profession specific to VI that teaches safe, efficient, and effective travel skills to persons of all ages and in all types of environments. However, the lack of standardized best practices for objective assessment of performance and the exposure of trainees to harm during training are key hurdles for O&M education success. To partially mitigate these drawbacks, we propose a virtual reality platform that can support O&M trainers in the evaluation and refinement of O&M practice, help O&M trainees learn new O&M techniques in a completely safe, yet realistic, environment, and raise awareness for VI in the general public. The proposed platform is tested with a proof-of-concept experiment that evaluates the clinical utility of a custom VI simulation, the immersivity of the virtual reality experience—a crucial attribute for training and educational purposes—and participants’ disability awareness and gained knowledge about the challenges faced by persons with VI in their daily life. The first concept is tested by assessing participants’ performance in virtual reality-based wayfinding tasks while the second and third are tested through a series of dedicated questionnaires. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Assistive technology; Disability; Human–computer interaction; Travel aid; Urban accessibility,Title_Abstract,True,
Scopus,journalPaper,2023,Effects of system- and media-driven immersive capabilities on presence and affective experience,VRS - Virtual Reality,B,"Virtual reality (VR) is receiving widespread attention as a delivery tool for exposure therapies. The advantage offered by VR over traditional technology is a greater sense of presence and immersion, which magnifies user effects and enhances the effectiveness of exposure-based interventions. The current study systematically examined the basic factors involved in generating presence in VR as compared to standard technology, namely (1) system-driven factors that are exclusive to VR devices while controlling general factors such as field of view and image quality; (2) media-driven factors of the virtual environment eliciting motivational salience through different levels of arousal and valence (relaxing, exciting and fear evoking stimuli); and (3) the effects of presence on magnifying affective response. Participants (N = 14) watched 3 different emotionally salient videos (1 × fear evoking, 1 × relaxing and 1 × exciting) in both viewing modes (VR and Projector). Subjective scores of user experience were collected as well as objective EEG markers of presence (frontal alpha power, theta/beta ratio). Subjective and objective presence was significantly greater in the VR condition. There was no difference in subjective or objective presence for stimulus type, suggesting presence is not moderated by arousal, but may be reliant on activation of motivational systems. Finally, presence did not magnify feelings of relaxation or excitement, but did significantly magnify users’ experience of fear when viewing fear evoking stimuli. This is in line with previous literature showing strong links between presence and generation of fear, which is vital in the efficacy of exposure therapies. © 2021, The Author(s).",Head-mounted display; Motivational salience; Presence; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,A method for evaluating the learning concentration in head-mounted virtual reality interaction,VRS - Virtual Reality,B,"In education, learning concentration is closely related to the quality of learning, and teachers can adjust their teaching methods accordingly to improve the learning outcomes of students. Particularly in head-mounted virtual reality interactions, current methods for assessing learning concentration cannot be fully applied to new interactive environments because immersion shaping and cognitive formation differ from the conventional education. Therefore, in this study, a learning concentration assessment method is proposed to measure the learning concentration of students in head-mounted virtual interaction, using the expression score, visual focus rate, and task mastery as evaluation indicators. In addition, the weights of the evaluation indicators can be configured to be included in the calculation of learning concentration depending on the characteristics of different types of courses. The results of a usability evaluation indicate that the learning concentration of students can be effectively evaluated using the proposed method. By developing and implementing strategies for optimizing learning effects, the learning concentration and assessment scores of students increased by 18% and 15.39%, respectively. © 2022, The Author(s).",Head-mounted virtual reality interaction; Learning concentration; Virtual reality education; Weight configuration,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Using virtual environments to facilitate refugee integration in third countries,VRS - Virtual Reality,B,"Virtual experiences (VEs) have significant potential to enrich emotional interactions, to encourage socialisation and improve communication. In education, VEs offer new approaches for delivering content. In this paper we consider the application of VEs for assisting refugees in Senegal to learn how to navigate the complexities of the UK health system; a substantial stumbling block for their integration into society and for their own health. Participants (N= 122), refugees awaiting to be repatriated, were exposed to material presented via three different media text, 360° videos and virtual reality (VR) across a total of seven different modalities. The experiment investigated specific attributes of the media that would facilitate refugees’ integration, such as knowledge received and retained, experience, usability and presence. The results show that interactive media, in particular VR, was significantly better across all tested attributes. © 2022, The Author(s).",Knowledge transfer; Media; Memory; Virtual experiences; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Application of virtual reality for peritoneal dialysis exchange learning in patients with end-stage renal disease and cognitive impairment,VRS - Virtual Reality,B,"Cognitive impairment is not uncommon in patients with end-stage renal disease and can make it more difficult for these patients to carry out peritoneal dialysis (PD) on their own. Their attempts to do so may result in adverse consequences such as peritonitis. PD exchange is a complex procedure demanding knowledge and skill which requires close supervision and guidance by a renal nurse specialist. In this study, a non-immersive virtual reality (VR) training program using a Leap motion hand tracking device was developed to facilitate patients’ understanding and learning of the PD exchange procedure before attempting real task practice. This study was a two-center single-blinded randomized controlled trial on 23 incident PD patients. Patients in the experimental group received 8 sessions of VR training, while patients in the control were provided with printed educational materials. The results showed that there were significant differences between the two groups in performance of the overall PD exchange sequence, especially on the crucial steps. VR had a patient satisfaction rate of 89%, and all patients preferred to have the VR aid incorporated in PD training. Our findings conclude VR can be a useful aid in the training and reinforcement of PD exchange procedures, with distinct merits of being free from restrictions of time, space, and manpower. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cognitive impairment; End-stage renal disease; Peritoneal dialysis; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Using virtual reality to train infection prevention: what predicts performance and behavioral intention?,VRS - Virtual Reality,B,"Training medical professionals for hand hygiene is challenging, especially due to the invisibility of microorganisms to the human eye. As the use of virtual reality (VR) in medical training is still novel, this exploratory study investigated how preexisting technology acceptance and in-training engagement predict VR hand hygiene performance scores. The effect of training in the VR environment on the behavioral intention to further use this type of training device (a component of technology acceptance) was also investigated. Participants completed a VR hand hygiene training comprising three levels of the same task with increasing difficulty. We measured technology acceptance, composed of performance expectancy, effort expectancy, and behavioral intention, pre- and post-training, and in-training engagement using adaptations of existing questionnaires. We used linear regression models to determine predictors of performance in level-3 and of behavioral intention to further use VR training. Forty-three medical students participated in this exploratory study. In-training performance significantly increased between level-1 and level-3. Performance in level-3 was predicted by prior performance expectancy and engagement during the training session. Intention to further use VR to learn medical procedures was predicted by both prior effort expectancy and engagement. Our results provide clarification on the relationship between VR training, engagement, and technology acceptance. Future research should assess the long-term effectiveness of hand hygiene VR training and the transferability of VR training to actual patient care in natural settings. A more complete VR training could also be developed, with additional levels including more increased difficulty and additional medical tasks. © 2022, The Author(s).",Engagement; Hand hygiene; Procedure training; Technology acceptance; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Correction to: Buddy biking: a user study on social collaboration in a virtual reality exergame for rehabilitation (Virtual Reality, (2023), 27, 1, (245-262), 10.1007/s10055-021-00544-z)",VRS - Virtual Reality,B,"The article ""Buddy biking: a user study on social collaboration in a virtual reality exergame for rehabilitation"", written by Emil Rosenlund Høeg et al., was originally published online on 27th, July 2021, without Open Access. After publication, the author decided to opt for Open Choice and to make the article an Open Access publication. Therefore, the copyright of the article has been changed to © The Author(s) 2021, and the article is forthwith distributed under the terms of the Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third-party material in this article is included in the article's creative commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article's creative commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0. © Springer-Verlag London Ltd., part of Springer Nature 2021.",,Title_Abstract,True,
Scopus,journalPaper,2023,Pneumatic and acoustic suit: multimodal haptic suit for enhanced virtual reality simulation,VRS - Virtual Reality,B,"A haptic device provides users with physical feedback to enhance their experience in virtual reality (VR). We developed a multimodal haptic suit, called as Pneumatic and Acoustic (PA) suit, which exhibits high-resolution haptic feedback, and applies high pressure and realistic vibration to a user. The PA suit artificially simulates the sensation of brief and strong collisions such as the impact of an explosion, ball, or fist. The pneumatic actuators, consisting of 40 air bladders, are designed as bellows-type pneumatic devices for vertical inflation. The air bladders are placed on the chest at an equal interval distance of 30 mm for high-resolution feedback. The acoustic actuators use an effective sound signal of a collision similar to realistic vibrations. This study aims to examine the effectiveness of our multimodal haptic suit in improving VR experience of users. The recognition tests clearly show that participants distinguish between the haptic patterns and position of collided virtual objects with the suit. The user study involving a collision of a ball shows that the PA suit transmits the approximate pressure of a real ball collision with artificial haptic feedback. Our receiving ball and explosion VR simulations confirm that the PA suit improves a VR experience depending on the types of actuators and VR contents. The results prove that the PA suit creates distinguishable haptic patterns for guiding a task and improves the VR experience of users with powerful and multimodal haptic feedback hence providing high-quality VR simulation. © 2023, The Author(s).",Acoustic vibration; Haptics; Human computer interaction; Pneumatic actuation; Virtual reality; Wearable,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Evaluating an augmented reality interface for sailing navigation: a comparative study with a immersive virtual reality simulator,VRS - Virtual Reality,B,"Sailing navigation is an activity that requires acquiring and processing information from the surrounding environment. The advancement of technology has enabled sailboats to have an increasing number of onboard sensors that make sailing more user-friendly. However, data provided by these sensors are still visualized on 2D digital displays that imitate traditional analog interfaces. Although these displays are strategically placed on the sailboat, the user needs to divert attention from the primary navigation task to look at them, thus spending a significant amount of cognitive resources. AR-based technologies have the potential to overcome these limitations by displaying information registered in the real environment, but there are no studies in the literature for validating the effectiveness of this technology in the field of sailing. Thus, we designed a head-mounted display AR-based interface to assist users in monitoring wind data to avoid user diversion from the primary task of sailing. We conducted a user study involving 45 participants in an Immersive Virtual Reality simulated environment. We collected objective and subjective measures to compare the AR-based interface with a traditional data visualization system. The AR-based interface outperformed the traditional data visualization system regarding reaction time, cognitive load, system usability, and user experience. © 2022, The Author(s).",Augmented reality; Cognitive load; Human–computer interaction; Nautical; Sailing; User study,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Differences in virtual and physical head orientation predict sickness during active head-mounted display-based virtual reality,VRS - Virtual Reality,B,"During head-mounted display (HMD)-based virtual reality (VR), head movements and motion-to-photon-based display lag generate differences in our virtual and physical head pose (referred to as DVP). We propose that large-amplitude, time-varying patterns of DVP serve as the primary trigger for cybersickness under such conditions. We test this hypothesis by measuring the sickness and estimating the DVP experienced under different levels of experimentally imposed display lag (ranging from 0 to 222 ms on top of the VR system’s ~ 4 ms baseline lag). On each trial, seated participants made continuous, oscillatory head rotations in yaw, pitch or roll while viewing a large virtual room with an Oculus Rift CV1 HMD (head movements were timed to a computer-generated metronome set at either 1.0 or 0.5 Hz). After the experiment, their head-tracking data were used to objectively estimate the DVP during each trial. The mean, peak, and standard deviation of these DVP data were then compared to the participant’s cybersickness ratings for that trial. Irrespective of the axis, or the speed, of the participant’s head movements, the severity of their cybersickness was found to increase with each of these three DVP summary measures. In line with our DVP hypothesis, cybersickness consistently increased with the amplitude and the variability of our participants’ DVP. DVP similarly predicted their conscious experiences during HMD VR—such as the strength of their feelings of spatial presence and their perception of the virtual scene’s stability. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Head-mounted display; Motion sickness; Motion-to-photon latency; Sensory conflict; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Seamless-walk: natural and comfortable virtual reality locomotion method with a high-resolution tactile sensor,VRS - Virtual Reality,B,"Efficient locomotion methods have been proposed to compensate for the limited space in real-world environments, and such methods offer users more immersive and natural experiences in relatively large virtual environments. The foot-based locomotion method is one of the best options for implementing natural locomotion using foot movement as an input. However, existing foot-based locomotion methods force users to wear equipment or take a video of the user’s body. These actions can cause discomfort, unnatural feelings, or privacy problems. Thus, we propose a Seamless-walk system that can seamlessly translate a real-world gait action to locomotion signals using a high-resolution tactile carpet sensor without requiring wearable equipment. The proposed method captures and analyzes high-resolution footprint information using a machine learning technique and calculates the user’s movement direction and speed in a real-time manner. In addition, the modular structure of Seamless-walk enables scalable installation of a tactile sensing platform at reasonable cost. Human tests (n = 80) confirmed that the proposed Seamless-walk system’s technical advantage increases usability. A 3D virtual world exploration game experiment revealed that the proposed method significantly increases comfort and overall naturalness. Additionally, The proposed method has no negative effects on exploration suitability, task load, simulator sickness, or the game experience. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Human computer interaction; Human-centered computing; Interaction paradigms; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2023,Optimized virtual reality-based Method of Loci memorization techniques through increased immersion and effective memory palace designs: a feasibility study,VRS - Virtual Reality,B,"For most, an improvement in memory would always be desirable, whether from the point of view of an aging individual with declining memory, or from the perspective of someone seeking to memorize large amounts of information in the shortest period of time. One way for people to improve upon their memory performance is by using the Method of Loci (MoL), a famously complex, ancient memorization technique for non-spatial information recall. With the use of virtual reality technology, this technique can finally be easily taught to individuals for use in their daily lives. In this paper, we present an exploration into this avenue of using MoL in virtual reality and report on the design and evaluation of our new virtual memory palace that aims to prove the feasibility of improving upon designs from other studies to optimize memory recall performance. An experiment was conducted to evaluate our VR MoL environment. The results from week 1 on the pre-test (M = 62.55, SD = 24.01) and post-test (M = 82.91, SD = 15.99) memory task showed an increase in the number of words remembered was statistically significant, t(20) = -2.34, p = 0.014 where participants were able to remember approximately 20.4% more non-spatial information, when compared to traditional memorization techniques. After a second use, participants improved, remembering 22.2% more non-spatial information on the pre-test (M = 63.44, SD = 26.64) and post-test (M = 85.67, SD = 16.10) memory task, indicating that the increase in number of words remembered was statistically significant, t(16) = -2.142, p = 0.024. The results suggest that the virtual memory palace experience could be optimized to help participants learn the MoL technique with very little training time and potentially produce significant improvements in recall performance as a result. © 2022, The Author(s).",Cognitive science; Improving memory recall; Memorization; Memory; Method of loci; Psychology; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Assessment of prospective memory after stroke utilizing virtual reality,VRS - Virtual Reality,B,"Prospective Memory (PM) is the ability to remember to do something in the future. It is often impaired after stroke and can impact on an individual’s level of independence and daily functioning. PM tasks have been criticized for their lack of ecological validity wherein test results may not be related to actual performance in daily life. With ecological validity in mind, the Virtual Reality Prospective Memory Shopping Task (VRPMST) was designed to assess two types of PM, time- and event-based. This study aimed to examine the ecological and convergent validity of the VRPMST in comparison to an experimental (Lexical Decision PM Task) and clinical measure of PM (Cambridge PM Test). Twelve individuals with stroke and 12 controls were administered three PM measures, three neuropsychological measures, and two user-friendliness questionnaires, one for the experimental PM measure and one for the VRPMST. Individuals with stroke showed impairments in PM compared to controls on all three PM measures, particularly time-based PM. Individuals with stroke were found to monitor time significantly less than controls on both the experimental PM measure and the VRPMST. The VRPMST was found to be sensitive in measuring PM, have better ecological validity when compared to the experimental PM measure, and good convergent validity. The findings of this study have helped to clarify that PM impairment does exist after stroke, possibly due to a problem in strategic monitoring. In addition, we have demonstrated how VR technology can be used to design a measure of cognitive function commonly impaired after stroke. © 2021, The Author(s).",Memory; Neuropsychological test; Psychometrics; Stroke; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Immersive virtual reality and passive haptic interfaces to improve procedural learning in a formal training course for first responders,VRS - Virtual Reality,B,"One key aspect for the safety and success of first responders’ operations is the compliance, during the intervention, with all the safety procedures and prescribed behaviors. Although real-world simulation exercises are considered as the best way to verify if operators are ready to handle emergency situations, they are not always a viable approach. Firefighting courses, for example, do not usually include this kind of activities, due to the numerous hazards related to deploying controlled fires for the simulation. However, traditional training approaches based on class lessons and multimedia learning material may not be particularly effective for teaching practical skills and procedural behaviors. In this work, the use of a Virtual Reality Training Simulation (VRTS) combined with passive haptic interfaces and a real-time fire simulation logic is investigated as a complement to a traditional video-based training approach used in the context of forest firefighting. The teaching of safety concepts and correct use of individual firefighting tools was selected as a use case, and a user study involving 45 trainees was carried out in the context of an existing training course. One third of the trainees attended the traditional video-based lessons of the course, whereas the remaining ones also took part to a practice training session, half of them with the devised VRTS, the others in the real world. Experimental results showed that the additional use of the devised VRTS improved the trainees’ procedural learning, as well as their motivation and perceived quality of the overall learning experience. © 2022, The Author(s).",Fire simulation; First responders; Forest firefighting; Passive haptics; Video-based training; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality applied to physiotherapy: a review of current knowledge,VRS - Virtual Reality,B,"Technological innovations have enabled physiotherapy to apply new possibilities in the rehabilitation of patients, especially in the use of virtual reality (VR). Although the literature provides several examples of VR applications, benefits, and barriers in physiotherapy, scholars obverse that there is still a dearth of studies that discuss and unify the results and impacts of this emerging technology on patients and physiotherapists. Thus, the aim of this study is to analyze the use of VR within physiotherapy and its impact on rehabilitation outcomes. A systematic literature review based on the PRISMA protocol was applied in this study. After searching on databases, such as Bireme, Cochrane, Emerald, Google Scholar, Lilacs, Medline, PEDro, PubMed, and Science Direct, we found 152 articles that complied with our protocol. The initial period of the search was open up to June 2020. Our results show an increased use of VR in neurology with elderly patients. We have identified underlying barriers (issues implementing VR, lack of protocols, and influence of patients) and benefits (effectiveness of treatment, motor development, and patient independence) of VR implementation. Finally, our study provides implications for VR in physiotherapy: a prominent increase in the use of VR in rehabilitation; value co-creation: interactions between patients and physiotherapists are crucial in the use of VR in physiotherapy; barriers related to technology, applicability, and the patient’s influence need to be overcome for VR practice to be used as a ‘business as usual’ modality in physiotherapy; the benefits of VR treatment can overcome the barriers faced by its use in rehabilitation. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Exergaming; Physiotherapy; Rehabilitation; Technology; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality application for real-time pedalling cadence estimation based on hip ROM tracking with inertial sensors: a pilot study,VRS - Virtual Reality,B,"Virtual reality (VR) applications on rehabilitation a home-base exercise experiences have boomed in the last decade. This is mainly because their entertainment capacity creates a sense of immersion in the users, which enhances adherence to their use. In addition, offering body-related visual feedback is a proven approach to the physical training towards a goal. Recent literature showed the exercise of pedalling has the potential to provide a high number of flexion and extension repetitions of the lower limb in reasonable therapeutic time periods to improve muscle activity, strength and balance in elders, but also motor improvements in patients with neurological injuries. The objective of this work is to present a low-cost wireless application in virtual reality (VR) for pedalling exercises. The platform developed consists of a VR headset and an inertial measurement unit (IMU). The VR headset processes the kinematic information of the IMU to estimate the cadence of the pedalling, while the IMU sensor tracks the angle of hip flexion/extension movement of the user. In order to confirm the suitability of this cadence estimation system, our approach is confronted with a cycling platform developed and validated in a previous study. In the present study, we carried out two repeated sessions with 13 subjects at 3 set speeds: slow (30 rpm), medium (60 rpm) and fast (90 rpm). The Spearman’s correlation (PC) between both systems for the 3 speeds and sessions shows high correlation values for low and medium speeds and moderate correlation for high speed. The SEM results for each system show low measurement error (about 1 cycle) for both systems at every target speed, except for the virtual cycling platform at the highest speed (SEM of VCP at 90 rpm = 3.24 cycles). The repeatability analysis based on ICC (3, 1) absolute agreement shows consistency in all measurements for both systems at high speed and also reflects the irregularity in measurements at low and medium speeds, where participants were less stable during testing due to entertainment from the VR system. All in all, it is concluded the validity of the cadence estimation system for pedalling exercises with low intensity. This development allows us to control the virtual environment by adapting the visual stimulus to cycling cadence. The proposed system can generate sensitive inputs to influence the user’s pedalling cadence. © 2022, The Author(s).",Cycling cadence; Feedback; Inertial unit sensor; Low-latency; Real-time tracking; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Cognitive load in immersive media settings: the role of spatial presence and cybersickness,VRS - Virtual Reality,B,"Faced with the ongoing diversification and commercial success of highly immersive media technologies (e.g., VR headsets), both content producers and scientific scholars have become highly invested in understanding the psychological consequences of experiencing media in these new and lifelike ways. While many studies underscore positive effects of high media immersivity—such as increased enjoyment or persuasive success—others warn about the intense cognitive load that technologies such as VR might put on their users. In a laboratory experiment with N = 121 participants, we compare the cognitive load experienced while watching a 360° video on a laptop screen or via an immersive VR head-mounted display. Furthermore, we scrutinize two prominent explanations for the additional cognitive load in immersive media settings, i.e., the role of spatial presence and cybersickness. As expected, the VR condition results in higher cognitive load, spatial presence, and cybersickness than the 2D condition. However, by means of a parallel mediation model, we observe that only cybersickness emerges as a meaningful mediator of participants’ strained cognitive capacity; spatial presence, on the other hand, remains statistically irrelevant in this regard. We discuss our findings considering implications for media producers and future research. © 2022, The Author(s).",Cognitive load; Cybersickness; Immersion; Immersive media; Spatial presence; Virtual reality,Keywords,True,
Scopus,journalPaper,2023,X-Board: an egocentric adaptive AR assistant for perception in indoor environments,VRS - Virtual Reality,B,"Augmented reality (AR) has the potential to become an effective assistive technology for emergencies in the future. However, raw AR content can confuse users’ visual perception and occlude information in the physical world. In this research, we propose X-Board, an X-ray visualization-based AR assistant for perception in indoor environments. In accordance with its design principles, X-Board provides visual–spatial cues by means of a grid mesh corresponding to the occluding surface in front of the target object. Meanwhile, the X-Board interacts with the physical world in real time, improving the coherence between the virtual and real worlds. To ensure the appropriate allocation of the user’s visual resources, the user’s visual intention is recognized based on gaze data to realize an adaptive display feature. The results of the user evaluation show that X-Board can effectively improve the accuracy and speed of the perception and reduce the cognitive load on users; thus, the usability of X-Board is confirmed. With X-Board, users could effectively perceive the spatial positions of their comrades in an indoor occluded environment in our simulated perception scenario. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Adaptive display; Augmented reality; Human–Computer interaction; Perception; Visual resources; X-ray visualization-based,Abstract_Keywords,True,
Scopus,journalPaper,2023,Using an immersive virtual reality bicycle simulator to evaluate hazard detection and anticipation of overt and covert traffic situations in young bicyclists,VRS - Virtual Reality,B,"Virtual reality (VR) offers an interactive and more engaging setting for studying hazard anticipation and anticipation in child cyclists who have a considerably higher risk for being involved in a traffic accident. Yet, reliability, validity and usability of VR for scientific purposes remain poorly documented. With the long-term goal in mind of developing fun and interactive training programmes, this study intends to test the effectiveness of a novel VR tool to promote cycling safety in children. A virtual traffic environment that reflects a typical Belgian city was created in Unity3D. Children (n = 130; 11.37 ± 0.54 years old; 92 girls and 38 boys), equipped with a HTC Vive, cycled on an instrumented bicycle through the virtual environment (VE) in which they had to negotiate 14 hazards in a safe manner. Participants’ speed and braking responses for the hazards were reported, while eye movements were recorded by means of the Pupil Labs eye tracking VR add-ons. Following the hazard anticipation test, children were questioned regarding perceived realism, simulator sickness and risky behaviours. After 4 weeks, 15 children performed the VR hazard anticipation test a second time. With respect to validity of the VR simulator, the results demonstrate that the overt hazards were fixated (p = 0.044) and braked for (p < 0.001) more, and fixated (p < 0.001) and braked for earlier (p < 0.001) compared to the covert hazards, which provides evidence for content validity of the VR simulator. The (weak) association between temperamental traits such as “errors and violations” and speed supports convergent validity. Furthermore, participants rated the VE as realistic to highly realistic which reflects face validity. Intraclass correlations suggest moderate test–retest reliability for all variables except first fixation rate. To improve ecological validity of hazard anticipation testing in cyclists, this study allowed the participants to cycle and fully interact with the VE. We found that the simulator provides a fun and realistic tool to document hazard anticipation skills in child bicyclists. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Bicycle simulator; Child cyclists; Hazard perception; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,User experiences of medical students with 360-degree virtual reality applications to prepare them for the clerkships,VRS - Virtual Reality,B,"For medical students, the transition from the preclinical to the clinical phase of their curriculum (clerkships) can result in increased levels of stress and anxiety. This is partly caused by low self-perception of preparedness. By using 360° video-based virtual reality it is possible to provide learners virtual access to clinical situations ahead of time. This technique can provide active and contextual user experiences and offers opportunities to demonstrate both behavioral skills and subject knowledge. We developed two 360° video-based virtual reality applications for medical students transitioning to the clerkships. In this study, we describe the development and evaluated the user experiences. Two virtual reality applications were developed for use in a small group learning session. One of the applications is an interactive virtual tour of a hospital ward, in which learners explore the Internal Medicine ward and learn about the roles of different health care professionals and their mutual interactions. In each room, the learners listen to a voice-over and look at hotspots to gather additional information. The other application has been developed to train students in observing (un)professional behavior of healthcare providers in their daily activities. An evaluation was performed by an anonymous explorative questionnaire with open and closed questions (Likert scales) regarding the user experience and cybersickness symptoms. In our study, 171 students used the applications and completed the questionnaire. For 63% of the respondents, this was their first experience with a virtual reality headset. Qualitative analysis showed that students evaluated the learning method as realistic, informative and enjoyable. Most students evaluated virtual reality as a good (59%) or excellent (26%) tool for learning. Forty-five percent of the students experienced physical discomfort, such as nausea, dizziness, headache and disorientation. In most cases, these complaints were mild, although a small number experienced severe nausea (n = 6) or severe headache (n = 2). Students suggested several areas of improvement including increase of display resolution and decrease of ambient noise causing distraction. 360° video-based virtual reality can successfully be implemented in the medical curriculum to create a realistic learning experience to prepare students for the clerkships. © 2023, The Author(s).",Active learning; Clerkships; Extended reality; Medical education; Technology enhanced learning; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Human interaction with virtual reality: investigating pre-evacuation efficiency in building emergency,VRS - Virtual Reality,B,"The current manuscript verifies the use of virtual reality (VR)-based methodology as a helpful way to study human behavior during the pre-evacuation period, considering the influence of pre-emergency activity (competitive tasks). Two conditions with different engagement levels (i.e., low and high) were set up, and sixty company workers were distributed across conditions randomly. Five types of evacuation behaviors were defined, and compliance behavior results showed most participants (66.7%) evacuated with the ISO-type evacuation alarm in low engagement condition, whereas only 20% of participants evacuated in high engagement situation. Statistical results confirmed the influence of pre-emergency activity on evacuation efficiency. Open-ended questions summarized three levels of knowledge background that justified the reasons/motivations behind pre-evacuation behaviors. simulator sickness, presence, and usability questionnaires confirmed the variable control between conditions. In summary, the VR-based methodology successfully reproduced evacuation behaviors similar to real situations, with the influence of pre-emergency activity. This study added a step to the efficacy of using VR as a tool to study human behavior during the pre-evacuation period and pointed out the need for the next generation of alarms, which will improve human safety in building emergencies. © 2022, The Author(s).",Emergency evacuation; Evacuation alarm; Virtual reality; VR-based evaluation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Moving from VR into AR using bio-cybernetic loops and physiological sensory devices for intervention on anxiety disorders,VRS - Virtual Reality,B,"Anxiety disorders comprise different clinical conditions that affect individuals in their personal, professional and social domains. The development of new intervention approaches for the treatment of anxiety disorders is crucial. As a step forward into promoting the well-being through adaptive physiological responses, we developed an augmented reality (AR)-based system using bio-cybernetic loops to create an adaptive system for exposure therapy in anxiety disorders. The system was built using open-source software (e.g., NyARToolkit and Unity 3D). AR technology uses computer-generated information to enrich the real world. It can be used with less intrusive devices to collect physiological data (e.g., Bitalino) describing human behavior in a cycle. In this context, our research project aims to study behavior during exposure to biologically relevant stimuli such as snakes. Phobia is described as an irrational fear to an object/stimulus. This fear triggers several physiological responses from sensors as increased heart rate (ECG) and skin conductance (EDA), which are responses from the autonomous nervous system. This approach can be used in several sessions, where the system through machine learning algorithms adapts the thresholds to the individual profile of each participant from historical data. Our study has been carried out in two stages: (1) The participants in a total of 35 students (30 males and 5 females with ages ranging from 19 to 29 years) were invited to fill a snake questionnaire (SNAQ). (2) A subsample was enrolled in an exposure session in AR using a virtual snake while collecting psychophysiological responses from sensors data. The results have shown increased physiological responses in two AR exposure sessions using snakes as stimuli. Therefore we conclude that the system was efficient to detect changes in physiological responses during the exposure sessions. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Adaptive systems; Augmented reality; Bitalino; NyARToolkit; Unity 3D,Abstract_Keywords,True,
Scopus,journalPaper,2023,How the physical appearance of companions affects females with high or low social physique anxiety: a virtual reality exercise study,VRS - Virtual Reality,B,"Technologies such as virtual reality (VR), an immersive computer-based environment that induces a feeling of mental and physical presence, are becoming increasingly popular for promoting participation in exercise. The purpose of this study was to explore changes in motivation and other psychological states when the physique of an exercise companion was altered during a VR-based exercise task, and whether trait social physique anxiety (SPA) altered these effects. Using a mixed experimental design, female participants (N = 43) categorised as high or low in SPA participated in two counterbalanced 10-min running tasks within a VR environment where the exercise companion was either overweight or in-shape. Across both running tasks, individuals with high SPA reported higher negative affect, pressure and tension, and lower perceived competencies, than those with low SPA. Pressure and tension were also higher when exercising with an in-shape companion than with an overweight companion for all participants. In addition, participants with high SPA reported a stronger preference to exercise with an overweight companion than those with low SPA in a real exercise setting, but not in a VR setting. The findings suggest that the physique of an exercise companion and the SPA of an exerciser have important, but independent, psychosocial effects during exercise. That an in-shape physique of a virtual exercise companion was not a deterrent among those with high SPA has provided preliminary evidence that VR-based exercise may be helpful among females who worry about their appearance or feel self-conscious while exercising. © 2022, The Author(s).",Exercise; Health promotion; Social physique anxiety; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Effects of physical walking on eyes-engaged target selection with ray-casting pointing in virtual reality,VRS - Virtual Reality,B,"Target selection in virtual reality (VR) is usually carried out with the need of visual attention. While target selection in VR has been extensively investigated in non-walking activities (e.g., sitting or standing), there have been few studies about eyes-engaged target selection during walking in virtual environments. Therefore, we conducted a comprehensive study to explore the effects of physical walking (as an independent variable with low, medium and high speeds) on eyes-engaged selection tasks with targets (three target sizes and three target depths) in two experiments: targets fixed in the virtual environment (Experiment One) and targets fixed to the virtual body (Experiment Two), respectively. Results showed that for Experiment One, the low walking speed led to the significantly longest task completion time, while the medium and high speeds had similar task completion time. For Experiment Two, higher walking speed led to longer task completion time. In both tasks, error rate significantly increased as walking speed increased. The effects of walking speed also varied across target size and target depth. We conclude our study with a set of design implications for target selection tasks when walking in VR environments. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Eyes-engaged interaction; Physical walking; Ray-casting selection; Target selection; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Teleporting through virtual environments: benefits of navigational feedback and practice,VRS - Virtual Reality,B,"Virtual environments (VEs) can be infinitely large, but movement of the virtual reality (VR) user is constrained by the surrounding real environment. Teleporting has become a popular locomotion interface to allow complete exploration of the VE. To teleport, the user selects the intended position (and sometimes orientation) before being instantly transported to that location. However, locomotion interfaces such as teleporting can cause disorientation. This experiment explored whether practice and feedback when using the teleporting interface can reduce disorientation. VR headset owners participated remotely. On each trial of a triangle completion task, the participant traveled along two path legs through a VE before attempting to point to the path origin. Travel was completed with one of two teleporting interfaces that differed in the availability of rotational self-motion cues. Participants in the feedback condition received feedback about their pointing accuracy. For both teleporting interfaces tested, feedback caused significant improvement in pointing performance, and practice alone caused only marginal improvement. These results suggest that disorientation in VR can be reduced through feedback-based training. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Feedback; Locomotion interface; Navigation; Remote data collection; Teleporting; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,User-centered redirected walking and resetting with virtual feelers,VRS - Virtual Reality,B,"With redirected walking (RDW), the exploration of an infinite virtual world with a small physical space has been enabled. This paper proposes a user-centered RDW (UC-RDW) and user-centered resetting (UC-R) that incorporates virtual feelers. This method performs redirection and resetting by only considering a user and obstacles. This means that the UC-RDW and UC-R perform redirection without complex prediction/optimization. If the virtual feelers collide with obstacle, the method redirects the user in the opposite direction from the obstacle. The following simulations obtained one condition (diamond-shaped with area ratio of 0.9, 1.0, and 1.2, angle between feelers of 10, and a length ratio of 0.5 between the major and minor axes of the diamond shape) that yields the best performance under various ray conditions. Then, the performance of the UC-RDW and UC-R were verified through live user experiments. Consequently, a lesser 10-m reset number, lesser increase in total simulator-sickness score, and shorter time to walk 10 m were obtained with the UC-RDW and UC-R. Furthermore, participants who were redirected through the UC-RDW and UC-R applied a larger redirection than other methods and stayed closer to the center of the tracked space than those using other methods. In addition, UC-RDW showed comparable performance to APF-RDW in medium and large-tracked space. Future works include verification of the UC-RDW and UC-R using a tracked space of irregular shape and multiple users. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Redirected walking; Resetting; Virtual feeler; Virtual reality,Keywords,True,
Scopus,journalPaper,2023,"Editorial: Virtual reality for therapy, psychological interventions, and physical and cognitive rehabilitation",VRS - Virtual Reality,B,[No abstract available],,Title,True,
Scopus,journalPaper,2023,Effects of enactment in virtual reality: a comparative experiment on memory for action,VRS - Virtual Reality,B,"Virtual reality (VR) is thought of as a promising educational medium, especially for learning actions, as it enables learning by enactment. Learning by enactment is associated with the enactment effect which describes a superior memory for enacted actions compared to actions which have not been enacted. To date, however, little is known about whether the enactment effect across different conditions of action learning can be found in VR which sets the stage for our first research question. Additionally, as a second research question, this study explores the extent to which the memory performance of learning by enactment in VR corresponds to learning by enactment in physical reality. We conducted a VR between subjects experiment with four groups (N = 112) that differed in terms of condition or environment. Participants were asked to remember short action phrases for a subsequent memory test. The results indicate that learning by enactment in VR outperforms learning by reading in VR but does not exceed observational learning in VR. Furthermore, the results demonstrate that the memory performance of learning by enactment in VR is similar to that in physical reality. These findings are highly relevant as they demonstrate the potential of VR as a new educational medium supporting learning by enactment. © 2022, The Author(s).",Actions; Enactment effect; Experimental design; Learning by enactment; Learning medium; Memory performance,Title_Abstract,True,
Scopus,journalPaper,2023,Buddy biking: a user study on social collaboration in a virtual reality exergame for rehabilitation,VRS - Virtual Reality,B,"Virtual reality (VR)-based rehabilitation is a growing technological field, which gradually becomes integrated into existing programs. However, technology has to support human behavior and -needs, including social relatedness, to achieve health-related outcomes. Elderly people have high risk of loneliness, and VR has technological affinity for natural social interaction. Previous studies have relied on competitiveness rather than collaborative elements, but research shows that competitiveness can lead to (feelings of) stress and aggressive behavior in some individuals. This article presents a mixed methods study to gather end-user feedback on a social VR scenario that encourages inter-player collaboration on a virtual tandem bike. Outpatients (n= 11 , 64% males, 60 ± 11 years) were invited to participate with a co-player (friend or family). Participants biked on average 10.7 (± 3) minutes with a mean speed of 14.8 kmph (± 5.8). The results indicate potential and feasibility for the collaborative social biking application. Participants reported excellent usability-scores (85 ± 5), high intrinsic motivation in all categories: enjoyment (6.5 ± 0.5), effort/importance (6.4 ± 0.3), relatedness (6.3 ± 0.7) and minimal increase in symptoms of nausea, oculomotor and disorientation. Furthermore, participants found the social aspect enjoyable, agreed that collaboration eased tasks and that they lost track of exercise duration. Interpersonal interaction between participants varied, but was mostly positively rated valence, even if the sense of copresence was limited by physical constraints and avatar representation. Most participants expressed that they would use the program again, but future studies should explore how to improve location and appearance of the virtual coactor, as well as implement additional tasks. © 2021, The Author(s).",Exergaming; Motivation; Older adults; Physical therapy; Social interaction; User-centered design; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Correction to: Human interaction with virtual reality: investigating pre-evacuation efficiency in building emergency (Virtual Reality, (2022), 10.1007/s10055-022-00710-x)",VRS - Virtual Reality,B,"In the original publication, the order of author affiliations was published wrongly. The correct order of affiliation is given below. This has been updated in the original publication. © 2022, Springer-Verlag London Ltd., part of Springer Nature.",,Title,True,
Scopus,journalPaper,2023,Can you hear it? Stereo sound-assisted guidance in augmented reality assembly,VRS - Virtual Reality,B,"Most augmented reality (AR) assembly guidance systems only utilize visual information. Regarding the sound, the human binaural effect helps users quickly identify the general direction of sound sources. At the same time, pleasant sounds can give people a sense of pleasure and relaxation. However, the effect on workers is still unknown when stereo sound and visual information are used together for assembly guidance. To assess the combination of sound and vision in AR assembly guidance, we constructed a stereo sound-assisted guidance system (SAG) based on AR. In our SAG system, we used the tone of a soft instrument called the Chinese lute as the sound source. To determine if SAG has an impact on assembly efficiency and user experience, we conducted a usability test to compare SAG with visual information alone. Results showed that the SAG system significantly improves the efficiency of assembly guidance. Moreover, simultaneous visual and auditory information processing does not increase user workload or learning difficulty. Additionally, in a noisy environment, pleasant sounds help to reduce mental strain. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Assembly guidance; Augmented reality; Multichannel human–computer interaction; Stereo sound,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Virtual reality and neuropsychological assessment: an analysis of human factors influencing performance and perceived mental effort,VRS - Virtual Reality,B,"This study aimed to compare a neuropsychological test tapping into executive control function, the Wisconsin Card Sorting Test (WCST), performed in either traditional paper-and-pencil (PP) or virtual reality (VR) modality, and to determine the role of human factors (i.e., sense of presence, cybersickness, field (in)dependence and video game experience) as contributors to performance and perceived mental effort. Indeed, if virtual assessment might bring the ecological dimension to controlled laboratory research, it is often suggested that human factors might bias performance. WCST performance and its associated perceived mental effort were compared between the two modalities (N = 107). In the VR modality (N = 52), a correlation matrix was conducted as well as a cluster analysis in order to build two experimental groups, or profiles, based on their subjective experience of VR. WCST performance and perceived mental effort were then compared between these two groups while controlling for age and education. Results outlined a similar WCST performance and perceived mental effort between the PP and VR modalities. However, when comparing the two VR groups, results suggest that an unfavorable profile for VR, i.e., less sense of presence, more cybersickness, more visual field dependence and less video game experience, is associated with greater perceived mental effort. These experimental findings enable outlining a new conceptual and methodological framework for the assessment of executive control task performance in VR. Results could help users to take human factors into consideration in order to fully exploit or predict the benefits of this tool. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Executive control of attention; Field dependence–independence; Sense of presence; Video game experience,Title_Abstract,True,
Scopus,journalPaper,2023,Development of a customizable interactions questionnaire (CIQ) for evaluating interactions with objects in augmented/virtual reality,VRS - Virtual Reality,B,"As new methods for interacting with systems are being developed for use within augmented or virtual reality, their impact on the quality of the user’s experience needs to be assessed. Although many instruments exist for evaluating the overall user experience or the computer interface used to complete tasks, few provide measures that can be used to evaluate the specific forms of interaction typically used in these environments. This paper describes the development of a customizable questionnaire for measuring the subjective user experience that focuses on the quality of the interactions with objects in augmented reality/virtual reality (AR/VR) worlds, which we are calling the Customizable Interactions Questionnaire, or (CIQ). The final questionnaire measures five factors that are related to user satisfaction while using the system: quality of interactions, assessment of task performance, comfort, quality of sensory enhancements, and consistency with expectations. © 2022, The Author(s).",Metrics; Questionnaire; Usability testing; User studies,Title_Abstract,True,
Scopus,journalPaper,2023,Aesthetic judgments of 3D arts in virtual reality and online settings,VRS - Virtual Reality,B,"Empirical aesthetics is beginning to branch off from conventional laboratory-based studies, leading to in-situ, immersive, often more accessible experiments. Here, we explored different types of aesthetic judgments of three-dimensional artworks in two contexts: virtual reality (VR), aiming for an immersive experience, and online settings aiming for an accessible setup for a remote audience. Following the pilot experiment conducted to select a set of 3D artworks, in the first experiment, participants freely engaged with virtual artworks via an eye-tracking-enabled VR headset and provided evaluations based on subjective measures of aesthetic experience such as ratings on liking, novelty, complexity, perceived viewing duration; and the objective viewing duration was also recorded. Results showed positive, linear, and mostly moderate correlations between liking and the other perceived judgment attributes. Supplementary eye-tracking data showed a range of viewing strategies and variation in viewing durations between participants and artworks. Results of the second experiment, adapted as a short online follow-up, showed converging evidence on correlations between the different aspects contributing to aesthetic judgments and suggested similarity of judgment strategies across contexts. In both settings, participants provided further insights via exit questionnaires. We speculate that both VR and online settings offer ecologically valid experimental contexts, create immersive visual arts experience, and enhance accessibility to cultural heritage. © 2022, The Author(s).",Aesthetic judgment; Art appreciation; Empirical aesthetics; Eye-tracking; Online experiment; Virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Exploring the future building: representational effects on projecting oneself into the future office space,VRS - Virtual Reality,B,"While virtual reality (VR) has been explored in the field of architecture, its implications on people who experience their future office space in such a way has not been extensively studied. In this explorative study, we are interested in how VR and other representation methods support users in projecting themselves into their future office space and how this might influence their willingness to relocate. In order to compare VR with other representations, we used (i) standard paper based floor plans and renders of the future building (as used by architects to present their creations to stakeholders), (ii) a highly-detailed virtual environment of the same building experienced on a computer monitor (desktop condition), and (iii) the same environment experienced on a head mounted display (VR condition). Participants were randomly assigned to conditions and were instructed to freely explore their representation method for up to 15 min without any restrictions or tasks given. The results show, that compared to other representation methods, VR significantly differed for the sense of presence, user experience and engagement, and that these measures are correlated for this condition only. In virtual environments, users were observed looking at the views through the windows, spent time on terraces between trees, explored the surroundings, and even “took a walk” to work. Nevertheless, the results show that representation method influences the exploration of the future building as users in VR spent significantly more time exploring the environment, and provided more positive comments about the building compared to users in either desktop or paper conditions. We show that VR representation used in our explorative study increased users’ capability to imagine future scenarios involving their future office spaces, better supported them in projecting themselves into these spaces, and positively affected their attitude towards relocating. © 2022, The Author(s).",Immersive VR environments; Job relocation; Sense of presence; User engagement; User experience,Abstract,True,
Scopus,journalPaper,2023,The influence of attentional engagement and spatial characteristics on time perception in virtual reality,VRS - Virtual Reality,B,"Virtual reality (VR) is a simulation tool that is being used extensively to study the effects of training and perception. However, several studies have shown that some aspects of perception within VR are not always accurate. The present study investigates the perception of time within a VR environment by asking for retrospective time judgments of the length of VR experiences. These environments varied in both the level of interaction with the VR environment and also the spatial qualities of the environment itself. The judged length of time did not significantly differ between conditions based on the level of activity in the environment, but the spatial characteristics of the VR environment did produce significantly different time estimations. This finding suggests that careful attention should be paid to what and how users are trained or evaluated in VR. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Attention; Spatial environment; Time perception; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Design and evaluation of a high− fidelity virtual reality manufacturing planning system,VRS - Virtual Reality,B,"Manufacturing applications of virtual reality (VR) technology are growing. The challenge is to design, integrate, and evaluate VR simulation for manufacturing Systems that improves the effectiveness of the planning process. In this paper, we discuss the technical infrastructure necessary to design a collaborative virtual manufacturing planning system. We describe the VR system setup and the integration of hardware and software to produce high-fidelity virtual simulation for manufacturing planning purposes. The designing guidelines are demonstrated by a high-fidelity VR simulation of a stamping process. The VR simulation also presents a method of visualizing computer-aided engineering content. Also, this paper assesses the factors that affect overall fidelity of the VR simulation. Objective evaluation of the VR simulation was conducted using the fidelity framework and the scales, whereas the subjective evaluation methods used were VR-simulation-driven data interpretation. The VR simulation was evaluated by a selective sample of 33 senior engineering students using a highly reliable scale (Cronbach’s Alpha =.93) questionnaire that was designed to evaluate functionality, performance, and experience. The results of the subjective evaluation validate the evaluation of objective scales to be high-medium for the VR system used (M = 5.24, M = 5.11) respectively. Significant positive relationships were found between all factors, except distraction, which had a significant negative relationship with fidelity. Overall, the realism and sensory systems factors were found to be the main significant factors affecting the fidelity of the VR system. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Digital twin; Fidelity; Manufacturing planning; Simulation; Virtual prototyping; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Step on it: asymmetric gain functions improve starting and stopping in virtual reality walking,VRS - Virtual Reality,B,"Transfer functions with a high translational gain can increase the range of walking in virtual reality. These functions determine how much virtual movements are amplified compared to the corresponding physical movements. However, it is unclear how the design of these functions influences the user’s gait and experience when walking with high gain values. In a mixed-methods study with 20 users, we find that their best transfer functions are nonlinear and asymmetrical for starting and stopping. We use an optimization approach to determine individually optimized functions that are significantly better than a common approach of using a constant gain. Based on interviews, we also discuss what qualities of walking matter to users and how these vary across different functions. Our work shows that it is possible to create high-gain walking techniques that offer dramatically increased range of motion and speed but still feel like normal walking. © 2022, The Author(s).",Acceleration; Gain; Locomotion; Non-isometric; Stopping; Transfer function; Virtual reality; Walking,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Affective experience in a virtual crowd regulates perceived travel time,VRS - Virtual Reality,B,"Time sometimes feels like it is flying by or slowing down. Previous research indicates objective number of items, subjective affect, and heart rate all can influence the experience of time. While these factors are usually tested in isolation with simple stimuli in the laboratory, here we examined them together in the ecological context of a virtual subway ride. We hypothesized that subjective affective experience associated with objective crowding lengthens subjective trip duration. Participants (N = 41) experienced short (1–2 min) immersive virtual reality subway trips with different levels of public crowding. Consistent with the immersive nature of decreased interpersonal virtual space, increased crowding decreased pleasantness and increased the unpleasantness of a trip. Virtual crowding also lengthened perceived trip duration. The presence of one additional person per square meter of the train significantly increased perceived travel time by an average of 1.8 s. Degree of pleasant relative to unpleasant affect mediated why crowded trips felt longer. Independently of crowding and affect, heart rate changes were related to experienced trip time. These results demonstrate socioemotional regulation of the experience of time and that effects of social crowding on perception and affect can be reliably created during a solitary virtual experience. This study demonstrates a novel use of Virtual Reality technology for testing psychological theories in ecologically valid and highly controlled settings. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Crowding; Emotion; Heart rate; Time perception; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Soundspace VR: spatial navigation using sound in virtual reality,VRS - Virtual Reality,B,"Prior research reveals that spatial navigation skills rely mostly in visual sensory abilities, but the study of how spatial processing operates in the absence of visual information is still incomplete. Therefore, a spatial navigation task in virtual reality using auditory cues was developed to study navigational strategies in blindfolded sighted individuals. Twenty healthy adult participants were recruited. The task consisted of a VR scene, in which participants were asked to localize a sound source and move to the target without visual information throughout the entire task. Task difficulty was manipulated by route length and complexity in three different difficulty levels repeated in two different trials. The first trial (learning) consisted of moving to the sound source and then returning to the starting point. The second trial (retrieval) consisted of the same task without the sound source but with auditory cues from obstacles to test spatial learning. Performance was assessed from behavioral measures of execution time, obstacle collisions, and prompts during the task execution. These variables were compared to established neuropsychological instruments for global cognition (Montreal Cognitive Assessment) and memory abilities (Wechsler Memory Scale-R). The results suggested that difficulty level affected navigation performance in both trials. Navigation performance was better in the retrieval trial, but both learning and retrieval trials were explained by global cognitive functioning. These data suggested the Soundspace VR as being effective to study spatial navigation in the absence of visual information and highlight the importance of auditory information from spatial sound cues for spatial navigation and spatial learning. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Sound cues; Spatial memory; Spatial navigation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Efficacy of adaptive cognitive training through desktop virtual reality and paper-and-pencil in the treatment of mental and behavioral disorders,VRS - Virtual Reality,B,"Cognitive deficits are a core feature of mental and behavioral disorders, leading to poor treatment adherence and functionality. Virtual reality (VR) methodologies are promising solutions for cognitive interventions in psychiatry once they provide greater ecological validity. This study assessed and compared two content-equivalent cognitive training (CT) interventions, delivered in desktop VR (Reh@City v2.0) and paper-and-pencil (Task Generator (TG)) formats, in patients with mental and behavioral disorders. 30 patients were randomly assigned to the Reh@City v2.0 group and the TG group. Both groups of patients underwent a time-matched 24-sessions intervention. Neuropsychological assessments were performed at baseline, post-intervention, and follow-up. A within-groups analysis revealed significant improvements in visual memory and depressive symptomatology after the Reh@City intervention. The TG group improved in processing speed, verbal memory, and quality of life (social relationships and environmental domains). Between groups, Reh@City led to a greater reduction in depressive symptomatology, whereas the TG group showed higher improvements in social relationships aspects of quality of life. At follow-up, previous gains were maintained and new improvements found in the Reh@City (global cognitive function, language, visuospatial and executive functions) and the TG groups (attention). The Reh@City significantly reduced depressive symptomatology, and the TG led to greater improvements in processing speed, abstraction, and social relationships domain of quality of life at follow-up. Both interventions were associated with important cognitive, emotional, and quality of life benefits, which were maintained after two months. Reh@City and TG should be considered as complementary CT methods for patients with mental and behavioral disorders. Trial registration The trial is registered at ClinicalTrials.gov, number NCT04291586. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cognitive training; Desktop virtual reality; Ecological validity; Mental and behavioral disorders; Psychiatric setting,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Psychological predictors of consumer-level virtual reality technology adoption and usage,VRS - Virtual Reality,B,"In recent years, virtual reality (VR) technology has been mainstreamed for at-home use, with various consumer-oriented devices released by media firms such as Meta, Google, Samsung, and HTC. The present research investigates the role of psychological traits—including immersive tendencies, absorption, sensation seeking, need for cognition, neophobia, and belief in science—as well as trait levels of individual innovativeness, self-perception of social well-being, and owner demographics, in predicting VR adoption rates and sustained use over time. Separate analyses were conducted for different classes of VR device (fixed, mobile, and standalone devices). In general, psychological factors generally emerged as more determinative of adoption than did demographics. Users’ immersive tendencies predicted earlier adoption of VR technology while absorption was associated with later adoption, with both predictive of higher overall initial usage of different types of devices. Additionally, perceiving oneself as socially successful was associated with higher initial VR usage, while a tendency to see one’s emotions as influenced by in-person rather than online contacts was negatively associated with usage. Finally, belief in science predicted greater consistency in usage over time while higher levels of absorption were associated with unstable usage patterns. These findings expand upon the limited work previously investigating the role of individual differences in adoption of VR and mark the promise of psychometrics for understanding the diffusion and continued usage of consumer-facing VR devices. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Absorption; Adoption; Immersion; Individual differences; Uses and gratifications; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A hybrid 2D–3D tangible interface combining a smartphone and controller for virtual reality,VRS - Virtual Reality,B,"Virtual reality (VR) controllers are widely used for 3D virtual object selection and manipulation in immersive virtual worlds, while touchscreen-based devices like smartphones or tablets provide precise 2D tangible input. However, VR controllers and touchscreens are used separately in most cases. This research physically integrates a VR controller and a smartphone to create a hybrid 2D–3D tangible interface for VR interactions, combining the strength of both devices. The hybrid interface inherits physical buttons, 3D tracking, and spatial input from the VR controller while having tangible feedback, 2D precise input, and content display from the smartphone’s touchscreen. We review the capabilities of VR controllers and smartphones to summarize design principles and then present a design space with nine typical interaction paradigms for the hybrid interface. We developed an interactive prototype and three application modes to demonstrate the combination of individual interaction paradigms in various VR scenarios. We conducted a formal user study through a guided walkthrough to evaluate the usability of the hybrid interface. The results were positive, with participants reporting above-average usability and rating the system as excellent on four out of six user experience questionnaire scales. We also described two use cases to demonstrate the potential of the hybrid interface. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Controller; Hybrid interface; Interaction paradigm; Smartphone; Tangible interface; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"A narrative review of immersive virtual reality’s ergonomics and risks at the workplace: cybersickness, visual fatigue, muscular fatigue, acute stress, and mental overload",VRS - Virtual Reality,B,"This narrative review synthesizes and introduces 386 previous works about virtual reality-induced symptoms and effects by focusing on cybersickness, visual fatigue, muscle fatigue, acute stress, and mental overload. Usually, these VRISE are treated independently in the literature, although virtual reality is increasingly considered an option to replace PCs at the workplace, which encourages us to consider them all at once. We emphasize the context of office-like tasks in VR, gathering 57 articles meeting our inclusion/exclusion criteria. Cybersickness symptoms, influenced by fifty factors, could prevent workers from using VR. It is studied but requires more research to reach a theoretical consensus. VR can lead to more visual fatigue than other screen uses, influenced by fifteen factors, mainly due to vergence-accommodation conflicts. This side effect requires more testing and clarification on how it differs from cybersickness. VR can provoke muscle fatigue and musculoskeletal discomfort, influenced by fifteen factors, depending on tasks and interactions. VR could lead to acute stress due to technostress, task difficulty, time pressure, and public speaking. VR also potentially leads to mental overload, mainly due to task load, time pressure, and intrinsically due interaction and interface of the virtual environment. We propose a research agenda to tackle VR ergonomics and risks issues at the workplace. © 2022, The Author(s).",Cybersickness; Ergonomics; Mental overload; Muscle fatigue; Stress; Virtual reality; Visual fatigue; Work,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Correction to: Eye tracking in virtual reality: a broad review of applications and challenges (Virtual Reality, (2023), 27, 2, (1481-1505), 10.1007/s10055-022-00738-z)",VRS - Virtual Reality,B,"In the original article, Figure 1 was published without part figure. The correct Fig. 1 is given below. The original article has been corrected. (Figure presented.) An HTC Vive Pro Eye HR HMD with integrated VOG-based eye trackers (left). A VR HMD with scleral search coil-based eye tracking system (right). Image courtesy of Whitmire et al. (2016) © 2023, The Author(s).",,Title,True,
Scopus,journalPaper,2023,Playing your pain away: designing a virtual reality physical therapy for children with upper limb motor impairment,VRS - Virtual Reality,B,"Children with upper limb motor impairment often undergo repetitive therapeutic physiotherapy sessions to minimize functional disabilities of the affected area. Even though therapeutic processes can improve functional outcomes and minimize persistent disabilities, patients often neglect to participate fully in physical therapies due to the associated procedural pain. Over recent decades, there has been a growing interest in designing non-pharmacological interventions which aim to minimize pain during physical therapies and improve functional outcomes. Via two interrelated studies, we explored the use of virtual reality (VR) as a tool to provide therapeutic physiotherapy for child patients in an out-patient hospital department. We found that VR is an effective solution for children with upper limb motor impairment undergoing painful therapeutic process within a hospital environment. VR can improve functional disabilities, alleviate perceived pain, reduce the perceived difficulty of rehabilitation exercises, increase exercise duration and produce positive emotions towards the therapy. Trial registration number and date of registration Protocol ID NCT03998995. Release Date: June 25, 2019. © 2021, The Author(s).",Children’s rehabilitation; Pain management; Patient-centred design; Upper limb motor impairment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Design and calibration of curved and see-through integral imaging 3D display,VRS - Virtual Reality,B,"Heads-up displays that are ‘see-through’ and ‘curved’ and capable of displaying 3D contents are considered crucial for augmented reality-based navigation in automobiles. Here we report the development, calibration and experimental evaluation of a 3D display system that satisfies the above requirements. Integral imaging is used as the 3D display technique, which is realized using a flexible ‘concave-micro-mirror array’ screen (equivalent of a ‘micro-lens array’, but working in reflection mode). The screen itself is fabricated as a holographic optical element. The holographic nature of the screen enables a ‘see-through’ effect. The 3D content to be displayed is served by a 2D projector as integral images. A novel calibration method is developed which employs diffusive markers, that are invisible to the naked eye, being placed at one corner of each elemental micro-mirror. The calibration enables proper treatment of the effects and artifacts caused by screen ‘curvature’, but the presence of markers itself does not degrade the display characteristics. A curved micro-mirror array screen of size 10 cm × 10 cm consisting of 100 × 200 elemental concave mirrors is fabricated as a flexible holographic optical element with diffusive markers of size 300 μ m × 300 μ m. The screen, when illuminated with a projector (that serves integral images), was able to reconstruct a 3D scene of size 10 cm × 10 cm with a depth of 5 cm. The novel calibration method employing diffusive markers demonstrates significant improvement in calibration accuracy. The curved and see-through nature of the display screen makes it a good choice for windshield displays. The reported system requires further improvements in enlarging the screen size and increasing depth of the 3D scene in order to meet real-world requirements, which can be achieved by scaling-up the system. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Calibration; Curved light field display; Holographic micro-mirror array; Integral imaging,Abstract,True,
Scopus,journalPaper,2023,Assessing perceptions to a virtual reality intervention to improve trunk control in Parkinson’s disease: a preliminary study,VRS - Virtual Reality,B,"Trunk control and postural instability remain critical markers of functional status in Parkinson’s Disease (PD). As pharmacological and invasive neuro-stimulation interventions provide only limited benefits for trunk and postural control, exercise-based interventions may provide the only effective path to improving functional outcomes for balance in PD. We describe the framework for a virtual reality (VR) graded exercise-based intervention focused on improving trunk mobility and control, including preliminary outcomes on perceptions and motion capabilities of individuals with PD. The study collected whole-body motion capture from 11 PD participants (8M, 3F; H&Y Stage I–III) as they performed tasks within a custom-designed set of VR therapies. Therapies involved static interactions (e.g., matching a cube sequence set to anthropometrically relevant locations to elicit specific trunk motions—Matchality), and more dynamic tasks (e.g., intercepting virtual fish jumping from a lake—Fishality, or a virtual session of dodgeball—Dodgeality). Participants were able to safely complete all tasks while performing trunk excursions requiring functionally relevant ranges of motion, and which altered across VR environment (F(1,10) = 8.319, p = 0.016). Overall, satisfaction with the MoVR therapy suite was high with 96% of responses showing agreement to questions relating to ‘immersion,’ ‘fun,’ and elements of player engagement. These results provide early safety, usability, and feasibility outcomes for VR interventions that require specific trunk excursions. Future work should confirm the effectiveness of VR interventions longitudinally on PD-specific trunk outcomes (e.g., trunk rigidity) and global balance/mobility measures associated with improved functional status. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Game development; Immersion; Parkinson’s disease; Posture; Trunk control; Virtual rehabilitation,Title_Abstract,True,
Scopus,journalPaper,2023,The use of augmented reality in the teaching and training of basic exercises involved in the non-surgical treatment of neck pain,VRS - Virtual Reality,B,"In this study, a new holographic supervision method was developed that enables patients to correctly apply the exercise movements associated with the non-surgical treatment of neck pain. The proposed method uses augmented reality technology to teach patients the correct exercise movements and supervise them, ensuring that they perform these exercises correctly. Previous studies have shown that patients generally do not adhere to prescribed treatments or may not perform the required exercises correctly. Thus, this system was developed to ease the workload of physical therapists and medical doctors. It uses a head-mounted device named HoloLens as well as some markers. Exercise movements are first taught to patients using holographic videos, guiding holograms, and sounds. The movements of the patients are subsequently observed using the sensors available on the HoloLens as well as markers placed around the patient to ensure that patients maintain the correct angle, duration, and the number of repetitions during the exercises. A methodology that employed finite-state machines was used to develop this software. To quantify the benefits of this system, tests were conducted on a total of 30 participants that were split equally into a control group and an experimental group. The duration, repetition, and angle error rates in the control group (who did not use the device) were 36.4%, 8%, and 27.3%, respectively; the corresponding error rates in the experiment group (who were provided with the device) were 0%, 0%, and 9.1%. Thus, a system has been developed that can address the issue of patients who do not adhere to treatment programs or apply them incorrectly. This study represents the first time that the exercises for the treatment of neck pain were taught and supervised with holograms instead of physical therapists. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",AR; Augmented reality; Medical training system; Mixed reality; Neck pain; Virtual rehabilitation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Co-design of avatars to embody auditory hallucinations of patients with schizophrenia: A study on patients’ feeling of satisfaction and psychiatrists’ intention to adopt the technology,VRS - Virtual Reality,B,"Auditory hallucinations are common and distressing symptoms of the schizophrenia disease. It is commonly treated with pharmacological approaches but, unfortunately, such an approach is not effective in all patients. In the cases in which the use of antipsychotic drugs is not possible or not recommended, psychotherapeutic interventions are used to help patients gain power and control against hearing voices. Recently, virtual reality technologies have been incorporated to this type of therapies. A virtual representation of their voice (avatar) is created in a controlled computer-based environment, and the patient is encouraged to confront it. Unfortunately, the software tools used in these therapies are not described in depth and, even more important, to the best of our knowledge, their usability, utility and intention to use by therapists, and patients have not been evaluated enough. The involvement of end users in the software development is beneficial in obtaining useful and usable tools. Hence, the two contributions of this paper are (1) the description of an avatar creation system and the main technical details of the configuration of auditory hallucination avatars, and (2) its evaluation from both the therapists’ and the patients’ viewpoints. The evaluation does not only focus on usability, but also assesses the acceptance of the technology as an important indicator of the future use of a new technological tool. Moreover, the most important results, the lessons learned and the main limitations of our study are discussed. © 2021, The Author(s).",Avatar-based therapy; Schizophrenia; Technology acceptance; Usability; User evaluation,Abstract,True,
Scopus,journalPaper,2023,"Effect of immersive visualization technologies on cognitive load, motivation, usability, and embodiment",VRS - Virtual Reality,B,"Virtual reality (VR) is a promising tool to promote motor (re)learning in healthy users and brain-injured patients. However, in current VR-based motor training, movements of the users performed in a three-dimensional space are usually visualized on computer screens, televisions, or projection systems, which lack depth cues (2D screen), and thus, display information using only monocular depth cues. The reduced depth cues and the visuospatial transformation from the movements performed in a three-dimensional space to their two-dimensional indirect visualization on the 2D screen may add cognitive load, reducing VR usability, especially in users suffering from cognitive impairments. These 2D screens might further reduce the learning outcomes if they limit users’ motivation and embodiment, factors previously associated with better motor performance. The goal of this study was to evaluate the potential benefits of more immersive technologies using head-mounted displays (HMDs). As a first step towards potential clinical implementation, we ran an experiment with 20 healthy participants who simultaneously performed a 3D motor reaching and a cognitive counting task using: (1) (immersive) VR (IVR) HMD, (2) augmented reality (AR) HMD, and (3) computer screen (2D screen). In a previous analysis, we reported improved movement quality when movements were visualized with IVR than with a 2D screen. Here, we present results from the analysis of questionnaires to evaluate whether the visualization technology impacted users’ cognitive load, motivation, technology usability, and embodiment. Reports on cognitive load did not differ across visualization technologies. However, IVR was more motivating and usable than AR and the 2D screen. Both IVR and AR rea ched higher embodiment level than the 2D screen. Our results support our previous finding that IVR HMDs seem to be more suitable than the common 2D screens employed in VR-based therapy when training 3D movements. For AR, it is still unknown whether the absence of benefit over the 2D screen is due to the visualization technology per se or to technical limitations specific to the device. © 2021, The Author(s).",Augmented Reality; Cognitive Load; Embodiment; Immersive Virtual Reality; Motivation; Usability,Abstract_Keywords,True,
Scopus,journalPaper,2023,Clinical effectiveness of virtual reality versus conventional clinic-based vestibular physical therapy on balance and function in active duty service members. A pilot randomized controlled trial,VRS - Virtual Reality,B,"Virtual reality (VR) may be useful during rehabilitation of service members with persistent vestibular impairment following concussion. Thirty-eight active duty US military service members with persistent balance impairment resulting from concussion were randomized into three groups [Conventional Vestibular Physical Therapy (CVPT, n = 13), Virtual Reality Vestibular Physical Therapy (VRVPT, n = 12), and Hybrid Virtual Reality and Conventional Vestibular Physical Therapy (HybridVPT, n = 13)] and were treated twice weekly for 6 weeks. Changes in clinical measures such as Activities-specific Balance Confidence (ABC) Scale, Dizziness Handicap Inventory (DHI), Functional Gait Assessment (FGA), and Sensory Organization Test (SOT) were assessed from pre-, mid-, and post-treatment scores. A significant main time effect was observed demonstrating clinical improvement over time (ABC: p <.001, η2p =.54; DHI: p <.001, η2p =.57; FGA: p <.001, η2p =.74; SOT: p <.001, η2p =.35). Both CVPT and HybridVPT groups demonstrated significant improvements in patient-reported confidence and function earlier in the treatment course (p <.005). FGA significantly and incrementally improved at each assessment time point in all treatment groups. The SOT significantly improved early in treatment in the CVPT group only and pre-to-post-treatment in the CVPT and VRVPT groups only. The HybridVPT group did not demonstrate any significant improvement with time in the instrumented SOT measure. In the comparison of pre-to-post-effects of VRVPT and HybridVPT effects compared to CVPT, there was no clear superiority or inferiority observed in either of the experimental treatments. This preliminary work shows initial efficacy of using VR-based therapy for concussed individuals allowing future work to personalize treatment that may improve adherence and engagement to therapy. © 2021, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.",Craniocerebral trauma; Data display; Physical therapy modalities; Sensation disorders; Therapeutics,Title_Abstract,True,
Scopus,journalPaper,2023,Machine learning classification analysis for an adaptive virtual reality Stroop task,VRS - Virtual Reality,B,"Advances in virtual environment (VE) technologies have afforded psychologists with high-dimensional virtual reality (VR) platforms that enhance the complexity and dimensionality of cognitive assessments. The Virtual Reality Stroop Task HMMWV (VRST; Stroop stimuli embedded within a virtual high mobility multipurpose wheeled vehicle) is a VR assessment involving both cognitive and affective components. There is a need for adaptive virtual environments (AVEs) that can adjust the complexity of environmental stimuli relative to the way the participant is performing. To develop the VRST into an AVE assessment, classifier algorithms must be developed. While previous research has explored classifier algorithms for modeling arousal and cognitive performance in the VRST, machine learning (ML) classifiers have not been developed for an adaptive VRST. The current study developed ML classifiers for an adaptive version of the VRST. The assessment of Naive Bayes (NB), k-Nearest Neighbors (kNN), and Support Vector Machines (SVM) machine learning classifiers found that SVM and NB classifiers tended to have the highest accuracies and greatest areas under the curve when classifying users as high or low performers. The kNN algorithms did not perform as well. As such, SVM and NB may be the best candidates for creation of an adaptive version of the VRST. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Adaptive virtual environment; Classification; Human–Computer interaction; Machine learning; Virtual reality stroop task,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Training in virtual reality enables learning of a complex sports movement,VRS - Virtual Reality,B,"Despite the increased use in sports, it is still unclear to what extent VR training tools can be applied for motor learning of complex movements. Previous VR studies primarily relate to realize performances rather than learning motor skills. Therefore, the current study compared VR with video training realizing the acquisition of karate technique, the Soto Uke moving forward in Zenkutsu Dachi, without being accompanied by a trainer or partner. Further analyses showed whether a less lavished forearm compared to a whole-body visualization in VR is necessary to acquire movements’ basics sufficiently. Four groups were tested: 2 groups conducted VR training (VR-WB: whole-body visualization, and VR-FA having only visualized the forearms), the third group passed through a video-based learning method (VB), and the control group (C) had no intervention. In consultation with karate experts, a scoring system was developed to determine the movements’ quality divided, into upper- and lower body performance and the fist pose. The three-way ANOVA with repeated measurements, including the between-subject factor group [VR-WB, VR-FA, VB, C] and the within-subject factors time [pre, post, retention] and body regions [upper body, lower body, fist pose], shows that all groups improved significantly (except for C) with the similar course after four training sessions in all body regions. Accordingly, VR training seems to be as effective as video training, and the transfer from VR-adapted skills into the natural environment was equally sufficient, although presenting different body visualization types. Further suggestions are made related to the features of future VR training simulations. © 2022, The Author(s).",Body visualization; Combat sports; Head-mounted display; Karate kumite; Motor learning; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2023,Relaxing in virtual reality: one synthetic agent relaxes all,VRS - Virtual Reality,B,"Virtual reality-based interventions have gained attention as innovative approaches in clinical settings. However, the use of virtual-based relaxation in reducing psychological distress and physiological activation, a common strategy in traditional interventions, is not well documented. This study aims at exploring the role of a non-familiar synthetic agent (SyA) as a resource to promote relaxation in a virtual environment (VE). Sixty-nine healthy participants were randomly assigned to three conditions: relaxing in VE while listening to relaxing instructions delivered by a SyA (n = 23), relaxing in the same VE while listening to the same instructions but aired by a radio set (n = 23; ‘active’ control group) and waiting to the end of the experience without relaxing instructions (n = 23; ‘passive’ control group). The instruction was preceded by an activation task (i.e., a matching game within a limited time). Our hypothesis claims that the presence of a humanoid-like figure that is strange to the participant (SyA) hinders the relaxing process. Data from several self-reports (Presence, Immersion, Cybersickness and emotional response) and from psychophysiology (respiratory rate—RR) revealed that no differences were found between the two groups that listened to the relaxing instructions (SyA and radio). Additionally, a significant decrease in RR recordings was only significant for these two relaxation conditions (SyA and radio), but not for the ‘passive’ control group. Results suggest that the presence of a non-familiar humanoid character was not perceived as a dissonant element in the VE setting and did not negatively influence the relaxation outcome. This study sets the ground for future studies that may provide an insight into the optimal characteristics of a SyA, contributing to the development of accessible and beneficial digital applications to a wide range of individuals in clinical and non-clinical contexts. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Relaxing; Synthetic agents; Virtual environments,Title_Abstract,True,
Scopus,journalPaper,2023,The efficiency of visually guided movement in real and virtual space,VRS - Virtual Reality,B,"There is an increasing interest in the use of low-cost virtual reality (VR) for training and simulation. As effective training for many tasks requires efficient sensory–motor coordination, we investigated the efficiency of visually guided movement in VR using a standardised Fitts’ tapping task. Throughput is a measure of movement efficiency and was significantly lower in VR than for a touchscreen. This difference was particularly marked for targets distributed in depth and is likely to reflect known limitations of VR visual display. The addition of haptic cues increased throughput slightly. The lower throughput in VR was due to both a decrease in the precision of pointing and an increase in movement time. Movement distances were equivalent for the touchscreen and VR, but on average slightly smaller than specified by the task. VR presentation also resulted in more numerous double touches on targets. There was evidence of a small and rapid learning effect for VR, but this was limited to the first block of 9 trials (252 movements). For tasks requiring skilled sensory–motor coordination, current low-cost VR may not provide the same transfer of training as the real world. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Human–computer interaction; Training; Virtual reality; Visual–motor control,Abstract_Keywords,True,
Scopus,journalPaper,2023,A cognitive model for emotional regulation in virtual reality exposure,VRS - Virtual Reality,B,"Virtual reality exposure (VRE) is an effective form of psychotherapy. However, theoretical frameworks for user experience of virtual reality (VR) have yet to be fully integrated with psychological theory, limiting optimisation of VRE. Presence, a sense of being in a specific place, is the dominant focus of VR/VRE research into emotion regulation. Critically, presence is subject to limitations that make it an impractical concept where precision is needed. More meaningful insights can be obtained by examining specific cognitive constructs. Additionally, presence is a technology-focused consideration. This review argues that in psychotherapy, it is the personal meaning of an environment, rather than the concrete properties of the environment itself, that informs interventions. Because personal meaning is subject to individual differences and most VRE scenarios are generic by nature, situational plausibility must be managed by the therapist. The cognitive person-focused model has been developed to address the limitations of the existing presence–emotion concepts. This model provides a much-needed psychological framework to inform and guide researchers and therapists. The framework also creates a scaffold to support additional variables of interest. As part of the model justification, an examination of presence literature limitations is included. It is hoped that validation and ongoing development of the model will help advance VRE research and therapy. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cognition; Emotional regulation; Presence; Psychotherapy; Virtual reality; Virtual reality exposure,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Virtual reality as a communication medium: a comparative study of forced compliance in virtual reality versus physical world,VRS - Virtual Reality,B,"There are reasons to consider virtual reality (VR) as a newly arrived communication medium that ought to be differentiated from all other forms of mediated communication, since it is the first and only medium with the potential to enable incorporation of the full spectrum of both verbal and non-verbal cues. The present paper is part of a broader scheme in investigating potential differentiations in interpersonal communication between the physical world and VR. Our experimental design builds upon the existing knowledge base of forced compliance experiments; the set-up involved a comparative study of two groups (N = 46) performing tasks under the authoritative influence of a researcher who applied persuasion techniques. Results indicate that VR-mediated communication is as intricate as face to face, since subjects were equally or more compliant, with the nature of information exchanged (e.g. fact-based, morality-based, etc.) being a contributing factor, whilst exemplifying under-development and future applications of VR collaborative environments. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cognitive dissonance; Computer-mediated communication; Face-to-face communication; Forced compliance; Virtual reality; VR-mediated communication,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Do we see rendered surface materials differently in virtual reality? A psychophysics-based investigation,VRS - Virtual Reality,B,"Synthesized surface materials are an essential visualization element to represent and simulate the appearances of virtual objects such as product prototypes. This paper investigates whether the perception of rendered surface materials would be different between a 3D immersive/VR viewing condition and a traditional 2D one. For rendered surface materials, roughness and specularity are the two major parameters that modulate the rendering outcome. In this study, we vary the two parameters and incorporate psychophysics techniques to derive a scale for measuring the perceivable changes of material appearance. Using the perceptual scale as the basis, we run a series of surface appearance matching tasks and compare the participants’ task performances in the VR viewing mode and the 2D viewing mode. The results show that in the VR viewing mode, the participants identify the matching materials at higher levels of accuracy and precision. These findings show that the depth impression in immersive viewing environments may result in a different perceptual response to the rendered surface materials. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Design evaluation; Immersive viewing environment; Material perception; Material visualization; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2022,Evaluation of the Oculus Rift S tracking system in room scale virtual reality,VRS - Virtual Reality,B,"In specific virtual reality applications that require high accuracy it may be advisable to replace the built-in tracking system of the HMD with a third party solution. The purpose of this research work is to evaluate the accuracy of the built-in tracking system of the Oculus Rift S Head Mounted Display (HMD) in room scale environments against a motion capture system. In particular, an experimental evaluation of the Oculus Rift S inside-out tracking technology was carried out, compared to the performance of an outside-in tracking method based on the OptiTrack motion capture system. In order to track the pose of the HMD using the motion capture system the Oculus Rift S was instrumented with passive retro-reflective markers and calibrated. Experiments have been performed on a dataset of multiple paths including simple motions as well as more complex paths. Each recorded path contained simultaneous changes in both position and orientation of the HMD. Our results indicate that in room-scale environments the average translation error for the Oculus Rift S tracking system is about 1.83 cm, and the average rotation error is about 0. 77 ∘, which is 2 orders of magnitude higher than the performance that can be achieved using a motion capture system. © 2022, The Author(s).",Head mounted displays; Room-scale virtual reality; Tracking technologies,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Reductions in sickness with repeated exposure to HMD-based virtual reality appear to be game-specific,VRS - Virtual Reality,B,"While head-mounted display (HMD) based gaming is often limited by cybersickness, research suggests that repeated exposure to virtual reality (VR) can reduce the severity of these symptoms. This study was therefore aimed at: (1) examining the exposure conditions required to reduce cybersickness during HMD VR; and (2) learning whether such reductions generalise from one HMD VR game to another. Our participants played two commercially-available HMD VR video games over two consecutive days. Their first exposure to HMD VR on both days was always to a 15-min virtual rollercoaster ride. On Day 1, half of our participants also played a virtual climbing game for 15-min, while the rest of them finished testing early. Participants in the latter group were only exposed to the climbing game late on Day 2. We found that sickness was significantly reduced for our participants on their second exposure to the virtual rollercoaster. However, sickness to the rollercoaster on Day 2 was unaffected by whether they had played the climbing game on Day 1. Sickness during virtual climbing was also unaffected by group differences in exposure to the virtual rollercoaster. This convergent evidence suggested that the reductions in cybersickness produced by repeated exposure to HMD VR were game-specific. While these benefits did not generalise to the second game, two 15-min exposures to the same HMD VR game was sufficient to significantly reduce cybersickness in this study. © 2022, The Author(s).",Computer games; Cybersickness; Head-mounted display; Motion sickness; Vection; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Controlling camera movement in VR colonography,VRS - Virtual Reality,B,"Immersive colonography allows medical professionals to navigate inside the intricate tubular geometries of subject-specific 3D colon images using Virtual Reality displays. Typically, camera travel is performed via Fly-Through or Fly-Over techniques that enable semi-automatic traveling through a constrained, well-defined path at user-controlled speeds. However, Fly-Through is known to limit the visibility of lesions located behind or inside haustral folds. At the same time, Fly-Over requires splitting the entire colon visualization into two specific halves. In this paper, we study the effect of immersive Fly-Through and Fly-Over techniques on lesion detection and introduce a camera travel technique that maintains a fixed camera orientation throughout the entire medial axis path. While these techniques have been studied in non-VR desktop environments, their performance is not well understood in VR setups. We performed a comparative study to ascertain which camera travel technique is more appropriate for constrained path navigation in immersive colonography and validated our conclusions with two radiologists. To this end, we asked 18 participants to navigate inside a 3D colon to find specific marks. Our results suggest that the Fly-Over technique may lead to enhanced lesion detection at the cost of higher task completion times. Nevertheless, the Fly-Through method may offer a more balanced trade-off between speed and effectiveness, whereas the fixed camera orientation technique provided seemingly inferior performance results. Our study further provides design guidelines and informs future work. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Colonography; Human-centered computing; Medical imagery; Navigation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Remote virtual whiteboard assistance for improving task performance during lunar surface operations,VRS - Virtual Reality,B,"During extravehicular activities (EVAs), astronauts are heavily dependent on the Mission Center (MCC) and their Intra-Vehicular Astronaut (IVA) counterparts. Each procedure step in a mission is relayed to the astronaut through a real-time voice loop, and emergency procedures are written on cuff checklists that astronauts must read from their spherically shaped helmet. In all situations, crew members heavily rely on IVA or MCC support, especially when they do not understand a procedure or need help with a specific problem. However, it can be hard to communicate procedures effectively due to a lack of visual diagrams and situational awareness between the two parties. To improve EVA efficiency, we investigated the use of a virtual whiteboard on a heads-up display during a lunar surface EVA task with virtual reality (VR). The virtual whiteboard allows MCC to send additional visual guidance (e.g., drawings and annotations) overlayed on the astronaut’s visual field of view to better assist with mission tasks. We conducted a between-subjects experiment where 21 participants were asked to accomplish a rover repair procedure, with (n = 11) and without (n = 10) the virtual whiteboard, using a VR lunar environment with support of a research proctor acting as MCC. The whiteboard group completed the rover procedure 39.1% faster than the non-whiteboard group, and this difference was statistically significant (p = 0.017). The total number of words exchanged during the experimental sessions was not statistically different between groups (p = 0.99). However, participants in the whiteboard group showed a tendency to talk less than their counterparts, while the research proctor in the whiteboard group showed a tendency to speak more. Finally, analysis of the spatial locations during the experiment indicated that whiteboard participants stayed closer to the rover, showing a better focus on the task at hand and therefore short completion times. The results of this experiment inform future development of AR spacesuit technologies for future planetary exploration EVA operations. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Extravehicular activity; Spacewalks; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Visual aspect-oriented modeling of explorable extended reality environments,VRS - Virtual Reality,B,"The availability of various extended reality (XR) systems for tracking users’ and objects’ behavior opens new opportunities for analyzing users’ and objects’ interactions and autonomous actions. Such analysis can be especially useful and attainable to domain experts when it is based on domain knowledge related to a particular application, liberating the analysts from going into technical details of 3D content. Analysis of XR users’ and objects’ behavior can provide knowledge about the users’ experience, interests and preferences, as well as objects’ features, which may be valuable in various domains, e.g., training, design and marketing. However, the available methods and tools for building XR focus on 3D modeling and programming rather than knowledge representation, making them unsuitable for domain-oriented analysis. In this paper, a new visual approach to modeling explorable XR environments is proposed. It is based on a semantic representation of aspects, which extend the primary code of XR environments to register their behavior in a form explorable with reasoning and queries, appropriate for high-level analysis in arbitrary domains. It permits domain experts to comprehend and analyze what happened in an XR environment regarding users’ and objects’ actions and interactions. The approach has been implemented as an extension to MS Visual Studio and demonstrated in an explorable immersive service guide for household appliances. The evaluation results show that the approach enables efficient development of explorable XR and may be useful for people with limited technical skills. © 2021, The Author(s).",3D web; Aspect-oriented modeling; Exploration; Extended reality; Ontologies; Semantic web,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,"Exploring structural relations among computer self-efficacy, perceived immersion, and intention to use virtual reality training systems",VRS - Virtual Reality,B,"The use of virtual reality (VR) training systems for education has grown in popularity in recent years. Scholars have reported that self-efficacy and interactivity are important predictors of learning outcomes in virtual learning environments, but little empirical research has been conducted to explain how computer self-efficacy (as a subcategory of self-efficacy) and perceived immersion (as a correlate of interactivity) are connected to the intention to use VR training systems. The present study aims to determine which factors significantly influence behavioral intention when students are exposed to VR training systems via an updated technology acceptance frame by incorporating the constructs of computer self-efficacy and perceived immersion simultaneously. We developed a VR training system regarding circuit connection and a reliable and validated instrument including 9 subscales. The sample data were collected from 124 junior middle school students and 210 senior high school students in two schools located in western China. The samples were further processed into a structural equation model with path analysis and cohort analysis. The results showed that the intention to use VR training systems was indirectly influenced by computer self-efficacy but directly influenced by perceived immersion (β = 0.451). However, perceived immersion seemed to be influenced mostly by learner interaction (β = 0.332). Among external variables, learner interaction (β = 0.149) had the largest total effect on use intention, followed by facilitating conditions (β = 0.138), computer self-efficacy (β = 0.104), experimental fidelity (β = 0.083), and subjective norms (β = 0.077). The moderating roles of gender differences, grade level, and previous experience in structural relations were also identified. The findings of the present study highlight the ways in which factors and associations are considered in the practical development of VR training systems. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Computer self-efficacy; Intention to use; Perceived immersion; TAM; VR training systems,Title_Abstract,True,
Scopus,journalPaper,2022,Effects of first- and third-person perspectives created using a head-mounted display on dart-throwing accuracy,VRS - Virtual Reality,B,"The first-person perspective (1PP) and third-person perspective (3PP) have both been adopted in video games. The 1PP can induce a strong sense of immersion, and the 3PP allows players to perceive distances easily. Virtual reality technologies have also adopted both perspectives to facilitate skill acquisition. However, how 1PP and 3PP views affect motor skills in the real world, as opposed to in games and virtual environments, remains unclear. This study examined the effects of the 1PP and 3PP on real-world dart-throwing accuracy after head-mounted display (HMD)-based practice tasks involving either the 1PP or 3PP. The 1PP group showed poorer dart-throwing performance, whereas the 3PP task had no effect on performance. Furthermore, while the effect of the 1PP task persisted for some time, that of task 3PP disappeared immediately. Therefore, the effects of 1PP HMD-based practice tasks on motor control transfer more readily to the real world than do those of 3PP tasks. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Agency; Body ownership; Distance perception; HMD; Human performance; Motor control; Size perception; Skill transfer; Visual perspective,Abstract,True,
Scopus,journalPaper,2022,Spatial transition management for improving outdoor cinematic augmented reality experience of the TV show,VRS - Virtual Reality,B,"There have been attempts to provide new cinematic experiences by connecting TV or movie content to suitable locations through augmented reality (AR). However, few studies have suggested a method to manage breakdowns in continuity due to spatial transitions. Thus, we propose a method to manage the spatial transition that occurs when we create a TV show trajectory by mapping TV show scenes with spatiotemporal information to the real world. Our approach involved two steps. The first step is to reduce the spatial transition considering the sequence, location, and importance of TV show scenes when creating the TV show trajectory in the authoring tool. The second is to fill the spatial transition with additional TV show scenes considering sequence, importance, and user interest when providing the TV show trajectory in the mobile application. The user study results showed that reducing spatial transition increases narrative engagement by allowing participants to see important content within the trajectory. The additional content in spatial transition decreased the physical demand and effort in terms of the perceived workload, although it increased the task completion time. Integrated spatial transition management improved the overall cinematic augmented reality (CAR) experience of the TV show. Furthermore, we suggest design implications for realizing the CAR of TV shows based on our findings. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Cinematic augmented reality; Continuity; Trajectory,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Virtual reality consumer experience escapes: preparing for the metaverse,VRS - Virtual Reality,B,"Virtual Reality (VR) experience escapes allow individuals to spend hours on end in immersive virtual environments and interact with content in a world that is providing shelter and illusion of an alternative reality – the metaverse. Discussions on possible risks have largely remained limited to usability challenges, while only a few studies reflect on social, psychological and physical implications this immersive technology exposes and the considerations consumers and businesses need to take. This paper critically reviews literature on escapism to discuss issues in the design and employment of virtual reality consumer experience escapes. Key issues relating to VR experience escapes and resulting effects on consumer health and well-being are discussed, emphasizing needed consumer-centered research and design. Future considerations include (1) Self-indulgent escapism through VR consumer experiences, (2) Ethical considerations in the design of VR consumer experience escapes, and (3) Purposeful design of VR consumer experiences escapes. A sequential research agenda is presented that integrates antecedents of VR experience escapes that connect to three main future research streams; designing purpose-driven VR consumer experience escapes, complementing methodologies for VR consumer experience research, and meaningful VR consumer experience escapes. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Consumer-centered design; Escapism; Metaverse; Research agenda; Virtual reality experience,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Augmented reality instructions for construction toys enabled by accurate model registration and realistic object/hand occlusions,VRS - Virtual Reality,B,"BRICKxAR is a novel augmented reality (AR) instruction method for construction toys such as LEGO®. With BRICKxAR, physical LEGO construction is guided by virtual bricks. Compared with the state of the art, accuracy of the virtual–physical model alignment is significantly improved through a new design of marker-based registration, which can achieve an average error less than 1 mm throughout the model. Realistic object occlusion is accomplished to reveal the true spatial relationship between physical and virtual bricks. LEGO players’ hand detection and occlusion are realized to visualize the correct spatial relationship between real hands and virtual bricks, and allow virtual bricks to be “grasped” by real hands. The major finding of the research is that the integration of these features makes AR instructions possible for small parts assembly, validated through a working AR prototype for constructing LEGO Arc de Triomphe and quantitative measures of the accuracies of registration and occlusions. In addition, a heuristic evaluation of BRICKxAR’s features has led to findings that the present method could advance AR instructions in terms of enhancing part visibility, match between mental models and visualization, alignment of physical and virtual parts in perspective views and spatial transformations, tangible user interface, consolidated structural diagrams, virtual cutaway views, among other benefits for guiding construction. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Accuracy; Assembly; Augmented reality; Instruction; Occlusion,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,"Short- and long-term learning of job interview with a serious game in virtual reality: influence of eyestrain, stereoscopy, and apparatus",VRS - Virtual Reality,B,"Purpose: Do apparatuses and eyestrain have effects on learning performances and quality of experience? Materials and Methods: 42 participants played a serious game simulating a job interview with a Samsung Gear VR Head-Mounted Display (HMD) or a computer screen. Participants were randomly assigned to 3 groups: PC, HMD biocular, and HMD stereoscopy (S3D). Participants played the game thrice. Eyestrain was assessed pre- and post-exposure with six optometric measures. Learning performances were obtained in-game. Quality of experience was measured with questionnaires. Results: eyestrain was higher with HMDs than PC based on Punctum Proximum of accommodation but similar between biocular and S3D. Knowledge gain and retention were similar with HMDs and PC based on scores and response time. All groups improved response time but without statistically significant differences between HMDs and PC. Visual discomfort difference was statistically significant between PC and HMDs (biocular and S3D). Flow difference was statistically significant between PC and HMDs (biocular and S3D), with the PC group reporting higher Flow than HMD-S3D. Conclusion: short-term learning is similar between PC and HMDs. Groups initially using HMDs continued improving during long-term learning but without statistically significant difference compared to PC. Eyestrain and visual discomfort were higher with HMDs than PC. Flow was higher with the PC group. Our results show that eyestrain does not seem to decrease learning. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Eyestrain; Head-Mounted Display; Learning; Serious Game; Stereoscopy; Virtual Reality,Title_Keywords,True,
Scopus,journalPaper,2022,Haptic/virtual reality orthopedic surgical simulators: a literature review,VRS - Virtual Reality,B,"This paper presents a review of surgical simulators, developed to enhance the learning process of surgical procedures, that involves bones, ranging from musculoskeletal system (orthopedics) and the skull (ENT and neurosurgeries). The paper highlights the specific challenges in terms of the extended reality representation of surgical training along with its latest advances. The study gathers journal and conference proceedings from various database sources (bibliographic databases and online search engines) that fulfills a predetermined eligibility criterion. From the search, 185 journals were found but only 144 met the inclusion criteria. Surgical simulators emerge as a promising alternative to aid residents in surgical training. It encompasses surgical procedures done in the craniomaxillofacial, joints, limbs and spine section of the human body. The study was partially supported by internal grant STG/19/047 from KU Leuven. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Haptics; Muscle memory; Orthopedics; Surgical simulator; Training,Title_Abstract,True,
Scopus,journalPaper,2022,Examining the impact of VR and MR on future teachers' creativity performance and influencing factors by scene expansion in instruction designs,VRS - Virtual Reality,B,"Unlike the traditional environment, VR and MR environments provide novel, natural, lifelike 3D user interfaces, which can stimulate the imagination of users. Whether VR and MR environments can influence the creativity and its impact factors (flow, attention, and relaxation) of future teachers' scene expansion in instruction design are the focus of the study. Moreover, the differences between the impacts of VR and MR environments on creativity and its impact factors are also questions worth exploring. In this study, we developed VR and MR experiment environments as creativity support systems to stimulate future teachers’ creativity before they carry out the instruction design of experimental courses. The results show that the creativity, flow, and attention of future teachers which use the VR and MR environments were higher than in the traditional environment. Moreover, the future teachers' creativity of scene expansion in MR environment was higher than VR environment, the future teachers' attention of VR environment was higher than MR environment, and the future teachers.' relaxation of VR environment was lower than the traditional environment. These findings provide a useful inspiration, i.e. too high concentration or too high relaxation is not conducive to the production of creativity. The MR creativity support system that provides moderate and balanced attention and relaxation can good stimulate the creativity of future teachers in the instruction design. We expect these findings to inspire the design of creativity support systems and foster future teachers’ creativity level in instructional design. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Attention; Creativity; Creativity support systems; Flow; Instruction design; Mixed reality (MR); Relaxation; Virtual reality (VR),Keywords,True,
Scopus,journalPaper,2022,Light direction estimation and hand touchable interaction for augmented reality,VRS - Virtual Reality,B,"Augmented reality is a technology that combines a virtual world with the real world. How to improve the realism of augmented reality is an important topic. One focus of this paper is lighting consistency between virtual and real world, and the other is interaction with virtual object using hands. Estimating lighting conditions through traditional methods often requires many prior knowledge of the scene. We propose a method that estimates the light direction based on shadows and foreground objects with only one scene image. We detect and calculate the relative direction of an object and its shadow in the scene to estimate the azimuth of the light, and use area size ratio of the object and its shadow to estimate the elevation angle of the light. We used some real scenes to test our method. However, the exact light direction of the real world is difficult to acquire, so we further verified our method by establishing a number of virtual scenes with preset light direction. Moreover, hand gesture-based human–computer interaction provides a natural and easy way for interaction. Traditional augmented reality interactions use markers or touch screens. We apply gesture recognition and hand touchable interaction in augmented reality, allowing the user’s hand to occlude the virtual object in the picture, and recognize the gesture. By adding estimated light and hand touchable interactions, we enhance the realism of augmented reality. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality (AR); Gesture recognition; Human–computer interaction; Light source estimation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,A framework for fidelity evaluation of immersive virtual reality systems,VRS - Virtual Reality,B,"Developments in visual and tracking systems have expanded virtual reality (VR) applications and led to VR becoming a powerful tool for decision making, planning, and conducting training and experiments across several fields. VR’s goal is to fully immerse a user in a virtual environment through simulating the same kinds of physical and psychological reactions they would experience in the real world. Fidelity is a common and useful concept for distinguishing different VR systems, as a common goal for VR is to provide a high-fidelity experience similar to the real world. The purpose of this study was to provide a comprehensive framework and a scale for evaluating the fidelity of VR systems by addressing their architecture and the factors that affect overall fidelity with respect to the digital sensory and tracking systems used. The proposed framework characterizes itself from other fidelity evaluation frameworks in the involvement of integration and synchronization of VR system data and devices as the main factors in fidelity evaluation. Also, it presents a scale for fidelity evaluation of VR systems and defines high-level useful concepts for distinguishing different VR systems with respect to fidelity. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Collaborative; Digital sensory system; Fidelity; Immersion; Simulation; Tracking system; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Virtual reality for the assessment and rehabilitation of neglect: where are we now? A 6-year review update,VRS - Virtual Reality,B,"Unilateral spatial neglect (USN) is a frequent repercussion of a cerebrovascular accident, typically a stroke. USN patients fail to orient their attention to the contralesional side to detect auditory, visual, and somatosensory stimuli, as well as to collect and purposely use this information. Traditional methods for USN assessment and rehabilitation include paper-and-pencil procedures, which address cognitive functions as isolated from other aspects of patients’ functioning within a real-life context. This might compromise the ecological validity of these procedures and limit their generalizability; moreover, USN evaluation and treatment currently lacks a gold standard. The field of technology has provided several promising tools that have been integrated within the clinical practice; over the years, a “first wave” has promoted computerized methods, which cannot provide an ecological and realistic environment and tasks. Thus, a “second wave” has fostered the implementation of virtual reality (VR) devices that, with different degrees of immersiveness, induce a sense of presence and allow patients to actively interact within the life-like setting. The present paper provides an updated, comprehensive picture of VR devices in the assessment and rehabilitation of USN, building on the review of Pedroli et al. (2015). The present paper analyzes the methodological and technological aspects of the studies selected, considering the issue of usability and ecological validity of virtual environments and tasks. Despite the technological advancement, the studies in this field lack methodological rigor as well as a proper evaluation of VR usability and should improve the ecological validity of VR-based assessment and rehabilitation of USN. © 2022, The Author(s).",Assessment; Neglect; Rehabilitation; Technology; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,The potential of immersive virtual reality for representations in design education,VRS - Virtual Reality,B,"This paper examines the potential of immersive virtual reality technology for design education. A quasi-experimental study has been conducted with 40 students of different expertise levels. The students analysed a design representation using one of the two visualisation technologies: immersive virtual reality (IVR) and non-immersive virtual reality (nIVR). The results show that the expertise in the used technology and the expertise in the design domain significantly affect design understanding. On the other hand, the effect of contextual expertise was not found significant. Spatial ability affected design understanding in nIVR but not in the IVR. Visualisation technology did not have an overall effect on understanding, but IVR helped students with lower expertise to understand specific aspects of a design better (e.g. rotation-based mechanisms). The study suggests that researchers and educators control the students’ expertise when assessing the effect of technology on design education. Overall, the results support the constructivist learning theory, as IVR can support context-dependent and context-independent understanding. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Constructivist learning theory; Design education; Design representation; Design understanding; Immersive virtual reality; Students’ expertise; Visualisation technology,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,NapWell: An EOG-based Sleep Assistant Exploring the Effects of Virtual Reality on Sleep Onset,VRS - Virtual Reality,B,"We present NapWell, a Sleep Assistant using virtual reality (VR) to decrease sleep onset latency by providing a realistic imagery distraction prior to sleep onset. Our proposed prototype was built using commercial hardware and with relatively low cost, making it replicable for future works as well as paving the way for more low cost EOG-VR devices for sleep assistance. We conducted a user study (n= 20) by comparing different sleep conditions; no devices, sleeping mask, VR environment of the study room and preferred VR environment by the participant. During this period, we recorded the electrooculography (EOG) signal and sleep onset time using a finger tapping task (FTT). We found that VR was able to significantly decrease sleep onset latency. We also developed a machine learning model based on EOG signals that can predict sleep onset with a cross-validated accuracy of 70.03%. The presented study demonstrates the feasibility of VR to be used as a tool to decrease sleep onset latency, as well as the use of embedded EOG sensors with VR for automatic sleep detection. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Electrooculography; Sleep onset; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Processing presence: how users develop spatial presence through an immersive virtual reality game,VRS - Virtual Reality,B,"A primary affordance of virtual reality (VR) headsets is to give the user spatial presence or the illusion of being in the virtual environment. Although considerable research connects VR to spatial presence, spatial awareness, and spatial ability, little is known about how users develop spatial presence in VR learning environments. This study addresses that gap by exploring spatial presence experience construction in a VR educational game and investigating whether users’ knowledge, game experience, and VR experience impact the establishment of spatial presence. In this study, 56 high school students played an immersive 3D VR cell biology game where players search for clues within a virtual cell to diagnose the cell. Findings suggest that players’ perceptions of spatial presence are linked to how they allocate their attention during the game, their level of interest in cellular biology, and their visual-spatial acuity, but are not linked to their game experience, VR experience, or prior knowledge of the content area. These results indicate that well-scaffolded, engaging virtual environments can foster spatial presence among users, regardless of prior knowledge or experience, and gives practitioners clues about how to design VR learning environments. © 2021, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.",Game-based learning; Immersive technology; Learner characteristics; Spatial ability; Spatial presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Point specification in collaborative visualization for 3D scalar fields using augmented reality,VRS - Virtual Reality,B,"We compared three techniques to specify 3D positions for collaborative augmented reality (AR) visualization. AR head-mounted displays allow multiple users to share the same physical space, while keeping seamless social interactions. Interactions being key parts of exploratory visualization tasks, we adapted from the virtual reality literature three distinct techniques to specify points in 3D space, such as for placing annotations for which they cannot rely on existing data objects. We evaluated these techniques on their accuracy and speed, the user’s subjective workload and preferences, as well as their co-presence, mutual understanding, and behavior in collaborative tasks. Our results suggest that all the three techniques provide good mutual understanding and co-presence among collaborators. They differ, however, in the way users behave, their accuracy, and their speed. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",3D point specification; 3D visualization; AR; CSCW,Title_Abstract,True,
Scopus,journalPaper,2022,Investigation of spatial ability test completion times in virtual reality using a desktop display and the Gear VR,VRS - Virtual Reality,B,"The interaction time of students who did spatial ability tests in a virtual reality environment is analyzed. The spatial ability test completion times of 240 and 61 students were measured. A desktop display as well as the Gear VR were used by the former group and by the latter one, respectively. Logistic regression analysis was used to investigate the relationship between the probability of correct answers and completion times, while linear regression was used to evaluate effects and interactions of following factors on test completion times: the users’ gender and primary hand, test type and device used. The findings were that while the completion times are not significantly affected by the users’ primary hand, other factors have significant effects on them: they are decreased by the male gender in itself, while they are increased by solving Mental Rotation Tests or by using the Gear VR. The largest significant increment in interaction time in virtual reality during spatial ability tests is when Mental Rotation Tests are accomplished by males with the Gear VR, while the largest significant decrease in interaction time is when Mental Cutting Tests are completed with a desktop display. © 2021, The Author(s).",Cognitive skills; Desktop display; Gear VR; Human-computer interaction; Interaction time; Mental rotation; Spatial ability; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Leading presence-based strategies to manipulate user experience in virtual reality environments,VRS - Virtual Reality,B,"Virtual reality (VR) has been widely used to simulate various real-like environments suitable to explore and interact, similar to being genuinely there (i.e., allowing presence). User experience in virtual environments (VE) is highly subjective, and presence-based self-reports have addressed its assessment; however, it is unclear how a diverse set of VR features relates to the subscales of the questionnaires (e.g., engagement, immersion, or Attention), which could be helpful to create and improve immersive VE. Consequently, most current studies have appealed to self-defined criteria to design their VE in response to a lack of accepted methodological frameworks. Therefore, we systematically reviewed the current publications to identify critical design elements to promote presence and realistic experiences in VR-games users. We extracted information from different databases (Scopus, Web of Science, PubMed, ACM, IEEE, Springer, and Scholar) and used inclusion and exclusion criteria to reduce the original set of 595 candidates to 53 final papers. The findings showed that better quality and quantity in resources allocation (software and hardware) and more accuracy in objects and characters, which all refer to higher immersion, provide Place Illusion (PI), i.e., the spatial dimension of presence. Furthermore, Scenario’s Realism, external stimuli, and coherent match between virtual and real worlds (including body representation) are decisive to set Plausibility Illusion (PSI), i.e., the dimension associated with coherence. Finally, performance feedback, character customization, and multiplayer mechanics are crucial to assure motivation and agency, which are user-exclusive but crucial to defining presence’s perception. Moreover, about 65% of the analyzed studies agreed that immersive media and social interaction could simultaneously influence PI and PSI. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Coherence; Immersion; Motivation; Presence; User experience; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Kinematics of aimed movements in ecological immersive virtual reality: a comparative study with real world,VRS - Virtual Reality,B,"Virtual reality (VR) has recently emerged as a promising technology to rehabilitate upper limb functions after stroke. To promote the recovery of functions, retraining physiological movement patterns is essential. However, it is still unclear whether VR can elicit functional movements that are similar to those performed in the real world (RW). This study aimed to investigate the kinematics of reach-to-grasp and transport movements performed in the real world and immersive VR by examining whether kinematic differences between the two conditions exist and their extent. A within-subject repeated-measures study was conducted. A realistic setup resembling a supermarket shelf unit was built in RW and VR. The analysis compared reaching and transport gestures in VR and RW, also considering potential differences due to: (i) holding the controller needed to interact with virtual items, (ii) hand dominance, and (iii) target positions. Ten healthy young adults were enrolled in the study. Motion data analysis showed that reach-to-grasp and transport required more time in VR, and that holding the controller had no effects. No major differences occurred between the two hands. Joint angles, except for thorax rotation, and hand trajectory curvature were comparable across conditions, suggesting that VR has the potentialities to retrain physiological movement patterns. Results were satisfying, though they did not demonstrate the superiority of ecological environments in eliciting natural gestures. Further studies should determine the extent of kinematic similarity required to obtain functional gains in VR-based upper limb rehabilitation. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Hand dominance; Immersive virtual reality; Kinematics; Movement analysis,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Satisfied or not: user experience of mobile augmented reality in using natural language processing techniques on review comments,VRS - Virtual Reality,B,"With the rapid developments and improvements in mobile technologies, mobile devices have become one of the primary interfaces of augmented reality technologies. In this study, we explore how user groups (satisfied vs. dissatisfied users) and domains (game vs. non-game domain) affect user experience, including perceived usefulness, usability, and affection using two-factor theory and analysis of review comments. We employ two linguistic approaches for processing the content of the comments. The results of a series of analyses of variance indicate that both user groups and domains significantly affect user-perceived usability and affection, while there is no effect of user groups on the perceived usefulness of mobile applications. Based on the findings of the study, notable implications and a few limitations are presented. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Mobile augmented reality; Review analysis; Two-factor theory; User satisfaction,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,MagicChem: a MR system based on needs theory for chemical experiments,VRS - Virtual Reality,B,"Real chemical experiments may be dangerous or pollute the environment; meanwhile, the preparation of drugs and reagents is time-consuming. Due to the above-mentioned reasons, few experiments can be actually operated by students, which is not conducive to the chemistry learning and the phenomena principle understanding. Recently, due to the impact of Covid-19, many schools adopt online teaching, which is even more detrimental to students’ learning of chemistry. Fortunately, MR(mixed reality) technology provides us with the possibility of solving the safety issues and breaking the space-time constraints, while the theory of human needs (Maslow’s hierarchical needs) provides us with a way to design a comfortable and stimulant MR system with realistic visual presentation and interaction. The paper combines with the theory of human needs to propose a new needs model for virtual experiment. Based on this needs model, we design and develop a comprehensive MR system called MagicChem, which offers a robust 6-DoF interactive and illumination consistent experimental space with virtual-real occlusion, supporting realistic visual interaction, tangible interaction, gesture interaction with touching, voice interaction, temperature interaction, olfactory interaction and virtual human interaction. User study shows that MagicChem satisfies the needs model better than other MR experimental environments that partially meet the needs model. In addition, we explore the application of the needs model in VR environment. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Chemical education; Mixed reality; Multi-camera collaboration; Virtual-real interaction; Virtual-real occlusion,Abstract_Keywords,True,
Scopus,journalPaper,2022,Exploiting fashion x-commerce through the empowerment of voice in the fashion virtual reality arena: Integrating voice assistant and virtual reality technologies for fashion communication,VRS - Virtual Reality,B,"The ongoing development of eXtended Reality (XR) technologies is supporting a rapid increase of their performances along with a progressive decrease of their costs, making them more and more attractive for a large class of consumers. As a result, their widespread use is expected within the next few years. This may foster new opportunities for e-commerce strategies, giving birth to an XR-based commerce (x-commerce) ecosystem. With respect to web and mobile-based shopping experiences, x-commerce could more easily support brick-and-mortar store-like experiences. One interesting and consolidated one amounts to the interactions among customers and shop assistants inside fashion stores. In this work, we concentrate on such aspects with the design and implementation of an XR-based shopping experience, where vocal dialogues with an Amazon Alexa virtual assistant are supported, to experiment with a more natural and familiar contact with the store environment. To verify the validity of such an approach, we asked a group of fashion experts to try two different XR store experiences: with and without the voice assistant integration. The users are then asked to answer a questionnaire to rate their experiences. The results support the hypothesis that vocal interactions may contribute to increasing the acceptance and comfortable perception of XR-based fashion shopping. © 2021, The Author(s).",Fashion retail; Mixed reality; Voice assistant; X-commerce,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,A VR-based volumetric medical image segmentation and visualization system with natural human interaction,VRS - Virtual Reality,B,"Volume rendering produces informative two-dimensional (2D) images from a 3-dimensional (3D) volume. It highlights the region of interest and facilitates a good comprehension of the entire data set. However, volume rendering faces a few challenges. First, a high-dimensional transfer function is usually required to differentiate the target from its neighboring objects with subtle variance. Unfortunately, designing such a transfer function is a strenuously trial-and-error process. Second, manipulating/visualizing a 3D volume with a traditional 2D input/output device suffers dimensional limitations. To address all the challenges, we design NUI-VR2, a natural user interface-enabled volume rendering system in the virtual reality space. NUI-VR2 marries volume rendering and interactive image segmentation. It transforms the original volume into a probability map with image segmentation. A simple linear transfer function will highlight the target well in the probability map. More importantly, we set the entire image segmentation and volume rendering pipeline in an immersive virtual reality environment with a natural user interface. NUI-VR2 eliminates the dimensional limitations in manipulating and perceiving 3D volumes and dramatically improves the user experience. © 2021, The Author(s).",Medical image segmentation; Natural user interaction; Transfer function; Virtual reality; Volume rendering,Abstract_Keywords,True,
Scopus,journalPaper,2022,PE-DLS: a novel method for performing real-time full-body motion reconstruction in VR based on Vive trackers,VRS - Virtual Reality,B,"Real-time full-body motion capture (MoCap) is becoming necessary for enabling natural interactions and creating deeper immersion in virtual reality (VR). To reduce the cost and complexity of MoCap systems, some studies attempt to track only the joint data of root and end effectors and reconstruct full-body motion by solving inverse kinematics (IK) problems. However, ensuring the accuracy of full-body motion reconstruction in real-time is challenging because the problem is inherently under-constrained. In this paper, we propose PE-DLS, a novel method to perform full-body motion reconstruction in two stages: pose estimation (PE) and damped least squares (DLS) optimization. First, we use analytical IK solvers to estimate the spine and limbs in sequence. To further improve model accuracy, we use the DLS method to optimize the results obtained from PE. To evaluate the model performance, we compare it with other methods in terms of the reconstruction error and computational time of full-body reconstruction via testing on publicly available datasets. These results indicate that PE-DLS outperforms other methods in terms of the mean per joint position error (2.11 cm) and mean per joint rotation error (10.75°) with low time cost (1.65 ms per frame). Furthermore, we implement a full-body MoCap system based on an HTC Vive headset and five Vive trackers. Live demos and qualitative comparisons show that our system achieves comparable quality to the commercial MoCap system. With high accuracy and low time cost, PE-DLS contributes to construct a real-time MoCap system in VR. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Full-body motion reconstruction; Inverse kinematics; Motion capture; Virtual reality; Vive tracker,Abstract_Keywords,True,
Scopus,journalPaper,2022,Task-specific virtual reality training on hemiparetic upper extremity in patients with stroke,VRS - Virtual Reality,B,"Task-specific training has been proven to be effective in promoting recovery of the hemiparetic upper extremities after a stroke. This study was to develop a task-specific VR (TS-VR) program using a leap motion controller device and the Unity3D game engine to promote recovery of the hemiparetic upper extremity in patients with stroke based on a hierarchy of seven functional tasks in the functional test for the hemiplegic upper extremity (FTHUE). The final version of the TS-VR was tested on 20 patients suffering from chronic stroke with upper-extremity hemiparesis over 2 weeks, 5 sessions per week, 30 min per session. Outcomes were assessed using the Fugl-Meyer assessment-upper extremity score (FMA-UE), the Wolf motor function test (WMFT), and the motor activity log (MAL) at the first (week 0), last (week 2), and follow-up sessions (week 5). Patients’ arm impairments were stratified into lower (levels 1–4) and higher (levels 5–7) functioning groups according to the FTHUE. Significant improvements were found after TS-VR training in FMA-UE total score and its subscores, and WFMT score among the three time occasions (p = 0.000), but no significant effect on grip strength was found. The higher-functioning group benefited more from the TS-VR, as indicated in outcome measures as well as amount of use score in MAL, but this was not the case for those in the lower-functioning group. Our findings show the TS-VR training was useful for upper-extremity recovery in patients with chronic stroke. It has potential to be applied in clinical settings in future. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Evaluation; Leap motion controller; Rehabilitation; Task-specific training; Training stroke; Upper limb; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2022,The ObReco-360°: a new ecological tool to memory assessment using 360° immersive technology,VRS - Virtual Reality,B,"One important feature of a neuropsychological test is its ecological validity, which defines how much patients’ test scores are linked to real-life functioning. However, many of the currently available neuropsychological tools show low to moderate levels of ecological validity. Virtual reality (VR) emerged as a possible solution that might enhance the ecological value of standard paper-and-pencil tests, thanks to the possibility of simulating realistic environments and situations where patients can behave as they do in real life. Moreover, a recent kind of virtual environments, the 360° spherical photos and videos, seems to guarantee high levels of graphical realism and lower technical complexity than standard VR, despite their limitations concerning interactive design. In this pilot study, we tested the possible application of 360° technology for the assessment of memory, developing an adaptation of a standardized test. We focused on Free Recall and Recognition accuracies as indexes of memory function, confronting and correlating the performances obtained by the participants in the standard and in the 360° test. The results, even if preliminary, support the use of 360° technology for enhancing the ecological value of standard memory assessment tests. © 2021, The Author(s).",360° video; Assessment; Memory; Object recognition; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Exploring the design space of eyes-free target acquisition in virtual environments,VRS - Virtual Reality,B,"Supporting smooth target acquisition is an important objective in immersive virtual reality (VR) environments. However, users were obliged to search for objects relying on the vision channel in traditional VR systems. Such eyes-engaged technologies may significantly degrade the interaction efficiency and user experience, particularly when users have to turn their head frequently to search for virtual objects in the limited field of view of a head-mounted display. In this paper, we report a two-stage study which investigates the capability of VR users to acquire spatial targets without eye engagement (i.e., eyes-free target acquisition). First, we measure the eyes-free performance of users in terms of control accuracy and subjective task load. Second, we evaluate the effects of eyes-free acquisition on memory capacity, spatial offset, and task completion time in the context of a VR game. Starting from a set of 54 spatial positions, we identify 18 optimal locations (half on the left side of the user’s body and half on the right) that allow both accurate and comfortable target acquisition without visual attention. After a short training period, users could accurately and quickly acquire 17 targets in a VR game with an average offset of 10.5 cm and an average completion time of 2.7 s. According to our results, we suggest how to optimize the spatial layout, number of targets, target locations, and interaction techniques for eyes-free acquisition in VR applications. Our work can serve as a foundation for future development of eyes-free methods of target acquisition in VR. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Eyes-free target acquisition; Games; Immersive environments; Spatial memory; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Performance enhancement of facial electromyogram-based facial-expression recognition for social virtual reality applications using linear discriminant analysis adaptation,VRS - Virtual Reality,B,"Recent studies have indicated that facial electromyogram (fEMG)-based facial-expression recognition (FER) systems are promising alternatives to the conventional camera-based FER systems for virtual reality (VR) environments because they are economical, do not depend on the ambient lighting, and can be readily incorporated into existing VR headsets. In our previous study, we applied a Riemannian manifold-based feature extraction approach to fEMG signals recorded around the eyes and demonstrated that 11 facial expressions could be classified with a high accuracy of 85.01%, with only a single training session. However, the performance of the conventional fEMG-based FER system was not high enough to be applied in practical scenarios. In this study, we developed a new method for improving the FER performance by employing linear discriminant analysis (LDA) adaptation with labeled datasets of other users. Our results indicated that the mean classification accuracy could be increased to 89.40% by using the LDA adaptation method (p <.001, Wilcoxon signed-rank test). Additionally, we demonstrated the potential of a user-independent FER system that could classify 11 facial expressions with a classification accuracy of 82.02% without any training sessions. To the best of our knowledge, this was the first study in which the LDA adaptation approach was employed in a cross-subject manner. It is expected that the proposed LDA adaptation approach would be used as an important method to increase the usability of fEMG-based FER systems for social VR applications. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Facial electromyogram; Facial-expression recognition; Linear discriminant analysis adaptation; Riemannian manifolds; Social virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Trick the body trick the mind: avatar representation affects the perception of available action possibilities in virtual reality,VRS - Virtual Reality,B,"In immersive Virtual Reality (VR), your brain can trick you into believing that your virtual hands are your real hands. Manipulating the representation of the body, namely the avatar, is a potentially powerful tool for the design of innovative interactive systems in VR. In this study, we investigated interactive behavior in VR by using the methods of experimental psychology. Objects with handles are known to potentiate the afforded action. Participants tend to respond faster when the handle is on the same side as the responding hand in bi-manual speed response tasks. In the first experiment, we successfully replicated this affordance effect in a Virtual Reality (VR) setting. In the second experiment, we showed that the affordance effect was influenced by the avatar, which was manipulated by two different hand types: (1) hand models with full finger tracking that are able to grasp objects, and (2) capsule-shaped—fingerless—hand models that are not able to grasp objects. We found that less than 5 mins of adaptation to an avatar, significantly altered the affordance perception. Counter intuitively, action planning was significantly shorter with the hand model that is not able to grasp. Possibly, fewer action possibilities provided an advantage in processing time. The presence of a handle speeded up the initiation of the hand movement but slowed down the action completion because of ongoing action planning. The results were examined from a multidisciplinary perspective and the design implications for VR applications were discussed. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Affordance; Avatars; Virtual hands; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Robustness and static-positional accuracy of the SteamVR 1.0 virtual reality tracking system,VRS - Virtual Reality,B,"The use of low-cost immersive virtual reality systems is rapidly expanding. Several studies started to analyse the accuracy of virtual reality tracking systems, but they did not consider in depth the effects of external interferences in the working area. In line with that, this study aimed at exploring the static-positional accuracy and the robustness to occlusions inside the capture volume of the SteamVR (1.0) tracking system. To do so, we ran 3 different tests in which we acquired the position of HTC Vive PRO Trackers (2018 version) on specific points of a grid drawn on the floor, in regular tracking conditions and with partial and total occlusions. The tracking system showed a high inter- and intra-rater reliability and detected a tilted surface with respect to the floor plane. Every acquisition was characterised by an initial random offset. We estimated an average accuracy of 0.5 ± 0.2 cm across the entire grid (XY-plane), noticing that the central points were more accurate (0.4 ± 0.1 cm) than the outer ones (0.6 ± 0.1 cm). For the Z-axis, the measurements showed greater variability and the accuracy was equal to 1.7 ± 1.2 cm. Occlusion response was tested using nonparametric Bland–Altman statistics, which highlighted the robustness of the tracking system. In conclusion, our results promote the SteamVR system for static measures in the clinical field. The computed error can be considered clinically irrelevant for exercises aimed at the rehabilitation of functional movements, whose several motor outcomes are generally measured on the scale of metres. © 2021, The Author(s).",Accuracy testing; HTC Vive PRO; SteamVR tracking; Tracker occlusion; Validation Study [Publication Type]; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Motion control of virtual reality based on an inertia-based sensing mechanism and a novel approach to redirected walking,VRS - Virtual Reality,B,"This research presents a motion control algorithm for constructing a portable virtual reality system, which can operate in any indoor or outdoor open space without the need for support from any pre-installed infrastructure. The real head and foot motions continuously measured by inertial sensors during natural walking are used as a part of the inputs to the algorithm to control the virtual walking motions of the user. In conjunction with such control, a novel approach to redirected walking is incorporated in the algorithm to continuously adjust the rotation of the virtual environment to redirect the user away from the boundary (i.e., walls and objects) of the real environment. Such an approach, namely the relative approach, adopts the directions and distances of the boundary relative to the user (i.e., the relative local information) instead of the absolute positions and orientations of the user for performing redirection. A ranging sensor is used for collecting the relative local information. The effectiveness of the algorithm was experimentally verified and demonstrated. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Inertial sensor; Motion control; Ranging sensor; Redirected walking; Virtual environment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Towards socialVR: evaluating a novel technology for watching videos together,VRS - Virtual Reality,B,"Social VR enables people to interact over distance with others in real-time. It allows remote people, typically represented as avatars, to communicate and perform activities together in a shared virtual environment, extending the capabilities of traditional social platforms like Facebook and Netflix. This paper explores the benefits and drawbacks provided by a lightweight and low-cost Social VR platform (SocialVR), in which users are captured by several cameras and reconstructed in real-time. In particular, the paper contributes with (1) the design and evaluation of an experimental protocol for Social VR experiences; (2) the report of a production workflow for this new type of media experiences; and (3) the results of experiments with both end-users (N = 15 pairs) and professionals (N = 22 companies) to evaluate the potential of the SocialVR platform. Results from the questionnaires and semi-structured interviews show that end-users rated positively towards the experiences provided by the SocialVR platform, which enabled them to sense emotions and communicate effortlessly. End-users perceived the photo-realistic experience of SocialVR similar to face-to-face scenarios and appreciated this new creative medium. From a commercial perspective, professionals confirmed the potential of this communication medium and encourage further research for the adoption of the platform in the commercial landscape. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Evaluation Protocol; Presence; Social VR; Togetherness; Virtual Reality; Volumetric Media,Keywords,True,
Scopus,journalPaper,2023,A virtual reality bus ride as an ecologically valid assessment of balance: a feasibility study,VRS - Virtual Reality,B,"Balance disorders can have substantial adverse implications on the performance of daily activities and lead to an increased risk of falls, which often have severe negative consequences for older adults. Quantitative assessment through computerized force plate-based posturography enables objective assessment of postural control but could not successfully represent specific abilities required during daily activities. The use of virtual reality (VR) could improve the representative design of functional activities and increase the ecological validity of posturographic tests, which would enhance the transferability of results to the real world. In this work, we investigate the feasibility of a simulated bus ride experienced in a surround-screen VR system to assess balance with increased ecological validity. Participants were first evaluated with a posturography test and then with the VR-based bus ride test, while the reactions of their centre of pressure were registered. Lastly, participants provided self-reported measures of the elicited sense of presence during the test. A total of 16 healthy young adults completed the study. Results showed that the simulation could elicit significant medial–lateral excursions of the centre of pressure in response to variations in the optical flow. Furthermore, these responses' amplitude negatively correlated with the participants' posturography excursions when fixating a target. Although the sense of presence was moderate, likely due to the passive nature of the test, the results support the feasibility of our proposed paradigm, based in the context of a meaningful daily living activity, in assessing balance control components. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Balance assessment; Ecological validity; Posturography; Virtual reality; Visual motion,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Virtual reality exergame for supplementing multimodal pain therapy in older adults with chronic back pain: a randomized controlled pilot study,VRS - Virtual Reality,B,"Immersive Virtual Reality (VR) with head-mounted displays (HMD) can be a promising tool for increasing adherence to exercise in older adults. However, there is little known about the effectiveness of an interactive multimodal therapy in VR for older chronic back pain (CBP) patients. The aim of the exploratory randomized controlled trial was to examine the preliminary effectiveness of a VR multimodal therapy for older adults with CBP in a laboratory setting over a period of four weeks. The intervention group (IG; n = 11) received a multimodal pain therapy in VR (movement therapy and psychoeducation) and the control group (CG; n = 11) received a conventional multimodal pain therapy (chair-based group exercises and psychoeducation in a group setting). Although the VR therapy (IG) did not reach the pain intensity reduction of the CG (IG: MD = 0.64, p =.535; CG: MD = 1.64, p =.07), both groups showed a reduction in pain intensity on the Numeric Rating Scale. The functional capacity in the IG improved from Visit 1, x¯ = 73.11% to Visit 2, x¯ = 81.82% (MD = 8.71%; p =.026). In the changes of fear avoidance beliefs and general physical and mental health, no significance was achieved in either group. Although the IG did not reach a significant pain intensity reduction compared to the CG, the results of the present study showed that a pain intensity reduction can be achieved with the current VR application. © 2022, The Author(s).",Chronic back pain; Multimodal pain therapy; Physical therapy; Psychotherapy; Serious gaming; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Immersive audio-visual scene reproduction using semantic scene reconstruction from 360 cameras,VRS - Virtual Reality,B,"As personalised immersive display systems have been intensely explored in virtual reality (VR), plausible 3D audio corresponding to the visual content is required to provide more realistic experiences to users. It is well known that spatial audio synchronised with visual information improves a sense of immersion but limited research progress has been achieved in immersive audio-visual content production and reproduction. In this paper, we propose an end-to-end pipeline to simultaneously reconstruct 3D geometry and acoustic properties of the environment from a pair of omnidirectional panoramic images. A semantic scene reconstruction and completion method using a deep convolutional neural network is proposed to estimate the complete semantic scene geometry in order to adapt spatial audio reproduction to the scene. Experiments provide objective and subjective evaluations of the proposed pipeline for plausible audio-visual VR reproduction of real scenes. © 2021, The Author(s).",3D reconstruction and completion; Audio-visual scene reproduction; Scene understanding; Spatial audio,Abstract,True,
Scopus,journalPaper,2022,Immersive virtual reality as an empirical research tool: exploring the capability of a machine learning model for predicting construction workers’ safety behaviour,VRS - Virtual Reality,B,"In recent years, research has found that people have stable predispositions to engage in certain behavioural patterns to work safely or unsafely, which vary among individuals as a function of their personality features. In this regard, an innovative machine learning model has been recently developed to predict workers’ behavioural tendency based on personality factors. This paper presents an empirical evaluation of the model’s prediction performance (i.e. the degree to which the model can generate similar results compared to reality) to address the issue of the model’s usability before it is implemented in real situations. As virtual reality allows a good grip on fidelity resembling real-world situations, it can stimulate more natural behaviour responses from participants to increase ecological validity of experimental results. Thus, we implemented a virtual reality experimentation environment to assess workers’ safety behaviour. The model’s prediction capability was then evaluated by comparing the model prediction results and workers’ safety behaviour as assessed in virtual reality. The comparison results showed that the model predictions on two dimensions of workers’ safety behaviour (i.e. task and contextual performance) were in good agreement with the virtual reality experimental results, with Spearman correlation coefficients of 79.7% and 87.8%, respectively. The machine learning model thus proved to have good prediction capability, which allows the model to help identify vulnerable workers who are prone to undertake unsafe behaviours. The findings also suggest that virtual reality is a promising method for measuring workers’ safety behaviour as it can provide a realistic and safe environment for experimentation. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Construction sector; Machine learning; Safety behaviour; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Could virtual reality applications pose real risks to children and adolescents? A systematic review of ethical issues and concerns,VRS - Virtual Reality,B,"Virtual reality technologies (VRTs) are high-tech human–computer interfaces used to develop digital content and can be applied to multiple different areas, often offering innovative solutions to existing problems. A wide range of digital games is being also developed with VRTs and together with their components, the games' structural elements are appealing to children and engaging them more in virtual worlds. Our research interest is directed towards children's development and the effects of VRTs within gaming environments. Contemporary psychology studies perceive human development as a holistic and lifelong process with important interrelationships between physical, mental, social and emotional aspects. For the objectives and scope of this work, we examine children development across three domains: physical, cognitive and psychosocial. In this context, the authors review the literature on the impact of VRTs on children, in terms of software and hardware. Since research requires an wide-ranging approach, we study the evidence reported on the brain and neural structure, knowledge, behaviour, pedagogy, academic performance, and wellness. Our main concern is to outline the emerging ethical issues and worries of parents, educators, ophthalmologists, neurologists, psychologists, paediatricians and all relevant scientists, as well as the industry’s views and actions. The systematic review was performed on the databases Scopus, IEEE Xplore, PubMed, and Google Scholar from 2010 to 2020 and 85 studies were selected. The review concluded that findings remain contradictory especially for the psychosocial domain. Official recommendations from organizations and well-documented researches by academics on child well-being are reassuring if health and safety specifications and particularly the time limit are met. Research is still ongoing, constantly updated and consist of a priority for the scientific community given that technology evolves. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Child development; Ethical issues; Games; Impact of technology; Safety; Virtual reality technologies,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Mixed reality or LEGO game play? Fostering social interaction in children with Autism,VRS - Virtual Reality,B,"This study extends the previous research in which it has been shown that a mixed reality (MR) system fosters social interaction behaviours (SIBs) in children with Autism Spectrum Condition (ASC). When comparing this system to a LEGO-based non-digital intervention, it has been observed that an MR system effectively mediates a face-to-face play session between a child with ASC and a child without ASC providing new specific advantageous properties (e.g. not being a passive tool, not needing to be guided by the therapist). Considering the newly collected multimodal data totaling to 72 children (36 trials of dyads, child with ASC/child without ASC), a first goal of the present study is to apply detailed statistical inference and machine learning techniques to extensively evaluate the overall effect of this MR system, when compared to the LEGO condition. This goal also includes the analysis of psychophysiological data and allows the context-driven triangulation of the multimodal data which is operationalized by (i) video-coding of SIBs, (ii) psychophysiological data, and (iii) system logs of user-system events. A second goal is to show how SIBs, taking place in these experiences, are influenced by the internal states of the users and the system. SIBs were measured by video-coding overt behaviours (Initiation, Response and Externalization) and with self-reports. Internal states were measured using a wearable device designed by the FuBIntLab (Full-Body Interaction Lab) to acquire: Electrocardiogram (ECG) and Electrodermal Activity (EDA) data. Affective sliders and State Trait Anxiety Scale questionnaires were used as self-reports. Repeated-measures design was chosen with two conditions, the MR environment and the traditional therapy LEGO. The results show that the MR system has a positive effect on SIBs when compared to the LEGO condition, with an added advantage of being more flexible. © 2021, The Author(s).",Children with Autism; Embodied interaction; Mixed reality; Multi-modal evaluation; Psychophysiology; Social initiation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Digital shinrin-yoku: do nature experiences in virtual reality reduce stress and increase well-being as strongly as similar experiences in a physical forest?,VRS - Virtual Reality,B,"Shinrin-yoku or forest bathing refers to a therapeutic, immersive nature experience that aids to improve well-being. The goal of the current research was to compare the effects of a physical urban nature versus virtual nature experience on stress, affect, vitality, and restoration. Previous research suggested that an immersive nature experience—such as shinrin-yoku—can be beneficial for health, but direct comparisons between physical and virtual reality (VR) experiences are scarce. In the current study, fifty participants navigated self-paced through a forest scene that was either an urban physical forest or an immersive VR forest with similar characteristics as the physical one. Before and after the intervention, we measured positive and negative affect, subjective vitality, and perceived daily stress. After the intervention, we measured perceived restorative outcomes. Results revealed that both VR and physical nature experience resulted in expected effects on well-being indicators: Affect was more positive and less negative, subjective vitality increased slightly, and stress decreased slightly after both interventions. There were no significant differences between the two settings on any of the variables, but slightly stronger effect sizes over time within the physical condition. Overall, these findings suggest that immersive VR nature experiences can have restoration effects similar to physical nature experiences, suggesting intervention strategies when physical nature options are scarce. © 2022, The Author(s).",Restoration; Shinrin-yoku; Stress; Urban forest; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Evaluation of visual-induced motion sickness from head-mounted display using heartbeat evoked potential: a cognitive load-focused approach,VRS - Virtual Reality,B,"Based on sensory conflict theory, motion sickness is strongly related to the information processing capacity or resources of the brain to cope with the multi-sensory stimuli experienced by watching virtual reality (VR) content. The purpose of this research was to develop a method of measuring motion sickness using the heart-evoked potential (HEP) phenomenon and propose new indicators for evaluating motion sickness. Twenty-eight undergraduate volunteers of both genders (14 females) participated in this study by watching VR content on both 2D and head-mounted devices (HMD) for 15 min. The responses of HEP measures such as alpha power, latency, and amplitude of first and second HEP components were compared using paired t-tests and ANCOVA. This study confirmed that motion sickness leads to a decline in cognitive processing, as demonstrated by increasing in alpha power of HEP. Also, the proposed indicators such as latency and amplitude of the HEP waveform showed significant differences during the experience of motion sickness and exhibited high correlations with alpha power measures. Latencies of the first HEP component, in particular, are recommended as better quantitative evaluators of motion sickness than other measures, following the multitrait-multimethod matrix. The proposed model for motion sickness was implemented in a support vector machine with a radial basis function kernel, and validated on twenty new participants. The accuracy, F1 score, precision, recall, and area under the curve (AUC) of the motion-sickness classification results were 0.875, 0.865, 0.941, 0.8, and 0.962, respectively. © 2021, The Author(s).",Cognitive load; Head-mounted display (HMD); Heartbeat evoked potential (HEP); Heart–brain synchronization; Visual-induced motion sickness (VIMS),Abstract,True,
Scopus,journalPaper,2022,Immersive machine learning for social attitude detection in virtual reality narrative games,VRS - Virtual Reality,B,"People can understand how human interaction unfolds and can pinpoint social attitudes such as showing interest or social engagement with a conversational partner. However, summarising this with a set of rules is difficult, as our judgement is sometimes subtle and subconscious. Hence, it is challenging to program Non-Player Characters (NPCs) to react towards social signals appropriately, which is important for immersive narrative games in Virtual Reality (VR). We collaborated with two game studios to develop an immersive machine learning (ML) pipeline for detecting social engagement. We collected data from participants-NPC interaction in VR, which was then annotated in the same immersive environment. Game design is a creative process and it is vital to respect designer’s creative vision and judgement. We therefore view annotation as a key part of the creative process. We trained a reinforcement learning algorithm (PPO) with imitation learning rewards using raw data (e.g. head position) and socially meaningful derived data (e.g. proxemics); we compared different ML configurations including pre-training and a temporal memory (LSTM). The pre-training and LSTM configuration using derived data performed the best (84% F1-score, 83% accuracy). The models using raw data did not generalise. Overall, this work introduces an immersive ML pipeline for detecting social engagement and demonstrates how creatives could use ML and VR to expand their ability to design more engaging experiences. Given the pipeline’s results for social engagement detection, we generalise it for detecting human-defined social attitudes. © 2022, The Author(s).",Artificial intelligence; Expressive body language; Gaming; Human–computer interaction; Virtual agents; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Sensorimotor adaptation in VR: magnitude and persistence of the aftereffect increase with the number of interactions,VRS - Virtual Reality,B,"In both prism and virtual reality experiments, it has been observed that visual displacement leads to an adaptation of the sensorimotor system. A characteristic of adaptation is the occurrence of the aftereffect, which is the spatial deviation of the movements in the direction opposite to the visual displacement. Prism adaptation experiments have shown that a higher number of interactions lead to an increased magnitude and persistence of the aftereffect. The aim of the present study was to investigate this relationship in virtual reality. After baseline measurement, the virtual environment was displaced visually. During this adaptation phase, the participants performed either zero, five, or thirty-five pointing movements. Afterwards, all participants performed the pointing movements without the visual displacement in the virtual environment. Performing five pointing movements during the adaptation phase was already sufficient to produce an aftereffect. With thirty-five pointing movements, both magnitude and persistence of the aftereffect increased. These results replicate studies of prism adaptation. Considering this, we briefly discuss the suitability of virtual reality as a research tool to study prism adaptation. © 2022, The Author(s).",Aftereffect; Number of interactions; Sensorimotor adaptation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Museum beyond physical walls: an exploration of virtual reality-enhanced experience in an exhibition-like space,VRS - Virtual Reality,B,"New media and technology are changing the museum experience in the twenty-first century. One such change is that of hybrid space in museums. A museum hybrid space combines physical artifacts co-located with virtual and augmented reality displays. Although the theory and technology exist to provide museums with hybrid space, there are few efforts to put hybrid space, particularly those that utilize commercial video games, into practice. The goal of this research is to explore how a certain type of museum hybrid space, namely, a virtual reality-enhanced environment relying on a commercial video game, can support and improve audience experiences. To reach this goal, a cognitive model is applied in the design of an experimental context that creates an exhibition-like environment for the viewer-participants. In the experiment, a virtual reality-enhanced environment is compared with two environments relying on commonly used media. Results show improvements in viewer-participants' experiences in terms of cognitive and edutainment aspects. Relying on a commercial video game, the VR-enhanced environment stimulated emotions and increased engagement in viewer-participants while helping them enjoy learning. The experimental context of this research can only approximate the full real-world experience that a museum visitor would have. However, the experimental context does provide the basic elements that a viewer would expect and associate with an exhibition, such as, objects for examination, labels, and didactic supports. Results from this research can encourage further investigations of hybrid space in other environments relying on various media types. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Commercial VR game; Exploratory research; Hybrid space; Museum experience; Qualitative approach; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,"Extended reality technologies in small and medium-sized European industrial companies: level of awareness, diffusion and enablers of adoption",VRS - Virtual Reality,B,"Augmented reality (AR) and virtual reality (VR), collectively referred to as “extended reality” (XR), have begun to diffuse in industry. However, the current levels of awareness, perceived limitations, and use of AR and VR, as well as the potential differences on these aspects between these technologies are still not well known. Moreover, it is unknown whether small and medium-sized enterprises (SMEs) differ from large companies on these issues. This research employed a mixed methods research design to address this gap by carrying out a cross-sectional survey (n = 208) to gauge European industrial companies’ level of AR and VR awareness and adoption, and by interviewing 45 companies in nine European countries in order to identify critical enabling factors in the adoption of XR for SMEs. Results show no statistical difference between the respondents’ perceptions toward AR and VR or in their use levels. Thus, examining AR and VR under the umbrella term XR seems justified, especially in the context of their organizational use. However, larger companies were found to be using XR more than SMEs. Analysis of interviews based on the technology–organization–environment framework also yielded several enabling factors affecting XR adoption and specified whether they are particularly highlighted in the SME context. Overall, this paper contributes to XR research by providing a holistic multi-country overview that highlights key issues for managers aiming to invest in these technologies, as well as critical organizational perspectives to be considered by scholars. © 2022, The Author(s).",Augmented reality; Extended reality; Industry 4.0; Small and medium-sized enterprises; Technology adoption; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Effects of display lag on vection and presence in the Oculus Rift HMD,VRS - Virtual Reality,B,"Head-mounted display (HMD)-based virtual reality (VR) is ideally suited for presence and generating compelling visual experiences of self-motion, but users can suffer from side effects associated with head-to-display lag. We used the Oculus Rift HMD (consumer release – CV1) to simulate forward self-motion in depth. Observers made continuous yaw head movements at approximately 0.5 Hz or 1.0 Hz while viewing these self-motion simulations. We examined the perceptual effects of increasing the display lag, by adding lag to the baseline lag of the system (estimated to be approximately 5.3 ms or 0.5 frames per second). We found that increasing the head-to-display lag up to 212 ms reduced the presence and the strength of vection. In addition, faster (1.0 Hz) head oscillations were found to generate weaker presence and vection in the virtual environment than the slower (0.5 Hz) head oscillations. We also found that a positive correlation between vection and presence (found previously) persists across a wide range of head-to-display lags, and, increasing lag from a very low baseline level still impaired both experiences. Both vection and presence in virtual environments can therefore be impaired by either increasing head-display lag or making more rapid angular head movements. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Head-mounted display; Presence; Vection; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Analysis of translation gains in virtual reality: the limits of space manipulation,VRS - Virtual Reality,B,"Physically walking in Virtual Reality (VR) creates a truly compelling user experience. Many navigation techniques for VR have been presented in the literature. The room-scale technique allows a natural and intuitive navigation through physically walking in the virtual environment, but it is limited to the available physical space. The dimensions of the virtual space can be extended by applying translation gains, i.e., a mapping of physical movements to virtual ones. Previous works have studied the threshold at which users detect the spatial manipulation. However, little is known about the user experience and usability beyond this threshold. This paper presents a user study with 110 participants that explores the effect of using translation gains beyond the detection threshold on cybersickness and presence. The objective of this paper is to assess whether translations gains higher than the ones used in redirection techniques can be used for walking in a bigger virtual environment than the tracking area without influencing user comfort and experience. Results showed no difference in presence scores and minimal cybersickness symptoms when using no gain and a 1.5 × gain, but started to be of concern with a 2 × gain. This contribution supports the use of translation gains and the development of novel applications that allow the exploration of bigger virtual environments, thus improving presence and user experience. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Human–computer interaction; Locomotion techniques; Translation gains; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Configuring a VR simulator for the evaluation of advanced human–machine interfaces for hydraulic excavators,VRS - Virtual Reality,B,"This study is aimed at evaluating the impact of different technical solutions of a virtual reality simulator to support the assessment of advanced human–machine interfaces for hydraulic excavator based on a new coordinated control paradigm and haptic feedbacks. By mimicking the end-effector movements, the control is conceived to speed up the learning process for novice operators and to reduce the mental overload on those already trained. The design of the device can fail if ergonomics, usability and performance are not grounded on realistic simulations where the combination of visual, auditory and haptic feedbacks make the users feel like being in a real environment rather than a computer-generated one. For this reason, a testing campaign involving 10 subjects was designed to discriminate the optimal set-up for the hardware to ensure a higher immersion into the VR experience. Both the audio–video configurations of the simulator (head-mounted display and surround system vs. monitor and embedded speakers) and the two types of haptic feedback for the soil–bucket interaction (contact vs. shaker) are compared in three different scenarios. The performance of both the users and simulator are evaluated by processing subjective and objective data. The results show how the immersive set-up improves the users’ efficiency and ergonomics without putting any extra mental or physical effort on them, while the preferred haptic feedback (contact) is not the more efficient one (shaker). © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Excavator coordinated control; Haptic control; Human–machine interface; Multi-sensory feedbacks; Virtual reality simulator,Abstract_Keywords,True,
Scopus,journalPaper,2022,"Correction to: Scene Walk: a non‑photorealistic viewing tool for first‑person video (Virtual Reality, (2021), 25, 4, (1171-1191), 10.1007/s10055-021-00523-4)",VRS - Virtual Reality,B,"A correction to this paper has been published: https://doi.org/10.1007/s10055-021-00545-y. © 2021, Springer-Verlag London Ltd., part of Springer Nature.",,Title,True,
Scopus,journalPaper,2022,Effectiveness of virtual reality exposure treatment for posttraumatic stress disorder due to motor vehicle or industrial accidents,VRS - Virtual Reality,B,"Virtual reality exposure treatment (VRET) for post-traumatic stress disorder (PTSD) is an emerging treatment. The purpose of this study was to examine the effectiveness and safety of VRET in patients with PTSD due to motor vehicle or industrial accidents. Twenty-six patients with PTSD (19 motor vehicle accidents and 7 industrial accidents) and eighteen subjects without PTSD were enrolled in five VRET sessions that were conducted using a head-mounted display. The VRET was based on systematic desensitization and included psychoeducation and training for breathing and relaxation techniques. The effectiveness of VRET was evaluated using the Posttraumatic Stress Disorder Checklist-Civilian version (PCL-C), Impact of Event Scale-Revised (IES-R), Clinical Global Impression-Severity scale (CGI-S), and Sheehan Disability Scale (SDS). Safety was assessed using the Simulator Sickness Questionnaire and Presence Questionnaire. After controlling for age, sex, marital status, job, economic status, and body mass index, we found that the CGI-S (F = 12.76, p = 0.001), PCL-C (F = 11.87, p = 0.002), IES-R (total score; F-8.31, p = 0.007), and SDS-A (F = 7.53, p = 0.010) scores in the VRET group were lower than those in the control group. Responses to the Simulator Sickness and Presence questionnaires did not differ significantly between the VRET and control groups (p > 0.05). In conclusion, for patients with PTSD due to motor vehicle accidents, VRET is a safe and potentially effective treatment method. Future randomized controlled studies are needed to provide stronger evidence for the effectiveness of VRET in patients with PTSD. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Exposure; Industrial accident; Motor vehicle accident; PTSD; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Taking real steps in virtual nature: a randomized blinded trial,VRS - Virtual Reality,B,"Studies show that green exercise (i.e., physical activity in the presence of nature) can provide the synergistic psychophysiological benefits of both physical exercise and nature exposure. The present study aimed to investigate the extent to which virtual green exercise may extend these benefits to people that are unable to engage in active visits to natural environments, as well as to promote enhanced exercise behavior. After watching a video validated to elicit sadness, participants either performed a treadmill walk while exposed to one of two virtual conditions, which were created using different techniques (360° video or 3D model), or walked on a treadmill while facing a blank wall (control). Quantitative and qualitative data were collected in relation to three overarching themes: “Experience,” “Physical engagement” and “Psychophysiological recovery.” Compared to control, greater enjoyment was found in the 3D model, while lower walking speed was found in the 360° video. No significant differences among conditions were found with respect to heart rate, perceived exertion, or changes in blood pressure and affect. The analysis of qualitative data provided further understanding on the participants’ perceptions and experiences. These findings indicate that 3D model-based virtual green exercise can provide some additional benefits compared to indoor exercise, while 360° video-based virtual green exercise may result in lower physical engagement. © 2022, The Author(s).",Green exercise; Immersive virtual environments; Mixed-methods; Virtual green exercise; Virtual reality,Keywords,True,
Scopus,journalPaper,2022,Multimodal markers for technology-independent integration of augmented reality devices and surgical navigation systems,VRS - Virtual Reality,B,"Augmented reality (AR) permits the visualization of pre-operative data in the surgical field of view of the surgeon. This requires the alignment of the AR device’s coordinate system with the used navigation/tracking system. We propose a multimodal marker approach to align an AR device with a tracking system: in our implementation, an electromagnetic tracking system (EMTS). The solution makes use of a calibration method which determines the relationship between a 2D pattern detected by an RGB camera and an electromagnetic sensor of the EMTS. This allowed the projection of a 3D skull model on its physical counterpart. This projection was evaluated using a monocular camera and an optical see-through device (HoloLens 2) (https://www.microsoft.com/en-us/hololens/) achieving an accuracy of less than 2.5 mm in the image plane of the HoloLens 2 (HL2). Additionally, 10 volunteers participated in a user study consisting of an alignment task of a pointer with 25 projections viewed through the HL2. The participants achieved a mean error of 2.7 1.3 mm and 2.9 2.9∘ in positional and orientation error. This study showcases the feasibility of the approach, provides an evaluation of the alignment, and finally, discusses its advantages and limitations. © 2022, The Author(s).",Alignment; Augmented reality; Calibration; Feasibility studies; Mixed reality; Navigation system; Surgical interventions,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Assessment of consumer VR-headsets’ objective and subjective field of view (FoV) and its feasibility for visual field testing,VRS - Virtual Reality,B,"Virtual reality as a research environment has seen a boost in its popularity during the last decades. Not only the usage fields for this technology have broadened, but also a research niche has appeared as the hardware improved and became more affordable. Experiments in vision research are constructed upon the basis of accurately displaying stimuli with a specific position and size. For classical screen setups, viewing distance and pixel position on the screen define the perceived position for subjects in a relatively precise fashion. However, projection fidelity in HMDs strongly depends on eye and face physiological parameters. This study introduces an inexpensive method to measure the perceived field of view and its dependence upon the eye position and the interpupillary distance, using a super wide angle camera. Measurements of multiple consumer VR headsets show that manufacturers’ claims regarding field of view of their HMDs are mostly unrealistic. Additionally, we performed a “Goldmann” perimetry test in VR to obtain subjective results as a validation of the objective camera measurements. Based on this novel data, the applicability of these devices to test humans’ field of view was evaluated. © 2022, The Author(s).",Eye; Eye relief; Field of view; HMD; Perimetry; Virtual reality; Visual field,Abstract_Keywords,True,
Scopus,journalPaper,2022,Deep-learning-based real-time silent speech recognition using facial electromyogram recorded around eyes for hands-free interfacing in a virtual reality environment,VRS - Virtual Reality,B,"Speech recognition technology is a promising hands-free interfacing modality for virtual reality (VR) applications. However, it has several drawbacks, such as limited usability in a noisy environment or a public place and limited accessibility to those who cannot generate loud and clear voices. These limitations may be overcome by employing a silent speech recognition (SSR) technology utilizing facial electromyograms (fEMGs) in a VR environment. In the conventional SSR systems, however, fEMG electrodes were attached around the user’s lips and neck, thereby creating new practical issues, such as the requirement of an additional wearable system besides the VR headset, necessity of a complex and time-consuming procedure for attaching the fEMG electrodes, and discomfort and limited facial muscle movements of the user. To solve these problems, we propose an SSR system using fEMGs measured by a few electrodes attached around the eyes of a user, which can also be easily incorporated into available VR headsets. To enhance the accuracy of classifying the fEMG signals recorded from limited recording locations relatively far from the phonatory organs, a deep neural network-based classification method was developed using similar fEMG data previously collected from other individuals and then transformed by dynamic positional warping. In the experiments, the proposed SSR system could classify six different fEMG patterns generated by six silently spoken words with an accuracy of 92.53%. To further demonstrate that our SSR system can be used as a hands-free control interface in practical VR applications, an online SSR system was implemented. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Deep learning; Facial electromyography; Human–computer interface; Myoelectric control; Silent speech recognition; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Intelligence augmentation: rethinking the future of work by leveraging human performance and abilities,VRS - Virtual Reality,B,"Nowadays, digitalization has an immense impact on the landscape of jobs. This technological revolution creates new industries and professions, promises greater efficiency and improves the quality of working life. However, emerging technologies such as robotics and artificial intelligence (AI) are reducing human intervention, thus advancing automation and eliminating thousands of jobs and whole occupational images. To prepare employees for the changing demands of work, adequate and timely training of the workforce and real-time support of workers in new positions is necessary. Therefore, it is investigated whether user-oriented technologies, such as augmented reality (AR) and virtual reality (VR) can be applied “on-the-job” for such training and support—also known as intelligence augmentation (IA). To address this problem, this work synthesizes results of a systematic literature review as well as a practically oriented search on augmented reality and virtual reality use cases within the IA context. A total of 150 papers and use cases are analyzed to identify suitable areas of application in which it is possible to enhance employees' capabilities. The results of both, theoretical and practical work, show that VR is primarily used to train employees without prior knowledge, whereas AR is used to expand the scope of competence of individuals in their field of expertise while on the job. Based on these results, a framework is derived which provides practitioners with guidelines as to how AR or VR can support workers at their job so that they can keep up with anticipated skill demands. Furthermore, it shows for which application areas AR or VR can provide workers with sufficient training to learn new job tasks. By that, this research provides practical recommendations in order to accompany the imminent distortions caused by AI and similar technologies and to alleviate associated negative effects on the German labor market. © 2021, The Author(s).",Artificial intelligence; Augmented reality; Changes in labor markets; Future of work; Human-enhancing technologies; Intelligence augmentation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Presence within the virtual reality environment of the international space station,VRS - Virtual Reality,B,"Virtual Reality (VR) is becoming an increasingly effective and powerful medium for learning, especially when applied to environments such as the International Space Station (ISS) that requires acquiring situational awareness (SA) and navigation. Research has shown, and continues to show, an encouraging array of positive learning outcomes when applying VR technology to support and improve learning. Findings include observing positive effects on spatial awareness (SA), astronaut navigation, and presence. Additionally, research has demonstrated that learners enjoy their VR experience and acknowledge the potential of VR in enhancing quality of instruction, especially regarding the immersive realism, or presence, that the virtual environment provides. It is argued that this sense of presence encourages and enhances self-paced learning and permits a more student-centered instructional approach. VR has been used in myriad fields and is not just an educational tool. But as an educational tool VR may be used to support only certain types of learning, because the medium may not work for all kinds of learning. That is because presence comprises several characteristics such as sensory effects, distraction, and realism. Also, the sense of presence may be affected by characteristics of a specific environment. To investigate this latter statement the objective of this study was to evaluate four components of presence (sensory, distraction, realism and involvement) while participants navigated through a VR-rendered ISS environment to assess sense of presence and to determine the level of presence in the virtual world of the ISS. The results indicate that when applied to these visual and kinesthetic modes of learning (the other modes being auditory and reading and writing), VR as an instructional tool is not superior to conventional learning. This result was borne out of the assessment of the comparisons between groups when completing navigation tasks and diagram tasks. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Immersive; Learning; Navigation; Presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Preliminary results of the impact of 3D-visualization resources in the area of graphic expression on the motivation of university students,VRS - Virtual Reality,B,"Augmented reality and virtual reality are innovative technologies applied to the area of graphic expression with increasing influence on the teaching–learning process. Although these innovative resources enable new forms of teaching, it remains unclear how these artificial applications can impact students’ motivation. The aim of this paper was to evaluate how virtual exercises increase the motivation level in different typologies of university students. The sample was composed of graduate (master’s degree) and undergraduate students (three engineering degrees) of the University of Cordoba. These tools were available to students through four devices: mobile phones, tablets, computers and virtual reality goggles. The motivation of the students was evaluated through the modified Instructional Materials Motivation Survey by the attention, relevance, confidence and satisfaction motivational model. The results obtained through a 5-point Likert scale showed that these innovative resources significantly improved the students’ motivation level, especially concerning the ‘relevance’ aspect (M = 4.01; SD = 0.98). The virtual resources also increased the understanding of the exercises and their spatial vision (M = 3.80; SD = 1.14). Of the total sample, 63.83% students considered the virtual reality goggles as the most suitable device to visualize graphic expression exercises. © 2021, The Author(s).",ARCS model; Augmented reality; Expression; Motivation level; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,The predictive role of body image and anti-fat attitudes on attentional bias toward body area in haptic virtual reality environment,VRS - Virtual Reality,B,"Evidence suggests that dissatisfaction with body image in women can be enhanced by exposure to media’s idealized images. The theory of social comparison and the avoidance hypothesis offer contradictory explanations of this relationship. We compare these two theories using a haptic virtual reality environment. We ask 42 female participants to interact with one of four types of virtual humans (VH) randomly assigned to them. The interaction task involves giving a virtual hug to a normal weight or overweight male or female VH. We verify the hypothesis that participants’ satisfaction with particular body parts and their anti-fat attitudes will determine the choice of the body area of the VH they will virtually touch. Our results show that: (1) touching VH lower torso is predicted by less anti-fat attitude, and avoidance of the upper torso and upper limb areas, and (2) touching VH shoulder and upper limbs areas is predicted by concerns with own stomach area and avoidance of VH lower torso and stomach waist areas. Our results tend to support the avoidance hypothesis as well as other research findings on anti-fat attitudes. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Anti-fat attitudes; Avoidance hypothesis; Body image; Haptic; Social comparison theory; Virtual humans,Title_Abstract,True,
Scopus,journalPaper,2022,"Effects of electrical brain stimulation on brain indices and presence experience in immersive, interactive virtual reality",VRS - Virtual Reality,B,"The subjective presence experience in virtual reality (VR) is associated with distinct brain activation patterns. Particularly, the dorsolateral prefrontal cortex (DLPFC) seems to play a central role. We investigated the effects of electric brain stimulation (transcranial direct current, tDCS) on the presence experience as well as on brain activity and connectivity. Thirty-eight participants received either anodal (N = 18) or cathodal (N = 20) stimulation of the DLPFC before interacting in an immersive VR as well as sham stimulation. During VR interaction, EEG and heart rate were recorded. After VR interaction, participants rated their subjective presence experience using standardized questionnaires. Cathodal stimulation led to stronger brain connectivity than sham stimulation. Increased brain connectivity was associated with numerically lower levels of subjective presence. Anodal stimulation did not lead to changes in brain connectivity, and no differences in subjective presence ratings were found between the anodal and sham stimulation. These results indicate that cathodal tDCS over the DLPFC leads to a more synchronized brain state, which might hamper the activity in networks, which are generally associated with the evolvement of the subjective presence experience. Our results underline the importance of the DLPFC for the presence experience in VR. © 2021, The Author(s).",Coherence; EEG; Presence; Transcranial direct current stimulation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,A review of cybersickness in head-mounted displays: raising attention to individual susceptibility,VRS - Virtual Reality,B,"Cybersickness still poses a significant challenge to the widespread usage of virtual reality, leading to different levels of discomfort and potentially breaking the immersive experience. Researchers have attempted to discover the possible fundamental causes of cybersickness for years. Despite the longstanding interest in the research field, inconsistent results have been drawn on the contributing factors and solutions to combating cybersickness. Moreover, little attention has been paid to individual susceptibility. A consolidated explanation remains under development, requiring more empirical studies with robust and reproducible methodologies. This review presents an integrated survey connecting the findings from previous review papers and the state of the art involving empirical studies and participants. A literature review is then presented, focusing on the practical studies of different contributing factors, the pros and cons of measurements, profiles of cybersickness, and solutions to reduce this phenomenon. Our findings suggest a lack of considerations regarding user susceptibility and gender balance in between groups studies. In addition, incongruities among empirical findings raised concerns. We conclude by suggesting points of insights for future empirical investigations. © 2022, The Author(s).",Cybersickness; Individual susceptibility; Literature review; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Pressure-sketch: a tablet-based design system in immersive VR,VRS - Virtual Reality,B,"Sketch design is generally a feasible concept within virtual reality (VR). However, VR controllers suffer from a fundamental problem that limits operability and immersion, which is the difficulty of imitating the texture of natural materials. To solve these challenges, we developed a tablet-based design system named Pressure-Sketch (PSK) around the tactile features of the digital tablet and stylus. PSK could provide an intuitive design workflow for VR. The digital tablet is covered with natural wood to give a more realistic feel. In addition, utilizing the pressure property, users can directly touch the material by the stylus to create lines with varying widths. An evaluated user study revealed that PSK could significantly improve designs’ accuracy and aesthetic quality. Moreover, the system has a better sense of immersion. We are willing to explore the differences between more types of materials on sketching in VR. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Integrating touch-based interactions into the design; Paint system; Virtual environment modeling; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Augmented reality-based border management,VRS - Virtual Reality,B,"A proficient border management has typically been tied to its capability to support information structuring and to make exchanges from the distributed sources. The lack of a proper access interface to information at the right time and the right place to conduct various activities. Augmented reality (AR) has been proposed as an efficient interface in order to improve the efficiency and the effectiveness of activities in the real world. There has only been a limited amount of research that has evaluated the effectiveness and usability of AR in the border management domain. This research aims to evaluate the effectiveness of border management and AR system integration to enhance the activities’ efficiency through improving the information retrieval process. The system development steps were adopted to design, develop, and evaluate the border management AR (BM AR) system. The system contains three AR services that include pointing to border objects, showing the borderline, and locating border objects. The system also integrates the information from different resources in an interoperable way using GIS web services. The results revealed the effectiveness of using AR for border activities, which can reduce the operating costs and effectively, access the required information for doing different activities in the border field. © 2022, The Author(s).",Augmented reality; Border management; Experimental study; Information integration,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,The effectiveness of a virtual reality attention task to predict depression and anxiety in comparison with current clinical measures,VRS - Virtual Reality,B,"Previous studies have revealed that attention and inhibition are impaired in individuals with elevated symptoms of depression and anxiety. Virtual reality (VR)-based neuropsychological assessment may be a valid instrument for assessing attention and inhibition given its higher ecological validity when compared to classical tests. However, it is still unclear as to whether a VR assessment can predict depression and anxiety with the same or higher level of effectiveness and adherence as classical neuropsychological measures. The current study examined the effectiveness of a new VR test, Nesplora Aquarium, by testing participants with low (N = 41) and elevated (N = 41) symptoms of depression and anxiety. Participants completed a continuous performance test where they had to respond to stimuli (species of fish) in a virtual aquarium, as well as paper-and-pencil and computerised tests. Participants’ performance in Nesplora Aquarium was positively associated with classic measures of attention and inhibition, and effectively predicted symptoms of depression and anxiety above and beyond traditional cognitive measures such as psychomotor speed and executive functioning, spatial working memory span. Hence, VR is a safe, enjoyable, effective and more ecological alternative for the assessment of attention and inhibition among individuals with elevated anxiety and depression symptoms. © 2021, The Author(s).",Anxiety; Attention; Cognitive ability; Depression; Response inhibition; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Enhancing mirror therapy via scaling and shared control: a novel open-source virtual reality platform for stroke rehabilitation,VRS - Virtual Reality,B,"Mirror therapy is increasingly used in stroke rehabilitation to improve functional movements of the affected limb. However, the extent of mirroring in conventional mirror therapy is typically fixed (1:1) and cannot be tailored based on the patient’s impairment level. Further, the movements of the affected limb are not actively incorporated in the therapeutic process. To address these issues, we developed an immersive VR system using HTC Vive and Leap Motion, which communicates with our free and open-source software environment programmed using SteamVR and the Unity 3D gaming engine. The mirror therapy VR environment was incorporated with two novel features: (1) scalable mirroring and (2) shared control. In the scalable mirroring, mirror movements were programmed to be scalable between 0 and 1, where 0 represents no movements, 0.5 represents 50% mirroring, and 1 represents 100% mirroring. In shared control, the contribution of the mirroring limb to the movements was programmed to be scalable between 0 to 1, where 0 represents 100% contribution from the mirroring limb (i.e., no mirroring), 0.5 represents 50% of movements from the mirrored limb and 50% of movements from the mirroring limb, and 1 represents full mirroring (i.e., no shared movements). Validation experiments showed that these features worked appropriately. The proposed VR-based mirror therapy is the first fully developed system that is freely available to the rehabilitation science community. The scalable and shared control features can diversify mirror therapy and potentially augment the outcomes of rehabilitation, although this needs to be verified through future experiments. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Illusion; Low cost; Mirror neurons; Motor control; Telehealth; Virtual rehabilitation,Title,True,
Scopus,journalPaper,2022,ViewfinderVR: configurable viewfinder for selection of distant objects in VR,VRS - Virtual Reality,B,"Selection is one of the fundamental user interactions in virtual reality (VR) and 3D user interaction, and raycasting has been one of the most popular object selection techniques in VR. However, the selection of small or distant objects through raycasting has been known to be difficult. To overcome this limitation, this study proposed a new technique called ViewfinderVR for improved selection of distant objects in VR, utilizing a virtual viewfinder panel with a modern adaptation of the through-the-lens metaphor. ViewfinderVR enables faster and more accurate target selection by allowing customization of the interaction space projected onto a virtual panel within reach, and users can select objects reflected on the panel with either ray-based or touch interaction. Experimental results of Fitts’ law-based tests with 20 participants showed that ViewfinderVR outperformed traditional raycasting in terms of task performance (movement time, error rate, and throughput) and perceived workload (NASA-TLX ratings), where touch interaction was superior to ray-based interaction. The associated user behavior was also recorded and analyzed to understand the underlying reasons for the improved task performance and reduced workload. The proposed technique can be used in VR applications to enhance the selection of distant objects. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Mid-air interaction; Selection techniques; Task performance; Virtual reality; Workload,Abstract_Keywords,True,
Scopus,journalPaper,2022,E-faceatlasAR: extend atlas of facial acupuncture points with auricular maps in augmented reality for self-acupressure,VRS - Virtual Reality,B,"Acupuncture is a form of alternative medicine from ancient times and adapted to modern medicine. One typical practice of acupuncture is putting pressure on specific sites (acupuncture points, or acupoints) in the body, called acupressure. Self-acupressure refers to people applying pressure on acupoints using fingers, palms, or special devices to themselves. It has been widely applied because of its convenience and effectiveness. However, localizing the acupoints in any acupuncture practice requires expert knowledge. Novices without any medical background typically find it difficult because of the lack of visual cues. Our previous prototype, FaceAtlasAR, stresses this issue by localizing and overlaying facial acupoints in the augmented reality (AR) context in real-time. This paper will present E-FaceAtlasAR, which extends FaceAtlasAR to show auricular zone maps in the meantime. The system first localizes facial acupoints and auricular zone map in an anatomical yet feasible way. Then, it overlays the requested acupoints and auricular zone maps on the ears in AR. We adopt Mediapipe, a cross-platform machine learning framework, to build the real-time pipeline that runs on desktop and Android phones. Also, we conducted extensive experiments to show its effectiveness and robustness. With this system, users, even not professionals, can position the acupoints quickly for their self-acupressure treatments. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Acupuncture; Augmented reality; Ear morphology; Face alignment; Mediapipe,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Accuracy study of the Oculus Touch v2 versus inertial sensor for a single-axis rotation simulating the elbow’s range of motion,VRS - Virtual Reality,B,"Virtual reality (VR) has emerged as a valid addition to conventional therapy in rehabilitation and sports medicine. This has enabled the development of novel and affordable rehabilitation strategies. However, before VR devices can be used in these situations, they must accurately capture the range of motion of the body-segment where they are mounted. This study aims to state the accuracy of the Oculus Touch v2 controller when used to measure the elbow’s motion in the sagittal plane. The controller is benchmarked against an inertial sensor (ENLAZATM), which has already been validated as a reliable measurement device. We have developed a virtual environment that matches both the Oculus Touch v2 and the inertial sensor orientations using a digital goniometer. We have also collected the orientation measurements given by each system for a set of 17 static angles that cover the full range of normal elbow flexion and hyperextension motion, in 10° intervals from − 10° (hyperextension) to 150° (flexion). We have applied the intra-rater reliability test to assess the level of agreement between the measurements of these devices, obtaining a value of 0.999, with a 95% confidence interval ranged from 0.996 to 1.000. By analyzing the angle measurement outcomes, we have found that the accuracy degrades at flexion values between 70° and 110°, peaking at 90°. The accuracy of Oculus Touch v2 when used to capture the elbow’s flexion motion is good enough for the development of VR rehabilitation applications based on it. However, the flaws in the accuracy that have been revealed in this experimental study must be considered when designing such applications. © 2022, The Author(s).",Accuracy; Elbow; Inertial measurement unit; Range of motion; Rehabilitation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,A comprehensive method to design and assess mixed reality simulations,VRS - Virtual Reality,B,"The scientific literature highlights how Mixed Reality (MR) simulations allow obtaining several benefits in healthcare education. Simulation-based training, boosted by MR, offers an exciting and immersive learning experience that helps health professionals to acquire knowledge and skills, without exposing patients to unnecessary risks. High engagement, informational overload, and unfamiliarity with virtual elements could expose students to cognitive overload and acute stress. The implementation of effective simulation design strategies able to preserve the psychological safety of learners and the investigation of the impacts and effects of simulations are two open challenges to be faced. In this context, the present study proposes a method to design a medical simulation and evaluate its effectiveness, with the final aim to achieve the learning outcomes and do not compromise the students' psychological safety. The method has been applied in the design and development of an MR application to simulate the rachicentesis procedure for diagnostic purposes in adults. The MR application has been tested by involving twenty students of the 6th year of Medicine and Surgery of Università Politecnica delle Marche. Multiple measurement techniques such as self-report, physiological indices, and observer ratings of performance, cognitive and emotional states of learners have been implemented to improve the rigour of the study. Also, a user-experience analysis has been accomplished to discriminate between two different devices: Vox Gear Plus® and Microsoft Hololens®. To compare the results with a reference, students performed the simulation also without using the MR application. The use of MR resulted in increased stress measured by physiological parameters without a high increase in perceived workload. It satisfies the objective to enhance the realism of the simulation without generating cognitive overload, which favours productive learning. The user experience (UX) has found greater benefits in involvement, immersion, and realism; however, it has emphasized the technological limitations of devices such as obstruction, loss of depth (Vox Gear Plus), and narrow FOV (Microsoft Hololens). © 2022, The Author(s).",Augmented reality; Cognitive load; Medical education; Mixed reality; Simulation; Stress,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Reducing cybersickness by implementing texture blur in the virtual reality content,VRS - Virtual Reality,B,"Virtual reality facilities have matured in recent years; however, cybersickness has yet remained a major issue in the virtual reality content. The researchers in this study attempt to simulate the different focal points and depth of field of human vision with an eye on reducing cybersickness by implementing blurring. This paper introduces a new type of technology—texture blur, which was tested in three experiments with different contents in this study. The participants were asked to verbally answer two questionnaires: a discomfort level questionnaire and a simulator sickness questionnaire (SSQ). The results were analyzed to compare the seriousness of the participants’ cybersickness with and without texture blur in the virtual reality content. The experiment results revealed that the participants with texture blur had lower discomfort level scores. The SSQ results also went in line with the previous questionnaire, showing better results with texture blur. Hence, the researchers concluded that texture blur can effectively reduce users’ cybersickness in virtual reality content. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Human factor; Texture blur; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Move your virtual body: differences and similarities in brain activation patterns during hand movements in real world and virtual reality,VRS - Virtual Reality,B,"Virtual reality (VR) is a promising tool for neurological rehabilitation, especially for motor rehabilitation. In the present study, we investigate whether brain activation patterns that are evoked by active movements are comparable when these movements are carried out in reality and in VR. Therefore, 40 healthy adults (20 men, mean age 25.31 years) performed hand movements and viewed these movements in a first-person view in reality, a VR scene showing realistic virtual hands, and a VR scene showing abstract virtual hands, in a randomized order. The VR conditions were presented via an immersive 3D head-mounted display system. EEG activity was assessed over the hand motor areas during and after movement execution. All three conditions led to typical EEG activation patterns over the motor cortex. Hence, brain activation patterns were largely comparable between conditions. However, the VR conditions, especially the abstract VR condition, led to a weaker hemispheric lateralization effect compared to the real-world condition. This indicates that hand models in VR should be realistic to be able to evoke activation patterns in the motor cortex comparable to real-world scenarios. © 2021, The Author(s).",Brain activity; Brain lateralization; Motor task; Presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,The effects of touchless interaction on usability and sense of presence in a virtual environment,VRS - Virtual Reality,B,"For software applications with a significant level of user involvement, the traditional concept of usability has evolved into the more complex idea of user experience, which also covers emotional, cognitive or physical responses. In virtual reality, user experience also depends on the user perception related to some peculiarities of immersive environments, where also the devices employed for user interaction play a determinant role. This has led to the design of the Presence Questionnaire (PQ) for the evaluation of the effectiveness of virtual environments. This work analyzes the effects of two different interaction modalities on usability and sense of presence: in particular, the Myo armband, a gesture-based device for touchless interaction, is compared with the Vive handheld controller bundled with the HTC Vive headset. A total of 84 subjects were recruited to test the virtual environment and asked them to fill in a questionnaire obtained by combining the Usability Metric for User eXperience (UMUX) questionnaire, the System Usability Scale (SUS) and the presence questionnaire (PQ), which was specifically designed for virtual environments. A comparison between the scores obtained for the two interaction modalities revealed which questionnaire items are significantly influenced by the input interface and deduce some insights about the consequences on human factors. © 2022, The Author(s).",Handheld controller; Presence; Touchless interaction; Usability; User experience; Virtual environment,Abstract,True,
Scopus,journalPaper,2022,"Focusing on cybersickness: pervasiveness, latent trajectories, susceptibility, and effects on the virtual reality experience",VRS - Virtual Reality,B,"Although virtual reality (VR) usage has become widespread in the last decade, its adoption has been hampered by experiences of user discomfort known as cybersickness. The present study, in line with the “2020 cybersickness R&D agenda”, sought to provide a broad examination of the cybersickness phenomenon, assessing its pervasiveness, latent trajectories, impacts on the VR experience, and predictor variables. The study was composed of 92 participants living in the Dominican Republic with ages ranging from 18 to 52 years (M = 26.22), who experienced a 10-min VR immersion in two environments designed for psychotherapy. The results indicated that cybersickness was pervasive, with 65.2% of the participants experiencing it, and 23.9% severely. Additionally, the latent trajectories of cybersickness were positive and curvilinear, with large heterogeneity across individuals. Cybersickness also had a substantive negative impact on the user experience and the intentions to adopt the VR technology. Finally, motion sickness susceptibility, cognitive stress, and recent headaches uniquely predicted greater severity of cybersickness, while age was negatively related. These combined results highlight the critical role that cybersickness plays on the VR experience and underscore the importance of finding solutions to the problems, such as technological advancements or special usage protocols for the more susceptible individuals. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Head-mounted displays; Motion sickness; Technology acceptance; Virtual presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,"Correction to: Intelligence augmentation: rethinking the future of work by leveraging human performance and abilities (Virtual Reality, (2022), 26, 3, (849-870), 10.1007/s10055-021-00590-7)",VRS - Virtual Reality,B,"In the original publication the citations of Figs. 1 and 2 and their captions are wrongly published. The correct Fig. 1 caption should read as ""A framework for using AR and VR in workforce training (images are own representations)"", and correct Fig. 2 caption should read as, ""Jobs losses and gains of the ‘Economic 4.0’ scenario compared to the basic scenario (Wolter et al. 2016)"". The citation of Fig. 1 should be cited under Sect. 4, and citation of Fig. 2 should be cited under Sect. 2.2. The original publication has been corrected. © Springer-Verlag London Ltd., part of Springer Nature 2021.",,Title,True,
Scopus,journalPaper,2022,Nonverbal behavior of interviewers influences the competence ratings of observers in recruitment interviews: a study investigating social influence using 360-degree videos with virtual reality and 2D screen displays,VRS - Virtual Reality,B,"This study examined whether an interviewer’s nonverbal behavior influences observers’ competence ratings in a recruitment interview using 360-degree videos experienced with immersive virtual reality (VR-cardboard) and 2D screen displays. Participants (n = 110) observed a recruitment interview and assessed three competences of the applicant (behavior in a team, customer care, and sales skill). We used a 2 × 2 design with the nonverbal behavior (positive vs. negative) of the interviewer and display type (VR-cardboard vs. 2D screen display) as between-subjects factors. After observing interview sequences and providing competence ratings, participants also rated different aspects of immersion using the augmented reality immersion questionnaire (ARI; Georgiou and Kyza in Int J Hum Comput Stud 98: 24–37, 2017) and their overall satisfaction with the experience. For two of the three competences (customer care and behavior in a team), we found that observers gave higher competence ratings when the interviewer’s nonverbal behavior was positive compared to when it was negative. This social influence effect was similar for 360-degree videos experienced with immersive VR and 2D screen displays. VR resulted in higher immersion than 2D screen displays regarding the dimensions of flow and presence. Our results suggest that the ARI questionnaire can be used to reliably assess 360-degree videos experienced with immersive VR and 2D screen displays. © 2021, The Author(s).",360-degree video; Immersion; Job recruitment interview; Nonverbal behavior; Social influence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Real-time camera-based eye gaze tracking using convolutional neural network: a case study on social media website,VRS - Virtual Reality,B,"Eye gaze tracking plays an important role in various fields including, human computer interaction, virtual and augmented reality and in identifying effective marketing solutions in affective manner. This paper addresses real-time eye gaze estimation problem using low resolution ordinary camera available in almost every desktop environment as opposed to gaze tracking technologies requiring costly equipment and infrared light sources. In this research, a camera based non-invasive technique has been proposed for tracking and recording gaze points. Further, the proposed framework was used to analyze gaze behavior of users on advertisements displayed on social media website. Eye gaze fixations data of 32 participants were recorded, and gaze patterns were plotted using Heat maps. In addition, the gaze driven interface was designed for virtual interaction tasks to assess the performance, and usability of our proposed framework. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Consumer attention; Consumer behavior; Eye gaze tracking; Non-intrusive; Social media; Video-oculography advertisement; Virtual reality; Vision research; Webcam,Abstract_Keywords,True,
Scopus,journalPaper,2022,The benchmark framework and exploratory study to investigate the feasibility of 360-degree video-based virtual reality to induce a full body illusion,VRS - Virtual Reality,B,"The feeling of ownership of a virtual body has been a topic of interest in recent years. In order to observe the mechanisms involved in the perception of the body illusion and its manipulation, the use of virtual reality (VR) has been essential. Various technical VR set-ups have been adopted by different authors to induce the sense of embodiment. Recently, 360-degree technology camera emerged as an innovative instrument to generate an immersive experience, with positive results in terms of involvement with the scenario. The current study aims to test the feasibility of the 360-degree video-based VR to induce a full body illusion. To do this, we evaluated two different groups receiving different levels of immersion: a 3D immersive video and a 2D non-immersive video. Self-reported sense of embodiment and heart rate variability (HRV) measures were analyzed. The results of the embodiment questionnaire showed that the immersive condition can trigger a full body illusion, with significant differences between the 3D and 2D conditions (ownership: p =.003, agency: p =.000, location: p =.013, haptic sensation: p =.027). No difference was found on the Root Mean Square of the Successive Differences (RMSSD) index—the beat-to-beat variance of the heart rate—of the HRV measure (first 90 s: p =.168, last 90 s: p =.401). Based on these results, future studies are needed to investigate the 360-degree video-based VR technology as a medium to generate the sense of embodiment. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",360-degree video; Body illusion; Embodiment; Immersive technology; Media comparison; Panorama camera,Title_Abstract,True,
Scopus,journalPaper,2022,Hyper-reoriented walking in minimal space,VRS - Virtual Reality,B,"We present a new reorientation technique, “hyper-reoriented walking,” which greatly reduces the amount of physical space required in virtual reality (VR) applications asking participants to walk along a grid-like path (such as the most common layout in department stores). In hyper-reoriented walking, users walk along the gridlines with a virtual speed of twice the speed of real walking and perform turns at cross-points on the grid with half the speed of the rotation speed in the physical space. The impact of the technique on participants’ sense of orientation and increase in simulator sickness was investigated experimentally involving 19 participants walking in a labyrinth of infinite size that included straight corridors and 90° T-junctions at the end of the corridors. Walking accuracy was assessed by tracking the position of the head mounted display, and cyber-sickness was recorded with the simulator sickness questionnaire and with open questions. Walking straight forward was found to closely match the ideal path, which is the grid line, but slight errors occasionally occurred when participants turned at the T-junctions. A correction algorithm was therefore necessary to bring users back to the gridline. For VR experiments in a grid-like labyrinth with paths of 5 m in length, the technique reduces required size of the tracked physical walking area to 3 m × 2 m. © 2021, The Author(s).",Natural walking; Redirected walking; Virtual locomotion; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Identifying physiological correlates of cybersickness using heartbeat-evoked potential analysis,VRS - Virtual Reality,B,"Many studies have consistently proven that repeatedly watching virtual reality (VR) content can reduce cybersickness. Moreover, the discomfort level decreases when the VR content includes an unusual orientation, such as an inverted scene. However, few studies have investigated the physiological changes during these experiences. The present study aimed to identify psychophysiological correlates, especially the neural processing, of cybersickness. Twenty participants experienced two types of VR orientation (upright and inverted), which were repeated three times. During the experience, we recorded the participants’ subjective levels of discomfort, brain waves, cardiac signals, and eye trajectories. We performed a heartbeat-evoked potential (HEP) analysis to elucidate the cortical activity of heartbeats while experiencing cybersickness. The results showed that the severity of cybersickness decreased as the participants repeatedly watched the VR content. The participants also reported less nausea when watching the inverted orientation. We only found a significant suppression at the fronto-central HEP amplitudes in the upright orientation for the physiological changes. This study provides a comprehensive understanding of bodily responses to varying degrees of cybersickness. In addition, the HEP results suggest that this approach might reflect the neural correlates of transient changes in heartbeats caused by cybersickness. © 2022, The Author(s).",Cybersickness; Heartbeat-evoked potential; Physiological responses; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,The temporal pattern of VR sickness during 7.5-h virtual immersion,VRS - Virtual Reality,B,"In this study, we assessed the relationship between exposure duration and VR sickness severity during 7.5-h virtual immersion. First, we showed that the VR sickness severity was positively correlated to the exposure duration: the longer participants were exposed to the VR environment, the more severe sickness symptoms they had. Second, we showed a dynamic sickness adaptation process during a long time of VR exposure: the sickness adaption effect that had already been established could be broken as the exposure duration continued to increase, and a new sickness adaption process would establish. Moreover, we showed a distinguishable symptom profile of HMD compared with LCD, which was insusceptible of exposure duration. This is the first report presenting the temporal pattern of VR sickness during such long-duration exposure. Our study could offer a predictive model of VR sickness severity level during long virtual immersion and provide suggestions for the use of VR technology for scientific study, clinical application, and business entertainment. © 2021, The Author(s).",Long-duration exposure; Sensory conflict; Sickness adaption effect; Virtual immersion; Virtual reality sickness,Keywords,True,
Scopus,journalPaper,2022,Arousing a wide range of emotions within educational virtual reality simulation about major depressive disorder affects knowledge retention,VRS - Virtual Reality,B,"The experience of using an educational application, concerning a major depressive disorder simulation, could be anything but pleasant, so the challenges of creating such an application are ample. In this research, the effects of the emotional experience of the players, deriving from the positive ending of the virtual reality (VR) simulation’s embedded narrative or the lack of it, are evaluated. Alongside the investigation of a possible link between the emotional impact of the simulation and information retention, the overall effect of the application in relation to VR presence and body ownership is appraised. Thirty participants over 18 years old tested the application, using an Oculus Rift head-mounted display with a joystick, and their data were recorded by a pre- and a post-questionnaire. The 30 participants have been separated into groups of 15, where the positive ending was accessible to only one of the two groups. The group which experienced the positive ending reported a significant correlation of emotional impact and knowledge retention. © 2021, The Author(s).",Affective states; Game narrative; Knowledge retention; Major depressive disorder; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Three-dimensional modeled environments versus 360 degree panoramas for mobile virtual reality training,VRS - Virtual Reality,B,"Virtual field trip is a way of providing users with some knowledge and exposure of a facility without requiring them to physically visit the location. Due to the high computational costs that are necessary to produce virtual environments (VEs), the potential for photorealism is sacrificed. Often these three-dimensional (3D) modeled applications use an unrealistic VE and, therefore, do not provide a full depiction of real-world environments. Panoramas can be used to showcase complex scenarios that are difficult to model and are computationally expensive to view in virtual reality (VR). Utilizing 360° panoramas can provide a low-cost and quick-to-capture alternative with photorealistic representations of the actual environment. The advantages of photorealism over 3D models for training and education are not clearly defined. This paper initially summarizes the development of a VR training application and initial pilot study. Quantitative and qualitative study then was conducted to compare the effectiveness of a 360° panorama VR training application and a 3D modeled one. Switching to a mobile VR headset saves money, increases mobility, decreases set-up and breakdown time, and has less spatial requirements. Testing results of the 3D modeled VE group had an average normalized gain of 0.03 and the 360° panorama group, 0.43. Although the 3D modeled group had slightly higher realism according to the presence questionnaire and had slightly higher averages in the comparative analysis questionnaire, the 360° panorama application has shown to be the most effective for training and the quickest to develop. © 2021, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.",Virtual field trip; Virtual laboratories; Virtual reality; Virtual training,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,“Passive or interactive virtual reality? The effectiveness for pain and anxiety reduction in pediatric patients”,VRS - Virtual Reality,B,"Invasive techniques such as venipuncture are painful procedures causing stress and anxiety, both in pediatric patients and in their carers. For this reason, efforts are being made to develop mitigating strategies for the patient’s pain and anxiety during the performance. To analyze and evaluate the effectiveness of the use of Virtual Reality distraction techniques as a measure of pain and anxiety reduction in pediatric patients and their parents. In addition, the effects of two modes of Virtual Reality (passive vs. interactive) were compared. A quasi-experimental study was carried out in the pediatric emergency department of a tertiary referral hospital in north Spain. The participants were children who underwent venipuncture for blood extraction and vascular cannulation. From the 124 patients, 51.6% (n = 64) were girls and 48.4% (n = 60) were boys (p = 0.574). The mean age was 8.4 years (SD: 4.1). The mean level of pain experienced was 2.33 (SD: 0.76) in the interactive VR group (n = 88) versus 2.67 (SD: 1.35) in patients with passive VR (n = 36) (p = 0.008); being the presence of anxiety in 27.3% (n = 24) of the cases treated with interactive Virtual Reality and in 88.9% (n = 32) of the patients with passive Virtual Reality (p = 0.000). The virtual reality is an effective method to reduce pain and anxiety levels in pediatric patients, with the effectiveness of interactive virtual reality and its use in the population aged 7–15 years being greater. © 2022, The Author(s).",Anxiety; Pain; Pediatric patients; Phlebotomy; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Spatial and attentional aftereffects of virtual reality and relations to cybersickness,VRS - Virtual Reality,B,"Cybersickness describes the nausea and discomfort that frequently emerges upon exposure to a virtual reality (VR) environment. The extent to which cybersickness leads to temporary constraints in cognitive functioning after VR exposure is a critical aspect of evaluating the risk to human safety where VR tasks are used for workforce training. Here, we examined whether VR exposure results in deteriorated cognitive spatial ability and attention, and if this possible deterioration is related to cybersickness. A standardized cognitive test battery consisting of Corsi blocks task (CBT), Manikin spatial task (MST), and color trails test (CTT-A and -B) was administered before and after participants were exposed to virtual reality (VR group), or engaged in interactive board games (control group). The performance of participants in CBT remained unchanged from pre-test to post-test in both groups, while performance in MST improved in the control and remained stable in VR group. Response times in CTT-A remained stable in the VR group but reduced significantly in the control group. Regarding CTT-B, participants from both groups became significantly faster in post-test. We did not observe any significant sex differences, or effects of past VR experience, across measures of cognitive performance or cybersickness. Crucially, no significant correlations were found between cognitive performance changes and cybersickness scores in any cases. The results provide encouragement for the use of VR in professional settings, suggesting that VR and cybersickness may minimally limit subsequent cognitive processing. However, it will be crucial to further examine the aftereffects in other cognitive functions. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cognitive aftereffects; Cybersickness; Multisensory integration; Self-motion; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,An interactive extended reality-based tutorial system for fundamental manual metal arc welding training,VRS - Virtual Reality,B,"Extended reality (XR) technology has been proven an effective human–computer interaction tool to increase the perception of presence. The purpose of this study is to develop an interactive XR-based welding tutorial system to enhance the learning and hands-on skills of novice welders. This study is comprised of two parts: (1) fundamental manual metal arc welding (MMAW) science and technology tutoring in a virtual reality (VR)-based environment, and (2) hands-on welding training in a mixed reality (MR)-based environment. Using the developed tutorial system, complicated welding process and the effects of welding process parameters on weld bead geometry can be clearly observed and comprehended by using a 3D interactive user interface. Visual aids and quantitative guidance are displayed in real time to guide novice welders through the correct welding procedure and help them to maintain a proper welding position. A user study was conducted to evaluate the learnability, workload, and usability of the system. Results show that users obtained significantly better performance by using the XR-based welding tutorial system, compared to those who were trained using the conventional classroom training method. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Extended reality; Manual metal arc welding; Mixed reality; Quantitative guidance; Virtual reality; Visual aids,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,The effect of reducing distraction on the flow-performance link in virtual experiential learning environment,VRS - Virtual Reality,B,"In virtual experiential learning environment (VELE), distraction can reduce learners’ flow experience, learning performance, and the link between them, which are important aspects for high-quality learning. Based on the weak association model (WAM), we proposed two potential guidelines to deal with distraction and then improve these aspects. Guideline1 is directly decreasing the amount of attractive but task-irrelevant distractors in VELE. Guideline2 is enhancing the congruence between distractors and primary task by guiding attention from task-irrelevant distractors to task-relevant elements. To explore the effect of the guidelines, this paper develops a prototype VR experiential learning system, based on which two experiments were performed. Experiment 1 and experiment 2, respectively, conducted a comparative experiment to test the effect of guideline 1 and guideline 2 on flow, performance, and flow-performance link. Results show that both guidelines helped enhance the learning performance without any damage on flow experience and alleviated the weak flow-performance link. The two guidelines provide easy ways to guide task-relevant attention to optimize VELE. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Distraction; Experiential learning; Flow-performance link; Guideline; Virtual reality,Keywords,True,
Scopus,journalPaper,2022,A novel virtual reality simulation training system with haptic feedback for improving lateral ventricle puncture skill,VRS - Virtual Reality,B,"The lateral ventricle puncture is an important step of external ventricular drain. It is one of the most basic but challenging skills that must be mastered by physicians. For improving the lateral ventricle puncture skill, a novel virtual reality simulation training system equipped with haptic feedback was developed in this paper. A series of experiments and questionnaires were conducted to evaluate the fidelity of simulated haptic force and the effectiveness of this system. Both the forces generated by the haptic device during the virtual puncture and that generated during puncturing on a pig brain were obtained and compared. The results indicate that these two forces have the similar varying tendency under different puncturing conditions. In addition, two groups of neurosurgical interns named A (trained without this system) and B (trained with this system) were selected to verify the effectiveness of this system. The operation metrics, including operative dictation, operation time, positioning of Kocher’s point, times of repeated punctures, and the punctured position on lateral ventricle, were assessed by the chief physician for both groups. The results show that group B achieved higher scores than group A in all operation metrics except only operative dictation (P = 0.001). This indicates that the proposed virtual training system is an effective aid in training neurosurgery physicians’ lateral ventricle puncture skill. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Haptic feedback; Lateral ventricle puncture; Surgery simulation; Training system; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Learner experience and evidence of cybersickness: design tensions in a virtual reality public transportation intervention for autistic adults,VRS - Virtual Reality,B,"People with autism spectrum disorders (ASD) exhibit a range of socio-communicative and behavioral deficits which leads to difficulties holding meaningful relationships and vocational opportunities. Unfortunately, it is oftentimes difficult for this population to transfer learned skills from controlled intervention contexts into the real-world. As a result, interest in using virtual reality (VR) to create naturalistic training contexts has grown. Research has provided evidence to support the benefits of using VR-based training for people with ASD. However, the emergence of commercially available head-mounted displays (HMD), and their association with cybersickness, has led many to wonder if people with ASD would continue to find VR as being acceptable if they were to be immersed within these devices. Further, people with ASD often have sensory integration disorders making the continued use of VR a potential ethical concern. This research examined the extent that adults with ASD from a day program felt symptoms of cybersickness while undergoing sessions of a VR-training program. The nature of learner experiences while using HMD were also explored. Research questions were addressed through multi-method procedures that utilized quantitative and qualitative data. Despite the presence of some cybersickness symptoms, participants found the experiences to be positive and acceptable. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Accessibility; Autism spectrum disorder; Cybersickness; Oculus rift; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,"The influence of personality, sound, and content difficulty on virtual reality sickness",VRS - Virtual Reality,B,"Virtual reality (VR) sickness is a condition that may occur during or after exposure to a virtual environment and can induce symptoms such as headache and eye strain. VR sickness can be influenced by several factors. One individual factor that might affect VR sickness is personality. Non-individual factors that may influence VR sickness are sound and content difficulty. The aim of this study was to observe the influence of personality, sound, and content difficulties on VR sickness in one specific game. An experiment with a VR game was conducted, involving 60 students representing six different personalities: Honesty-Humility, Emotionality, eXtraversion, Agreeableness, Conscientiousness, and Openness to Experience. Participants were instructed to complete a visual search game with different levels of sound and content difficulty. VR sickness was assessed in each condition using a VR Sickness Questionnaire consisting of two dimensions: oculomotor (consists of general discomfort, fatigue, eye strain, and difficulty in focusing) and disorientation (consists of headache, fullness of head, blurred vision, dizziness with eyes closed, and vertigo). Data were processed using descriptive and separate mixed ANOVA. The results showed that the effect of personality was significant for the VR sickness dimensions oculomotor and disorientation. The Emotionality personality reported the highest oculomotor and disorientation scores. There was no significant difference in oculomotor and disorientation scores based on sound and content difficulty. The implications of the results were discussed. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Disorientation; Level of difficulty; Oculomotor; Personality; Sound; VR sickness,Title_Abstract,True,
Scopus,journalPaper,2021,Kinematics of direct reaching in head-mounted and stereoscopic widescreen virtual environments,VRS - Virtual Reality,B,"This research investigated the effects of virtual displays on kinematic parameters during direct pointing at a virtual target. Two virtual displays, three egocentric distances, and three indices of difficulty (IDs) were the independent variables considered in the study. Twelve participants (M = 29.8 ± 3.45 years of age) with normal visual acuity performed a pointing movement in the two VR displays, a stereoscopic widescreen display (SWD) and a head-mounted display (HMD). The movement data were compiled using a motion system. The outcomes revealed that peak velocity and reaction time differed significantly between the SWD and HMD conditions; peak velocity was higher and reaction time was shorter with the SWD than with the HMD. Nonetheless, the effective movement time and confirmation time were not significantly different between the two VR displays. In addition, the distance judgment accuracies of the HMD and SWD were 96% and 86%, respectively; distance was underestimated in the HMD and overestimated in the SWD. Moreover, both peak velocity and reaction time were significantly lower at high ID than at low and medium IDs. The results suggested that using an SWD could be more effective and efficient than an HMD for restoring motor function in upper and lower limbs. On the other hand, an HMD might be appropriate for applications which require exocentric distance judgment precision, such as architecture or medical visualization. Moreover, such findings provide valuable information for developers and designers of human interfaces and applications in virtual reality. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Head-mounted display; Kinematics; Stereoscopic widescreen display,Abstract,True,
Scopus,journalPaper,2021,Influence of navigation parameters on cybersickness in virtual reality,VRS - Virtual Reality,B,"Cybersickness remains a major challenge in the virtual reality community. It occurs mainly when navigating in a 3D immersive virtual environment. Several parameters are known to influence the users’ cybersickness level while navigating, that can be either technological or neuro-psychological. This study investigates two of these parameters that are the distance from a virtual barrier and the choice of the navigation interface. An experiment was performed for each of these parameters to evaluate their influence on the variation of cybersickness. For each experiment, participants were asked to navigate in a large virtual room with walls that were textured with a black and white lined pattern to voluntarily exacerbate cybersickness. The level of cybersickness was collected through subjective (Simulator Sickness Questionnaire) and behavioral (evolution of postural sway) measurements. Results allow drawing suggestions for optimal navigation, so that cybersickness can be significantly reduced, thus providing with enhanced user experience. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Navigation; Navigation device; Parameters; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"Correction to: Predicting user visual attention in virtual reality with a deep learning model (Virtual Reality, (2021), 25, 4, (1123-1136), 10.1007/s10055-021-00512-7)",VRS - Virtual Reality,B,"In the original publication, the spelling of fifth author was wrong. The correct spelling is ’Praben Hansen.’ The original publication has been corrected. © 2021, Springer-Verlag London Ltd., part of Springer Nature.",,Title,True,
Scopus,journalPaper,2021,The assessment of virtual reality training in antromastoidectomy simulation,VRS - Virtual Reality,B,"Virtual reality (VR) may be a good alternative for cadaveric temporal bone surgical dissection courses, which are an important part of otolaryngology resident’s training. The aim of the study was to assess the VR temporal bone surgery simulator in an antromastoidectomy simulation. The VR system was based on the Geomagic Touch Haptic Device from 3D System. The research was designed as a prospective study, with three sessions of VR simulation training. The group of four ENT specialists unexperienced in otosurgery and 11 otorhinolaryngology residents performed a series of virtual dissections of a VR temporal bone model. Two experts with a broad experience in ear surgery participated in the study as supervisors for all the participants. At the end of each session, the experts controlled the accuracy of the simulated surgery performance assigning positive points for each correctly performed step and negative points for each mistake. After each session, participants of the study were asked to fill in the questionnaire concerning their impression of a VR system simulation. The evaluation of every simulation (total score) was based on the duration of a VR session, the quality of performance (positive points) and the number of mistakes (negative points). During consecutive VR sessions, evident shortening of the length of performance, as well as an improvement in the quality of performance and reduction in mistakes, was observed. Sixty percent of study participants answered that signaling damage to the critical elements was good (40%—sufficient), and 67% assessed that they had made a progress in consecutive sessions. After three sessions, 100% of participants indicated higher self-confidence in relation to their own surgical skills. Also, all the participants indicated that VR training should be included in a routine educational program for medical students. VR training provides a structured, safe and supportive environment to familiarize oneself with complex anatomy and practical skills. © 2021, The Author(s).",Antromastoidectomy performance; Medical sciences; Simulation model; Surgery training; Temporal bone; Virtual education; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Surface Prediction for Spatial Augmented Reality Applications,VRS - Virtual Reality,B,"In spatial augmented reality applications, incorrect projection mapping may occur when projecting images onto moving non-rigid surfaces. This may detract from the user experience, as the image may not be perceived as originally intended. This is especially apparent when using low-cost projectors and cameras or when surfaces are moving quickly. In this paper, an algorithm is developed which predicts the motion of a non-rigid surface, so that when an image is being projected onto the surface, the projection “matches” the surface shape, while using low-cost equipment. Using an interconnected mass–spring–damper system to model the surface, the surface position is predicted using a Kalman filter-based algorithm, which also compensates for the processing delays and fast-moving surfaces. To accurately model real-world materials, the mass–spring system parameters are found using a system identification approach. When the prediction algorithm is implemented experimentally, in real-time, the results show convergent results in the sense that the surface predictions converge to the measured position of a non-rigid surface. The error results show that the algorithm is both accurate and robust and can currently be applied in spatial augmented reality applications. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Control theory; Spatial augmented reality; Surface representations,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Cybersickness in current-generation virtual reality head-mounted displays: systematic review and outlook,VRS - Virtual Reality,B,"Cybersickness (CS) is a term used to refer to symptoms, such as nausea, headache, and dizziness that users experience during or after virtual reality immersion. Initially discovered in flight simulators, commercial virtual reality (VR) head-mounted displays (HMD) of the current generation also seem to cause CS, albeit in a different manner and severity. The goal of this work is to summarize recent literature on CS with modern HMDs, to determine the specificities and profile of immersive VR-caused CS, and to provide an outlook for future research areas. A systematic review was performed on the databases IEEE Xplore, PubMed, ACM, and Scopus from 2013 to 2019 and 49 publications were selected. A summarized text states how different VR HMDs impact CS, how the nature of movement in VR HMDs contributes to CS, and how we can use biosensors to detect CS. The results of the meta-analysis show that although current-generation VR HMDs cause significantly less CS (p< 0.001), some symptoms remain as intense. Further results show that the nature of movement and, in particular, sensory mismatch as well as perceived motion have been the leading cause of CS. We suggest an outlook on future research, including the use of galvanic skin response to evaluate CS in combination with the golden standard (Simulator Sickness Questionnaire, SSQ) as well as an update on the subjective evaluation scores of the SSQ. © 2021, The Author(s).",Cybersickness; Head-mounted display; Immersive virtual reality; Visually induced motion sickness,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,A framework for constructing and evaluating the role of MR as a holographic virtual guide in museums,VRS - Virtual Reality,B,"Mixed reality (MR) is a cutting-edge technology at the forefront of many new applications in the tourism and cultural heritage sector. This study aims to reshape the museum experience by creating a highly engaging and immersive museum experience for visitors combing real-time visual, audio information and computer-generated images with museum artefacts and customer displays. This research introduces a theoretical framework that assesses the potential of MR guidance system in usefulness, ease of use, enjoyment, interactivity, touring and future applications. The evaluation introduces the MuseumEye MR application in the Egyptian Museum, Cairo using mixed method surveys and a sample of 171 participants. The results of the questionnaire highlighted the importance of the mediating the role of the tour guide in enhancing the relationship between perceived usefulness, ease of use, multimedia, UI design, interactivity and the intention of use. Furthermore, the results of this study revealed the potential future use of MR in museums and ensured sustainability and engagement past the traditional visitor museum experience, which heightens the economic state of museums and cultural heritage sectors. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Holographic system; Microsoft HoloLens; Mixed reality; Museums; Virtual guide,Abstract_Keywords,True,
Scopus,journalPaper,2021,Comparing the effectiveness of fire extinguisher virtual reality and video training,VRS - Virtual Reality,B,"Fire is a major hazard in built environments. Fires in buildings cause fatalities, serious injuries and tremendous damage. Most fires can be extinguished in the early stages of the fire’s development, with the right equipment and correct use of the equipment. However, as there can be as little as a few minutes between a fire starting and very dire consequences, rapid and correct responses are critical. Implementing effective training solutions is necessary to enable members of the public, who are not experts in fire safety, to use a fire extinguisher correctly. This can assist to build resilience to fires. In recent decades, virtual reality (VR) has aroused the fire safety community’s attention, as a smart, safe and effective training method compared to the traditional methods of lectures, non-interactive videos, and brochures. VR has been used for training for fire emergency preparedness and to collect data about evacuee decision-making, but VR has rarely been applied to a fully immersive training experience about fire extinguishers operation steps. Fire extinguisher operation steps are Pull, Aim, Squeeze and Sweep. Each step is critical to quickly extinguish a fire. This paper compares fire extinguisher training using a VR simulation with a non-interactive training video and evaluates the trainees learning of a fire extinguisher’s basic operation steps, in terms of knowledge acquisition, retention of information and change of self-efficacy. The results showed that the VR trainees scored better than video trainees, in terms of knowledge acquisition, even if the same trend was observed for long term retention of information. It was also observed that VR training provided a higher increment of self-efficacy right after the training. The VR group participants had maintained the same level of self-efficacy even 3–4 weeks after the training, while the video group had shown a significant drop of self-efficacy. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Fire extinguishers; Fire safety; Serious games; Training; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Development of alternative reality environments for spacecraft habitat design evaluation,VRS - Virtual Reality,B,"Alternative reality (XR) tools are becoming more commonplace in the realm of architecture, engineering, and construction (AEC); however, these digitally immersive technologies vary greatly in their degree of virtuality and individual capabilities. While many studies detail the performance of one specific XR technology for a particular use case, little work exists comparing numerous modern XR technologies in a side-by-side manner for a single use case. In this work, we construct four equal-fidelity, alternative reality environments for the application of spacecraft habitat design evaluation, starting with a baseline habitat mockup constructed in physical reality (PR). Three digital environments—augmented reality, hybrid reality (HR), and virtual reality—were modeled after the PR environment and developed in parallel. The implementation of each environment was carefully documented, along with relative strengths and weaknesses associated with both development and use. Additionally, we have developed a novel HR implementation that includes realistic and intuitive haptics, hand tracking, a fully virtual audiovisual scene, and responsive habitat elements, all wirelessly synched with a game engine and spatially synched with the PR environment. In sum, this work serves as a resource for those considering XR technologies for any variety of applications, particularly in AEC disciplines. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",AEC; Design evaluation; Hybrid reality; Mixed reality; Prototyping; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Mixed reality system for nondestructive evaluation training,VRS - Virtual Reality,B,"nondestructive evaluation (NDE) is an analysis technique used to evaluate the properties of a material, component, structure or system without causing damage. In this paper, we introduce a novel mixed reality system for NDE training. In particular, we model and simulate the inspected object and the inspection probe. The operator trainees with the wearable headsets are able to move and zoom the inspected object in a mixed environment, i.e., reality with virtual objects overlaid. In addition, the trainees use their gaze, gesture, and voice to control and interact with the virtual objects within the NDE training session. They can also access the help manual in order to follow the training instruction. The system is successfully operated on HoloLens, the state-of-the-art mixed reality headset. Evaluational results demonstrate that the use of mixed reality training provides significant benefit for the potential technician trainees. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Mixed reality; Nondestructive evaluation; Simulation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,A meta-analysis of the virtual reality problem: Unequal effects of virtual reality sickness across individual differences,VRS - Virtual Reality,B,"Practical applications of virtual reality (VR), defined as a three-dimensional digital representation of a real or imagined space, have become increasingly popular and are now applied in workplace training, physical rehabilitation, psychological therapy, and many other settings. Feelings akin to motion sickness, called VR sickness, can arise from interacting with VR programs, and researchers have shown that certain aspects of the user, such as gender and age, may predict the occurrence of VR sickness. The unequal effects of VR sickness are a dire concern and the application of VR is unfair to certain users if they are prone to sickness. For instance, a workplace VR training program could result in disparate treatment if women experience more VR sickness than men. To investigate this notion, we perform a meta-analysis on the relationship between VR sickness and a wide array of potential antecedents. The results demonstrate that motion sickness susceptibility, gender, real-world experience, technological experience, possessing a neurological disorder, and possessing a relevant phobia all significantly relate to VR sickness; however, no moderating effects produced recurrent significant results. These results were partially explained by the current dominant framework for VR sickness, postural instability theory, but some findings were not predicted by the theory. Therefore, we support that (a) VR sickness produces unequal effects across multiple individual differences; (b) these effects appear resilient across applications of VR programs, and (c) further research is needed to develop theory and identify explanatory mechanisms that detail these relationships. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Head-mounted displays; Individual differences; User attributes; Virtual reality; Virtual reality sickness; Virtual reality-induced symptoms and effects,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,A simulator for both manual and powered wheelchairs in immersive virtual reality CAVE,VRS - Virtual Reality,B,"A large number of people in the world need to use a wheelchair because of different disabilities. Driving a wheelchair requires complex physical and cognitive abilities which need to be trained. Virtual training helps users acquire driving skills in a safe environment. The aim of this paper is to describe and technically validate simulation models for both manual (MW) and powered wheelchairs (PW) based on immersive virtual reality CAVE (VR). As VR system, the Gait Real-time Analysis Interactive Lab (GRAIL) was used, a CAVE equipped with a motion platform with two degrees of freedom and an optoelectronic motion capture system. A real wheelchair was positioned onto the motion platform with rear wheels free to turn in MW modality, and a commercial joystick was installed on an armrest to simulate the PW modality. Passive markers were used to track the wheel rotation, the joystick and the user hand motion. Custom D-flow applications were developed to manage virtual scene response to user actions. Overground tests, based on single wheel rotation, were performed to verify the simulation model reliability. Quantitative results demonstrated that the MW simulator kinematics was consistent with a real wheelchair overground in the absence of wheel slip and inertia (median error for MW 0.40 °, no systematic bias p = 0.943, high correlation rho > 0.999, p < 0.01). The proposed solution is flexible and adaptable to different wheelchairs, joysticks and optoelectronic systems. The main limitation is the absence of force feedback. Nevertheless, it is a reliable prototype that can be used to validate new virtual scenarios as well as for wheelchair training. The next steps include the system validation with real end users and assessment of the simulator effectiveness as a training tool. © 2021, The Author(s).",Immersive virtual reality; Manual wheelchair; Powered wheelchair; Simulating system,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,A study on the verification of the effect of sensory extension through cutaneous sensation on experiential learning using VR,VRS - Virtual Reality,B,"This study verifies the effects of learning experiences by measuring the degrees of awareness of direct experience felt by users detecting both temperature and wind occurring within a virtual space as real cutaneous sensations. Learning while using virtual reality (VR)—with a focus on audiovisual vividness, tactile interactivity, and locomotive interactivity—was thus compared to VR utilization learning with the aforementioned wind and temperature sensations. The results show a much higher level of vividness and presence for those that experienced VR learning with cutaneous sensations. This confirms that a user feels an experience more vividly, and therefore as being closer to real life, when his or her senses are engaged, particularly through the use of skin sensations. Furthermore, when asked to describe their experiences, most of the group studied (95.8%) selected the “Exploratory Stage” (allowing users to believe they can feel, touch, and manipulate objects) and the “Spectator Stage” (looking at an object as if it were there). The results demonstrate a difference from previous VR learning that provides no cutaneous sensations and concludes that experiential learning can therefore be enhanced. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Authentic virtual reality; Cutaneous sensation; Experiential learning; Presence; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Acceptance of dance training system based on augmented reality and technology acceptance model (TAM),VRS - Virtual Reality,B,"The advancement in Computer Vision (CV) has evolved drastically from image processing to object recognition, tracking video, restoration of images, three-dimensional (3D) pose recognition, and emotion analysis. These advancements have eventually led to the birth of Augmented Reality (AR) technology, which means embedding virtual objects into the real-world environment. The primary focus of this research was to solve the long-term learning retention and poor learning efficiency for mastering a dance skill through the AR technology based on constructivism learning theory, Dreyfus model and Technology Acceptance Model (TAM). The problem analysis carried out in this research had major research findings, in which the retention and learning efficiency of a dance training system were predominantly determined through the type of learning theory adopted, learning environment, training tools, skill acquisition technology and type of AR technique. Therefore, the influential factors for the user acceptance of AR-based dance training system (ARDTS) were based on quantitative analysis. These influential factors were determined to address the problem of knowledge gap on acceptance of AR-based systems for dance education through self-learning. The evaluation and testing were conducted to validate the developed and implemented ARDTS system. The Technology Acceptance Model (TAM) as the evaluation model and quantitative analysis was done with a research instrument that encompassed external and internal variables. TAM consisted of 37 items, in which six factors were used to assess the new developed ARDTS by the authors and its acceptability among 86 subjects. The current study had investigated the potential use of AR-based dance training system to promote a particular dance skill among a sample population with various backgrounds and interests. The obtained results support a general acceptance towards ARDTS among the users who are interested in exploring the cutting-edge technology of AR for gaining expertise in a dance skill. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Dance learning; Interactive system; Kinect V2; User acceptance,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Performance prediction at single-action level to a first-person shooter video game,VRS - Virtual Reality,B,"Serious games, professional entertainment (e.g. e-sport), or the immersive simulation of critical scenarios by means of virtual reality belong to the scope of the video game industry. With implications in economic (gambling) and professional careers (e-sports), researchers have focused on the high-level analysis of the win/lose chances and on the player’s profile (good/bad performers). At the low-level analysis, the prediction of player’s performance at single-action level, such as in the case of hits in a first-person shooter (FPS) video game, to the best of our knowledge, has not been undertaken yet. In this study we hypothesize that VR video games, embed enough contextual information to predict performance in an FPS game at single-action level. For this purpose, we developed an FPS video game and a single-shoot level prediction model based on virtual world contextual information. Eighteen students of the University of Granada without previous experience in the game played for 45–50 min and generated 600–1200 events each. Every event, which was composed of twenty-contextual-player-centred components of the virtual scenario, were transmitted on-line to a remote server to perform predictions. Data from fifteen out of eighteen participants were used to train the model prediction model. After training, the model predicted “hit”/”miss” with a mean accuracy of 74.1%. In a broad vision, our results suggest that immersive virtual environments bear enough contextual information for accurate predictions even at single-action level. In a closed-loop design, this finding could be used (e.g. in defence, professional e-sports, etc.) to anticipate participant’s actions/decisions before they are taken, and modify the virtual scenario (e.g. abort mission, change environmental conditions) or drive the player (e. g. suggest options, relieve of command, etc.) according to the purpose of the mission. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",First-person shooter; Performance; Prediction; Video game; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Predicting user visual attention in virtual reality with a deep learning model,VRS - Virtual Reality,B,"Recent studies show that user’s visual attention during virtual reality museum navigation can be effectively estimated with deep learning models. However, these models rely on large-scale datasets that usually are of high structure complexity and context specific, which is challenging for nonspecialist researchers and designers. Therefore, we present the deep learning model, ALRF, to generalise on real-time user visual attention prediction in virtual reality context. The model combines two parallel deep learning streams to process the compact dataset of temporal–spatial salient features of user’s eye movements and virtual object coordinates. The prediction accuracy outperformed the state-of-the-art deep learning models by reaching record high 91.03%. Importantly, with quick parametric tuning, the model showed flexible applicability across different environments of the virtual reality museum and outdoor scenes. Implications for how the proposed model may be implemented as a generalising tool for adaptive virtual reality application design and evaluation are discussed. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Deep learning model; Eye tracking; Virtual reality; Visual attention,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,A sliding mode-based approach to motion cueing for virtual reality gaming using motion simulators,VRS - Virtual Reality,B,"Motion simulators have been of significant importance for the aviation sector in training pilots. However, the present boom in the utilization of robotics for virtual reality (VR) gaming has given rise to a new application of motion simulators. Motion cueing algorithms (MCA) play a key role in mapping the motions from a gaming scenario to the workspace of a simulator. This workspace is small (as compared to the gaming world), and on reaching the boundary, it becomes necessary to saturate the motion. Each degree of freedom, in the Cartesian space, is saturated between two fixed extremities.This hampers the perception of motion of a user enjoying the scenario. In order to address this practical problem, we make an attempt to enlarge the workspace and develop a mathematical methodology to prevent the simulator from exiting a non-cuboidal workspace. To do so, we propose sliding mode-based cueing algorithm (SMCA), which makes the simulator to slide in close proximity across the boundary of workspace. We make use of discrete-time models to present this methodology in order to ensure straightforward implementation by researchers in the future. Veracity of SMCA is testified by means of experimentation on SP7 motion simulator.The experimental results give evidence of a 57% increase in the considered sub-workspace, thereby reducing the relative necessity to saturate the motions as compared to classical MCA. This leads to a better experience of a user enjoying the VR scenario. On the other hand, the following drawbacks are reported: (1) necessity to analytically model the workspace boundary and ensuring that it is smooth with nonzero gradient, (2) SMCA parameter selection is more cumbersome than classical MCA, thereby making its utility restricted to recorded scenarios. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Motion cueing; Motion simulators; Parallel manipulators; Sliding mode control; Virtual reality gaming,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Hand-adaptive user interface: improved gestural interaction in virtual reality,VRS - Virtual Reality,B,"Most interactive user interfaces (UIs) for virtual reality (VR) applications are based on the traditional eye-centred UI design principle, which primarily considers the user’s visual searching efficiency and comfort, but the hand operation performance and ergonomics are relatively less considered. As a result, the hand interaction in VR is often criticized as being less efficient and precise. In this paper, the user’s arm movement features, such as the choice of the hand being used and hand interaction position, are hypothesized to influence the interaction results derived from a VR study. To verify this, we conducted a free hand target selection experiment with 24 participants. The results showed that (a) the hand choice had a significant effect on the target selection results: for a left hand interaction, the targets located in spaces to the left were selected more efficiently and accurately than those in spaces to the right; however, in a right hand interaction, the result was reversed, and (b) the free hand interactions at lower positions were more efficient and accurate than those at higher positions. Based on the above findings, this paper proposes a hand-adaptive UI technique to improve free hand interaction performance in VR. A comprehensive comparison between the hand-adaptive UI and traditional eye-centred UI was also conducted. It was shown that the hand-adaptive UI resulted in a higher interaction efficiency and a lower physical exertion and perceived task difficulty than the traditional UI. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Adaptive user interface; Free hand interaction; Hand choice; Performance evaluation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Cost–benefit analysis of virtual reality-based training for emergency rescue workers: a socio-technical systems approach,VRS - Virtual Reality,B,"Virtual reality (VR) is widely recognised as a promising technology for training emergency first responders and other safety–critical workers. It is uniquely able to immerse trainees in extreme situations that are too risky or dangerous to be examined in traditional real-world safety training. Most organisations seeking to implement VR safety training often limit their decisions to financial and technological factors. However, in this paper, we argue that a socio-technical systems approach is required to better appreciate the social costs and benefits of VR training, which are important for a successful implementation. The paper also reports our own research on a real-world implementation of VR safety training for the Mine Rescue Brigades in New South Wales, Australia. The training—conducted in both fully immersive (360 VR) and non-immersive (Desktop VR) virtual reality—involved a search and rescue operation which was necessitated by an underground fire at the bottom of the transport drift in a coal mine. Following this training, the 368 trainees not only completed a post-training questionnaire, but also were interviewed, to assess their training experiences in the VR environment. The findings provide a comprehensive account of the social costs and benefits of adopting VR as a safety training tool. Overall, the trainees perceived the benefits to far outweigh the costs, with an overall high inclination to recommend the VR training to other colleagues. Desktop VR was found to be as fit for delivering successful training as the more immersive 360 VR. However, this Desktop VR generated considerably less motion sickness in trainees. These findings should help organisations and training providers decide on: (1) whether or not to invest in VR safety training solutions; (2) which type technology/method of delivery to use. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",,Title_Abstract,True,
Scopus,journalPaper,2021,"Motivations, design, and preliminary testing for a 360° vision simulator",VRS - Virtual Reality,B,"Contemporary virtual reality systems enable academics to more efficiently explore and analyze complex three-dimensional (3D) content, but their utility is limited by visual short-term memory. Janus, a geometry agnostic shader script, circumvents this cognitive limitation by automatically rendering complex object meshes to fit entirely within the field-of-view of consumer head-mounted displays. The resulting 360° vision experience represents an advantage over existing scientific data visualization tools, which have sought to replicate real-world viewing experiences but have inadvertently replicated associated limitations as well. By presenting data in such a way so as to effectively circumvent cognitive loads associated with body (or object) movement, academics can use the Janus shader to more readily engage in the exploratory analysis of complex 3D data sets, thereby facilitating scientific insight. This paper explores the motivations and design of the Janus shader and describes preliminary results from user testing conducted under controlled conditions. For the 24 study participants (N = 24), statistically significant time-to-completion decreases were observed for spatial analysis tasks taking place in intervention (Janus-enabled) VR scenes of low-to-moderate complexity. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Academic technologies; Cognitive load; Scientific data visualization; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Exploring colour in context using Virtual Reality: Does a room change how you feel?,VRS - Virtual Reality,B,"The colour-in-context theory suggests that our reactions to colour vary depending on the context in which the colour is presented. Our understanding of how colour affects mood in different contexts is not well understood. We used Virtual Reality to explore mood and valence (colour preference) responses to colours in three different contexts: a living room, a hospital waiting room, and an empty cube-shaped room. Our hypothesis was that mood and preference responses to colour would vary depending on the virtual environment in which it was presented. Members of the general public participated in this prospective, within-participant case-crossover experimental study. Participants were randomised to one of the eight clusters, with five different colours presented in each cluster. Forty colours were investigated in total. Participants used a Google Daydream View head-mounted display to view the three virtual room environments, which each appeared ‘painted’ in one of the five different colours. Participants provided mood and valence responses at each exposure. Random effects logistic regression was used to explore responses to the colours in context. A total of 745 people participated. In one cluster, the mood and valence responses were significantly different in response to the same colour(s) in different rooms, indicating that context can impact mood and valence responses to colours. Virtual Reality is a feasible methodology to study colour in context. We found that the context in which a colour is presented can impact mood and valence responses, but this was not consistent across clusters. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Colour preference; Healthcare design; Mood; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"Testing the construct validity of a soccer-specific virtual reality simulator using novice, academy, and professional soccer players",VRS - Virtual Reality,B,"Virtual reality (VR) provides the potential for immersive and engaging training solutions for improving sport performance. However, if VR training is to be adopted and used in an effective and evidence-based fashion, a more rigorous assessment of the validity of the simulation is required. Construct validity is the degree to which the simulation provides an accurate representation of core features of the task. In the context of sport, if the training drills in the VR environment are a true representation of the skills needed in the real world, then those that excel at the sport in the real world should also excel in the virtual one. In this experiment, we examined the construct validity of a soccer-specific VR simulator by recruiting professional, academy, and novice players. Seventeen participants in each group completed four VR soccer drills, and the VR software provided scores relating to performance and process (e.g., passing accuracy, composure, reaction time, and adaptability). Based on these scores, an algorithm gave a diagnostic score relating to the predicted ability of the player. Results showed that this VR platform successfully differentiated between participants of differing skill levels. These results provide some support for the construct validity of this VR simulator and suggest at least partial overlap between the perceptual-cognitive and motor skills needed to perform well across ‘real’ and virtual environments. Further work is needed to explore the validity and fidelity of the simulation before its adoption as a training device can be fully endorsed. © 2020, The Author(s).",Football; Perceptual-cognitive expertise; Simulation; Skill acquisition; Training,Title_Abstract,True,
Scopus,journalPaper,2021,Validity of primary driving tasks in head-mounted display-based driving simulators,VRS - Virtual Reality,B,"The development of new car interior concepts requires tools, particularly in development phases before concept milestones, which enable subjective experiences and evaluations in static and driving situations. On the one hand, variant comparisons are necessary; on the other hand, the level of immersion should be high enough that participants can behave as they would in real cars. Virtual reality technologies and especially head-mounted displays are generally very suitable for such evaluations with the exception being in state-of-the-art driving simulators. Therefore, a validation study was undertaken in which primary driving tasks in two HMD-based simulators were compared with test runs in a real car. The difference between the simulators was only the state of the motion base (enabled vs. disabled). In both simulators and the test runs in the real car, four identical scenarios (straight, curves, overtaking and junction) were carried out. Since the focus is primarily on subjective ratings and gaze behaviour when evaluating new car interior concepts, in this study gaze behaviour was also priority. In addition, driving dynamics parameters were measured. The results reveal that the participants show more valid behaviour in the dynamic system than in the static simulator condition. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Behavioural validity; Driving simulation; Eye tracking; Gaze behaviour; Head-mounted display,Abstract,True,
Scopus,journalPaper,2021,A review of the evidence for training effectiveness with virtual reality technology,VRS - Virtual Reality,B,"Prior to adopting new technologies for training, evaluations must be executed to demonstrate their benefit. Specifically, the appeal of virtual reality has led to applications across domains. While many evaluations have been conducted on their effectiveness, there has yet been a review to summarize and categorize the evidence on training outcomes. To assess the benefits these new technologies may bring to the trainee, a review of the research on the training effectiveness with virtual reality (VR) technology that was conducted. The goal for this review was to take a domain-agnostic perspective to identify the knowledge, skills, and abilities (KSAs) that have been trained effectively or enhanced with the use of VR. This review searched the related literature within multiple databases and found publications that met the search criteria from 1992 to 2019. A discussion of previous VR training reviews is first presented, followed by an in-depth evaluation of the literature that met the inclusion criteria. Three distinct categories of KSAs were identified consistently: psychomotor performance, knowledge acquisition, and spatial ability. Recommendations to support achievement of training outcomes utilizing VR training systems are provided. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Knowledge acquisition; Psychomotor; Spatial ability; Training effectiveness; Virtual environment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Single-channel EEG measurement of engagement in virtual rehabilitation: a validation study,VRS - Virtual Reality,B,"Stroke rehabilitation suffers from low levels of patient engagement, impeding recovery. Virtual rehabilitation (VR) approaches can improve patient outcomes; however, there is limited understanding of the participant’s user experience and the field lacks a validated, objective measure of VR engagement. A neurophysiological measure of engagement in healthy adults was therefore examined, to inform future clinical studies. Twenty-four participants (Mage 26.7 years, range 18–47) interacted with a tabletop VR system (Elements DNA, or EDNA), after which they rated their experience on the presence questionnaire (PQ). Separately, participants completed tasks eliciting low (resting eyes-open and -closed) and high (EDNA VR and roller coaster simulation) levels of engagement while continuous electroencephalogram (EEG) was recorded from a single, left pre-frontal electrode. EEG differences between the resting and simulation conditions included an increase in theta power (p < 0.01) and a decrease in alpha power (p < 0.01). Importantly, theta power in simulation conditions correlated with PQ scores expressing the hands-on EDNA VR experience (rs = 0.38–0.48). In conclusion, the current results provide proof of concept that increased frontal theta power in healthy adults provides a valid measure of user engagement in VR simulation and participation. As the practical potential of VR is increasingly realised in stroke rehabilitation, objective EEG-based measures of engagement may provide a convenient and sensitive technique to assist in evaluating these interventions. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Electroencephalogram; Engagement; Presence; Rehabilitation; Virtual reality,Keywords,True,
Scopus,journalPaper,2022,Democratizing AI in biomedical image classification using virtual reality,VRS - Virtual Reality,B,"Artificial intelligence models can produce powerful predictive computer vision tools for healthcare. However, their development simultaneously requires computational skill as well as biomedical expertise. This barrier often impedes the wider utilization of AI in professional environments since biomedical experts often lack software development skills. We present the first development environment where a user with no prior training can build near-expert level convolutional neural network classifiers on real-world datasets. Our key contribution is a simplified environment in virtual reality where the user can build, compute, and critique a model. Through a controlled user study, we show that our software enables biomedical researchers and healthcare professionals with no AI development experience to build AI models with near-expert performance. We conclude that the potential role for AI in the biomedical domain can be realized more effectively by making its development more intuitive for non-technical domain experts using novel modes of interaction. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Deep learning; Machine learning; Neural nets; Virtual reality; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Performance evaluation of AR/VR training technologies for EMS first responders,VRS - Virtual Reality,B,"The first responder training sector presents crucial difficulties on adopting “future of work” online training principles because physical (muscle) memory is considered as important as cognitive memory. It is obvious that physical memory cannot be obtained by existing screen- and paper-based trainings. This paper presents a novel training framework for first responders that leverages augmented reality and virtual reality technologies. The framework incorporates novel design thinking processes that are implemented for the design of the training experiences. In addition, a qualitative and quantitative analysis of various metrics such as performance, time on task, accuracy and learning rate are developed to analyze the effectiveness of the proposed framework. A special use case of the emergency medical services called the ambulance bus is investigated and it is shown that the proposed training methodology improved the accuracy of the first responders by a factor of 46% and the speed on executing tasks by 29%. © 2020, This is a U.S. government work and its text is not subject to copyright protection in the United States; however, its text may be subject to foreign copyright protection.",Augmented reality; Evaluation; First responders; Learning technologies; Training; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,"RUN: rational ubiquitous navigation, a model for automated navigation and searching in virtual environments",VRS - Virtual Reality,B,"By now, the realm of virtual reality is abuzz with high-quality visuals, enough to simulate a real-world scene. The use of intelligence in virtual reality systems, however, is a milestone yet to be achieved to make possible seamless realism in a virtual environment. This paper presents a model, rational ubiquitous navigation to improve believability of a virtual environment. The model intends to augment maturity of a virtual agent by inculcating in it the human-like learning capability. A novel approach for automated navigation and searching is proposed by incorporating machine learning in virtual reality. An intelligent virtual agent learns objects of interest along with the paths followed for navigation. A mental map is molded dynamically as a user navigates in the environment. The map is followed by the agent during self-directed navigation to access any known object. After reaching at a location where an object of interest resides, the required object is selected on the basis of front-facet feature. The model is implemented in a case-study project learn objects on path (LOOP). Twelve users evaluated the model in the immersive maze-like environment of LOOP. Results of the evaluation assure applicability of the model in various cross-modality applications. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Automated navigation; Intelligent virtual reality systems; Machine learning in VR; Object-based searching,Abstract_Keywords,True,
Scopus,journalPaper,2021,"The relationships between the exploration of virtual space, its presence and entertainment in virtual reality, 360º and 2D",VRS - Virtual Reality,B,"This research investigates the relationships between the way virtual space is explored, the perception of presence and the degree of entertainment experienced during the experience. All participants (N = 147) interact with an omnidirectional video clip in three different conditions (VR, 360º, 2D). Throughout the two experimental sessions, affective, cognitive, and behavioural information is collected from the participant, which allows us to relate their interactive behaviour, their perception of presence and degree of entertainment. The possible influence of experience with interactive systems on current interactive behaviour is also analysed. The results highlight the complex relationships between these nuclear dimensions of VR and indicate the existence of two types of exploratory behaviour that we have called interface dependent and interface independent. When the first is present, there is no connection with the positive perception of presence and entertainment, but there is in the second. This typology shows the need to consider the learning processes in the access to the content through the interface in digital interactive systems such as VR and 360º. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Entertainment; Navigation; Presence; Virtual Reality; Virtual Space,Title_Keywords,True,
Scopus,journalPaper,2021,Immersive training of first responder squad leaders in untethered virtual reality,VRS - Virtual Reality,B,"We present the VROnSite platform that supports immersive training of first responder units’ on-site squad leaders. Our training platform is fully immersive, entirely untethered to ease use and provides two means of navigation—abstract and natural walking—to simulate stress and exhaustion, two important factors for decision making. With the platform’s capabilities, we close a gap in prior art for first responder training. Our research is closely interlocked with stakeholders from multiple fire brigades to gather early feedback in an iterative design process. In this paper, we present the system’s design rationale, provide insight into the process of training scenario development and present results of a user study with 41 squad leaders from the firefighting domain. Virtual disaster environments with two different navigation types were evaluated using quantitative and qualitative measures. Participants considered our platform highly suitable for training of decision making in complex first responder scenarios and results show the importance of the provided navigation technologies in this context. © 2020, The Author(s).",,Title,True,
Scopus,journalPaper,2021,Contributions of pictorial and binocular cues to the perception of distance in virtual reality,VRS - Virtual Reality,B,"We assessed the contribution of binocular disparity and the pictorial cues of linear perspective, texture, and scene clutter to the perception of distance in consumer virtual reality. As additional cues are made available, distance perception is predicted to improve, as measured by a reduction in systematic bias, and an increase in precision. We assessed (1) whether space is nonlinearly distorted; (2) the degree of size constancy across changes in distance; and (3) the weighting of pictorial versus binocular cues in VR. In the first task, participants positioned two spheres so as to divide the egocentric distance to a reference stimulus (presented between 3 and 11 m) into three equal thirds. In the second and third tasks, participants set the size of a sphere, presented at the same distances and at eye-height, to match that of a hand-held football. Each task was performed in four environments varying in the available cues. We measured accuracy by identifying systematic biases in responses and precision as the standard deviation of these responses. While there was no evidence of nonlinear compression of space, participants did tend to underestimate distance linearly, but this bias was reduced with the addition of each cue. The addition of binocular cues, when rich pictorial cues were already available, reduced both the bias and variability of estimates. These results show that linear perspective and binocular cues, in particular, improve the accuracy and precision of distance estimates in virtual reality across a range of distances typical of many indoor environments. © 2021, The Author(s).",Cue combination; Distance perception; Size constancy; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Implementation of escape room system based on augmented reality involving deep convolutional neural network,VRS - Virtual Reality,B,"Escape room is a live-action adventure game, where the players search clues, solve puzzles and achieve the assigned tasks. This paper proposed a novel escape room system combining augmented reality and deep learning technology. The system adopts a client–server architecture and can be divided into the server module, the smart glasses module and the player–hardware interaction module. The player–hardware interaction module consists of subsystems each of which includes a Raspberry Pi 3. HoloLens is used as the smart glasses in the experiment of the paper. The server communicates with all the Raspberry Pis and HoloLens through TCP/IP protocol and manages all the devices to achieve the game flow by following the process timeline. The smart glasses module provides two display modes, i.e., the AR 3D models display and the 2D text clues display. In the first mode, the SDK Vuforia is used for detection and tracking of markers. In the second mode, the scene images captured by HoloLens camera are sent to the pre-trained image classifier based on deep convolutional neural network. Considering both the image category and the game status value, the server decides the text clue image to be displayed on HoloLens. The accuracy of the image classification model reaches 94.9%, which can be correctly classified for a certain rotation angle and partial occlusion. The integration of AR, deep learning, electronics and escape room games opens up exciting new directions for the development of escape room. Finally, a built mini-escape room is analyzed to prove that the proposed system can support more complicated narratives showing the potential of achieving immersion. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Convolutional neural network; Escape room game; Image classification,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Using virtual reality for professional training practices: exploring the factors of applying stereoscopic 3D technologies in knowledge transfer,VRS - Virtual Reality,B,"Current research has constantly highlighted the significance of developing a learning culture with a robust knowledge sharing and transfer process in knowledge-based societies. The emergence of stereoscopic 3D virtual technologies provides organizations with an opportunity to develop immersive and virtual professional training practices for employees’ transformative learning. This research gathered data from a survey of 326 respondents to investigate employees’ virtual learning experiences in a virtual learning environment (VLE). The results show that a successful VLE model could possibly enhance employees’ learning motivation, process and satisfaction. The empirical evidence of this research suggests that there are three key components of framework for actual implementation; they are (1) the careful selection of VLE; (2) the appropriate design of pedagogical strategies; and (3) the effective use of virtual stimuli which involves the factors of the stereoscopic 3D visualization, telepresence and multisensory interactions. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Knowledge transfer; Learning science; Professional training; Stereoscopic 3D technology; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2022,Comparing augmented reality visualization methods for assembly procedures,VRS - Virtual Reality,B,"Assembly processes require now more than ever a systematic way to improve efficiency complying with increasing product demand. Several industrial scenarios have been using augmented reality (AR) to enhance environments with different types of information and influence the overall user satisfaction and performance. The purpose of this work is to evaluate three different AR-based methods that can be used to support users during the execution of assembly procedures. The AR methods evaluated are handheld mobile AR, indirect AR (showing the augmented scene on a monitor) and see-through head-mounted display. A user study was performed to assess performance, mental and physical workload, as well as acceptance of the aforementioned methods. Results from a thirty participants study did not reveal a best method in terms of performance and user preference, showing that all methods are adequate to support users. However, the study highlights the strengths and weaknesses of each method, which may lead to potential advantages in specific use cases. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Computer-aided manufacturing; Head-mounted display; Indirect augmented reality; Mobile augmented reality; User study,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Ellic’s Exercise Class: promoting physical activities during exergaming with immersive virtual reality,VRS - Virtual Reality,B,"This work presents the design and evaluation of a set of three mini exercise games (exergames), called “Ellic’s Exercise Class,” which allows people to play in virtual reality (VR) using a head-mounted display (HMD) with the intention to promote physical activities. The exergames require the player to move hands, arms, and body to interact with sporting gameplay events. The game design methodology considers both display advantages and physical limitations in VR technologies. The games are evaluated in a usability study with the goal to understand participants’ performance, gaming experience, effectiveness in motivating them to move, and their exercise intensity levels. In the study, we compare play of games in a VR environment simulated through an HMD with the play in front of a standard large flat-screen display (LFD), while ensuring the difficulty level of gameplay to be same in both. The participants’ moving distances and their answers in questionnaires show that they become more active and engaged when playing exergames with HMD, but according to heart rate data, there is not significant evidence that playing with HMD would be better than that with LFD in terms of increasing the energy expenditure. We discuss the findings in accordance with the relationship and independence between engagement and exertion in exergaming. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Entertainment computing; Exergaming; Human–computer interaction; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,A mobile VR-based respiratory biofeedback game to foster diaphragmatic breathing,VRS - Virtual Reality,B,"Virtual reality (VR) has become popular in mental health research. Several studies have explored the use of VR in the context of biofeedback protocols. In the present paper, we report on the development and evaluation of a VR-based respiratory biofeedback game to foster diaphragmatic breathing. The game integrates respiratory biofeedback, restorative VR and gamification. The game is designed to run on a mobile, all-in-one VR headset. Notably, an integrated VR hand controller is utilized as a sensor to detect respiration-induced movements of the diaphragm. In a longitudinal within-subjects study, we explored the feasibility of the game and tested the effectiveness of six training sessions. Participants reported a pleasant user experience. Moreover, the results show that the brief VR-based breathing training increased perceived breath awareness, improved diaphragmatic breathing, increased relaxation, decreased perceived stress, reduced symptoms of burnout and boosted relaxation-related self-efficacy. Future studies need to address the generalizability and long-term stability of the results, compare the approach with existing treatments and fine-tune the training components. © 2020, The Author(s).",Diaphragmatic breathing; Respiratory biofeedback; Self-efficacy; Serious game; Stress reduction; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Using virtual reality to optimize assessment of sociomoral skills,VRS - Virtual Reality,B,"Sociocognitive evaluation is an important component of comprehensive neuropsychological assessment. However, concerns have been raised as to whether traditional assessment methods such as paper-and-pencil questionnaire adequately represent real-life abilities. Virtual reality (VR) has the potential to increase ecological value by providing experimental conditions that are similar to those in a real-world environment. This project aimed to explore the potential benefits of using VR in the assessment of adolescent sociocognitive skills, specifically with regard to sociomoral decision-making and reasoning. A computer-based version and a VR version of the So-Moral task were used to compare the performance of adolescents aged 12–25 (n = 30) on sociomoral skills. In both versions, participants were presented with everyday sociomoral dilemmas and were asked to explain how they would react (sociomoral decision-making) and why (sociomoral maturity). The Interpersonal Reactivity Index and the Immersive Tendencies Questionnaire were completed to investigate the association between sociomoral skills, empathy and sense of presence. In both versions of the task, participants provided similar levels of sociomoral decision-making (F(1,26)=2.05, p = 0.16) and maturity (F(1,26)=1.92 , p = 0.18). Empathy was associated with presence (r = 0.39, p = 0.048) and with sociomoral maturity (r = 0.46, p = 0.01) only when assessed in VR, explaining a significant 21% of the variability in outcome. Together, these results support the notion of a disparity between static and dynamic sociocognitive assessment tools and suggest that the association between sociocognitive skills and underlying social or affective substrates may be susceptible to stimuli saliency and presentation. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Adolescents; Assessment; Empathy; Social cognition; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Enhancing stroke assessment simulation experience in clinical training using augmented reality,VRS - Virtual Reality,B,"The development of extended reality in recent years is opening doors for using extended reality devices (virtual reality, augmented, and mixed reality devices) in education and healthcare. The purpose of this pilot study was to test the use of augmented reality in teaching healthcare practitioners in a stroke assessment simulation designed for clinical training, where students at nursing school are targets in the study. To conduct our feasibility, a simulation application was developed for the mixed reality device that projects a human face displaying facial drooping (a symptom of stroke) onto a computerized training mannequin. Nursing students were then placed in a clinical simulation wherein they wore the mixed reality device and performed an assessment of their mannequin patient to identify the symptom of stroke and act accordingly. The students completed a survey following their simulations, and then provided feedback on the device and the quality of their experience. The results of the study show that most students enjoyed the simulation and felt that extended reality would be a very useful educational tool for clinical training and healthcare. Further development of the program and device is underway, and future tests will be conducted. The results from this study will be helpful in further progressing the development of extended reality, and the use of these devices in healthcare training. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Clinical training; FAST stroke assessment; Stroke simulation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Estimating cybersickness from virtual reality applications,VRS - Virtual Reality,B,"Cybersickness is a known issue in virtual reality affecting a notable percentage of the populations. However, predicting the level and incidence of cybersickness in new systems is difficult. Past publications were analyzed for their factors and resulting cybersickness scores. These factors were then used to develop three predictive models using demographics, software, and hardware factors. Using demographic information alone explained 44.2% of the adjusted variance in a linear model. Using hardware and software factors alone explained 55.3% of the adjusted variance in a linear model. Using demographics, software, and hardware factors did not use a linear model, but rather had an average residual error of 1.03. This residual error is an estimate of how far the predicted cybersickness score is from the actual score. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Displays; VIMS; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Effects of dynamic field-of-view restriction on cybersickness and presence in HMD-based virtual reality,VRS - Virtual Reality,B,"The phenomenon of cybersickness is currently hindering the mass market adoption of head-mounted display (HMD) virtual reality (VR) technologies. This study examined the effects of dynamic field-of-view (FOV) restriction on the cybersickness generated by ecological HMD-based gameplay. Forty participants were exposed to a commercially available HMD game (Marvel Powers United VR) under both unrestricted FOV and dynamic FOV restriction conditions across three sessions. Participants had their spontaneous postural instability measured before entering VR. Then, during/following each of these 10-min exposures to HMD VR, they rated their cybersickness, vection (illusory self-motion), and feelings of presence. Individual differences in spontaneous postural instability were found to predict cybersickness during HMD VR gameplay. Cybersickness severity increased steadily over the course of each VR exposure and was significantly reduced by dynamic FOV restriction. Presence also increased steadily over the course of each VR exposure and was positively correlated with vection. We conclude that: (1) postural instability can identify people who are more susceptible to cybersickness, (2) vection can increase an HMD user’s feelings of presence, and (3) dynamic FOV restriction can serve as a viable countermeasure to cybersickness. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Head-mounted display; Presence; Vection; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,"Memory for a virtual reality experience in children and adults according to image quality, emotion, and sense of presence",VRS - Virtual Reality,B,"Numerous studies have explored the effects of virtual reality (VR) on adults’ cognition. Little is known, however, of these effects in children. The aim of this study was to explore, in both children and adults, the respective roles of the specific factors of VR, such as immersion, sense of presence and emotion, on memory performance. To do so, we used a head-mounted display to present a VR experience in which we manipulated immersion by varying 3D asset quality (High and Low) and emotion by presenting negative, neutral and positive stimuli. 48 adults (Mage = 20.65) and 40 children (Mage = 11.63) were both divided into two experimental groups (High vs. Low 3D model quality). Valence, arousal, and sense of presence were self-assessed by means of questionnaires, while memory of the presented stimuli was assessed using a free recall task. We also performed physiological measurements to provide objective support for our data. Results showed that memory performance was better for emotional than for neutral stimuli regardless of age group, even though children seemed to avoid looking at negative stimuli compared to neutral ones. Memory was predicted by arousal and presence in adults and only by arousal in children. Memory was not impaired by using poor image quality when highly arousing content was displayed. This study revealed that, contrary to adults, the use of poor image quality did not protect children from strong emotional experiences in VR. The roles of familiarity and arousal are discussed to help explain these results. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Adults; Children; Emotion; Head-mounted display; Memory; Physiology; Presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Exploring sensorimotor performance and user experience within a virtual reality golf putting simulator,VRS - Virtual Reality,B,"In light of recent advances in technology, there has been growing interest in virtual reality (VR) simulations for training purposes in a range of high-performance environments, from sport to nuclear decommissioning. For a VR simulation to elicit effective transfer of training to the real-world, it must provide a sufficient level of validity, that is, it must be representative of the real-world skill. In order to develop the most effective simulations, assessments of validity should be carried out prior to implementing simulations in training. The aim of this work was to test elements of the physical fidelity, psychological fidelity and construct validity of a VR golf putting simulation. Self-report measures of task load and presence in the simulation were taken following real and simulated golf putting to assess psychological and physical fidelity. The performance of novice and expert golfers in the simulation was also compared as an initial test of construct validity. Participants reported a high degree of presence in the simulation, and there was little difference between real and virtual putting in terms of task demands. Experts performed significantly better in the simulation than novices (p =.001, d = 1.23), and there was a significant relationship between performance on the real and virtual tasks (r =.46, p =.004). The results indicated that the simulation exhibited an acceptable degree of construct validity and psychological fidelity. However, some differences between the real and virtual tasks emerged, suggesting further validation work is required. © 2020, The Author(s).",Construct validity; Simulation; Sport; Training; VR,Title_Abstract,True,
Scopus,journalPaper,2021,An electromyogram-based tapping gesture model with differentiated vibration feedback by low-fidelity actuators,VRS - Virtual Reality,B,"Among other aspects like attention and focus, the effectiveness of actions, such as training in percussion, also relies on the correct gesture and strength of the individual carrying out the activity repetitively to develop proper muscle memory and train motor skills. To make this process economical and efficient, we propose a system that models the feature of direction and strength of a tap and receives the corresponding active haptic feedback in an immersive virtual environment. We propose a novel tapping gesture model that takes electromyogram and vibration feedback into consideration. We also propose an approach of designing vibration modes that follow the mechanical stimulation principles and generate distinguishable vibration feedback with low-fidelity actuators. We developed a prototype using myoelectric and haptic apparatus (Myo armband and HTC VIVE controllers) and evaluated our system by conducting a user study with 50 participants. The evaluation shows that the users can distinguish three common materials (wood, rubber, and aluminum), and our system allows them to actively adjust their direction and strength to grasp the correct point while tapping. We demonstrate the efficacy of our tapping model on a virtual Chinese chimes application and observe positive feedback from both novices and expert users. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Electromyography; Haptic feedback; Immersive virtual reality; Low-fidelity actuators; Mechanical stimulation; Vibration model,Keywords,True,
Scopus,journalPaper,2021,Immersive Virtual Reality in K-12 and Higher Education: A systematic review of the last decade scientific literature,VRS - Virtual Reality,B,"There has been an increasing interest in applying immersive virtual reality (VR) applications to support various instructional design methods and outcomes not only in K-12 (Primary and Secondary), but also in higher education (HE) settings. However, there is a scarcity of studies to provide the potentials and challenges of VR-supported instructional design strategies and/or techniques that can influence teaching and learning. This systematic review presents a variety of studies that provide qualitative and/or quantitative data to investigate the current practices with VR support focusing on students’ outcomes, performance, alongside with the benefits and challenges of this technology concerning the analysis of visual features and design elements with mobile and desktop computing devices in different learning subjects. During the selection and screening process, forty-six (n = 46) articles published from the middle of 2009 until the middle of 2020 were finally included for a detailed analysis and synthesis of which twenty-one and twenty-five in K-12 and HE, respectively. The majority of studies were focused on describing and evaluating the appropriateness or the effectiveness of the applied instructional design processes using various VR applications to disseminate their findings on user experience, usability issues, students’ outcomes, and/or learning performance. This study contributes by reviewing how instructional design strategies and techniques can potentially benefit students’ learning performance using a wide range of VR applications. It also proposes some recommendations to guide and lead effective instructional design settings in several teaching and learning contexts to outline a more accurate and up-to-date picture of the current state of literature. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Human–computer interface; Immersive technologies; Simulations; Systematic review; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Quantitative measures of the visually evoked sensation of body movement in space (Vection) using Electrovestibulography (EVestG),VRS - Virtual Reality,B,"Vection is defined as an illusory self-motion sensation induced in stationary observers that can be experienced in a real/virtual world. Vection, as a result of immersion in virtual reality (VR) environments, can subsequently lead to a sense of inability to maintain postural control and cause cybersickness symptoms. The multisensory integration of visual and vestibular (balance) information plays a vital role in vection. The etiology of vection perception, as well as, the vestibular response change while experiencing vection is poorly understood. This study explores vestibular response change following vection in 20 individuals (10 females, 26.45 ± 4.40 (SD) years). Vection was induced in participants using an immersive VR roller-coaster. The vestibular response was measured simultaneously using a noninvasive method called Electrovestibulography (EVestG). The detected field potentials and the time intervals between the field potentials were extracted from the recorded EVestG signals corresponding to four segments of the VR roller-coaster trajectory namely Stationary, Up movement, Down movement, and slopes and turns (Mix). The results show that the Stationary segment is significantly different (P < 0.05) from other dynamic segments when the average field potential of the right and left ear are subtracted. Furthermore, the Stationary segment shows longer time intervals between field potentials compared to those of the other segments in the right ear. These observations suggest that the combined effect of the visually induced sensation of self-motion together with a concurrent/co-occurring stress/anxiety factor can affect the vestibular activity in an excitatory way. Increased excitatory vestibular activity implies increased feeling of imbalance and more likelihood of experiencing cybersickness by the participants. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Afferent; Efferent; EVestG; Vestibular; Virtual reality vection; Visual,Abstract_Keywords,True,
Scopus,journalPaper,2021,"A virtual classroom can elicit teachers’ speech characteristics: evidence from acoustic measurements during in vivo and in virtuo lessons, compared to a free speech control situation",VRS - Virtual Reality,B,"To achieve pedagogic goals and deal with environmental constraints such as noise when lecturing, teachers adapt their speech production in terms of frequency, intensity, and temporal aspects. The mastery of appropriate vocal skills is key to teachers’ speech intelligibility, health, and educational effectiveness. This project tests the relevance of virtual reality (VR) for training teachers’ vocal skills by simulating a lesson in a realistic VR environment characterized by adjustable constraints such as background noise and fidgety children. The VR environment depicts an elementary school classroom with 16 pupils aged 9 to 12 years old animated with typical childlike actions. To validate this virtual classroom in terms of speech characteristics, we conducted acoustic analyses on the speech productions of 30 female teachers in three conditions: (1) giving a free speech while facing the experimenter (control), (2) teaching in their usual classroom (in vivo), and (3) teaching the same lesson in a virtual classroom (in virtuo). The background noise in the VR setting was adjusted for each talker so it was similar to the level measured in vivo. Repeated measures ANOVAs showed that teachers significantly increased their voice frequency, intensity, and intonation and made longer pauses while speaking in vivo and in virtuo, compared to the control condition (p <.001). These voice and speech adaptations (partly related to background noise), the strong feeling of presence, and the lack of side effects suggest that the virtual classroom may facilitate voice training and rehabilitation for teachers. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Acoustic measurement; Lombard speech; Speech and language therapy; Teacher training; Virtual reality; Vocal behavior,Abstract_Keywords,True,
Scopus,journalPaper,2021,An evaluation of a virtual atlas of portion sizes (VAPS) mobile augmented reality for portion size estimation,VRS - Virtual Reality,B,"Food portion size estimation is a critical yet challenging task in dietary assessment. Augmented reality technology enables the presentation of food dimensions and volume in a virtual three-dimensional object. It has the potential to improve perception and estimation of portion sizes. This study aims to develop and evaluate a novel mobile augmented reality application, namely Virtual Atlas of Portion Sizes (VAPS), as a portion size estimation aid. The development methodology of VAPS involves food photography, reconstruction of 3D models using photogrammetry method and presenting them in an AR environment. The 3D food models displayed in either semi-transparent or vivid mode for users to perform food portion estimation. Users can then resize and rotate the 3D models to fit the virtual model with the actual food. A total of thirty-six participants were involved in the evaluation and were divided into a health science and a non-health science background group. VAPS received good usability level with 76 SUS score. In terms of task completion time, unsurprisingly, the health science group performed faster. However, both groups have equivalent accuracy on the food portion estimation task using VAPS: 22.5% for non-health science group and 26.6% for health science group. The health science group liked and have better accuracy in vivid 3D food models (37.5%). Meanwhile, the non-health science group preferred semi-transparent 3D food models, but the accuracy is not significantly different between semi-transparent (25%) and vivid 3D food model (20%). Results demonstrate the potential of VAPS to aid in portion size estimation for dietary assessment, and participants’ feedback will be incorporated in the future for improvement of the app. © 2020, The Author(s).",Augmented reality; Food atlas; Food portion; Human–computer interaction; Portion estimation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Virtual reality and augmented reality in social learning spaces: a literature review,VRS - Virtual Reality,B,"In this survey, we explore Virtual Reality and Augmented Reality within social learning spaces, such as classrooms and museums, while also extending into relevant social interaction concepts found within more reality-based and social immersive media frameworks. To provide a foundation for our findings we explore properties and interactions relevant to educational use in social learning spaces; in addition to several learning theories such as constructivism, social cognitive theory, connectivism, and activity theory, within a CSCL lens, to build a theoretical foundation for future virtual reality/augmented reality educational frameworks. Several virtual reality/augmented reality examples for learning are explored, and several promising areas to further research, such as a greater focus on accessibility, the interplay between the physical and virtual environments, and suggestions for updated learning theory foundations, are proposed. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Collaborative; Education; Mixed reality; Multi-user; Social learning spaces; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Accurate and stable alignment of virtual and real spaces using consumer-grade trackers,VRS - Virtual Reality,B,"The world space defined by a traditional virtual reality application is generally regarded as separate from the real-world space in which users actually exist. In this paper, we present a method that enables such detached spaces to connect in an integrated space. In particular, we show that by using a specially manufactured calibration board and three consumer-grade position tracking devices, a reference coordinate system can easily be set up in the physical space, whose geometric relationship with the virtual reality space is estimated with high numerical accuracy and stability. Then, we demonstrate that, combined with traditional computer vision techniques for marker tracking, the presented technique allows colocated users from virtual, augmented, and mixed realities to cooperate with each other while making effective use of technologies from other realities. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Consumer-grade trackers; Extended-reality applications; Numerical accuracy and stability; Space alignment algorithms; Virtual and real worlds,Abstract,True,
Scopus,journalPaper,2021,Promoting eco-driving behavior through multisensory stimulation: a preliminary study on the use of visual and haptic feedback in a virtual reality driving simulator,VRS - Virtual Reality,B,"This paper describes the design and preliminary test of a virtual reality driving simulator capable of conveying haptic and visual messages to promote eco-sustainable driving behavior. The driving simulator was implemented through the Unity game engine; a large street environment, including high-speed and urban sections, was created to examine different driving behaviors. The hardware setup included a gaming driving seat, equipped with a steering wheel and pedals; the virtual scenarios were displayed through an Oculus Rift headset to guarantee an immersive experience. Haptic stimulation (i.e., vibrations) was delivered to the driver through the accelerator pedal, while visual stimuli (i.e., icons and colors) were shown on a virtual head-up display. The sensory feedbacks were presented both alone and in combination, providing information about excessive acceleration and speed. Four different virtual scenarios, each one including a distracting element (i.e., navigator, rain, call, and traffic), were also created. Ten participants tested the simulator. Fuel consumption was evaluated by calculating a mean power index (MPI) in reference to the sensory feedback presentation; physiological reactions and responses to a usability survey were also collected. The results revealed that the haptic and visuo-haptic feedback were responsible for an MPI reduction, respectively, for 14% and 11% compared with a condition of no feedback presentation; while visual feedback alone resulted in an MPI increase of 11%. The efficacy of haptic feedback was also accompanied by a more relaxing physiological state of the users, compared with the visual stimulation. The system’s usability was adequate, although haptic stimuli were rated slightly more intrusive than the visual ones. Overall, these preliminary results highlight how promising the use of the haptic channel can be in communicating and guiding the driver toward a more eco-sustainable behavior. © 2021, The Author(s).",Eco-driving; Haptics; Multisensory; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Social viewing in cinematic virtual reality: a design space for social movie applications,VRS - Virtual Reality,B,"Since watching movies is a social experience for most people, it is important to know how an application should be designed for enabling shared cinematic virtual reality (CVR) experiences via head-mounted displays (HMDs). Viewers can feel isolated when watching omnidirectional movies with HMDs. Even if they are watching the movie simultaneously, they do not automatically see the same field of view, since they can freely choose their viewing direction. Our goal is to explore interaction techniques to efficiently support social viewing and to improve social movie experiences in CVR. Based on the literature review and insights from earlier work, we identify seven challenges that need to be addressed: communication, field-of-view (FoV) awareness, togetherness, accessibility, interaction techniques, synchronization, and multiuser environments. We investigate four aspects (voice chat, sending emotion states, FoV indication, and video chat) to address some of the challenges and report the results of four user studies. Finally, we present and discuss a design space for CVR social movie applications and highlight directions for future work. © 2020, The Author(s).",360° video; Cinematic virtual reality; Interactive TV; Omnidirectional video; Social viewing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Virtual Observations: a software tool for contextual observation and assessment of user’s actions in virtual reality,VRS - Virtual Reality,B,"In this paper, we present ‘Virtual Observation’ (VO) a software tool for contextual observation and assessment of user’s directly from within the virtual reality (VR) simulation framework. Unlike other recording systems, the VO system described in this paper focuses on recording and reconstructing VR user’s positional, rotational and input data to recreate the same experience the user had with a VR simulation. Different from animation-based approaches, VO records user inputs and reconstructs the simulation from them and the user positional data. Moreover, the system allows the broadcast of this information to a remote machine enabling remote live observation of the simulation. Datasets recorded by the system can be shared by exporting them as XML files or, optionally, into a standalone online application, such as browser WebGL, allowing researchers, developers and educators to share and review a VR user simulation through a free-moving camera using a web browser. In this paper, the consistency of the data generated from the software by the client, server and reconstructed datasets acquired during real-time live observations was evaluated. We conclude that this Virtual Observation software offers detailed reconstruction of low-level information and visual information of user actions during simulations for both live and offline observations. We envision that our system will be of benefit for researchers, developers and educators that work with VR applications. © 2020, The Author(s).",Observing; Replaying; Reviewing; Simulations; Virtual Observation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Fear and loathing in VR: the emotional and physiological effects of immersive games,VRS - Virtual Reality,B,"Compared to traditional screen-based media, virtual reality (VR) generally leads to stronger feelings of presence. The current study aimed to investigate whether playing games in VR resulted in a stronger sense of presence than playing on a TV, and whether these feelings of presence affected players’ emotional and physiological responses to the games. Two experiments were conducted among 128 students, comparing the effects of playing either a survival horror game (N = 59) or a first-person shooter (N = 69) on a TV or in VR on physiological and subjective fear, hostility and enjoyment. Results showed that playing games in VR resulted in a stronger sense of presence, lower heart rate variability and a stronger subjective sense of fear. The feeling of presence thereby mediated the effects of VR on fear. The effects of playing a first-person shooter in VR on hostility were mixed, and gaming in VR was not more enjoyable than on TV. Regardless of the type of game or display medium, hostility increased significantly post-play. This study provides evidence that commercial VR games can affect feelings of presence and the physiological and emotional state of players. © 2021, The Author(s).",Aggression; Fear; Measurements; Physiological; Presence; Video games; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Perceptual self-position estimation based on gaze tracking in virtual reality,VRS - Virtual Reality,B,"The depth perception of human visual system is divergent between virtual and real space; this depth discrepancy affects the spatial judgment of the user in a virtual space, which means the user cannot precisely locate their self-position in a virtual space. Existing localization methods ignore the depth discrepancy and only concentrate on increasing location accuracy in real space. Thus, the discrepancy always exists in virtual space, which induces visual discomfort. In this paper, a localization method based on depth perception is proposed to measure the self-position of the user in a virtual environment. Using binocular gaze tracking, this method estimates perceived depth and constructs an eye matrix by measuring gaze convergence on a target. Comparing the eye matrix and camera matrix, the method can automatically calculate the actual depth of the viewed target. Then, the difference between the actual depth and the perceived depth can be explicitly estimated without markers. The position of the virtual camera is compensated by the depth difference to obtain perceptual self-position. Furthermore, a virtual reality system is redesigned by adjusting the virtual camera position. The redesigned system makes users feel that the distance (from the user to an object) is the same in virtual and real space. Experimental results demonstrate that the redesigned system can improve the user’s visual experiences, which validate the superiority of the proposed localization method. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Depth perception; Gaze tracking; Human–computer interaction; Stereo vision; Visual discomfort,Title_Abstract,True,
Scopus,journalPaper,2021,A novel method for VR sickness reduction based on dynamic field of view processing,VRS - Virtual Reality,B,"In this paper, we proposed a novel method for virtual reality (VR) sickness reduction based on dynamic field of view (FOV) processing. Dynamic FOV processing is performed based on the estimated VR sickness for each video frame. The level of sickness is estimated using VR sickness model, which is obtained by defining the relationship between the motion information and the measured VR sickness. For motion information analysis, subregion-based correspondence points tracking is used to efficiently remove outliers and prevent prediction error propagation. Amount of head dispersion is used as a quantitative VR sickness measure, which can be calculated from inertial measurement unit sensor in VR devices. The optimal FOV range was determined by experimentally validating a minimum FOV that can effectively reduce VR sickness with almost negligible loss in presence. The simulation results show a significant decrease of 37% compared to full FOV viewing, when FOV is dynamically varied between full and 60°. © 2020, The Author(s).",Dynamic FOV processing; Sickness measurement; VR contents motion analysis; VR sickness,Abstract,True,
Scopus,journalPaper,2021,"Virtual reality and 360° panorama technology: a media comparison to study changes in sense of presence, anxiety, and positive emotions",VRS - Virtual Reality,B,"Recently, 360° panorama technologies have been used to create videos and pictures of real and virtual environments, thus opening new possibilities for psychological research. The aim of this study is to compare a 360° real panorama environment to a computer-simulated one to verify if they are equally efficient in generating sense of presence, emotions, and relaxation in individuals. The study employs a 3 × 2 mixed factorial design. Forty participants took part in the study and were assessed on self-reported anxiety and mood levels before and during the virtual reality (VR) experience of a relaxing video in computer graphics or shot in 360°. After the experience, sense of presence and experience ratings were also collected. Heart rate data during the experience were also used. Both inferential and Bayesian analyses showed a lack of effect of the manipulation: there is no difference between a 360° panorama environment and a simulated environment in generating sense of presence, anxiety reduction, and in improving emotional states. These results highlight the feasibility of using a 360° real panorama VR if the participants’ task is passive and requires no active exploration of the environment, as the development of the 360° video is easier and cheaper than the one required by a computer-simulated environment. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",360° Technology; Anxiety; Case comparison; Computer-simulated environment; Mood; Sense of presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Surgical navigation system for brachytherapy based on mixed reality using a novel stereo registration method,VRS - Virtual Reality,B,"In this study, we present a novel mixed reality navigation system to facilitate brachytherapy, which is an effective method for curing cancer. The accuracy of needle positioning is a vital problem in brachytherapy that can influence the treatment effect. The purpose of the developed system is to help doctors more quickly and easily position the needles and improve seed accuracy in brachytherapy surgery. Based on mixed reality and a multi-information fusion method, a successful fusion of medical images and a preoperative plan for real patients was achieved, allowing doctors to gain an intuitive understanding of the tumor. Image recognition and pose estimation were used to track the needle punctures in real time and perform registration processes. After global registration using an iterative closest-point algorithm with a pattern tracker, medical images and volume renderings of organs, needles and seeds were aligned with the patient. Based on a phantom experiment, the average needle location error was 0.961 mm, and the angle error was 1.861°. The accuracy of needle insertion was 1.586 mm, and the angle error was 2.429°. This research presents the design and validation of a surgical navigation system for thoracoabdominal brachytherapy based on mixed reality. The proposed system was validated through both phantom and animal experiments. The results indicated that the proposed system achieves clinically acceptable accuracy and can aid doctors in performing surgery based on a visualized plan. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Brachytherapy; Mixed reality; Stereo registration; Surgical navigation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,A scoping review of the educational uses of 6DoF HMDs,VRS - Virtual Reality,B,"Head-mounted displays offering 6° of freedom have not been sufficiently researched in terms of their impact on users' learning and skills. The issue is multi-dimensional, heterogeneous, and complex. The paper presents a scoping review aiming to map and review the existing literature on the matter. The areas in which they have been mostly used, the benefits, and the negative effects they may have had, were examined. Eighty-seven articles were identified and analyzed. Out of them, only fourteen were considered as having adequate statistical power. Most had relatively small sample sizes and number of interventions, while university students were the most frequent target group. The review identified a total of twenty-seven distinct learning domains in which head-mounted displays offering six degrees of freedom were applied, with medical science being the most common one. The results in the reviewed papers (in terms of knowledge or skills) demonstrated that these devices outperform other tools. Moreover, they appear to have a positive effect on users' engagement, motivation to learn, immersion, and enjoyment. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Degrees of freedom; Education; Fully immersive virtual reality; Head-mounted displays; Scoping review,Keywords,True,
Scopus,journalPaper,2021,Eye movements to absent objects during mental imagery and visual memory in immersive virtual reality,VRS - Virtual Reality,B,"The role of eye movements in mental imagery and visual memory is typically investigated by presenting stimuli or scenes on a two-dimensional (2D) computer screen. When questioned about objects that had previously been presented on-screen, people gaze back to the location of the stimuli, even though those regions are blank during retrieval. It remains unclear whether this behavior is limited to a highly controlled experimental setting using 2D screens or whether it also occurs in a more naturalistic setting. The present study aims to overcome this shortcoming. Three-dimensional (3D) objects were presented along a circular path in an immersive virtual room. During retrieval, participants were given two tasks: to visualize the objects, which they had encoded before, and to evaluate a statement about visual details of the object. We observed longer fixation duration in the area, on which the object was previously displayed, when compared to other possible target locations. However, in 89% of the time, participants fixated none of the predefined areas. On the one hand, this shows that looking at nothing may be overestimated in 2D screen-based paradigm, on the other hand, the looking at nothing effect was still present in the 3D immersive virtual reality setting, and thus it extends external validity of previous findings. Eye movements during retrieval reinstate spatial information of previously inspected stimuli. © 2020, The Author(s).",Eye movements; Eye tracking; Mental imagery; Virtual reality; Visual memory,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Comparison of spatial orientation skill between real and virtual environment,VRS - Virtual Reality,B,"Virtual reality (VR) is a promising tool and is increasingly used in many different fields, in which virtual walking can be generalized through detailed modeling of the physical environment such as in sports science, medicine and furthermore. However, the visualization of a virtual environment using a head-mounted display (HMD) differs compared to reality, and it is still not clear whether the visual perception works equally within VR. The purpose of the current study is to compare the spatial orientation between real world (RW) and VR. Therefore, the participants had to walk blindfolded to different placed objects in a real and virtual environment, which did not differ in physical properties. They were equipped with passive markers to track the position of the back of their hand, which was used to specify each object’s location. The first task was to walk blindfolded from one starting position to different placed sport-specific objects requiring different degrees of rotation after observing them for 15 s (0°, 45°, 180°, and 225°). The three-way ANOVA with repeated measurements indicated no significant difference between RW and VR within the different degrees of rotation (p > 0.05). In addition, the participants were asked to walk blindfolded three times from a new starting position to two objects, which were ordered differently during the conditions. Except for one case, no significant differences in the pathways between RW and VR were found (p > 0.05). This study supports that the use of VR ensures similar behavior of the participants compared to real-world interactions and its authorization of use. © 2021, The Author(s).",Head-mounted display; Spatial orientation; Virtual reality; Visual perception,Abstract_Keywords,True,
Scopus,journalPaper,2021,The effect of paper-based manual and stereoscopic-based mobile augmented reality systems on knowledge retention,VRS - Virtual Reality,B,"Augmented reality technology is attracting increased attention, given the popularity of the smartphone. The utilisation of the smartphone with AR technology is also enabling mobile augmented reality (MAR) to become accessible. Stereoscopic vision offers users the benefit of depth perception, which can help improve user memory. Therefore, this study aims to investigate the effect on knowledge retention using MAR with stereoscopic vision. An experiment to compare a stereoscopic-based MAR application and a paper-based manual was designed to test the participants’ knowledge retention on a product part and disassembly process. The developed MAR adopted a smartphone and headset case to provide a stereoscopic view to the user. The experiment consisted of both pre-test and post-test phases where the pre-test phase examined the participant’s knowledge about the product, and the post-test phase focused on how much information the participants could recall. The post-test phase was conducted after 48 h following the pre-test phase since long-term memory required several hours in which to stabilise. The results showed that the MAR group had an advantage over the paper-based manual group for both information retentions having a mean score of 8.89 from the MAR group, compared with 6.33 for the paper-based manual group. Moreover, the MAR group was observed to have fewer errors in the post-test with a mean score of 0.53, whereas the paper-based achieved a score of 2.2. This result indicated that the MAR group and the paper-based manual group had equivalent performance in the completion time without the assistance tool. In addition, MAR had a better mean score for the subjective feedback (usefulness = 4.47, ease of use 4.18 and satisfaction = 4.07). © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Human–computer interaction; Knowledge retention; Long-term memory; Mobile augmented reality; Stereoscopic vision,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Immersive virtual reality as physical therapy in older adults: present or future (systematic review),VRS - Virtual Reality,B,"Increased life expectancy leads to an increase in the number of older adults and in the prevalence of aging-associated diseases and disabilities. Active aging strategies—particularly those based on physical exercise therapy— have great positive impact on older people’s health. Virtual reality (VR) represents an innovative approach to involve and motivate patients during therapy sessions. Exergaming programs based on VR technologies and commercially available gaming platforms are amenable to therapeutic use in older adults, according to various systematic reviews and meta-analyses. The use of immersive virtual reality (IVR) in the field of rehabilitation or physical skills training in seniors is understudied.In the present systematic review, we analyze the therapeutic use and application of IVR in older adults through physical activity. We describe the populations studied, the conditions of IVR application (device, session, physical, and virtual environments), its potential benefits, and its limitations. We found that most studies are feasibility pilot experiences, where the use and acceptability of the immersive platform were evaluated. Cumulative data suggest that the use of IVR with therapeutic intent in senior populations is in early stages of clinical development and shows promise as a complementary tool in the fields of health, rehabilitation, and active aging. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Aged; Exercise therapy; Rehabilitation; Virtual reality; Virtual reality exposure therapy; Virtual reality immersion therapy,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"Framework for developing alternative reality environments to engineer large, complex systems",VRS - Virtual Reality,B,"We present a framework for alternative reality (XR) technologies to enable an understanding of what constitutes an XR environment when used in the context of design and engineering of large, complex systems.The framework provides guidelines for implementing the corresponding desired human sensory experience. This work is founded on the existing literature which defines theoretical Spectra, such as Fidelity, to systematically characterize an XR environment taxonomy. We identify landmarks for four XR categories within these Spectra and provide definitions that can used to establish a common vernacular. We further map these to specific human sensing modalities that are influenced by XR, such as tactility and vision, and define the technical requirements needed to augment the human experience in the desired XR environment. Finally, we connect the theoretical elements to the technical requirements to create an integrated XR framework.The utility of this framework is demonstrated in a case study addressing the use of XR technologies for five stakeholder groups involved in the evaluation of spacecraft habitat design and operations. This demonstrates the utility of the proposed XR taxonomy in a spacecraft habitat design process, which could be extended to other similar applications. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Hybrid reality; Spacecraft design; Virtual reality,Keywords,True,
Scopus,journalPaper,2021,"Virtual, mixed, and augmented reality: a systematic review for immersive systems research",VRS - Virtual Reality,B,"Immersive systems can be used to capture new data, create new experiences, and provide new insights by generating virtual elements of physical and imagined worlds. Immersive systems are seeing increased application across a broad array of fields. However, in many situations it is unknown if an immersive application performs as well or better than the existing application in accomplishing a specific task. The purpose of this study is to conduct a systematic review of the literature that addresses the performance of immersive systems. This review assesses those applications where experiments, tests, or clinical trials have been performed to evaluate the proposed application. This research addresses a broad range of application areas and considers studies that compared one or more immersive systems with a control group or evaluated performance data for the immersive system pre- and post-test. The results identify those applications that have been successfully tested and also delineate areas of future research where more data may be needed to assess the effectiveness of proposed applications. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",AR; Augmented reality; Empirical research; Immersive systems; Mixed reality; MR; Systematic review; Virtual reality; VR,Title_Keywords,True,
Scopus,journalPaper,2021,Exploring the effect of an augmented reality literacy programme for reading and spelling difficulties for children diagnosed with ADHD,VRS - Virtual Reality,B,"Children diagnosed with attention deficit hyperactivity disorder (ADHD) experience a variety of difficulties related to three primary symptoms: hyperactivity, inattention and impulsivity. The most common type of ADHD has a combination of all three symptom areas. These core symptoms may negatively impact the academic and social performance of children throughout their school life. The AHA (ADHD-Augmented) project focused specifically on the impact of digital technologies’ intervention on literacy skills of children that participated in the pilot study and were diagnosed with ADHD prior to the intervention. Existing research has shown that augmented reality (AR) can improve academic outcomes by stimulating pupils’ attention. AHA project aimed at implementing an evidence-based intervention to improve ADHD children’s reading and spelling abilities through the enhancement of an existing literacy programme with AR functionality. The present paper reports preliminary findings of the pilot study aimed at evaluating the effectiveness of the AHA system in promoting the acquisition of literacy skills in a sample of children diagnosed with ADHD compared to the literacy programme as usual. Background information on the main characteristics and difficulties related to the teaching and learning process associated with children diagnosed with ADHD are first introduced; the design and methodology of the AHA project intervention are also described. The preliminary findings have shown that AHA project succeeded in delivering an AR solution within an existing online literacy programme, which integrates a set of specific technologies and supports interactive educational content, services, assessment, and feedback. © 2020, The Author(s).",ADHD; Augmented reality; Behavioural monitoring; Literacy programme; Reading and spelling difficulties,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Assessing the learning and transfer of gaze behaviours in immersive virtual reality,VRS - Virtual Reality,B,"Virtual reality (VR) has clear potential for improving simulation training in many industries. Yet, methods for testing the fidelity, validity and training efficacy of VR environments are, in general, lagging behind their adoption. There is limited understanding of how readily skills learned in VR will transfer, and what features of training design will facilitate effective transfer. Two potentially important elements are the psychological fidelity of the environment, and the stimulus correspondence with the transfer context. In this study, we examined the effectiveness of VR for training police room searching procedures, and assessed the corresponding development of perceptual-cognitive skill through eye-tracking indices of search efficiency. Participants (n = 54) were assigned to a VR rule-learning and search training task (FTG), a search only training task (SG) or a no-practice control group (CG). Both FTG and SG developed more efficient search behaviours during the training task, as indexed by increases in saccade size and reductions in search rate. The FTG performed marginally better than the CG on a novel VR transfer test, but no better than the SG. More efficient gaze behaviours learned during training were not, however, evident during the transfer test. These findings demonstrate how VR can be used to develop perceptual-cognitive skills, but also highlight the challenges of achieving transfer of training. © 2021, The Author(s).",Fidelity; Police; Policing; Training; Validity; VR,Title_Abstract,True,
Scopus,journalPaper,2021,"Virtual experience, real consequences: the potential negative emotional consequences of virtual reality gameplay",VRS - Virtual Reality,B,"As virtual reality (VR) technology enters mainstream markets, it is imperative that we understand its potential impacts on users, both positive and negative. In the present paper, we build on the extant literature’s focus on the physical side effects of VR gameplay (e.g., cybersickness) by focusing on VR’s potential to intensify users’ experiences of negative emotions. We first conducted a preliminary survey to assess users’ emotional responses during VR gameplay, with the results suggesting that certain VR situations can in fact produce intense negative emotional experiences. We then designed an interactive scenario intended to elicit low to moderate amounts of negative emotion, wherein participants played out the scenario in either VR (using the HTC Vive) or on a laptop computer. Compared to the participants who enacted the scenario on the laptop, those in the VR condition reported higher levels of absorption, which in turn increased the intensity of their negative emotional response to the scenario. A follow-up questionnaire administered several hours later revealed that the intensified negative emotions resulting from VR had a significant positive correlation with negative rumination (i.e., harmful self-related thoughts related to distress). These results show that VR gameplay has the potential to elicit strong negative emotional responses that could be harmful for users if not managed properly. We discuss the practical and policy implications of our findings. © 2020, The Author(s).",,Title_Abstract,True,
Scopus,journalPaper,2022,Comparative evaluation of WIMP and immersive natural finger interaction: a user study on CAD assembly modeling,VRS - Virtual Reality,B,"Virtual and augmented reality allows the utilization of natural user interfaces, such as realistic finger interaction, even for purposes that were previously dominated by the WIMP paradigm. This new form of interaction is particularly suitable for applications involving manipulation tasks in 3D space, such as CAD assembly modeling. The objective of this paper is to evaluate the suitability of natural interaction for CAD assembly modeling in virtual reality. An advantage of the natural interaction compared to the conventional operation by computer mouse would indicate development potential for user interfaces of current CAD applications. Our approach bases on two main elements. Firstly, a novel natural user interface for realistic finger interaction enables the user to interact with virtual objects similar to physical ones. Secondly, an algorithm automatically detects constraints between CAD components based solely on their geometry and spatial location. In order to prove the usability of the natural CAD assembly modeling approach in comparison with the assembly procedure in current WIMP operated CAD software, we present a comparative user study. Results show that the VR method including natural finger interaction significantly outperforms the desktop-based CAD application in terms of efficiency and ease of use. © 2021, The Author(s).",Assembly modeling; Computer-aided design; Natural user interface; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Emotional responses to watching and touching 3d emotional face in a virtual environment,VRS - Virtual Reality,B,"Facial expressions play a crucial role in modulating the emotional responses in the viewers. Touch is an important factor in shaping human emotions and social communication. The objective of this study is to investigate the effects of viewing and touching a virtual emotional face on the emotional responses of a viewer/toucher. In the case of touching the model, the effects of physical properties, namely stiffness and texture, are examined. Emotional facial expressions for neutrality, anger, fear, disgust, happiness, surprise, and sadness are developed and experimentally validated for the visual stimuli whereas four combinations of stiffness/texture properties are examined for the physical properties (low/high stiffness and smooth/rough texture). 25 participants viewed and touched the virtual emotional face and reported their respective emotional responses. The results showed that watching angry, happy, surprised, and sad faces significantly increased their anger, happiness, surprise, and sadness levels, respectively (p < 0.05). Watching a scared or a sad face significantly modulated the participants’ surprise levels (p < 0.05). On the other hand, viewing and touching an angry face significantly reduced the surprise level in the toucher (p < 0.05). As for differences based on physical properties, our results suggested that viewing and touching the disgusted face significantly modulated sadness. In particular, high stiffness/rough texture condition resulted in a significant increase in sadness while viewing and touching the disgusted face, compared to the high stiffness/smooth texture condition (p < 0.01). These conclusions suggest that viewing and touching an emotional face in a virtual environment modulates the emotional responses in the viewer/toucher. Findings of this study help the field of virtual reality to expand to a greater understanding of building emotionally compelling interpersonal interactions in the virtual environments. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Emotions; Facial expressions; Haptic interaction; Virtual environments,Abstract,True,
Scopus,journalPaper,2021,Haptic-enabled virtual training in orthognathic surgery,VRS - Virtual Reality,B,"Orthognathic surgery (OGS) is a very complex surgical procedure aiming to correct a wide range of skeletal and dental irregularities, including jaws and teeth misalignments. It requires a precise pre-surgical planning and high surgical skills that are traditionally acquired through years of hands-on training in the operating room or in laboratory-based surgical practices using cadavers or models. Although modern engineering technologies have led to the development or computer-aided surgical procedures and systems, surgical training in OGS still relies on the traditional physical hands-on approach. This paper presents the results of an investigation carried out with the aim to evaluate the use of haptics and virtual reality technologies as an OGS training tool. Three case studies corresponding to cephalometry training, osteotomy training and surgery planning training were conducted. Participants comprised novices and experts in the area of OGS. Surgical skills, performance and confidence of trainees, in addition to reducing execution times and errors associated with the traditional OGS process, indicate that the haptic-enabled virtual reality approach is an effective training tool. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Cephalometry; Haptics; Orthognathic surgery (OGS); Osteotomy; Surgery planning; Surgical skills; Surgical training; Virtual reality (VR),Abstract_Keywords,True,
Scopus,journalPaper,2022,Using virtual reality for dynamic learning: an extended technology acceptance model,VRS - Virtual Reality,B,"Virtual reality (VR) is being researched and incorporated into curricula and training programs to expand educational opportunities and enhance learning across many fields. Although researchers are exploring the learning affordances associated with VR, research surrounding students’ perceptions of the technology, and intentions to use it for training has been neglected. The goal of this research was to determine the factors that influence students’ intention to use VR in a dynamic learning environment. An extended Technology Acceptance Model (TAM) was developed that incorporates factors related to education and the use of VR technology in training environments. Confirmatory factor analysis (CFA) and structural equation modeling (SEM) processes were employed. Nine of 14 hypotheses in the original model were supported, and eight of the nine predictor factors of the model were determined to directly or indirectly impact behavioral intention (BI). The original TAM factors had the strongest relationships. Relationships between factors particularly relevant to VR technology and learning were also supported. The results of this study may guide other educators interested in incorporating VR into a dynamic learning environment. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Aviation education; Dynamic learning; Education technology; Student perception; Technology acceptance model; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Co-located (multi-user) virtual rehabilitation of acquired brain injury: feasibility of the Resonance system for upper-limb training,VRS - Virtual Reality,B,"Upper-limb virtual rehabilitation (VR) in adult acquired brain injury (ABI) is based largely on systems administered on a one-to-one basis. Multi-user interaction between co-located participants may offer advantages over single-user methods. The present study examined the feasibility of deploying a co-located VR system (Resonance) in a clinical setting. Following a baselining period, 5 patients with ABI completed 12 Resonance sessions over 4–6 weeks. Feasibility criteria included recruitment, intervention delivery, attrition, user experience, and suitability of outcome measures. Individual participant motor proficiency (box and blocks task) was examined using a time-series analysis with reliable change indices and curve fitting. All feasibility criteria were satisfied, with positive reports of user experience. Repeated collection of outcome measures was successfully integrated into the training schedule. Time-series analysis was successfully conducted, providing a detailed account of individual training-related change. Within a clinical setting, it was feasible to deliver Resonance and regularly monitor motor function. User feedback regarding the co-located VR intervention was generally positive, but expectations regarding the level of immersion may need to be managed. Individual time-series analysis is recommended as an adjunct to group-based analysis in future VR research. These findings can inform the design of a clinical trial. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Brain injury; Group therapy; Rehabilitation; Time-series analysis; Upper limb; Virtual reality,Keywords,True,
Scopus,journalPaper,2021,Comparison of gaze accuracy and precision in real-world and virtual reality,VRS - Virtual Reality,B,"Virtual reality (VR) is popular across many fields and is increasingly used in sports as a training tool. The reason, therefore, is recently improved display technologies, more powerful computation capacity, and lower costs of head-mounted displays for VR. As in the real-world (R), visual effects are the most important stimulus provided by VR. However, it has not been demonstrated whether the gaze behavior would achieve the same level in VR as in R. This information will be important for the development of applications or software in VR. Therefore, several tasks were designed to analyze the gaze accuracy and gaze precision using eye-tracking devices in R and VR. 21 participants conducted three eye-movement tasks in sequence: gaze at static targets, tracking a moving target, and gaze at targets at different distances. To analyze the data, an averaged distance with root mean square was calculated between the coordinates of each target and the recorded gaze points for each task. In gaze accuracy, the results showed no significant differences between R and VR in gaze at static targets (1 m distance, p > 0.05) and small significant differences at targets placed at different distances (p < 0.05), as well as large differences in tracking the moving target (p < 0.05). The precision in VR is significantly worse compared to R in all tasks with static gaze targets (p < 0.05). On the whole, this study gives a first insight into comparing foveal vision, especially gaze accuracy and precision between R and VR, and can, therefore, serve as a reference for the development of VR applications in the future. © 2020, The Author(s).",Accuracy; Eye-tracking; Gaze behavior; Head-mounted display; Precision; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,An improved colour binary descriptor algorithm for mobile augmented reality,VRS - Virtual Reality,B,"The incorporation of both virtual content and real world in augmented reality (AR) allows real-time engagement with the virtual objects. The selection of an appropriate tracking algorithm is important to optimise the performance of mobile AR applications given the limited processing capabilities and memories of mobile devices like smartphones. Tracking in AR consists of four essential components, namely detector, descriptor, matcher, and pose estimator. Since a descriptor substantially affects the overall performance of a mobile AR application, it must have short computational time and remains invariant to scale, rotation, and lighting changes. Studies have proposed Fast Retina Keypoint (FREAK) descriptor as the most suitable descriptor for mobile AR applications. Unlike other greyscale descriptors, FREAK has shorter computational time and is less likely to be affected by scale and rotation changes. However, it overlooks the vital colour space information. Focusing on enhancing the efficiency and robustness of FREAK, this study proposed the use of CRH-FREAK (RGB + HSV) descriptor and applied the vertical concatenation technique that combined all extracted keypoints vertically. The robustness of the proposed descriptors against scale, rotation, and lighting changes was verified using Mikolajczyk and Amsterdam Library of Object Images (ALOI) datasets. The developed CRH-FREAK descriptors used six colour spaces to describe the keypoints, which made them slower than the original FREAK. However, the size reduction of CRH-FREAK from 512 bits to 128 bits in this study successfully reduced the computational time to 29.49 ms, which was found comparable to the original FREAK. The improved efficiency and robustness of a 128-bit CRH-FREAK descriptor benefit the future development of mobile AR applications that remain invariant to scale, rotation, and lighting changes. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Colour descriptor; FREAK; HSV; Mobile augmented reality; RGB,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,Evaluation of the impact of different levels of self-representation and body tracking on the sense of presence and embodiment in immersive VR,VRS - Virtual Reality,B,"The main goal of this paper is to investigate the effect of different types of self-representations through floating members (hands vs. hands + feet), virtual full body (hands + feet vs. full-body avatar), walking fidelity (static feet, simulated walking, real walking), and number of tracking points used (head + hands, head + hands + feet, head + hands + feet + hip) on the sense of presence and embodiment through questionnaires. The sample consisted of 98 participants divided into a total of six conditions in a between-subjects design. The HTC Vive headset, controllers, and trackers were used to perform the experiment. Users were tasked to find a series of hidden objects in a virtual environment and place them in a travel bag. We concluded that (1) the addition of feet to floating hands can impair the experienced realism (p= 0.039), (2) both floating members and full-body avatars can be used without affecting presence and embodiment (p> 0.05) as long as there is the same level of control over the self-representation, (3) simulated walking scores of presence and embodiment were similar when compared to static feet and real walking tracking data (p> 0.05), and (4) adding hip tracking overhead, hand and feet tracking (when using a full-body avatar) allows for a more realistic response to stimuli (p= 0.002) and a higher overall feeling of embodiment (p= 0.023). © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Body representation; Body tracking; Embodiment; Presence; Virtual reality,Keywords,True,
Scopus,journalPaper,2022,"Cyber sickness in low-immersive, semi-immersive, and fully immersive virtual reality",VRS - Virtual Reality,B,"It is known that virtual reality (VR) experience may cause cyber sickness. One aspect of VR is an immersion or otherwise sense of presence, the sense of feeling oneself in a virtual world. In this paper an experiment which was conducted in order to find the link between level of immersion and cyber sickness felt by participants is presented. Eighty-nine participants aged between 19 and 36 years have been equally divided into four groups with different level of VR immersion. The low-immersive group was represented by PC with monoscopic screen, the semi-immersive group was represented by CAVE with stereoscopic projector, the fully immersive group was represented by VR head-mounted display, and the last group was the control group without any kind of immersion. The task for the participants was to navigate through the maze for a specified amount of time (10 min). The Simulator Sickness Questionnaire was used as a subjective measure tool for cyber sickness level and Grooved Pegboard Test for assessing the fine dexterity, both before and after the experiment. Regarding the time spend in VR the fully immersive environment had the biggest problems as more than half of the participants had to stop before 10 min (p < 0.001). Concerning the cyber sickness, the significant increase in nausea score between pre-test and post-test scores has been observed in semi-immersive group (p = 0.0018) and fully immersive group (p < 0.0001). The increase in oculomotor score was smaller. The significant difference was noted only in fully immersive group (p = 0.0449). In spite of great nausea factor after the VR immersion the participants did not show a decrease of fine dexterity in any group (p < 0.001). © 2021, The Author(s).",Cyber sickness; Dexterity test; Head-mounted display; Stereoscopic projection; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Virtual body representation for rehabilitation influences on motor performance of cerebral palsy children,VRS - Virtual Reality,B,"Game-based virtual reality systems have been shown to enhance motor function, motivation and therapy adherence in cerebral palsy (CP) children. In these systems, several types of virtual body representations have been implemented, however without conclusive support of guidelines nor the most appropriate choice for enhancing motor performance. Thus, the purpose of this study is to examine how the subjective experience of seeing and controlling a half-body avatar, or an abstract hand representation in a moderate immersion virtual environment (VE), for training upper limb movements may affect CP children’s motor performance. To achieve that purpose, a game-like VE for training the reaching-releasing of objects was designed. Unlike previous studies, relevant task performance and cost function metrics were obtained from the analysis of kinematic and kinetic parameters of movement. Results show that visualizing the hand movement through an abstract object makes children perform faster, correct less to produce smoother movements, and use less mechanical energy than visualizing the arm movement through a realistic Avatar. These effects were more noticeable in the reaching than in the releasing phase of the task. Based on these findings, some recommendations are provided for the effective design and use of VE’s for upper limb rehabilitation of CP children. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Cerebral palsy; Movement analysis; Rehabilitation; Virtual body representation; Virtual environments,Abstract,True,
Scopus,journalPaper,2021,An experimental investigation of menu selection for immersive virtual environments: fixed versus handheld menus,VRS - Virtual Reality,B,"With the development of consumer-grade virtual reality (VR) systems, the interface and interaction design for immersive virtual environments have become a critical issue for VR application designers and developers. The previous design experience from conventional VR applications may not be effective anymore. This work investigated two types of menu interfaces (fixed menu and handheld menu) and three selection techniques, hand pointing with button press (Hand-BP), head pointing with button press (Head-BP), and head pointing with dwell (Head-DW), based on performance and subjective assessment. Results showed that the best performing selection technique for fixed menu was Hand-BP, while Head-BP was the most suitable and preferred selection technique for handheld menu. The limitations of different menu interfaces and selection techniques were discussed, and solutions were proposed to overcome the limitations that were found in the experiment. Based on the findings of this study, design guidelines were provided to help designers and developers choose the right menu interface and selection technique for different contexts of use and user groups. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",3D selection; Menu interface; Virtual environment,Abstract,True,
Scopus,journalPaper,2021,My hands? Importance of personalised virtual hands in a neurorehabilitation scenario,VRS - Virtual Reality,B,"We have developed a novel and affordable way to texture virtual hands from individually taken photographs and integrated the virtual hands into a mixed reality neurorehabilitation system. This mixed reality system allows for serious game play with mirrored and non-mirrored hands, designed for patients with unilateral motor impairments. Before we can ethically have patients use the system, we must show that embodiment can be achieved for healthy users. We compare our approach’s results to previous work in the field and present a study with 48 healthy (non-clinical) participants targeting visual fidelity and self-location. We show that embodiment can be achieved for mirrored and non-mirrored hand representations and that the higher realism of virtual hands achieved by our texturing approach alters perceived embodiment. We further evaluate whether using virtual hands resized to the individual’s hand size affects embodiment. We present a 16-participant study where we could not find a significant difference with personal resized hands. In addition to rehabilitation contexts, our findings have implications for the design and development of applications where embodiment is of high importance, such as surgical training and remote collaboration. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Neurorehabilitation therapy; Virtual embodiment; Virtual hands reconstruction; Virtual interaction; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Landmine detection training simulation using virtual reality technology,VRS - Virtual Reality,B,"Landmines are frequently used for defence and attack. Thus, landmine detection is vital to preventing the damages incurred. Various landmine detection methods have been developed from the past to the present. Thus, a need has arisen for the employment of qualified personnel in this field. In today’s landmine detection training, soldiers are first subjected to theoretical training about landmine types. After this stage, they attend practical training in the field. However, when training is given in this way, sample application fields do not contain all possible terrain and weather conditions. Also, many accidents and injuries occur during this practical training. To overcome the disadvantages elucidated above, a landmine detection training simulation was developed in this study, which supports real terrain conditions and creates a safe detection environment. In this study, the developed simulation software was based on virtual reality technology. The interaction with the computer is both provided by a detector appearance control device (DACD) controller developed for this simulator and by Kinect controller. The simulator has remarkable benefits concerning time and cost when compared with the landmine detection training given today. At the same time, detections performed in a safe environment without any risk factors and real land conditions were provided close to reality. Also, the actual space coordinates of the joints in the human body can be detected correctly using the DACD controller, as much as the Kinect device. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Kinect V2; Landmine detection training; Landmine sweeping; Simulation; Unity 3D; VR,Title_Abstract,True,
Scopus,journalPaper,2021,Evoking emotions in virtual reality: schema activation via a freeze-frame stimulus,VRS - Virtual Reality,B,"Virtual reality can be used for educational purposes, particularly in demanding professions such as firefighting. Such virtual training may be useful for preparing trainees for distress, fear, or frustration experienced during real rescue operations. Evoking cognitive schemas, especially of other people, during training appears to be crucial as well, as the greatest stressors in the firefighting profession are social. Based on interviews with firefighters, two types of people (children and young women) were chosen as stimuli in the research design. In an experimental study with three iterations, the stimuli designed to evoke the schema of a significant other were implemented in a virtual reality simulator to evoke a cognitive schema in firefighters through emotions (positive and negative) and several dimensions of stress. The first iteration of the study did not yield expected results, as the stimulus (a child’s toy) was not as suggestive and vivid as it was expected. In the second attempt, the stimulus was improved and evoked feelings of challenge, harm, and loss in the participants. In the third iteration, the stimulus was changed once more (to a white dress) and this time it evoked negative emotions of fear, anger, guilt, and sadness. However, after correcting for multiple comparisons, only results regarding emotional response remained statistically significant. The results are discussed in light of cognitive schemas’ activation, and perspectives for further research in this scope are proposed. Due to research outcomes, the issues of manipulation checks in experimental psychology and limitations of the VR technology are taken into consideration. © 2020, The Author(s).",Cognitive schema; Emotions; Serious games; Simulation; Training; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Automatic detection and classification of emotional states in virtual reality and standard environments (LCD): comparing valence and arousal of induced emotions,VRS - Virtual Reality,B,"The following case study was carried out on a sample of one experimental and one control group. The participants of the experimental group watched the movie section from the standardized LATEMO-E database via virtual reality (VR) on Oculus Rift S and HTC Vive Pro devices. In the control group, the movie section was displayed on the LCD monitor. The movie section was categorized according to Ekman's and Russell's classification model of evoking an emotional state. The range of valence and arousal was determined in both observed groups. Valence and arousal were measured in each group using a Self-Assessment Manikin (SAM). The control group was captured by a camera and evaluated by Affdex software from Affectiva in order to compare valence values. The control group showed a very high correlation (0.92) between SAM and Affdex results. Having considered the Affdex results as a reference value, it can be concluded that SAM participants evaluated their emotions objectively. The results from both groups show that the movie section is supposed to evoke negative emotion. Negative emotion was perceived more intensely than its counterpart, positive emotion. Using virtual reality to evoke negative emotion (anger) has confirmed that VR triggers a significantly stronger intensity of emotion than LCD. © 2021, The Author(s).",Arousal; Classification; Emotions; Valence; Virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Conceptualising touch in VR,VRS - Virtual Reality,B,"How touch is conceptualised matters in shaping technical advancements, bringing opportunities and challenges for development and design and raising questions for how touch experience is reconfigured. This paper explores the notion of touch in virtual reality (VR). Specifically, it identifies how touch ‘connection’ is realised and conceptualised in virtual spaces in order to explore how digital remediation of touch in VR shapes the sociality of touch experiences and touch practices. Ten participants from industry and academia with an interest in touch in virtual contexts were interviewed using an in-depth semi-structured approach to elicit experiences and perspectives around the role of touch in VR. Data analysis shows the growing value and significance of touch in virtual spaces and reveals particular ways in which touch is talked about, implemented and conceptualised. It highlights changes for the sociality of touch through participants’ conceptualisations of touch as replication and illusion, and how the body is brought into this ‘touch’ space. These perspectives of touch shape who touches, what is touched and how it is touched and set an agenda for the types of touch that are facilitated by VR. The findings suggest ways in which technological techniques can be employed towards interpretive designs of touch that allow for new ways to look at touch and haptics. They also show how touch is distorted and disrupted in ways that have implications for disturbing established ‘real world’ socialities of touch as well as their renegotiation by users in the space of digitally mediated touch in VR. © 2020, The Author(s).",Design; Illusion; Immersive virtual reality; Reproduction; Social perspectives; Touch,Abstract_Keywords,True,
Scopus,journalPaper,2021,Fooling the size–weight illusion—Using augmented reality to eliminate the effect of size on perceptions of heaviness and sensorimotor prediction,VRS - Virtual Reality,B,"Augmented reality, whereby computer-generated images are overlaid onto the physical environment, is becoming significant part of the world of education and training. Little is known, however, about how these external images are treated by the sensorimotor system of the user – are they fully integrated into the external environmental cues, or largely ignored by low-level perceptual and motor processes? Here, we examined this question in the context of the size–weight illusion (SWI). Thirty-two participants repeatedly lifted and reported the heaviness of two cubes of unequal volume but equal mass in alternation. Half of the participants saw semi-transparent equally sized holographic cubes superimposed onto the physical cubes through a head-mounted display. Fingertip force rates were measured prior to lift-off to determine how the holograms influenced sensorimotor prediction, while verbal reports of heaviness after each lift indicated how the holographic size cues influenced the SWI. As expected, participants who lifted without augmented visual cues lifted the large object at a higher rate of force than the small object on early lifts and experienced a robust SWI across all trials. In contrast, participants who lifted the (apparently equal-sized) augmented cubes used similar force rates for each object. Furthermore, they experienced no SWI during the first lifts of the objects, with a SWI developing over repeated trials. These results indicate that holographic cues initially dominate physical cues and cognitive knowledge, but are dismissed when conflicting with cues from other senses. © 2021, The Author(s).",Holograms; Perception; Sensorimotor control; Virtual reality; Weight illusions,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Simulation-based surgical training systems in laparoscopic surgery: a current review,VRS - Virtual Reality,B,"Simulation-based training has been widely used in medical education. More specifically, various systems for minimally invasive surgery training have been proposed in the past two decades. The aim of this article is to review and summarize the existing simulation-based training systems for laparoscopic surgery in terms of their technical realizations. Forty-three training systems were found and analyzed. These training systems generally consist of training tasks, a visualization interface, and an instrument interface. Three different approaches—physical, virtual, and augmented reality—to implement visualization interfaces are discussed first. Then, haptic feedback, performance evaluation, and guidance methods are summarized. Portable devices to enable at-home training and instrument tracking technologies to support visualization, evaluation, and guidance are also presented. Based on survey of the relevant literature, we propose several recommendations to design the next-generation training systems in laparoscopic surgery. Novel guidance and assessment schemes with augmented reality visualization are recommended to design an intelligent surgical training simulator. This intelligent simulator enhances the training procedure and ultimately improves the patient safety. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Haptic guidance; Laparoscopy; Objective assessment; Surgical training; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Sustained inattentional blindness in virtual reality and under conventional laboratory conditions,VRS - Virtual Reality,B,"Virtual reality (VR) might increase the ecological validity of psychological studies as it allows submerging into real-life experiences under controlled laboratory conditions. We intended to provide empirical evidence for this claim at the example of the famous invisible gorilla paradigm (Simons and Chabris in Perception, 28(9), 1059–1074, 1999). To this end, we confronted one group of participants with a conventional 2D-video of two teams passing basketballs. To the second group of participants, we presented the same stimulus material as a 3D360°-VR-video and to a third group as a 2D360°-VR-video. Replicating the original findings, in the video condition, only ~ 30% of the participants noticed the gorilla. However, in both VR-conditions, the detection rate was increased to ~ 70%. The illusion of spatial proximity in VR enhances the salience of the gorilla, thereby enhancing the noticing rate. VR mimics the perceptual characteristics of the real world and provides a useful tool for psychological studies. © 2020, The Author(s).",Ecological validity; Invisible gorilla; Sustained inattentional blindness; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,A model for physics-based fire simulation and analysis,VRS - Virtual Reality,B,"This paper describes the development of a three-dimensional (3D) physics-based fire simulation model that employs the incompressible Navier–Stokes equations to realistically emulate the combustion process. Then, we animate the 3D interactive burning processes by rendering a flame under a set of influence factors and with various solid boundaries and obstacles. It is insufficient to simply create a virtual reality-based fire model. Instead, evaluating the similarity and accuracy of the models requires data processing for the virtual flames. In this paper, detailed data are extracted from the simulation results to compute the fire’s geometrical features and the distribution of the density and velocity fields. Using methods for video-based fire detection, some visual features of the simulated-fire videos are extracted and compared with those of real fires. The results show the capability of the physics-based fire model in representing some features of real flames. The proposed quantitative analysis of virtual flames serves to evaluate the similarity between a virtual and a real fire. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Fire; Fire detection; Flame; Navier–stokes equations; Physics-based simulation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,A use case study comparing augmented reality (AR) and electronic document-based maintenance instructions considering tasks complexity and operator competency level,VRS - Virtual Reality,B,"Augmented reality (AR) is more and more used in the industrial context for maintenance, assembly operation. However, owing to the evolution and maturity of the technology, it is still necessary to evaluate the usage and benefits of AR in an industrial context to go further than proof of concept. As such, this paper proposes an analysis of AR case study in the literature. Then, this paper presents a methodology for comparing AR and electronic document-based complex maintenance instructions considering tasks complexity and operator competency level. The main results show that the consultation duration of the AR tablet is 34% statistically faster than the PDF tablet. It also shows that error concerning similar objects is reduced thanks to AR. Moreover, the study specifically focuses on types of task that worth using AR. However, the study shows that usability of the AR device is less well rated than the PDF for “beginner–intermediate” operators. Finally, an in-depth analysis permits extracting recommendations of the different factors to take into account with industrial AR applications. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Augmented reality; Case studies in industry; Industry 4.0,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Towards estimating affective states in Virtual Reality based on behavioral data,VRS - Virtual Reality,B,"Inferring users’ perceptions of Virtual Environments (VEs) is essential for Virtual Reality (VR) research. Traditionally, this is achieved through assessing users’ affective states before and after being exposed to a VE, based on standardized, self-assessment questionnaires. The main disadvantage of questionnaires is their sequential administration, i.e., a user’s affective state is measured asynchronously to its generation within the VE. A synchronous measurement of users’ affective states would be highly favorable, e.g., in the context of adaptive systems. Drawing from nonverbal behavior research, we argue that behavioral measures could be a powerful approach to assess users’ affective states in VR. In this paper, we contribute by providing methods and measures evaluated in a user study involving 42 participants to assess a users’ affective states by measuring head movements during VR exposure. We show that head yaw significantly correlates with presence, mental and physical demand, perceived performance, and system usability. We also exploit the identified relationships for two practical tasks that are based on head yaw: (1) predicting a user’s affective state, and (2) detecting manipulated questionnaire answers, i.e., answers that are possibly non-truthful. We found that affective states can be predicted significantly better than a naive estimate for mental demand, physical demand, perceived performance, and usability. Further, manipulated or non-truthful answers can also be estimated significantly better than by a naive approach. These findings mark an initial step in the development of novel methods to assess user perception of VEs. © 2021, The Author(s).",Affective States; Affective VR; Sensor Data; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Development of a virtual reality laboratory stressor,VRS - Virtual Reality,B,"This research report describes the development of a virtual reality (VR) laboratory stressor to study the effects of exposure to stressful events. The aim of the research was to develop a VR simulation that would evoke stressor responses at a level that was tolerable for participants. Veterans with and without warzone-related posttraumatic stress disorder (PTSD) were presented with VR simulations of combat stressors. There was one complaint of feeling hot during simulations but no incidents of simulator sickness. Participants denied experiencing the simulations as overly distressing, and there were no reports of any distress or problems related to study participation when they were contacted two weeks after the VR challenge. Simulations elicited moderate levels of anxiety and mild levels of dissociation that were significantly greater in Veterans with PTSD. Simulations were less successful in eliciting differential heart rate reactivity and stress hormone secretion, though history of civilian trauma exposure was associated with elevated heart rates during the second simulation. The study demonstrated that the VR paradigm was feasible and tolerable and that it holds promise as a new method with which to conduct controlled laboratory research on the effects of exposure to stressful events. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Experimental psychopathology; PTSD; Research methods; Veterans; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Contextual priming to increase the sense of presence in virtual reality: exploratory study,VRS - Virtual Reality,B,"Sense of presence plays an essential role in realistic answers in virtual reality and is, therefore, relevant to promote. Prior studies suggest that a relationship exists between familiarity and presence through the memory process. Indeed, the recall of memories could reactivate the mental state and emotions associated with their context. The present study thus examined the effectiveness of contextual priming in enhancing the sense of presence in a virtual environment. The reading of an article primed participants. In the university priming condition, the article consisted of a description of the university, place well-known by the participants and relating to the virtual environment used subsequently. In the non-university condition, the article dealt with the organization of household chores. After the reading, the researcher immersed participants in a virtual classroom and had to discuss the article for three minutes. Finally, participants were invited to complete several questionnaires. Participants in the university priming condition reported a higher sense of presence during the virtual task than participants in the neutral priming group, especially regarding reality judgment. These findings suggest that contextual priming can promote a sense of presence through the activation of familiarity. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Contextual priming; Priming; The sense of familiarity; The sense of presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,A VR training system for learning and skills development for construction workers,VRS - Virtual Reality,B,"There is a looming shortage of well-trained professionals in the wood construction workforce. To challenge this shortage, we developed a simulated learning environment that leverages a novel Virtual Reality (VR) system to train novice workers in wooden wall construction. A comprehensive task analysis was first used to best identify training requirements. Then, a virtual building site was modeled and a 3D video tutorial was implemented using a VR Head-Mounted Display (HMD). To evaluate the effectiveness of this tool, participants who learned via the VR training tool were compared with participants who instead only had simple 2-D instructional video training. VR training resulted in better retention, task performance, learning speed, and engagement than the video training counterpart, maintaining system usability. This demonstrates that VR is a viable training tool for the construction sector and can produce benefits beyond those of traditional video training. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Human-computer interaction; Virtual reality; Virtual training; Workforce development,Abstract_Keywords,True,
Scopus,journalPaper,2021,Designing virtual environments for attitudes and behavioral change in plastic consumption: a comparison between concrete and numerical information,VRS - Virtual Reality,B,"Starting from the pro-environmental potential of virtual reality (VR), the aim was to understand how different statistical information formats can enhance VR persuasive potential for plastic consumption, recycling and waste. Naturalistic, immersive virtual reality environments (VREs) were designed ad hoc to display three kinds of statistical evidence formats, featured as three different formats (i.e., numerical, concrete and mixed). Participants were exposed only to one of the three formats in VR, and their affect, emotions, sense of presence, general attitudes toward the environment, specific attitudes and behavioral intentions toward plastic, use, waste, recycle, as well as their social desirability proneness were measured. Numerical format was the least effective across all dimensions. Concrete and mixed formats were similar. Social desirability only partially affected participants’ attitudes and behavioral intentions. Numerical format did not increase the persuasive efficacy of statistical evidence displayed in VR, with respect to visual alone. Implications and future directions for designing effective VRE promoting pro-environmental behaviors were discussed. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Concrete; Format; Numerical; Plastic; Statistical evidence; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Comparing touch-based and head-tracking navigation techniques in a virtual reality biopsy simulator,VRS - Virtual Reality,B,"Recently, virtual reality (VR) technologies started gaining momentum in surgical simulation-based training by allowing clinicians to practice their skills before performing real procedures. The design of such simulators is usually focused on the primary operative tasks to be taught, but little attention is paid to secondary tasks that the user needs to perform, such as changing his/her point of view when manipulating the surgical instruments. More particularly, it is not clear how to design appropriate interaction techniques for those tasks, and how the fidelity of these interactions can impact the user’s performance on such systems. In this paper, we compare two viewpoint changing techniques having two different levels of interaction fidelity during needle insertion in a semi-immersive VR (SIVR) biopsy trainer. These techniques were designed based on observing clinicians performing actual biopsy procedures. The first technique is based on tracking the user’s head position (high interaction fidelity), while the second technique is touch-based with the user utilizing his/her non-dominant hand fingers to manipulate the point of view on a touch screen (moderate interaction fidelity). A user study was carried out to investigate the impact of the interaction fidelity of the viewpoint changing task (secondary task) on the user’s performance during the needle insertion task (main task). Twenty-one novice participants were asked to perform several trials of a needle insertion task while using the navigation techniques (within-subject design). Objective and subjective measures were recorded to compare the task performance (time to accomplish the task, precision of the tumor sampling, and errors) and user experience for both techniques. The results show that the touch-based viewpoint changing technique improves the users’ task completion performance during needle insertion while maintaining a similar level of needle manipulation accuracy as compared to the head-tracking technique. These results suggest that high interaction fidelity is not always necessary when designing surgical trainers. This also highlights the importance of designing appropriate interactions for secondary tasks because they can influence the user’s primary task performance in VR simulators. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Biopsy trainer; Interaction design; Interaction fidelity; Surgical training,Title_Abstract,True,
Scopus,journalPaper,2020,To move or not to move? Analyzing motion cueing in vehicle simulators by means of massive simulations,VRS - Virtual Reality,B,"Motion platforms and motion cueing algorithms (MCA) have been included in virtual reality applications for several decades. They are necessary to provide suitable inertial cues in vehicle simulators. However, the great number of operational constraints that these devices and algorithms suffer, namely limited physical space, elevated costs, absence of sufficient power, difficulty of tuning and lack of standardized assessment methods, have hindered their widespread use. This work tries to clarify open questions in the field, such as: How important is MCA tuning? How much does size, number of DOF and power/latency matter? Can the absence of motion be better than poor motion cueing? What are the key factors that should be addressed to enhance the design of these devices? Although absolute certain answers cannot be given, this paper tries to clarify these research questions by performing massive experiments with simulated motion platforms of different types, sizes and powers. The information obtained from these experiments will be important to customize the design of real devices for this particular use. Ideally, subjective experiments with human experts would have been preferred. However, the use of simulated devices allows comparing many different motion platforms. In this paper, forty of these devices are simulated, optimized by means of a heuristic algorithm and compared with objective indicators in order to measure their relative performance using the classical MCA, something that would require an unreasonable amount of effort with real users and real devices. The obtained results show that MCA tuning is of the utmost importance in motion cueing. They also suggest that high power can usually compensate for lack of size and that a 6-DOF motion platform slightly improves the performance of a 3-DOF motion platform. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Massive simulation; Motion cueing; Motion platform; Particle swarm optimization; Vehicle simulator; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2019,Real-time manufacturing modeling and simulation framework using augmented reality and stochastic network analysis,VRS - Virtual Reality,B,"While the development of augmented reality (AR) technologies has made it possible to assign real-time features to many systems and applications, these trends are rare in manufacturing modeling and simulation. This research study proposes a real-time manufacturing layout modeler and material flow simulator. The manufacturing devices of interest are positioned using AR labels, and the generated layout is converted into a stochastic Petri net model, where the validity of material flow and other criteria are checked. In order to overcome the limitations of the Petri net model and enhance analytical functionalities, stochastic network analyses are embedded into the framework. The layout model with greater uncertainty is analyzed, and manufacturing performance indicators such as cycle time, throughput, and work-in process are estimated. The proposed framework is not simply an integration of AR techniques and manufacturing simulations, but provides an efficient AR labeling architecture for large-scale manufacturing environments, and is suitable for a fast, real-time rendering. In order to verify the effectiveness of the proposed framework, real-time modeling and simulation examples were used as case studies. The results showed that the proposed system contributes to more accurate layout design and simulation analysis by using the embedded AR techniques and queuing network methods. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Manufacturing layout design; Manufacturing simulation; Petri net; Stochastic network; Virtual manufacturing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Revisiting affordance perception in contemporary virtual reality,VRS - Virtual Reality,B,"Virtual reality (VR) applications have rapidly gained renewed popularity and are extensively employed for replicating real-life scenarios that may otherwise be impractical to recreate. All such VR applications require that the environments being used provide high levels of immersion and mimic their real-world counterpart in terms of size, distance, depth, and action capabilities. Many VR applications being developed for training and entertainment require users to traverse an immersive virtual environment (IVE), where determining whether one can pass through an opening or aperture is one of the most frequently made decisions. In this experiment, we empirically compare passability judgments made in an IVE to those made in the real world. Participants judged whether they could pass through various widths of an adjustable sliding doorway in the real world and in a to-scale virtual replica viewed through an HTC Vive head-mounted display. If uncertain of their initial judgments, participants were permitted to walk towards the doorway. Results indicate that participants accurately perceive their ability to pass through doorways in both the real world and VR. However, participants in VR required more exposure to dynamic information via movement through the IVE in order to reach a real-world level of perceptual accuracy. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Affordances; Body scaling; Passability; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Game play in virtual reality driving simulation involving head-mounted display and comparison to desktop display,VRS - Virtual Reality,B,"Previous studies have reported the effect of driving simulator games on simulator sickness and eye symptoms experienced by users; however, empirical results regarding the game experience using commercial virtual reality head-mounted displays (VR-HMDs) are lacking. We conducted an experiment where participants played a driving simulator game (Live for Speed) displayed through an Oculus Rift DK2 for up to 120 min. Game play duration was recorded. Game experience was surveyed using questionnaires about simulator sickness, eye symptoms, and game engagement. The results showed that the average game play duration for this specific driving simulation game was approximately 50 min. Simulator sickness was negatively correlated with affordable play duration using the VR-HMD. We also found that age was negatively correlated with game play duration. There were no differences between those who did and did not wear frame glasses. In addition, we compared the VR-HMD game play and traditional desktop LCD game play, in terms of simulator sickness, subjective eye symptoms, game engagement, and game performance. The results showed that VR-HMD game play in the driving simulation game was similar to the experience using the desktop LCD display, except for a moderately increased level of simulator sickness. These findings provide new data about VR-HMD’s impact on game play and will inform game designers, players, and researchers for their choices and decisions on proper game duration and the type of devices. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Driving simulation games; Eye symptoms; HMD; Simulator sickness; User experience; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Visuomotor adaptation to excessive visual displacement in video see-through HMDs,VRS - Virtual Reality,B,"A video see-through head-mounted display (VSHMD) is a modified HMD having an additional small digital camera set (see-through camera set) attached in front of the HMD, which allows users to view the real scene along with virtual information in digitally mixed form. However, although VSHMD has potential utility in augmented reality applications, the visual displacement problem must be overcome. This problem is caused by the distance between the see-through camera and human eye and induces visuomotor performance deterioration. Previous studies have revealed that human adaptation improves the visuomotor performance over time, by rearranging the proprioception. In this study, we extend the visual displacement excessively to 300 mm and investigate the eye–hand and eye–foot visuomotor coordination in two experiments. In Experiment 1, the prism adaptation paradigm is used to compare task performance under various visual displacement conditions. In Experiment 2, the procedures of Experiment 1 are implemented on 3 consecutive days to evaluate the relatively long-term adaptation trend. The results reveal distinct adaptations under all conditions. When excessive visual displacement is unavoidable, sufficient training can improve task performance, similar to the previously discovered perceptual adaptation. However, with increased visual displacement, the task performance improvement decelerates significantly. This improvement attenuation increases as the task performance becomes close to that achieved under bare eye conditions. Although humans can adapt to a large amount of visual displacement, a serious usage problem arises because of this slow adaptation improvement trend. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Adaptation; Aftereffect; Video see-through HMD; Visual displacement; Visuomotor; VSHMD,Abstract,True,
Scopus,journalPaper,2019,Augmenting the learning experience in primary and secondary school education: a systematic review of recent trends in augmented reality game-based learning,VRS - Virtual Reality,B,"There is a significant body of research relating to augmented reality (AR) uses for learning in the primary and the secondary education sectors across the globe. However, there is not such a substantial amount of work exploring the combination of AR with game-based learning (ARGBL). Although ARGBL has the potential to enable new forms of teaching and transform the learning experience, it remains unclear how ARGBL applications can impact students’ motivation, achievements, and learning performance. This study reports a systematic review of the literature on ARGBL approaches in compulsory education considering the advantages, disadvantages, instructional affordances, and/or effectiveness of ARGBL across various primary and secondary education subjects. In total, 21 studies published between 2012 and 2017 in 11 indexed journals were analysed, with 14 studies focusing on primary education and 7 on secondary. The main findings from this review provide the current state of the art research in ARGBL in compulsory education. Trends and the vision towards the future are also discussed, as ARGBL can potentially influence the students’ attendance, knowledge transfer, skill acquisition, hands-on digital experience, and positive attitude towards their learning. This review aims to lay the groundwork for educators, technology developers, and other stakeholders involved in the development of literacy programmes for young children by offering new insights with effective advice and suggestions on how to increase student motivation and improve learning outcomes and the learning experience by incorporating ARGBL into their teaching. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Game-based learning; Primary education; Secondary education; Systematic review,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Evaluating the effectiveness of mixed reality music instrument learning with the theremin,VRS - Virtual Reality,B,"Learning music is a challenging process that requires years of practice to master, either with lessons from a professional teacher or through self-teaching. While practicing, students are expected to self-evaluate their performance which may be difficult without timely feedback from a professional. Research into computer-assisted music instrument tutoring (CAMIT) attempts to address this through the use of emerging technologies. In this paper, we study CAMIT for mixed reality (MR) by developing MR:emin, an immersive MR music learning environment for the theremin, an electronic music instrument that is controlled without physical contact. MR:emin integrates a physical theremin with the immersive learning environment. To better understand the effectiveness of such environments, we perform a user study with MR:emin comparing traditional music learning with two virtual learning environments, an immersive one and a non-immersive one. In a between-groups study, 30 participants were trained to play a sequence of notes on the theremin using one of the three training environments. Results of our statistical analysis show that performance error during training is significantly smaller in the immersive MR environment. This does not necessarily lead to improved performance after training; analysis of post-training improvement indicates that immersive training results in the smallest amount of improvement. Participants, however, indicate that the MR:emin environment is more engaging and increases confidence during practice. We discuss potential factors leading to the decrease in learning and provide some environment guidelines to aid in the design of engaging immersive music learning environments. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Immersive learning environments; Learning transfer; Mixed reality; Music pedagogy; Training,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,"Virtual and augmented reality effects on K-12, higher and tertiary education students’ twenty-first century skills",VRS - Virtual Reality,B,"The purpose of this review article is to present state-of-the-art approaches and examples of virtual reality/augmented reality (VR/AR) systems, applications and experiences which improve student learning and the generalization of skills to the real world. Thus, we provide a brief, representative and non-exhaustive review of the current research studies, in order to examine the effects, as well as the impact of VR/AR technologies on K-12, higher and tertiary education students’ twenty-first century skills and their overall learning. According to the literature, there are promising results indicating that VR/AR environments improve learning outcomes and present numerous advantages of investing time and financial resources in K-12, higher and tertiary educational settings. Technological tools such as VR/AR improve digital-age literacy, creative thinking, communication, collaboration and problem solving ability, which constitute the so-called twenty-first century skills, necessary to transform information rather than just receive it. VR/AR enhances traditional curricula in order to enable diverse learning needs of students. Research and development relative to VR/AR technology is focused on a whole ecosystem around smart phones, including applications and educational content, games and social networks, creating immersive three-dimensional spatial experiences addressing new ways of human–computer interaction. Raising the level of engagement, promoting self-learning, enabling multi-sensory learning, enhancing spatial ability, confidence and enjoyment, promoting student-centered technology, combination of virtual and real objects in a real setting and decreasing cognitive load are some of the pedagogical advantages discussed. Additionally, implications of a growing VR/AR industry investment in educational sector are provided. It can be concluded that despite the fact that there are various barriers and challenges in front of the adoption of virtual reality on educational practices, VR/AR applications provide an effective tool to enhance learning and memory, as they provide immersed multimodal environments enriched by multiple sensory features. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Higher education; K-12; Tertiary education; Twenty-first century skills; Virtual and augmented reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,An evaluation of multimodal interaction techniques for 3D layout constraint solver in a desktop-based virtual environment,VRS - Virtual Reality,B,"We propose a new approach to the 3D layout problems based on the integration of constraint programming and virtual reality interaction techniques. Our method uses an open-source constraint solver integrated in a popular 3D game engine. We designed multimodal interaction techniques for the system, based on gesture and voice input. We conducted a user study with an interactive task of laying out room furniture to compare and evaluate the mono- and multimodal interaction techniques. Results showed that voice command provided the best performance and was most preferred by participants, based on the analysis of both objective and subjective data. Results also revealed that there was no significant difference between the voice and multimodal input (voice and gesture). Our original approach opens the way to multidisciplinary theoretical work and promotes the development of high-level applications for the VR applications. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",3D layout; Constraint solver; Interaction techniques; Multimodality; Virtual environments,Abstract,True,
Scopus,journalPaper,2019,Shopping in virtual reality: a study on consumers’ shopping experience in a stereoscopic virtual reality,VRS - Virtual Reality,B,"The popularity of home-based stereoscopic television provides researchers and practitioners with possibilities of bringing stereoscopic virtual reality (StereoVR) at consumers’ home. To further the investigation on the potential development of applying StereoVR in retailing, this research focuses on understanding consumers’ shopping experiences in this new platform. The research team believes that the use of StereoVR has potentials to become a new arena for interactive business. To explore these potential uses of technology in retailing, the team designed and built a StereoVR, called “FutureShop,” for implementing a virtual fashion retailing practices as well as collecting consumers’ responses for further development. Participants are asked to complete a shopping process from product selection to purchase in FutureShop. The factors examined in this research included the consumers’ purchase intention, interactive shopping and hedonic shopping experience. The findings and implications suggest that the StereoVR can make a significant contribution in creating more interactive experiences for apparel retailing by enhancing consumers’ hedonic shopping experiences in the StereoVR. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Consumer research; Retailing; Shopping experiences; Stereoscopic displays; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,An interactive cameraless projector calibration method,VRS - Virtual Reality,B,"The geometric calibration of projectors is a demanding task in many areas related to computer vision, virtual reality or augmented reality, to name some. Up to date, different methods have been proposed to retrieve the intrinsic and extrinsic parameters of projectors. During the last 20 years, researchers have used cameras as means to calibrate projectors in order to automatize the process. However, this might add: (1) complexity in terms of mathematical formulation; (2) restrictions in terms of camera locations relative to projectors; and (3) additional errors (those due to the camera calibration itself). Most of these camera-based methods make use of planar homographies, and others require an extended calibration process (for both the camera and the projector). In this paper, we present an approach that combines a direct transformation method (DLT) with projected augmented reality to perform an interactive calibration of projectors without the need for cameras. This method is based on non-coplanar points and 2D/3D correspondences, which are interactively established. Intrinsic and extrinsic calibration is achieved in a single step, making use of the DLT. The method rescues old approaches to calibrate projectors, but brings the new capabilities of interactive systems, all integrated in a single software solution. We conduct different experiments by considering two projector setups and two different sets of control points, proving that the accuracy of the method in the real space can be between one and two pixels. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Calibration; Cameraless; DLT; Interactive; Photogrammetry; Projector,Abstract_Keywords,True,
Scopus,journalPaper,2020,A virtual reality application for augmented panoramic mountain images,VRS - Virtual Reality,B,"Virtual reality is a powerful interaction mechanism that holds the promise of engaging users, not only for entertainment, but also for social and environmental purposes. In this paper we present PeakLensVR, a virtual reality mobile application that enables users to capture panoramic mountain images with their mobile devices and later visualize such images, enriched with metadata about the peaks visible from the capture point, with a low-end VR device. The application exploits a multi-stage data processing pipeline, which comprises the following steps: (1) the acquisition of a sequence of frames with the mobile phone camera and their annotation with sensor readings captured during the shooting session; (2) the creation of a panoramic image from the acquired frames, with state-of-the art stitching algorithms; (3) the registration of the panoramic image to the mountain skyline in view, by comparing the image skyline with a virtual profile extracted from the NASA SRTM Digital Elevation Model of the Earth; (4) the enrichment of the registered panoramic image with markers and metadata (name, altitude, etc.) of the peaks in view, by querying the OpenStreetMap GIS. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Environmental monitoring; Location-based systems; Mobile applications; Mountain identification; Remote sensing; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Negative effects of network latencies in immersive collaborative virtual environments,VRS - Virtual Reality,B,"The present work aims to investigate the negative effects of network latencies in immersive collaborative virtual environments. A user study was conducted to determine the impact of those delays on the performance of users.Participants of the study played a simple cooperative game designed for two players. The goal of the game was to correctly place bicolored cubes into their specific destinations. Since each player only saw the colors of the cubes of his or her partner, both players had to visually and verbally exchange information to complete the game. Each pair of participants played the game under four different latency conditions. The task performance was measured by the time needed to place one cube successfully. Besides, other subjectively observable variables were investigated. The results of the study show that a high end-to-end delay between two VR stations has an adverse effect on the users’ task performance, the amount of mutual understanding and the perceived workload. For the co-presence metric, i.e., the perceived amount of togetherness inside the virtual environment, no significant correlation to the network delay could be determined. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Co-presence; Distributed virtual reality; Immersive collaborative virtual environment; Latency; Mutual understanding; Perceived workload; Performance,Keywords,True,
Scopus,journalPaper,2020,Influence of presentation means on industrial product evaluations with potential users: a first study by comparing tangible virtual reality and presenting a product in a real setting,VRS - Virtual Reality,B,"Nowadays, virtual reality allows products to be presented to potential users, but as they cannot feel them physically, their perception of some product attributes can be distorted. Conversely, the mixture of visual and touch feelings that tangible virtual reality (TVR) offers could act as a similar approach to knowing products in real settings. This is a first study to compare the evaluation of product attributes presented in a real setting and by tangible virtual reality to verify the possible equivalence of both means. The semantic differential method was used to evaluate product attributes by creating a semantic scale with 16 bipolar pairs. Seventy-seven people (mean age of 21.7) evaluated one product by both means in an alternate viewing order. The results revealed that the product that was chosen was rated with more positive attributes in some bipolar pairs when experienced via TVR, while it was better rated in others when experienced in a real environment. The Wilcoxon test (α = 0.05) corroborated that the presentation means used to evaluate the product influenced the evaluation of 15 of 16 attributes. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Attributes evaluation; Industrial design; Product presentation; Tangible virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,An experimental paradigm for the assessment of realistic human multitasking,VRS - Virtual Reality,B,"Human multitasking has been evaluated with paradigms that administered two—rarely three—concurrent tasks. In everyday life, however, we usually face an ever-changing sequence of distinct concurrent tasks. Available studies therefore provided valuable insights into our ability for dual tasking, but they did not address the natural interplay of dual tasking and task switching. The present study was undertaken to explore the feasibility of two new paradigms which replicate that interplay in virtual reality. We used car driving simulator software to implement a virtual car-driving task as well as a virtual street-crossing task. Either task was administered alone, as well as concurrently with a battery of loading tasks that mimicked activities of everyday life. The loading tasks used different sensory modalities, different cognitive processes, and different output channels and were presented in an ever-changing sequence. Cronbach’s alpha scores of key registered variables were high, which indicates that our approach is reliable. Driving and street-crossing performance deteriorated under multitask conditions, which indicates that our approach is sensitive to multitasking. This is the first study to demonstrate the feasibility of an experimental paradigm for the assessment of natural multitasking, i.e., of combined dual tasking and task switching. This paradigm could be of interest for basic science as well as for prevention and rehabilitation settings. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Car driving; Ecological validity; Human cognition; Multitasking costs; Street crossing,Abstract,True,
Scopus,journalPaper,2020,Adapting UTAUT2 to assess user acceptance of an e-scooter virtual reality service,VRS - Virtual Reality,B,"A virtual reality (VR) technology innovation experience service was designed to promote electric two-wheelers (E2Ws). An understanding of the factors that will have an impact on VR service adoption in experiencing an E2W ride is important. This study adapts the Unified Theory of Acceptance and Use of Technology (UTAUT2) to investigate the factors that may influence user acceptance of fully immersive VR as compared to desktop VR. A within-subjects design enabled 56 participants to evaluate both VR systems. The results indicate that the model constructs of performance expectancy, hedonic motivation, and facilitating conditions are useful predictors of the behavioral intention to use VR systems. Although these factors were significantly higher for fully immersive VR, both VR systems can yield a positive influence on behavioral intention. Based on these findings, several implications for developers and suggestions for future research are provided. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Immersive experience; Technology acceptance model; User experience; UTAUT2; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,Flexible data structures for dynamic virtual auditory scenes,VRS - Virtual Reality,B,"Virtual environments and their contents are dynamically changing, but also need to respond to the user immediately. While managing a dynamic scene is a common and well-understood problem for visual rendering, additional challenges exist for the high-quality audio rendering of such scenes. Audio rendering differs in a key aspect: Sound waves propagate substantially slower than light. For the acoustics in scenes of large dimensions, it is not sufficient to regard just the state at the current time. The sound propagation times become so significant (perceptible) that the past of the objects matter, making a time history of the scene necessary. Particularly the conjunction of multithreading and low-latency audio processing makes the description of the virtual acoustic scene a problem on its own. This paper presents a novel solution to this acoustic-related problem. We discuss the challenges of realizing a real-time auralization on modern (non-real-time) operating systems and state the main requirements of the data structure. A hierarchical state-based data structure with time history is presented, which not only fulfills the requirements for outdoor auralizations but also has key advantages for indoor simulations—such as room acoustics. A key feature is the integral support of atomic scene modifications, allowing several modifications to be performed at the same time. The presented concept is very modular and beneficial for a wide range of applications. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",3D sound; Audio rendering; Auralization; Interaction; Virtual acoustics; Virtual reality,Keywords,True,
Scopus,journalPaper,2021,From TAM to AVRTS: development and validation of the attitudes toward Virtual Reality Technology Scale,VRS - Virtual Reality,B,"To address a deficiency of scales measuring attitudes toward virtual reality technology, the Attitudes toward Virtual Reality Technology Scale (AVRTS) was developed following a three-step procedure. Items were generated based on literature and in a focus group pilot study (n = 20). Using factor analysis with maximum likelihood extraction and direct oblimin rotation, the initial scale was created after administration of these items (n = 314) and further refined after a second survey (n = 473). In the final solution, a total of 22 items clustered into a three-factor solution with the factors being “ease of use” (alpha =.858), “usefulness” (alpha =.857), and “enjoyment” (alpha =.919) and overall reliability of.910. Construct validity was established by correlating the AVRTS with the Technology Readiness Index and an access barrier scale, and internal validity was established by correlating the scale with its sub-scales. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Attitudes; Attitudes toward Virtual Reality Technology Scale (AVRTS); Enjoyment; Scale development; Technology Acceptance Model (TAM); Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,"Correction to: Virtual memory palaces: immersion aids recall (Virtual Reality, (2019), 23, 1, (1-15), 10.1007/s10055-018-0346-3)",VRS - Virtual Reality,B,"The article “Virtual memory palaces: immersion aids recall,” written by Eric Krokos, Catherine Plaisant and Amitabh Varshney, was originally published electronically on the publisher’s Internet portal (currently SpringerLink) on May 16, 2018, without open access. With the author(s)’ decision to opt for Open Choice the copyright of the article changed to © The Author(s) [2018] and the article is forthwith distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creat iveco mmons .org/licen ses/by/4.0/), which permits use, duplication, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made. © 2018, The Author(s).",,Title,True,
Scopus,journalPaper,2020,"Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments",VRS - Virtual Reality,B,"A potential application of gesture recognition algorithms is to use them as interfaces to interact with virtual environments. However, the performance and the user preference of such interfaces in the context of virtual reality (VR) have been rarely studied. In the present paper, we focused on a typical VR interaction scenario—answering Yes/No questions in VR systems to compare the performance and the user preference of three types of interfaces. These interfaces included a head gesture interface, a hand gesture interface and a conventional gamepad interface. We designed a memorization task, in which participants were asked to memorize several everyday objects presented in a virtual room and later respond to questions on whether they saw a specific object through the given interfaces when these objects were absent. The performance of the interfaces was evaluated in terms of the real-time accuracy and the response time. A user interface questionnaire was also used to reveal the user preference for these interfaces. The results showed that head gesture is a very promising interface, which can be easily added to existing VR systems for answering Yes/No questions and other binary responses in virtual environments. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Hand gesture; Head gesture; Usability; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2020,Body size illusions influence perceived size of objects: a validation of previous research in virtual reality,VRS - Virtual Reality,B,"Previous research indicates that the size of the own body affects the judgment of objects’ size, depending on the amount of subjective ownership toward the body (Van der Hoort et al. in PLOS ONE 6(5):e20195, 2011). We are the first to transfer this own-body-size effect into a virtual environment. In a series of three experiments, participants (N = 68) had to embody small, medium, and large avatars and judge the size of objects. Body ownership was manipulated using synchronous and asynchronous touch. We also included a new paradigm with an additional change of perspective to induce stronger ownership (Experiment 2). Additionally, we assessed whether the visibility of the body during the judgment phase influenced the results (Experiment 3). In all three experiments, we found an overestimation in a small and an underestimation in a large body compared to a medium body. However, size estimation did not depend on the degree of ownership despite clear differences in self-reported ownership. Our results show that a virtual reality scenario does not require a visuotactile manipulation of ownership in order to evoke the own-body-size effect. Our validation of the effect in a virtual setting may be helpful for the design of clinical applications. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Avatars; Embodiment; Own-body-size effect; Ownership; Size perception; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Improved force JND in immersive virtual reality needle insertion simulation,VRS - Virtual Reality,B,"Haptic feedback in immersive virtual reality (IVR) systems is critical to enable a more intuitive and natural way of interacting with virtual objects. IVR-based haptic medical simulations such as needle insertion procedures have the potential to enhance clinicians’ haptic expertise. This work is a preliminary study on the use and implementation of IVR for needle simulators. Although few studies have quantified haptic skills such as force Just Noticeable Difference (JND) with the single finger, none have measured the force JND as recommended in the standard needle insertion protocol in an IVR environment. The hypothesis of this study is that there will be an improvement of force perception in the IVR, compared to that of the non-immersive virtual reality (NIVR) which facilitates the use of IVR for medical simulations. This paper emphasized on two objectives: firstly, the development of the observer state model for both the IVR and NIVR and the theoretical analysis of the psychophysical measures in both of the environments. Secondly, measures of force JND with the three fingers and comparison of these measures in NIVR to that of the IVR using psychophysical study with the force matching task, constant stimuli method, and isometric force probing stimuli to validate the model. Twenty voluntary subjects performed the experiment in both of the environments. Mean force JND and standard deviation of the JND were found to be 9.12% and 3.75% in the NIVR and 5.91% and 3.65% in the IVR (p value < 0.0001) which are in the same range of JNDs found in the literature (5–10%) for the NIVR using a single finger. Surprisingly, the results showed a better force JND in the IVR compared to that of the NIVR. Also, a simple state observer model was proposed to explain the improvement of force JND in the IVR. This study would quantitatively reinforce the use of IVR for the design of various medical simulators. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Force JND; Immersive virtual reality; Isometric probing; Non-immersive virtual reality; Observer state model; Psychophysical study,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,The effect of a virtual reality learning environment on learners’ spatial ability,VRS - Virtual Reality,B,"This study employed electroencephalography to record event-related potentials to investigate the difference in learning performance between learners with different levels of spatial ability in a traditional learning environment that utilizes presentation slides and a learning environment that incorporates virtual reality (VR). Thirty-two university students participated in the experiment. The N1 and P2 components were results that indicated selective attention at an early stage; their amplitudes were proportional to the cognitive loads. The experiment results revealed that the main effect of learning environment was significant. The N1 and P2 components had larger amplitudes and indicated higher cognitive loads in the presentation slides-based environment than in the VR-based environment. The main effect of spatial ability was also significant. The N1 and P2 amplitudes evoked in the high spatial ability (HSA) learners were smaller than those evoked in the low spatial ability (LSA) learners, indicating that the LSA learners possessed fewer cognitive resources and bore relatively high cognitive loads. The interaction effect of learning environment and spatial ability was significant. Larger P2 amplitude was observed in LSA learners in the presentation slides-based environment than in the VR-based environment, implying that VR facilitates the reduction of cognitive loads in LSA learners. The P2 amplitude detected in HSA learners did not show any significant difference in both learning environments, indicating that the VR-based learning environment did not enhance their learning. This result supports the ability-as-compensator hypothesis to a certain extent. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Cognitive load; Education; Event-related potential (ERP); Spatial ability; Virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Verification of the possibility and effectiveness of experiential learning using HMD-based immersive VR technologies,VRS - Virtual Reality,B,"This paper examines the possibility of experiential learning in a virtual space using head-mounted-display-based immersive virtual reality (VR) technologies. Experiential learning refers to learning through direct experiences in the context of learning. Realistically, experiential learning is impossible in most cases, but VR technologies allowing direct interaction with virtual environments and objects are being developed and commercialized. These technologies are predicted to enhance vividness, interactivity, presence, flow, and experientiality, and increase the expectations of the possibility of experiential learning using VR. Thus, in this study, an experiment was conducted to verify such possibility. The analysis of the experiment results showed that the tactile interactivity and presence improved with the use of enhanced interaction technologies in VR, and in terms of experientiality, the experiment participants became highly aware of the “exploratory stage,” referring to the level of experience of being exposed to an interesting site and directly touching an object in the currently enhanced VR in providing direct tactile and locomotive interactivity. Furthermore, the fact that the learning effect is also partially enhanced was discovered. Accordingly, it was determined that experiential learning using VR is possible based on the experiment results, which showed that the enhanced vividness and interactivity of VR technologies allow the users to closely recognize virtual experiences as direct experiences, and that the learning effect is enhanced. It was also determined that experiential learning in a virtual environment that is identical to an experience in reality would be made possible in the near future based on continued technological development. © 2018, The Author(s).",Authentic virtual reality; Experiential learning; Locomotive interactivity; Presence; Tactile interactivity; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2020,Don’t make me sick: investigating the incidence of cybersickness in commercial virtual reality headsets,VRS - Virtual Reality,B,"The resurgence of interest in the use of virtual reality (VR) technology for research and entertainment purposes has led to an increase in concerns about human factor issues inherent in VR technology. One issue that has received a great deal of attention from researchers and end users is cybersickness, which refers to a constellation of unpleasant physiological symptoms, such as nausea and dizziness, experienced as result of exposure to a virtual environment. While cybersickness is not a new issue, it is expected to become even more prevalent with the recent proliferation of commercially available VR headsets, such as Oculus Rift and HTC Vive, underscoring the importance of investigating the prevalence of cybersickness while using these headsets. Accordingly, in two experiments, the current study examined whether cybersickness was a persistent issue in the state-of-the-art commercial-grade VR headsets and compared the incidence and severity of cybersickness across Oculus Rift CV1, HTC Vive and a desktop display. Consistent with prior work into head-mounted displays and cybersickness, results indicated that participants in the Oculus Rift CV1 and HTC Vive conditions experienced more severe cybersickness symptoms, when compared to those in the desktop display condition. Oculus Rift CV1 and HTC Vive did not significantly differ from each other and were comparable in the severity of cybersickness symptoms they induce. The current study demonstrated that cybersickness was still a prevalent human factor issue in such modern VR headsets as Oculus Rift and HTC Vive, highlighting the importance of devising strategies to mitigate cybersickness in VEs. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Head-mounted displays; HMD; Human factors of VR; Motion sickness; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Feasibility and usability of a virtual reality intervention to enhance men’s awareness of testicular disorders (E-MAT),VRS - Virtual Reality,B,"Testicular cancer is the most common cancer among men younger than 50, and benign testicular disorders such as torsion and epididymitis can be life-threatening if left untreated. Men’s awareness of testicular disorders is lacking, and their intentions to see help for symptoms of testicular disease are low. This study aimed to describe the development, feasibility, and usability of a virtual reality (VR) intervention designed to enhance men’s awareness of testicular disorders (E-MAT). We designed E-MAT as a three-level VR experience and tested its feasibility and usability with 15 men recruited from a university. Following exposure to the intervention, participants filled a 43-item questionnaire. Participants agreed that the technology was comfortable to use, testicular disorders were well represented, the use of light humor was appropriate, and the scientific facts were easy to understand. Participants also agreed that the intervention was suited for men from different sociodemographic backgrounds and felt confident using VR. Overall, participants perceived the intervention as user-friendly, enjoyable, and aesthetically appealing. To the best of our knowledge, VR has not been used to promote men’s health in the past, let alone increasing their awareness and help seeking for testicular disorders. We recommend testing the effectiveness of E-MAT and making it available on public platforms that men can access at their own leisure. VR can be used in future interventions to educate men about various health topics. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Feasibility; Health promotion; Testicular cancer; Testicular diseases; Usability; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,HapticSnakes: multi-haptic feedback wearable robots for immersive virtual reality,VRS - Virtual Reality,B,"Haptic feedback plays a large role in enhancing immersion and presence in VR. However, previous research and commercial products have limitations in terms of variety and locations of delivered feedbacks. To address these challenges, we present HapticSnakes, which are snake-like waist-worn robots that can deliver multiple types of feedback in various body locations, including taps-, gestures-, airflow-, brushing- and gripper-based feedbacks. We developed two robots, one is lightweight and suitable for taps and gestures, while the other is capable of multiple types of feedback. We presented a design space based on our implementations and conducted two evaluations. Since taps are versatile, easy to deliver and largely unexplored, our first evaluation focused on distinguishability of tap strengths and locations on the front and back torso. Participants had highest accuracy in distinguishing feedback on the uppermost regions and had superior overall accuracy in distinguishing feedback strengths over locations. Our second user study investigated HapticSnakes’ ability to deliver multiple feedback types within VR experiences, as well as users’ impressions of wearing our robots and receiving novel feedback in VR. The results indicate that participants had distinct preferences for feedbacks and were in favor of using our robots throughout. Based on the results of our evaluations, we extract design considerations and discuss research challenges and opportunities for developing multi-haptic feedback robots. © 2019, The Author(s).",Airflow; Design; Feeding; Gestures; Haptic; Multipurpose; Robot; Taps; Torso; Wearable,Title,True,
Scopus,journalPaper,2020,Self-position awareness-based presence and interaction in virtual reality,VRS - Virtual Reality,B,"The awareness of self-position, which refers to the location of our cyclopean eye, is essential to the experience of being immersed and for interacting with a virtual environment. Currently, individual variability in the cyclopean eye location is not considered when presenting virtual reality content. The effect of the cyclopean eye shift was analyzed with theoretical and experimental methods, and it was found that human vision is sensitive to the visual direction offset caused by the cyclopean eye shift. To compensate for the cyclopean eye shift, an original model is proposed with shifting the 3D camera system accordingly. The proposed method is expected to make all individuals with different cyclopean eye shifts perceive the virtual environment from the same viewpoint as the designer desired and may achieve more accurate interaction. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Accurate interaction; Cyclopean eye; Immersed experience; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Open data exploration in virtual reality: a comparative study of input technology,VRS - Virtual Reality,B,"In this article, we compare three different input technologies (gamepad, vision-based motion controls, room-scale) for an interactive virtual reality (VR) environment. The overall system is able to visualize (open) data from multiple online sources in a unified interface, enabling the user to browse and explore displayed information in an immersive VR setting. We conducted a user interaction study (n= 24 ; n= 8 per input technology, between-group design) to investigate experienced workload and perceived flow of interaction. Log files and observations allowed further insights and comparison of each condition. We have identified trends that indicate user preference of a visual (virtual) representation, but no clear trends regarding the application of physical controllers (over vision-based controls), in a scenario that encouraged exploration with no time limitations. © 2019, The Author(s).",3D gestural input; Comparative study; Gamepad; Room-scale virtual reality; Virtual reality; Vision-based motion controls,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Detecting rigid links between sensors for automatic sensor space alignment in virtual environments,VRS - Virtual Reality,B,"                             Simultaneous use of multiple sensor systems provides improved accuracy and tracking range compared to use of a single sensor system for virtual reality applications. However, calibration of multiple sensor technologies is non-trivial and at a minimum will require significant, and likely regular, user actioned calibration procedures. To enable ambient sensor calibration, we present techniques for automatically identifying relations between rigidly linked 6DoF and 3DoF sensors belonging to different sensor systems for body tracking. The techniques allow for subsequent automatic alignment of the sensor systems. Two techniques are presented, analysed in simulation for performance under varying noise and latency conditions, and are applied to two case studies. The first study identified sensors tracked by a gold standard rigid body tracker with one of six rigid bodies tracked by the first generation Kinect sensor with each sensor identified correctly in at least 76% of estimates. The second case study was an interactive version of the system that can detect a change in sensor configuration in 1–2 s and only requires movements of less than 15 cm or 90                              ∘                             . Our methods represent a key step in creating highly accessible multi-device 3D virtual environments.                          © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Calibration; Input devices; Sensors; Tracking; Usability,Abstract,True,
Scopus,journalPaper,2019,Editorial: themed issue on enhanced educational experience in virtual and augmented reality,VRS - Virtual Reality,B,"VR and AR can bring changes not only to the everyday life of society, but also in humans’ cognitive capacity to perceive and imagine, giving them the opportunity to experience what is not possible in the real world, and equipping them with the possibility to see objects, places and situations that cannot be seen in reality. Technologies change the field of education, the way people learn and the way they acquire knowledge; consequently, changes need to be made in the way students are taught and in how their knowledge is assessed when using these technologies. In the context of VR/AR educational experiences, significant transformations are needed; in particular, these experiences move away from local teaching to translocal teaching as the virtual world gives students the possibility to disconnect from their physical environment. VR/AR can be combined with haptic solutions and ideas of gamification to support learning and provide diverse experiences. Both the socialisation process and the environment are partly transformed, and virtual communities are created which, in turn, advance new challenges to the teaching and learning process. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Education; Knowledge acquisition; Learning process; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2020,Automatic synthesis of explosion sound synchronized with animation,VRS - Virtual Reality,B,"Sound is an essential element for enhancing the realness of virtual world. Currently, there are some remarkable works on the synthesis of fluid sound, such as fire and water. However, little attention has been paid to synthesizing explosion sound. This paper proposes an automatic method for synthesizing explosion sounds that are synchronized with the visual phenomena of explosion animations, including fireball generation and flame combustion. Such two types of visual animation correspond to two types of sound, which we name as explosive sound and combustion noise, respectively. For the synthesis of explosive sound, firstly, the occurrence time and duration of explosion sound are determined according to the dynamic process of fuel consumption, and then, the corresponding explosive sound is extracted from the recording examples according to the high-frequency content. For the combustion noise, we propose a synthesis method of combustion noise on the basis of the timbre similarity between sound examples and low-frequency combustion noise generated by a physical method. Finally, the two types of sound are blended respecting the occurrence and duration of the explosions and combustions parts detected in the visual stream. Our experiments and the user study show the results of our method and demonstrate the effectiveness of our method. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Audiovisual synchronization; Combustion noise; Explosive sound; Immersive virtual reality; Sound synthesis,Keywords,True,
Scopus,journalPaper,2020,Development and validation of a simulation workload measure: the simulation task load index (SIM-TLX),VRS - Virtual Reality,B,"Virtual reality (VR) simulation offers significant potential for human factors training as it provides a novel approach which enables training in environments that are otherwise dangerous, impractical or expensive to simulate. While VR training has been adopted in many environments, such as heavy industry, surgery and aviation, there remains an inadequate understanding of how virtual simulations impact cognitive factors. One such factor, which needs careful consideration during the design of VR simulations, is the degree of mental or cognitive load experienced during training. This study aimed to validate a newly developed measure of workload, based on existing instruments (e.g. the NASA-TLX), but tailored to the specific demands placed on users of simulated environments. While participants completed a VR puzzle game, a series of experimental manipulations of workload were used to assess the sensitivity of the new instrument. The manipulations affected the questionnaire subscales (mental demands; physical demands; temporal demands; frustration; task complexity; situational stress; distraction; perceptual strain; task control; presence) as predicted in all cases (ps <.05), except for presence, which displayed little relationship with other aspects of task load. The scale was also found to have good convergent validity with an alternate index of task load. The findings support the sensitivity of the new instrument for assessing task load in virtual reality. Overall, this study contributes to the understanding of mental workload in simulated environments and provides a practical tool for use in both future research and applications in the field. © 2019, The Author(s).",Cognitive load; Learning; Training; Virtual reality; Workload,Abstract_Keywords,True,
Scopus,journalPaper,2020,Effects of voluntary heart rate control on user engagement and agency in a virtual reality game,VRS - Virtual Reality,B,"It has been demonstrated that virtual reality (VR) exposure can affect the subjective experience of different situations, cognitive capabilities or behavior. It is known that there is a link between a person’s physiological state and their psychological self-report and user experience. As an immersive experience can affect users’ physiological data, it is possible to adapt and enhance the content of a virtual environment in real-time base on physiological data feedback (biofeedback). With the rapid evolution of the physiological monitoring technologies, it is now possible to exploit different modalities of biofeedback, in a cheap and non-cumbersome manner, and study how they can affect user experience. While most of the studies involving physiological data use it as a measuring tool, we want to study its impact when direct and voluntary physiological control becomes a mean of interaction. To do so, we created a two-parts protocol. The first part was designed to categorize the participants on their heart rate control competency. In the second part of the study, we immersed our participants in a VR experience where they must control their heart rate to interact with the elements in the game. The results were analyzed based on the competency distribution. We observed consistent results between our competency scale and the participants’ control of the biofeedback game mechanic. We also found that our direct biofeedback mechanic is highly engaging. We observed that it generated a strong feeling of agency, which is linked with users’ level of heart rate control. We highlighted the richness of biofeedback as a direct game mechanic, prompting interesting perspective for personalized immersive experiences. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Agency; Biofeedback; User engagement; User study; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,"Correction: Mixed prototypes for the evaluation of usability and user experience: simulating an interactive electronic device (Virtual Reality, (2018), 10.1007/s10055-018-0356-1)",VRS - Virtual Reality,B,"Correction to: Virtual Reality https ://doi.org/10.1007/s1005 5-018-0356-1 In the original publication of the article, the following corrections have to be noted down. Section 3.3.5 Is: For the comparative evaluation of emotions that the product and its MP cause to users, we use expressive characters that represent a set of fourteen universal emotions, as proposed by Desmet (2003). Figure 11 shows these emotions as presented by Caicedo and Desmet (2009) and currently used in PrEmo ® Instrument (a proprietary tool used to evaluate the user’s emotional response when interacting with a product or interface). Should be: For the comparative evaluation of emotions that the product and its MP cause to users, we use expressive characters that represent a set of fourteen universal emotions, as proposed by Desmet et al. (2000) and Desmet (2003). Figure 11 shows these emotions as presented by Caicedo and Desmet (2009) and Laurans and Desmet (2017) and currently used in PrEmo ® Instrument (a proprietary tool used to evaluate the user’s emotional response when interacting with a product or interface). Legend from Figure 11 Is: Fig. 11 Fourteen universal feelings. (Reproduce with permission from Caicedo and Desmet 2009) Should be: Fig. 11 Fourteen universal feelings (reproduced with permission from Caicedo and Desmet 2009 and Laurans and Desmet 2017) Legend from Table 3 Is: Table 3 Emotional domains for expressive characters. (Reproduced with permission from Caicedo and Desmet 2009; Laurans and Desmet 2012) Should be: Table 3 Emotional domains for expressive characters (reproduced with permission from Caicedo and Desmet 2009; Laurans and Desmet 2017) List of References (exclude) Laurans G, Desmet P (2012) Indroducing PREMO2: new directions for the non-verbal measurement of emotion in design. Central Saint Martins College Of Arts & Design, London, p 13 (include) Desmet P, Hekkert P, Jacobs JJ (2000) When a car makes you smile: development and application of an instrument to measure product emotions. In NA—Advances in Consumer Research Vol. 27, eds. Stephen J. Hoch and Robert J. Meyer, Provo, UT: Association for Consumer Research, pp. 111–117. Laurans G, Desmet P (2017) Developing 14 animated characters for non-verbal self-report of categorical emotions. J of Design Research, Vol. 15, No. 3/4, pp. 214–233. © Springer-Verlag London Ltd., part of Springer Nature 2018.",,Title_Abstract,True,
Scopus,journalPaper,2019,An augmented reality application for improving shopping experience in large retail stores,VRS - Virtual Reality,B,"In several large retail stores, such as malls, sport or food stores, the customer often feels lost due to the difficulty in finding a product. Although these large stores usually have visual signs to guide customers toward specific products, sometimes these signs are also hard to find and are not updated. In this paper, we propose a system that jointly combines deep learning and augmented reality techniques to provide the customer with useful information. First, the proposed system learns the visual appearance of different areas in the store using a deep learning architecture. Then, customers can use their mobile devices to take a picture of the area where they are located within the store. Uploading this image to the system trained for image classification, we are able to identify the area where the customer is located. Then, using this information and novel augmented reality techniques, we provide information about the area where the customer is located: route to another area where a product is available, 3D product visualization, user location, analytics, etc. The system developed is able to successfully locate a user in an example store with 98% accuracy. The combination of deep learning systems together with augmented reality techniques shows promising results toward improving user experience in retail/commerce applications: branding, advance visualization, personalization, enhanced customer experience, etc. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",3D visualization; Augmented reality; Deep learning; Human–computer interaction; Retail stores; Smart shopping; User experience,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,A virtual reality approach to mindfulness skills training,VRS - Virtual Reality,B,"Virtual reality (VR) is increasingly incorporated into psychotherapy, with the literature documenting its effectiveness in treating anxiety disorders, pain, and stress. Few studies have incorporated VR into mindfulness interventions. The present study examined the efficacy of VR in facilitating mindfulness. It was hypothesised that a brief mindfulness intervention integrated with VR would lead to a greater elevation in mindfulness than conventional mindfulness practice. Thirty-two adults (16 males) aged 18–65 (M = 27.25, SD = 6.04) were randomly allocated to either a control condition, in which participants listened to a mindfulness audio track, or an experimental condition, in which participants received mindfulness practice in a VR beach environment. The Toronto Mindfulness Scale (Lau et al. J clin psychol 62(12):1445–1467 2006) was used to assess two mindfulness factors: curiosity and decentring. Although participants in the experimental condition experienced an increase in mindfulness, VR was not significantly more effective in facilitating mindfulness overall, although the VR condition was characterised by a significantly greater increase in decentring. Replication and investigation of causative mechanisms is necessary to further understand the distinct increase in decentring observed during VR-enhanced mindfulness training in this study. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Brief mindfulness intervention; Mindfulness skills; Randomised control trial; Toronto Mindfulness Scale; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Adaptive virtual reality-based training: a systematic literature review and framework,VRS - Virtual Reality,B,"Virtual reality (VR) provides the capability to train individuals to deal with complex situations by immersing them in a virtual environment. VR-based training has been used in many domains; however, in order to be effective, the training should be adapted based on user’s capabilities, performance, and needs. This study provided a framework for adaptive VR-based training including performance measures, adaptive logic, and adaptive variables. A systematic review of literature was conducted using Compendex, Web of Science, and Google Scholar databases to identify the adaptive VR-based training approaches used in different domains. Results revealed that adaptive VR-based training can be improved by using real-time kinematic/kinetic data and physiological measures from the user, incorporating offline measures such as trainee’s profile information, providing adaptations on controlled elements in the simulation, adjusting feedback content, type, and timing, and using reinforcement learning algorithms. The recommendations provided in this study need to be further validated using longitudinal studies comparing adaptive and non-adaptive training approaches. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Adaptive training; Framework; Personalization; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,"Encoding immersive sessions for online, interactive VR analytics",VRS - Virtual Reality,B,"Capturing and recording immersive VR sessions performed through HMDs in explorative virtual environments may offer valuable insights on users’ behavior, scene saliency and spatial affordances. Collected data can support effort prioritization in 3D modeling workflow or allow fine-tuning of locomotion models for time-constrained experiences. The web with its recent specifications (WebVR/WebXR) represents a valid solution to enable accessible, interactive and usable tools for remote VR analysis of recorded sessions. Performing immersive analytics through common browsers however presents different challenges, including limited rendering capabilities. Furthermore, interactive inspection of large session records is often problematic due to network bandwidth or may involve computationally intensive encoding/decoding routines. This work proposes, formalizes and investigates flexible dynamic models to volumetrically capture user states and scene saliency during running VR sessions using compact approaches. We investigate image-based encoding techniques and layouts targeting interactive and immersive WebVR remote inspection. We performed several experiments to validate and assess proposed encoding models applied to existing records and within networked scenarios through direct server-side encoding, using limited storage and computational resources. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Data quantization; Immersive analytics; Session encoding; Virtual reality; WebVR; WebXR,Keywords,True,
Scopus,journalPaper,2019,Testing the effectiveness of virtual reality as a defusion technique for coping with unwanted thoughts,VRS - Virtual Reality,B,"Negative thoughts are experienced by as many as 80–99% of the population. These thoughts are associated with a variety of negative consequences, including negative mood, decreased task performance and the development of psychopathology. One technique employed in contextual behavioral therapies to help cope with negative thoughts is cognitive defusion. Cognitive defusion techniques undermine potential negative effects of thinking by teaching clients to get some distance from their thoughts. Virtual reality (VR) is the computer-generated simulation of a three-dimensional environment that users can interact in. VR is of increasing interest to applied psychologists due to its potential for exposure learning. One area where VR may be effective is helping people to cope with negative thoughts. The current study examined the impact of a VR task as a cognitive defusion technique on participants’ relationship with a negative self-referential thought (e.g., “I am a failure”). Thirty participants were randomly assigned to one of two conditions (i.e., defusion VR and control VR). Participants were tested pre- and post-VR task on a state measure of cognitive defusion and ratings of their self-referential negative thought. The results indicated that a defusion VR task facilitates the management of negative thoughts and leads to an increase in state defusion. The findings are discussed in terms of their implications for the use of VR techniques in dealing with negative thoughts. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Acceptance and commitment therapy; Cognitive defusion; Negative thoughts; Randomized controlled trial; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,EVA: EVAluating at-home rehabilitation exercises using augmented reality and low-cost sensors,VRS - Virtual Reality,B,"Over one billion people in the world live with some form of disability. This is incessantly increasing due to aging population and chronic diseases. Among the emerging social needs, rehabilitation services are the most required. However, they are scarce and expensive what considerably limits access to them. In this paper, we propose EVA, an augmented reality platform to engage and supervise rehabilitation sessions at home using low-cost sensors. It also stores the user’s statistics and allows therapists to tailor the exercise programs according to their performance. This system has been evaluated in both qualitative and quantitative ways obtaining very promising results. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",3D visualization; Augmented reality; Deep learning; Human–computer interaction; Low-cost sensors; Rehabilitation exercises,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Virtual reality crowd simulation: effects of agent density on user experience and behaviour,VRS - Virtual Reality,B,"Agent-based crowd simulations are used for modelling building and space usage, allowing designers to explore hypothetical real-world scenarios, including extraordinary events such as evacuations. Existing work which engages virtual reality (VR) as a platform for crowd simulations has been primarily focussed on the validation of simulation models through observation; the use of interactions such as gaze to enhance a sense of immersion; or studies of proxemics. In this work, we extend previous studies of proxemics and examine the effects of varying crowd density on user experience and behaviour. We have created a simulation in which participants walk freely and perform a routine manual task, whilst interacting with agents controlled by a typical social force simulation model. We examine and report the effects of crowd density on both affective state and behaviour. Our results show a significant increase in negative affect with density, measured using a self-report scale. We further show significant differences in some aspects of user behaviours, using video analysis, and discuss how our results relate to VR simulation design for mixed human–agent scenarios. © 2018, The Author(s).",Agent density; Crowd simulation; User experience,Title_Abstract,True,
Scopus,journalPaper,2019,Editorial for special issue on Virtual Reality and Augmented Reality,VRS - Virtual Reality,B,[No abstract available],,Title,True,
Scopus,journalPaper,2020,Application of immersive technologies and natural language to hyper-redundant robot teleoperation,VRS - Virtual Reality,B,"This work presents an analysis of immersive realities and natural language applied to the teleoperation of hyper-redundant robots. Such devices have a large number of degrees of freedom, so they often exhibit complex configurations frustrating their spatial understanding. This work aims to contrast two hypotheses; first, if immersive interfaces enhance the telepresence and efficiency against conventional ones; and second, if natural language reduces workload and improves performance against other conventional tools. A total of 2 interfaces and 6 interaction tools have been tested by 50 people. As a result, immersive interfaces were more efficient, improved situational awareness and visual feedback, and were chosen by 94% of participants against conventional ones. On the other hand, participants performed better using natural language than conventional tools despite having less previous experience with the first ones. Additionally, according to 52% of the population, the preferred interaction tool was a mixed strategy that combined voice recognition and hand gestures. Therefore, it is concluded that immersive realities and natural language should play a very important role in the near future of hyper-redundant robots and their teleoperation. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Hyper-redundant robot; Mixed reality; Natural language; Soft robot; Teleoperation; Virtual reality,Keywords,True,
Scopus,journalPaper,2019,Virtual reality to improve group work skill and self-directed learning in problem-based learning narratives,VRS - Virtual Reality,B,"Problem-based learning (PBL) is a type of student-centred educational approach where students learn a topic via their experience in solving open-ended problems. In PBL, elements of active, interactive, and collaborative learning are incorporated to allow teachers to observe their students’ learning process. The contexts of problems in PBL are in the form of cases and narratives of the real world where there are no right or wrong answers. Virtual reality (VR) is a part of mixed reality where a real-world environment is made to be virtual online. In VR, real-world elements are rendered as 3D objects. By adopting VR technologies in PBL, this will enable teachers to form virtual world narratives and cases akin to problems in a real world. VR allows elements of active, interactive, and collaborative to be integrated in students’ learning process. This study explores how VR may be adopted to enable PBL and improve group work skill as well as self-directed learning. The gained research results indicated that PBL with the presentation problem scenario in VR environment is effective in enhancing the group work skills and self-directed learning among students. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Biodiversity; Learning interaction; Problem-based learning; Self-directed learning; Virtual reality; VR scenario,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Evaluating discrete viewpoint control to reduce cybersickness in virtual reality,VRS - Virtual Reality,B,"Cybersickness in virtual reality (VR) is an ongoing problem, despite recent advances in head-mounted displays (HMDs). Discrete viewpoint control techniques have been recently used by some VR developers to combat cybersickness. Discrete viewpoint techniques rely on reducing optic flow via inconsistent displacement, to reduce cybersickness when using stationary HMD-based VR systems. However, reports of their effectiveness are mostly anecdotal. We experimentally evaluate two discrete movement techniques; we refer to as rotation snapping and translation snapping. We conducted two experiments measuring participant cybersickness levels via the widely used simulator sickness questionnaire (SSQ), as well as user-reported levels of nausea, presence, and objective error rates. Our results indicate that both rotation snapping and translation snapping significantly reduced SSQ by 40% for rotational viewpoint movement, and 50% for translational viewpoint movement. They also reduced participant nausea levels, especially with longer VR exposure. Presence levels, error rates, and performance were not significantly affected by either technique. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Vection; Virtual reality; Visually induced motion sickness,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,"Users’ psychophysiological, vocal, and self-reported responses to the apparent attitude of a virtual audience in stereoscopic 360°-video",VRS - Virtual Reality,B,"This research analyzes the psychological reactions of participants to the neutral, positive, or negative attitudes of a virtual audience recorded in 360°-video. Participants were asked to deliver three speeches, each accompanied by a different type of reaction from the virtual audience. Measures of user state included questionnaires, psychophysiological measures, and voice recordings. The results showed that, compared to the neutral audience, the negative audience elicited increases in skin conductance level and heart rate variability, decreases in voice intensity, and a higher ratio of silent parts in the speech, as well as a more negative self-reported valence, higher anxiety, and lower social presence. These findings evidence that, even if some attributes that are considered to be central to immersive experiences are lacking, (i.e., interactivity or avatar representation of the user), 360°-video recreations of virtual environments lead to realistic reactions on users. These results support the effectiveness of 360°-video virtual audiences for public speaking training and social anxiety treatment. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",360°-video; Presence; Psychophysiology; Public speaking; Virtual reality,Keywords,True,
Scopus,journalPaper,2019,Elucidating the impact of critical determinants on purchase decision in virtual reality products by Analytic Hierarchy Process approach,VRS - Virtual Reality,B,"Virtual Reality (VR) is a technology that uses a specialized user interface to connect people to the virtual world, where they can enjoy a multitude of visual, auditory, olfactory, and tactile experiences. In this study, the analytic hierarchy process method is used to analyze the following four factors that affect consumers’ purchase decisions with respect to VR products: VR system requirements, purchase-oriented marketing of VR products, VR application types, and user-friendliness of VR products. Additionally, this study also examines consumer preferences for various VR products. Despite the interviewees’ concerns regarding computing power, product pricing, after-sales services, knowledge learning, and user-friendliness, the results of this study reveal that what they need most are VR products that can be used on the go. Therefore, portable VR is expected to be the trend as people are hoping to use VR applications whenever and wherever they wish to use them. This research provides the theoretical and managerial implications for understanding the potential trends for VR products and thus develops the appropriate marketing strategies for them. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Analytic hierarchy process (AHP); Purchase intention; Virtual reality (VR); VR device,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Virtual memory palaces: immersion aids recall,VRS - Virtual Reality,B,"Virtual reality displays, such as head-mounted displays (HMD), afford us a superior spatial awareness by leveraging our vestibular and proprioceptive senses, as compared to traditional desktop displays. Since classical times, people have used memory palaces as a spatial mnemonic to help remember information by organizing it spatially and associating it with salient features in that environment. In this paper, we explore whether using virtual memory palaces in a head-mounted display with head-tracking (HMD condition) would allow a user to better recall information than when using a traditional desktop display with a mouse-based interaction (desktop condition). We found that virtual memory palaces in HMD condition provide a superior memory recall ability compared to the desktop condition. We believe this is a first step in using virtual environments for creating more memorable experiences that enhance productivity through better recall of large amounts of information organized using the idea of virtual memory palaces. © 2018, The Author(s).",3D navigation; Education; Experimental methods; HMD; Immersion; Perception; Presence; Psychology; Training; User study; Visualization,Abstract,True,
Scopus,journalPaper,2018,Virtual grasps recognition using fusion of Leap Motion and force myography,VRS - Virtual Reality,B,"Hand gesture recognition is important for interactions under VR environment. Traditional vision-based approaches encounter occlusion problems, and thus, wearable devices could be an effective supplement. This study presents a hand grasps recognition method in virtual reality settings, by fusing signals acquired using force myography (FMG), a muscular activity-based hand gesture recognition method, and Leap Motion. We conducted an experiment where participants performed grasping of virtual objects with VR goggles on their head, an FMG band on their wrist, and a Leap Motion positioned either on the desk or on the goggles (two experimental settings). The FMG, Leap Motion, and fusion of both signals were used for training and testing a simple, but effective linear discriminant analysis classifier, as well as three other mainstream classification algorithms. The results showed that the fusion of both signals achieved a significant improvement in classification accuracy, compared to using Leap Motion alone in both experimental settings. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Force myography; Grasp classification; Hand gesture recognition; Leap Motion; VR,Abstract,True,
Scopus,journalPaper,2019,Leveraging advertising to a higher dimension: experimental research on the impact of virtual reality on brand personality impressions,VRS - Virtual Reality,B,"As new advertising formats emerge, research regarding their effectiveness is called for. Since the emergence of virtual reality (VR) technology, several brands have started implementing VR advertising in their communication strategies. The research objective of this study is to examine whether mobile VR advertising can positively impact managerially relevant brand outcome variables [brand personality (BP) impressions, brand attitude and purchase intentions] compared to traditional 2D advertising. An experimental between-subjects design comprising 160 respondents reveals a significant difference in impact between 2D and VR advertising on three out of five BP dimensions (namely ‘excitement’, ‘sophistication’ and a marginal effect on ‘ruggedness’). Furthermore, mobile VR advertising evokes significantly more positive consumer attitudes and higher purchase intentions than mobile 2D ads. The findings of this study imply that mobile VR advertising can be effective for brand managers who are in search of bringing their brands to life and who aim to obtain more positive consumer attitudes and higher purchase intentions towards their brands. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Advertising; Brand attitude; Brand personality; Purchase intentions; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,A comparison of two-dimensional prediction tracing and a virtual reality patient methods for diagnosis and treatment planning of orthognathic cases in dental students: a randomized preliminary study,VRS - Virtual Reality,B,"Virtual reality patient (VR patient), a simulated patient module in a virtual reality environment allowing manipulation of the upper and lower jaws and chin in three planes of space, was developed to help students understand diagnosis and treatment planning of orthognathic surgical procedures. The objective was to compare student understanding in diagnosing and treatment planning complex orthognathic cases using the VR patient versus a conventional 2D prediction tracing method and to determine feasibility of utilizing VR methods. Thirty third year dental students were assigned randomly to an experimental (VR patient) or control (2D tracing) group. The dependent variables were a multiple choice question (MCQ) examination, baseline and exit surveys, and written case analysis of two cases. Student–teacher interactions were recorded for both length and type of interaction. Data were evaluated using descriptive and inferential statistics. The students’ performance on the MCQ examinations improved immediately following the educational intervention (p <.05). However, no significant difference was found between the 2 groups on the written case analysis and pre-test, post-test, and follow-up MCQ examinations. The effect size of the intervention ranged from.14 to.90 and differed greatly between the written responses to the two cases. Intra- and inter-rater reliability of the written response scoring was found to be reliable and reproducible (>.928). Dental students were able to improve their understanding of diagnosis and treatment planning of orthognathic cases using both 2D prediction tracing and the VR patient methods. The method of scoring the written responses was reliable and reproducible and should be used for future full-scale studies. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Dental education; Oral surgery; Orthodontics; Orthognathic surgical prediction; Simulated patient; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,"How latency, action modality and display modality influence the sense of agency: a virtual reality study",VRS - Virtual Reality,B,"The objective of this study was to investigate the influences of latency (i.e., technical system response time), action modality (button press, voice command) and display modality (head-mounted display, monitor) on the sense of agency (SOA). SOA is the experience of controlling one’s own actions and their corresponding effects in the environment. The N = 31 (48% female, with a mean age of 24) participants had to interact repeatedly with three different objects (lamp, tablet and computer) in a virtual environment (presented on a monitor or via a head-mounted display) by using a voice command or pressing a button to turn the objects on. The objects reacted after a specific technical system response delay (150, 450 and 750 ms). Results showed that the SOA was weaker for actions employing voice commands opposed to button presses, except for the explicit SOA in the monitor condition. Higher latencies diminished the explicit, but not the implicit SOA. Neither the explicit nor the implicit SOA was significantly affected by the display modality. The findings in part support the weighting process of different agency cues of the underlying framework, and we propose to extend this model by a sense of presence. Users seem to react as if they have the impression that they are not able to control the technical system properly if they interact through a voice command. Therefore, human–computer interface designers could take account of our findings regarding the modality of an action by providing additional feedback cues to increase the SOA for interactions with voice interfaces. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",HMD; Human–computer interaction; Latency; Sense of agency; Virtual reality; Voice command,Title_Keywords,True,
Scopus,journalPaper,2020,Lessons learned from supplementing archaeological museum exhibitions with virtual reality,VRS - Virtual Reality,B,"Archaeological excavations provide us with important clues about the past. Excavated artefacts represent an important connection to civilisations that no longer exist and help us understand some of their customs, traditions and common practices. With the help of academics and practitioners from various disciplines, the results of archaeological excavations can be analysed and a body of knowledge about the corresponding society can be created and shared with members of the general public. Museums have traditionally served the purpose of communicating this knowledge and backing it up with the help of the excavated artefacts. Many museum visitors, however, find it difficult to develop a coherent understanding of the corresponding society only based on the artefacts and annotations shown in museums. Effective modern techniques that have high potential in helping museum visitors with better understanding of the past are 3D reconstruction and virtual reality. 3D reconstruction offers a cost-effective way of recreating historical settlements in a computer-generated virtual environment, while virtual reality helps with immersing people into such environments and reaching a high degree of realism. With the help of these technologies, it becomes possible to relive history, imagine yourself being a part of the reconstructed society and learn about its culture firsthand. The combination of 3D reconstruction and virtual reality represents a very powerful learning tool; however, this tool has been rarely used in a museum setting and its correct use has not been properly investigated. In this paper, we present a study into using virtual reality in itinerant archaeological exhibitions. We discuss the lessons we have learned from developing an interactive virtual reality simulation of the Neolithic settlement of La Draga. These lessons feature our analysis of qualitative and quantitative feedback of museum visitors, as well as what we have learned from analysing their navigation and interaction patterns. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Gamified experiences; Virtual heritage; VR analytics,Title_Abstract,True,
Scopus,journalPaper,2019,Personalized planning and training system for brachytherapy based on virtual reality,VRS - Virtual Reality,B,"This paper presents a virtual planning and training system for brachytherapy based on virtual reality (VR). The purpose of this system is to facilitate preplanning and help doctors to more quickly and easily improve their skills in brachytherapy surgery. For unskilled doctors, this system can increase confidence and support the successful completion of surgery. The system is based on a VR system that can deliver a fully immersive training environment and involves three functions: simulation of the relocation template, arrangement of the puncture needles and the implantation of the seeds. We used Student’s t test to verify the efficiency of the system. Participants were required to complete both the training and a questionnaire to supply feedback. The participants subsequently performed simulation surgery on a dummy and reviewed their cumulative errors compared with the ideal condition. The results demonstrate that this VR system is able to effectively train unskilled doctors, especially young doctors. This system is appreciated by a variety of people, especially inexperienced and younger users, as it is easy to use and provides an enjoyable learning experience. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Accuracy improvement; Simulated surgery; Tumour brachytherapy; Virtual reality training system,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,The joint influence of social status and personal attitudes in a contact and open versus a noncontact and homophobic culture on the virtual Midas touch,VRS - Virtual Reality,B,"Alongside the highly rapid development of virtual reality technology, embodied agents will become soon a common element of human–computer interactions. Our study analyzed the interactional influence of social status, personal attitudes (homophobia and social status importance), and culture on the efficiency of the virtual Midas touch effect. From a human perspective, we focused on the cultural background related to the social norms of touch, homophobia, and social status importance. In Poland, a noncontact culture, men avoid same-gender touch and also score very high on male homophobia. Catalonia, on the other hand, has a contact culture, where same-gender male touch is rather common and natural. Catalonia is also one of the most inclusive and open societies in the world. From an embodied agent’s perspective, we asked whether the agent’s social status influences compliance with virtual touch. We used a modified paradigm of the ultimatum game to observe whether Polish and Catalan men are more compliant when touched by high- or low-status agents. Our results suggest that the virtual interpersonal touch and social status importance influence compliance with a moderating effect of culture. We found also a significant effect of the offer’s value and a moderating effect of culture and homophobia on compliance. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Culture; Homophobia; Immersive virtual reality; Midas touch; Social status; Ultimatum game,Abstract_Keywords,True,
Scopus,journalPaper,2020,An augmented reality-based training system with a natural user interface for manual milling operations,VRS - Virtual Reality,B,"This study developed an augmented reality (AR)-based training system for conventional manual milling operations. An Intel RealSense R200 depth camera and a Leap Motion controller were mounted on an HTC Vive head-mounted display to allow users freely walk around in a room-size AR environment to operate a full-size virtual milling machine with their barehands, using their natural operation behaviors, as if they were operating a real milling machine in the real world, without additional worn or handheld devices. GPU parallel computing was used to handle dynamic occlusions and accelerate the machining simulation to achieve a real-time simulation. Using the developed AR-based training system, novices can receive a hands-on training in a safe environment, without any injury or damage. User test results showed that using the developed AR-based training resulted in lower failure rates and inquiry times than using video training. Users also commented that the AR-based training was interesting and helpful for novices to learn the basic manual milling operation techniques. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Manual milling operation; Natural operation behavior; Occlusion,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Mixed prototypes for the evaluation of usability and user experience: simulating an interactive electronic device,VRS - Virtual Reality,B,"Mixed prototyping technology can be used to represent and simulate the behavior of interactive products at low cost and with great flexibility. This preliminary experimental research intends to verify the suitability of using this technology in the evaluation of usability and user experience of interactive products. Users and experts evaluate the mixed prototype of an image projector with regard to its own usability, and also with respect to its ability to be used to evaluate usability and user experience aspects of the real projector. Users perform tasks on both the real projector and its mixed prototype. In regard to these comparative performance evaluations, time to perform the task and number of errors show a clear positive relationship with the difficulty of the task for both mixed prototype and real projector. In regard to more subjective UX evaluations, the results show to be congruent. However, we realize that emotions assigned to the mixed prototype are influenced by the “fascination” that augmented reality arises in individuals. Experts evaluate the mixed prototype with respect to aspects of the interaction with the product that it is able to simulate, and with respect to the classes of products best suited to be prototyped. They highlighted the possibility of using the technology to evaluate product performance, ergonomics, and operation. In regard to the classes of products to be prototyped, experts’ suggestions coincided with the classes of products that have already been used in research: household appliances, information and communication devices, and automotive parts and accessories. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Mixed prototyping; Product prototyping; Usability; User experience,Abstract_Keywords,True,
Scopus,journalPaper,2020,Level of immersion affects spatial learning in virtual environments: results of a three-condition within-subjects study with long intersession intervals,VRS - Virtual Reality,B,"Virtual reality and immersive technologies are used in a variety of learning and training applications. However, higher levels of immersion do not always improve learning. The mixed results in the literature may partly arise from the use of between-subjects designs, insufficient time intervals between sessions in within-subjects designs, and/or overreliance on binary comparisons of immersion levels. Our study examined the influence of three levels of audiovisual immersive technology on spatial learning in virtual environments, using a within-subjects design with long intersession intervals. Performance on object recognition and discrimination was improved in the highest immersion condition, whereas performance on directional bearings showed a U-shaped relationship with level of immersion. Examination of our data suggests that these results likely would not have been found had we used a between-subjects design or a binary comparison, thus demonstrating the value of our approach. Results suggest that different levels of immersion may be better suited to more or less cognitively complex types of spatial learning. We discuss challenges and opportunities for future work. © 2020, This is a U.S. government work and its text is not subject to copyright protection in the United States; however, its text may be subject to foreign copyright protection.",Head-mounted display; Immersion; Longitudinal design; Spatial audio; Spatial learning; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2020,Improvement potentials in balance and visuo-motor reaction time after mixed reality action game play: a pilot study,VRS - Virtual Reality,B,"Over the past several decades, there has been much interest regarding the effects of video game play. Research has evaluated the behavioral, cognitive, and psychological effects of video game play, and found individuals who play games using complex and dynamic game settings and require focused attention with quick responses, such as action video games, demonstrate improved cognitive performance. Action video game training has also yielded rehabilitation efficacy for a myriad of patients, such as stroke patients and individuals with vestibular and balance deficits. Recently, new technology platforms have been developed such as mixed reality (MR); however, little to no research has been conducted examining the effects of advanced technology platforms. The purpose of the current study was to explore whether MR game training improved visuo-motor reaction time and balance performances. Fourteen participants (11 male; 18–50 years of age) underwent baseline and post-intervention assessments, and then complete 8 h of MR action game training. Visuo-motor reaction times were obtained via computerized Automated Neurocognitive Assessment Metric testing and balance measures were obtained via Sensory Organizational Testing. Baseline and post-intervention comparisons were analyzed using Wilcoxon signed-rank tests. Baseline and post-intervention visuo-motor reaction time and balance comparisons revealed that MR game training yielded significant reaction time improvements (p < 0.05) and vestibular performances (p < 0.05) following game training. Our results suggest visuo-motor reaction time and balance performances were significantly improved following of 8 h of action MR game training. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Action games; Balance; Mixed reality; Reaction time; Visuo-motor,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Effects of steering locomotion and teleporting on cybersickness and presence in HMD-based virtual reality,VRS - Virtual Reality,B,"While head-mounted display-based virtual reality (VR) can produce compelling feelings of presence (or “being there”) in its users, it also often induces motion sickness. This study compared the presence, cybersickness and perceptions of self-motion (or “vection”) induced when using two common methods of virtual locomotion: steering locomotion and teleporting. In four trials, conducted over two separate days, 25 participants repeatedly explored the “Red Fall” virtual environment in the game Nature Treks VR for 16 min at a time. Although steering locomotion was found to be more sickening on average than teleporting, 9 participants reported more severe sickness while teleporting. On checking their spontaneous postural activity before entering VR, these “TELEsick” participants were found to differ from “STEERsick” participants in terms of their positional variability when attempting to stand still. While cybersickness was not altered by having the user stand or sit during gameplay, presence was enhanced by standing during virtual locomotion. Cybersickness was found to increase with time in trial for both methods of virtual locomotion. By contrast, presence only increased with time in trial during steering locomotion (it did not vary over time when teleporting). Steering locomotion was also found to generate greater presence for female, but not male, participants. While there was not a clear advantage for teleporting over steering locomotion in terms of reducing cybersickness, we did find some evidence of the benefits of steering locomotion for presence. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Head-mounted display; Motion sickness; Presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Behavioural intentions of using virtual reality in learning: perspectives of acceptance of information technology and learning style,VRS - Virtual Reality,B,"The use of virtual reality (VR) has become a viable alternative to conventional learning methods in various knowledge domains. Wearable head-mounted displays (HMDs) are devices that provide users with an immersive VR experience. To investigate the direct determinants affecting students’ reasons for HMD use in learning, hypotheses relating to information technology acceptance and Kolb’s learning styles were proposed and tested in this study. Participants were recruited through stratified random sampling according to the population ratio of colleges at a university in Taiwan. Students were shown a video on VR applications in learning, after which an online survey was completed. In total, 387 questionnaires were collected of which 376 were valid. An inference analysis of the samples was performed by structural equation modelling with eight exogenous latent variables, namely the four constructs of the unified theory of acceptance and use of technology (UTAUT) and the four modes of Kolb’s learning styles. All eight variables pointed to one endogenous latent variable: behavioural intention. The results showed all four constructs of the UTAUT to have a positive and significant effect on students’ behavioural intention to use HMDs in learning and only the concrete experience mode of Kolb’s learning styles to have a positive and significant effect. Based on these findings, this study provides suggestions on how to encourage HMDs use in learning to VR developers and educational institutions. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Behavioural intentions; Learning styles; UTAUT; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Systematic review and meta-analysis of augmented reality in educational settings,VRS - Virtual Reality,B,"Augmented reality (AR) is an important technology to enhance learning experiences. Many studies have been conducted to establish the tendencies, affordances and challenges of this technology in educational settings. However, these studies have little analyzed important issues such as the special needs of specific users or the impact of AR on education through the quantitative analysis of the data. This paper presents a literature review that covers 61 studies published between 2012 and 2018 in scientific journals and conference proceedings. As a result, it identifies the status and tendencies in the usage of AR in education, the impact of this technology on learning processes, open questions as well as opportunities and challenges for developers and practitioners. The results indicate that AR has a medium effect on learning effectiveness (d =.64, p <.001). The most reported advantages of AR systems in education are “learning gains” and “motivation.” Otherwise, it is also important to mention that only one of the AR systems of the studies includes accessibility features, which represents a setback in terms of social inclusion. Therefore, given the apparent multiple benefits of using AR systems in educational settings, stakeholders have great opportunities to develop new and better systems that benefit all learners. This technology covers a wide range of topics, target groups, academic levels and more. This could be an indicator that AR is achieving maturity and has successfully taken root in educational settings. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Education; Inclusive learning; Information technologies; Literature review; Meta-analysis,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,UnrealROX: an extremely photorealistic virtual reality environment for robotics simulations and synthetic data generation,VRS - Virtual Reality,B,"Data-driven algorithms have surpassed traditional techniques in almost every aspect in robotic vision problems. Such algorithms need vast amounts of quality data to be able to work properly after their training process. Gathering and annotating that sheer amount of data in the real world is a time-consuming and error-prone task. These problems limit scale and quality. Synthetic data generation has become increasingly popular since it is faster to generate and automatic to annotate. However, most of the current datasets and environments lack realism, interactions, and details from the real world. UnrealROX is an environment built over Unreal Engine 4 which aims to reduce that reality gap by leveraging hyperrealistic indoor scenes that are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline to generate raw data and ground truth annotations. This virtual reality environment enables robotic vision researchers to generate realistic and visually plausible data with full ground truth for a wide variety of problems such as class and instance semantic segmentation, object detection, depth estimation, visual grasping, and navigation. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Grasping; Robotics; Synthetic data,Title_Abstract,True,
Scopus,journalPaper,2020,Users’ domain knowledge prediction in e-learning with speech-interfaced augmented and virtual reality contents,VRS - Virtual Reality,B,"E-learning provides an individualized course path which provides a user the convenience of pacing ones way through a particular course. One of the key privileges it offers is the flexibility of the course and consistent delivery of the material. The proposed system predicts the user’s domain knowledge with the help of the lectures knowledge that a particular user completes and also a lecture’s knowledge gets updated with respect to the users’ knowledge who searches for it. The dependency among the domains also plays a vital role in updating ones’ domain knowledge. The dependencies can be determined by constructing a fuzzy cognitive map. This helps in determining the user’s knowledge in other domains also. The lectures of the proposed system include Augmented Reality and Virtual Reality contents which give an interactive learning experience to the users. The user commands are accepted as audio signals, processed, classified and mapped to the system commands to make it to respond. This proposed work uses the combination of discrete wavelet transform and wavelet packet decomposition for feature extraction and artificial neural network for classification. © 2017, Springer-Verlag London Ltd.",Augmented reality; Fuzzy cognitive maps; Neural network; Signal processing; Virtual reality; Wavelet transformation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Derivation and simulation verification of the relationship between world coordinates and local coordinates under virtual reality engine,VRS - Virtual Reality,B,"As a new human–computer interaction technology, virtual reality technology has been widely used in education, military, industry, art and entertainment, etc. Unity3D is one of the most popular virtual reality product development engines in the world. Coordinate transformation is the mathematical basis for space transformation in virtual reality technology. This paper explains the mechanism of Euler rotation causing the phenomenon of gimbal lock from the perspective of mathematical principles and derives the importance of Unity3D using quaternion for rotation calculation. Based on the Unity3D quaternion rotation calculation, the world–local coordinate transformation relationship of the child object under the Unity3D engine is deeply deduced and verified, which lays a theoretical foundation for the in-depth development of virtual reality products based on Unity3D. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Coordinate transformation; Gimbal lock; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Real-time body tracking in virtual reality using a Vive tracker,VRS - Virtual Reality,B,"Due to recent improvements in virtual reality (VR) technology, the number of novel applications for entertainment, education, and rehabilitation has increased. The primary goal of these applications is to enhance the sense of belief that the user is “present” in the virtual environment. By tracking the user’s skeleton in real-time, it is possible to synchronize the avatar’s motions with the user’s motions. Although current common devices implement body tracking to a certain degree, most approaches are limited by either high latency or insufficient accuracy. Due to the lack of positional and rotation data, the current VR applications typically do not represent the user’s motions. In this paper, we present an accurate, low-latency body tracking approach for VR-based applications using Vive Trackers. Using a HTC Vive headset and Vive Trackers, we have been able to create an immersive VR experience, by animating the motions of the avatar as smoothly, rapidly and as accurately as possible. An evaluation showed our solution is capable of tracking both joint rotation and position with reasonable accuracy and a very low end-to-latency of 6.71±0.80ms. Due to this merely imperceptible delay and precise tracking, our solution can show the movements of the user in real-time in order to create deeper immersion. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Full-body avatar; HTC Vive tracker; Inverse kinematics; Low-latency; Real-time tracking; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,Subjective visual vertical in virtual reality (Curator SVV): validation and normative data,VRS - Virtual Reality,B,"Subjective visual vertical (SVV) assesses the ability to perceive verticality, which is a measure of vestibular otolithic function. Vestibular lesions influence this perception of verticality. We developed a method using virtual reality (VR) display and an Android software application named ‘Curator SVV’. The virtual reality SVV (Curator SVV) consisted of ten readily identifiable artworks projected by a Samsung phone S6 which is inserted into a virtual reality headset. In the first study, 20 patients had there SVV assessed with two devices: (1) a commercially available SVV measurement device (VestiTest®) and (2) a virtual reality SVV using the Curator SVV application. In a second study, 32 healthy subjects had their SVV assessed by the Curator SVV application whilst sitting in a chair. In the first study, there was no significant difference (p = 0.44, paired t test and p = 0.01, test of equivalence) between results obtained by Curator SVV and the commercially available device. In the second study, the average angle measured for healthy subjects was 0.00° ± 0.85°. The normal range (mean ± 2 SD) was ± 2° in standard upright position. We were able to demonstrate that the Curator SVV can be readily employed as an objective, non-invasive and affordable means of assessing otolith function in the clinical context. We validated this novel methodology by finding strong quantitative parity between a standard commercial SVV unit and the VR Curator SVV method. Our very lightweight and mobile device can be employed in clinical contexts including at the bedside and in different head and body positions. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Curator; Otolith; Samsung Gear VR; SVV; Upright; Vestibular,Title_Abstract,True,
Scopus,journalPaper,2020,Virtual reality: an aid as cognitive learning environment—a case study of Hindi language,VRS - Virtual Reality,B,"The objective of this research is to propose a dynamic composition of behaviour-rich interactive 3D scene as a virtual environment for cognitive support using visual and linguistic analytics. Based on the constructivist theory of learning through visual cognition by Winn (A conceptual basis for educational applications of virtual reality, Technical report TR 93-9, Washington Technology University, 1993), we propose our work with a grounding that visual data are easy to comprehend for a person having linguistic learning difficulties. Virtual reality provides an environment for learners to actively pursue their knowledge needs by applying their theories in the ‘real world’. Therefore, we focus our work on generating an interactive virtual environment. It decreases cognitive load for the person with difficulties in comprehension, especially in language reading, e.g. dyslexia. Our prior work related to the proposed research is named as Preksha—an automatic Hindi text visualizer. To the best of the knowledge of the authors, Preksha is the only known visualization work for an Indian Language, viz. Hindi. Belonging to morphologically-rich and free-word order Indian languages, this work on the Hindi Language is a novel interdisciplinary approach to develop a virtual environment for cognitive support. Application of an automatic text visualization with a suitable learning paradigm in a virtual environment is another novelty of this research. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Artificial intelligence; Automatic text visualization; Cognitive learning; Natural language processing; Virtual reality; Visual cognition; WebVR,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Operator training simulators in virtual reality environment for process operators: a review,VRS - Virtual Reality,B,"Given the factors such as safety, profitability, and environmental concerns at stake, operator training is an everlasting and vital process in the process industry. An inevitable need for skilled operators in the chemical industry leads to search for novel and effective training methodologies. Consequently, dynamic simulation techniques have been considered as a tool to educate and train inexperienced personnel as expected by the industry. Traditional training methodologies are hardly sufficient to instruct the operators for seldom-occurring perilous situations. Conventional operator training simulators (OTS) are generally effective, but they lack to give operators the actual feel of the scenarios. Training effectiveness can be enhanced by providing operators with a sense of realism. Therefore, integration of OTS with virtual reality (VR-OTS) certainly comes out to be an alternative. VR-OTS can replicate emergency conditions, accidents, and investigate safety protocols. In this work, we discuss the need for virtual reality (VR) in OTS, merits of VR-OTS, and the role of training assessment methods. Contributions of OTS Authors’ in process industry from year 2000 to mid-2017 are reviewed and discussed extensively. The review shows that VR-OTS provides tangible benefits over its conventional counterparts in terms of improved safety of plant, increased productivity, and environmental protection. Finally, this paper outlines future scopes that the current researcher may consider to focus for the increased and improved VR-OTS usage. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Operator training simulators (OTS); Process simulators; Training assessment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Evaluation of augmented reality technology for the design of an evacuation training game,VRS - Virtual Reality,B,"Building evacuation training systems and training employees in an organization have a vital role in emergency cases in which people need to know what to do exactly. In every building, procedures, rules, and actions are attractively shown on the walls, but most of the people living in that building are not aware of these procedures and do not have any experience what to do in these dangerous situations. In order to be able to apply these procedures properly in an emergency situation, community members should be trained with the state-of-the-art equipment and technologies, but to do so, up-front investment and development of such a system are necessary. In this study, augmented reality (AR) technology was applied to realize a game-based evacuation training system that implements gamification practices. The architectural plans of a university were used to model the floors and the relevant environment. Employees are trained to learn how to reach the nearest exit location in the event of a fire or earthquake, and also, the system provides the shortest path for the evacuation. In addition to these features, our training game has educational animations about the fire, chemical attack, and earthquake events. A mobile application was implemented to train employees working in the building and inform them to know how to escape in an emergency situation. The technology acceptance model and the related questionnaire form were applied, and the response of 36 participants was analyzed. It was demonstrated that AR and relevant tools provide a flexible environment to develop evacuation systems in a university, our mobile application enabled participants to be trained in a realistic environment, and trainees were highly satisfied with the system. Educational animations were also another benefit for the trainees. © 2019, The Author(s).",Animation; ARKit framework; Augmented reality; Evacuation training system; Game engine; Software; Training; Unity3D,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Exploiting sensing devices availability in AR/VR deployments to foster engagement,VRS - Virtual Reality,B,"Currently, in all augmented reality (AR) or virtual reality (VR) educational experiences, the evolution of the experience (game, exercise or other) and the assessment of the user’s performance are based on her/his (re)actions which are continuously traced/sensed. In this paper, we propose the exploitation of the sensors available in the AR/VR systems to enhance the current AR/VR experiences, taking into account the users’ affect state that changes in real time. Adapting the difficulty level of the experience to the users’ affect state fosters their engagement which is a crucial issue in educational environments and prevents boredom and anxiety. The users’ cues are processed enabling dynamic user profiling. The detection of the affect state based on different sensing inputs, since diverse sensing devices exist in different AR/VR systems, is investigated, and techniques that have been undergone validation using state-of-the-art sensors are presented. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Affect state detection; Engagement; Interpretation of interaction; Multimodal affect state detection,Abstract,True,
Scopus,journalPaper,2020,Evaluating the user interface and experience of VR in the electronic commerce environment: a hybrid approach,VRS - Virtual Reality,B,"In recent years, to attract global manufacturers, media reporters, and consumer attention, a popular term in search engines has been virtual reality (VR). In addition to video, entertainment, education, and medical, VR can be used in electronic commerce (E-commerce). Currently, most E-commerce platforms present products using 2D images, text-based interface or animation. However, consumers want to know more about the products while online shopping and compare the product’s material, color, and other elements. Therefore, combining VR with E-commerce can improve the user’s shopping experience. This study will focus on the characteristics of VR and E-commerce. This feature can be used to interactively study UI and UX between users and designers to generate design criteria. Experimental design is carried out through the modified SEM-CPU method. Then, we use the think-aloud protocols, Questionnaire for User Interaction Satisfaction, and User Experience Questionnaire to understand the user’s thoughts on UI and UX. Moreover, this study uses focus group to interview experts to obtain expert opinions on VR E-commerce. Induction and integration of data is carried out to produce design criteria by experimental results. Finally, the Delphi method is utilized to validate and evaluate the practicability of the design criterion. Through the “UI and UX design criteria of VR” generated by this study, many important design elements can be proposed and assessed in VR E-commerce. In closing, this study provides design and development suggestions for VR developers to improve the positive influence of UX. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Electronic commerce (E-commerce); User experience (UX); User interface (UI); Virtual reality (VR),Abstract_Keywords,True,
Scopus,journalPaper,2020,Physical discomfort and eye movements during arbitrary and optical flow-like motions in stereo 3D contents,VRS - Virtual Reality,B,"Users of stereo 3D technology commonly report physical discomfort during or after exposure of stereo 3D contents. The discomfort has been associated with sensation of arbitrary and optical flow-like self-motion. However, there is no information on whether arbitrary motion induces stronger physical discomfort compared with optical flow-like motion. To address this research gap, we investigate physical discomfort among players and spectators of stereo 3D contents using eye tracking and Simulator Sickness Questionnaire. Thirty participants (N= 30) acted as players and spectators of a first-person shooter (FPS) and a car racing game. The FPS and the car racing game produce a sensation of arbitrary and optical flow-like self-motion, respectively. Experimental results show that the FPS game induces more severe physical discomfort than its racing counterpart (p< 0.0083 , with a Bonferroni correction to the p value). We also found that severeness of oculomotor symptoms can be predicted using two eye movements metrics: the amount of fixational eye movements and viewing duration at the center of the screen. Our study implies that one should pay particular attention to different types of self-motion in stereo 3D contents regardless of whether the user controls or solely watches the contents. Our study also suggests that physical discomfort can be reduced by decreasing the frequency of fixational eye movements while prolonging the duration of each fixation at the center of screen. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Cybersickness; Eye tracking; Human factors in virtual reality; Stereo 3D,Keywords,True,
Scopus,journalPaper,2019,Do virtual reality head-mounted displays make a difference? A comparison of presence and self-efficacy between head-mounted displays and desktop computer-facilitated virtual environments,VRS - Virtual Reality,B,"Virtual reality (VR) has made it possible for users to access novel digital experiences. An interesting question that arises in the context of VR is whether it appears or feels different to users when different virtual environments are used. This study investigates the effect of VR head-mounted display (HMD) and desktop computer-facilitated VR on users’ sense of presence (spatial presence and immersion) and task-oriented self-efficacy when exposed to an earthquake education VR system. A quasi-experiment design was used with a sample of 96 university students. The results revealed that the VR system had positive impacts on the users’ earthquake preparedness self-efficacy. Although the experiment group (n = 39) had repeated experiences, as they first used desktop VR followed by VR HMD for the same content, users indicated a higher sense of spatial presence and immersion while using VR HMD than when using desktop VR. In addition, a VR HMD single-group pre- and posttest experimental design was performed with 20 participants, and the differences between the pretest and posttest measurements of earthquake preparedness and self-efficacy were determined to be significant. The qualitative results reveal that the visual stimulus and motion are relevant in composing the VR experience. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Head-mounted display; Immersion; Self-efficacy; Spatial presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Tadpole VR: virtual reality visualization of a simulated tadpole spinal cord,VRS - Virtual Reality,B,"Recent advances in “developmental” approach (combining experimental study with computational modeling) of neural networks produce increasingly large data sets, in both complexity and size. This poses a significant challenge in analyzing, visualizing and understanding not only the spatial structure but also the behavior of such networks. This paper describes a virtual reality application for visualization of two biologically accurate computational models that model the anatomical structure of a neural network comprised of 1500 neurons and over 80,000 connections. The visualization enables a user to observe the complex spatiotemporal interplay between seven unique types of neurons culminating in an observable swimming pattern. We present a detailed description of the design approach for the virtual environment, based on a set of initial requirements, followed up by the implementation and optimization steps. Lastly, the results of a pilot usability study are being presented on how confident participants are in their ability to understand how the alternating firing pattern between the two sides of the tadpole’s body generates swimming motion. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Data visualization; Immersive; Neuroscience; Scientific visualization; Virtual reality; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Assessing hands-free interactions for VR using eye gaze and electromyography,VRS - Virtual Reality,B,"With the increasing popularity of virtual reality (VR) technologies, more efforts have been going into developing new input methods. While physical controllers are widely used, more novel techniques, such as eye tracking, are now commercially available. In our work, we investigate the use of physiological signals as input to enhance VR experiences. We present a system using gaze tracking and electromyography on a user’s forearm to make selection tasks in virtual spaces more efficient. In a study with 16 participants, we compared five different input techniques using a Fitts’ law task: Using gaze tracking for cursor movement in combination with forearm contractions for making selections was superior to using an HTC Vive controller, Xbox gamepad, dwelling time, and eye-gaze dwelling time. To explore application scenarios and collect qualitative feedback, we further developed and evaluated a game with our input technique. Our findings inform the design of applications that use eye-gaze tracking and forearm muscle movements for effective user input in VR. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Electromyography; Eye gaze; Physiological sensing; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2020,Virtual prototyping in the design of see-through features in mobile machinery,VRS - Virtual Reality,B,"Limited visibility from a mobile machine cab can decrease task performance and lead to accidents. Therefore, it is important to consider visibility issues already in the design phase. This paper describes the use of virtual prototyping in the evaluation of see-through features of mobile work machines. The goal is to evaluate whether two different machine boom transparency levels have an effect on task performance. In addition, two alternative placements of overlaid information in the operators’ field of view are assessed. A within-subject design was used in this study. Based on the results, there was no significant difference in performance between the transparency levels. However, the test participants preferred a transparency level of 70–80% (where 0% is completely opaque). Similar results were found with the placement of the overlaid information, which had no significant effect on task performance. Both placements, on the windscreen and on the tunnel wall, were equally favoured by the participants. The findings of this study contribute to the design of see-through features for mobile work machines. In addition, the study demonstrates the use of virtual prototyping in the design of novel features in human–machine systems. © 2019, The Author(s).",Human factors and ergonomics; See-through technology; Virtual prototyping; Virtual reality,Keywords,True,
Scopus,journalPaper,2019,Haptic virtual reality and immersive learning for enhanced organic chemistry instruction,VRS - Virtual Reality,B,"Human–Computer interaction, including technology-aided instruction, is beginning to focus on virtual reality (VR) technology due to its ability to support immersive learning, teaching through simulation, and gamification of learning. These systems can deliver high-level multisensory learning experiences that are important in the teaching of many subjects, especially those involving abstract concepts or requiring spatial skills, such as organic chemistry. Haptic experiences with VR, however, remain a challenge. In addition, development has focused on general entertainment/gaming; VR systems in chemistry implement simulations of the chemistry laboratory and other advanced systems whereas those that support safe, game-like, immersive and multisensory learning of organic chemistry with haptics at pre-university education levels are scarce. We developed the VR Multisensory Classroom (VRMC) as an immersive learning environment within a VR head-mounted display, where learners employ hand movements to build hydrocarbon molecules and experience haptic feedback through gloves with built-in sensors and hand-tracking with the Leap Motion system. We report here the evaluation of the first prototype by learners from diverse backgrounds who reported on the ability of the VRMC to support high engagement, motivation, interest and organic chemistry learning as well as diverse learning styles. The VRMC is a novel VR classroom that supports immersive learning in molecular organic chemistry with haptics for multisensory learning. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Chemistry education; Gamification; Hands-on learning; Haptics; Hydrocarbons; Immersive learning; Introductory chemistry; Middle school science; Organic chemistry; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,A recommender system to generate museum itineraries applying augmented reality and social-sensor mining techniques,VRS - Virtual Reality,B,"Nowadays, museums offer technological and digital options to enrich the user experience in a visit. However, questions arise like which exhibition/museum could I visit? How to tour it and get the best experience? These questions are not easy to answer, because they do not represent tasks straightforward. Considering that the experiences of visiting a museum are now available in social networks, in which users describe, rate, and disseminate a work of art/exhibition of a museum, this information can be mined to generate tour recommendations in museums. Such recommendations could be improved by combining and applying data mining obtained from Internet of Things sensors installed in museums. In this paper, a hybrid approach to make recommendations for museum visits is proposed. It includes an Internet of Things architecture of beacons, incorporating some technologies based on semantic analysis, data mining, and machine learning. This approach integrates and combines data sources for generating and recommending indoor and outdoor itineraries for museums, which are visualized with augmented reality. The itinerary is built, taking into consideration opinions and assessments from social networks, the semantic classification of museums, and cultural activities, as well as data measured by beacon sensors in museum exhibitions. The result is a customized tour with augmented reality that contains a set of recommendations of how to visit a set of museums and obtain a better experience of the visit. A prototype of mobile application is available in the Google Play, called the “Historic Center,” with almost 500 downloads and an acceptable evaluation. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Internet of things; Mobile devices; Semantic classification; Tour recommendations,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Advantages and limits of virtual reality in learning processes: Briviesca in the fifteenth century,VRS - Virtual Reality,B,"Two teaching methodologies are presented and compared in this study: on the one hand, semi-guided tours in immersive virtual reality and, on the other, viewing video renderings of 3D environments. The two techniques are contrasted through 3D modeling of a fifteenth-century Spanish town called Briviesca, in an immersive environment, viewed with Oculus Rift. The suitability of virtual reality for teaching is assessed through questions on historical knowledge and urban layout. The understanding of the undergraduate students is evaluated, through questionnaires, after the viewing sessions. The responses of the students underline the effectiveness of the two methodologies: Video screenings received higher scores for historical ideas and the virtual tour was the most effective method at conveying knowledge learnt while viewing. Additionally, two user movements for controlling the virtual reality environment were tested: (1) gamepad locomotion and (2) roomscale movements combined with teleporting. The clear advantage of the second option was the total lack of motion sickness effects. However, the natural tendency using teleporting was to move very quickly through the city areas with no singular buildings and to spend more time in front of these types of buildings. They therefore missed visual information related to the first areas while retaining more information related to those buildings. Finally, the spatial location of singular buildings was clearly better acquired with the virtual tour. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Active learning; Cultural heritage; Game engine; Immersive environments; Learning; Oculus Rift; Presence; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Can’t touch this: the impact of augmented reality versus touch and non-touch interfaces on perceived ownership,VRS - Virtual Reality,B,"The rise of augmented reality (AR) technology presents e-retailers with new opportunities. According to previous research, it is a technology that can positively affect engagement, brand recall and purchase confidence. Mobile-enabled augmented reality differs from regular mobile phone use as the technology virtually overlays images or information to the real environment. As the use of a touch screen device (i.e. smartphone vs. laptop) has previously been found to positively affect feelings of perceived ownership, the current study examines whether the possibility to virtually manipulate a product on a mobile AR application would have an even stronger effect. This is examined for products with either material properties (i.e. products that require the examination of sensory information) or geometric properties (i.e. products that can be examined via written and/or visual information). The findings reveal that AR does indeed result in higher levels of perceived ownership, particularly in case of material products. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Augmented reality; Mobile commerce; Perceived ownership; Touch; Virtual product interaction,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Evaluating the effectiveness of learning design with mixed reality (MR) in higher education,VRS - Virtual Reality,B,"Virtual reality (VR) is rapidly developed and bringing advancement in various related technologies through the virtual world. It has high potential and plays an important role in education and training fields. Mixed reality (MR) is a type of hybrid system that involves both physical and virtual elements. While VR/MR has proved to be an effective way to improve the learning attitude and effectiveness for secondary students, however, not much work has been conducted on university students to compare the MR experience and traditional teaching approaches in learning design subjects. In this project, we investigated the effectiveness of students in learning design subjects with the support of MR. The effectiveness was measured based on their creativity and systematic approaches in design. Pretests and posttests were conducted to measure the learning effects. We also compared the learning effectiveness of a student’s study with the MR and traditional teaching materials. Nonparametric analyses were conducted to investigate whether the improvements were significant. Experimental results showed that after studying with the support of the MR technology, the students’ abilities in geometric analysis (mean difference = 4.36, p < 0.01) and creativity (mean difference = 1.59, p < 0.05) were significantly improved. The students’ ability in model visualization was also significantly better than the control group (mean difference = 3.08, p < 0.05). It indicated that the results were positive by using the MR to support their study. The MR was also better than using traditional teaching notes in various measured effects. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Design; Education; Extended reality; HoloLens; Learning; Mixed reality; University; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,The impact of the input interface in a virtual environment: the Vive controller and the Myo armband,VRS - Virtual Reality,B,"Gesture-based touchless devices are becoming a widespread alternative to traditional gaming devices such as joysticks or gamepads. However, the impact of such devices on the user experience has to be evaluated, especially if we consider that most users are more familiar with classical handheld gaming controllers. In virtual reality applications, they influence not only the traditional usability, but also the user perception related to some peculiarities of immersive environments. In this paper, we evaluate both these aspects by comparing the user experience with the Myo armband touchless interface and the Vive controller distributed with the HTC Vive headset. We focused on a virtual navigator we developed for HTC Vive to allow users exploring the organs of the human body and navigating inside them. We recruited 78 subjects to test the virtual environment and asked them to fill in a questionnaire: we combined two generic purpose questionnaires focusing on the system usability (UMUX and SUS) and a presence questionnaire, which was specifically designed for virtual environments. We conducted a statistical analysis to study the effects of a touchless interaction on the user experience. The results revealed a better usability of the Vive controller, even though the effort to learn how to use the two devices is similar. In particular, difficulties in using Myo have a significant impact on immersion and adaptation in the virtual environment. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Gesture; Presence; Touchless interaction; Usability; User experience; Virtual environment,Abstract,True,
Scopus,journalPaper,2019,A comparative study between a virtual reality heart anatomy system and traditional medical teaching modalities,VRS - Virtual Reality,B,"The aim of using virtual reality (VR) as a medical training tool is to offer additional means to teach students and to improve the quality of medical skills. A novel system was developed to fulfil the requirements of modern medical education and overcome the challenges faced by both students and lecturers in the process of knowledge transfer. A heart three-dimensional model presented in a virtual reality (VR) environment has been implemented in order to facilitate a new educational modality. This paper reports the outcome of a comparative study between traditional medical teaching modalities and virtual reality technology. This study was conducted in the Faculty of Medicine in the University of Jordan. The participants were asked to perform system trials and experiment with the system by navigating through the system interfaces, as well as being exposed to the traditional physical model of the human heart that is currently used in the faculty during practical anatomy sessions. Afterwards, they were asked to provide feedback via a comparative questionnaire. The participants’ replies to the questions regarding the Physical Heart Model and VR heart anatomy system were assessed for reliability using Cronbach’s alpha. The first group’s (Physical Heart Model questions) α value was 0.689. The second group’s (VR heart anatomy system questions) α value was 0.791. Comparing students’ experience results between the traditional method (Physical Heart Model) and the VR heart anatomy system, the mean scores showed a distinct increase in the values. This indicates that the developed system enhanced their experience in anatomy learning and the provided tools improved their understanding of heart anatomy. Results demonstrated the usefulness of the system by showing a higher satisfaction rate for the provided tools regarding structure and visualisation. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Anatomy; Comparative study; Heart; Medical education; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,Follower behavior under stress in immersive VR,VRS - Virtual Reality,B,"Understanding human decision making is a key requirement to improve crowd simulation models so that they can better mimic real human behavior. It is often difficult to study human decision making during dangerous situations because of the complexity of the scenarios and situations to be simulated. Immersive virtual reality offers the possibility to carry out such experiments without exposing participants to real danger. In the real world, it has often been observed that people tend to follow others in certain situations (e.g., unfamiliar environments or stressful situations). In this paper, we study human following behavior when it comes to exit choice during an evacuation of a train station. We have carried out immersive VR experiments under different levels of stress (alarm only or alarm plus fire), and we have observed how humans consistently tend to follow the crowd regardless of the levels of stress. Our results show that decision making is strongly influenced by the behavior of the virtual crowd: the more virtual people running, the more likely are participants to simply follow others. The results of this work could improve behavior simulation models during crowd evacuation, and thus build more plausible scenarios for training firefighters. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Crowd following; Immersive VR; Studies of human behavior,Abstract,True,
Scopus,journalPaper,2020,Challenges in passenger use of mixed reality headsets in cars and other transportation,VRS - Virtual Reality,B,"This paper examines key challenges in supporting passenger use of augmented and virtual reality headsets in transit. These headsets will allow passengers to break free from the restraints of physical displays placed in constrained environments such as cars, trains and planes. Moreover, they have the potential to allow passengers to make better use of their time by making travel more productive and enjoyable, supporting both privacy and immersion. However, there are significant barriers to headset usage by passengers in transit contexts. These barriers range from impediments that would entirely prevent safe usage and function (e.g. motion sickness) to those that might impair their adoption (e.g. social acceptability). We identify the key challenges that need to be overcome and discuss the necessary resolutions and research required to facilitate adoption and realize the potential advantages of using mixed reality headsets in transit. © 2019, The Author(s).",Augmented reality; In-car; In-flight; Mixed reality; Passenger; Transportation; Travel; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,"Correction to: Comparing head gesture, hand gesture and gamepad interfaces for answering Yes/No questions in virtual environments (Virtual Reality, (2020), 24, 3, (515-524), 10.1007/s10055-019-00416-7)",VRS - Virtual Reality,B,"In the original publication of the article, the set of Equations 1 was wrongly typeset. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",,Title,True,
Scopus,journalPaper,2015,Simulator sickness incidence and susceptibility during neck motion-controlled virtual reality tasks,VRS - Virtual Reality,B,"To determine the incidence, severity, and predisposing factors to simulator sickness (SS) when using the neck virtual reality (VR) device in asymptomatic individuals to understand the risk of provoking SS in the development of neck VR as a rehabilitation tool. Thirty-two participants used the VR system. Postural stability was measured before and after each VR module [range of motion (ROM), velocity, and accuracy]. The duration of each module was recorded, and participants reported their SS using a visual analogue scale (SS–VAS)/100 mm. Following the VR assessment, participants completed the Motion Sickness Susceptibility Questionnaire (MSSQ) (child and adult subsections) and Simulator Sickness Questionnaire (SSQ). The incidence of motion sickness during the VR immersion was 28 %, and the mean severity was 17.2 mm on VAS. There was a significant difference in ROM time, total time, MSSQ score, and SSQ score (p < 0.05) between those who reported any level of SS–VAS and those with no SS–VAS. The SS–VAS score displayed significant positive correlations with SSQ score, change in postural stability time pre to post, ROM time, and total time. Results indicate a relatively high incidence but low severity of SS which was associated with the MSSQ child subsection score and exposure time. © 2015, Springer-Verlag London.",Motion sickness; Neck; Rehabilitation; Velocity; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,Localization of self-generated synthetic footstep sounds on different walked-upon materials through headphones,VRS - Virtual Reality,B,"This paper focuses on the localization of footstep sounds interactively generated during walking and provided through headphones. Three distinct experiments were conducted in a laboratory involving a pair of sandals enhanced with pressure sensors and a footstep synthesizer capable of simulating two typologies of surface materials: solid (e.g., wood) and aggregate (e.g., gravel). Different sound delivery methods (mono, stereo, binaural) as well as several surface materials, in the presence or absence of concurrent contextual auditory information provided as soundscapes, were evaluated in a vertical localization task. Results showed that solid surfaces were localized significantly farther from the walker’s feet than the aggregate ones. This effect was independent of the used rendering technique, of the presence of soundscapes, and of merely temporal or spectral attributes of sound. The effect is hypothesized to be due to a semantic conflict between auditory and haptic information such that the higher the semantic incongruence the greater the distance of the perceived sound source from the feet. The presented results contribute to the development of further knowledge toward a basis for the design of continuous multimodal feedback in virtual reality applications. © 2015, Springer-Verlag London.",Interactive auditory feedback; Localization; Walking,Abstract,True,
Scopus,journalPaper,2015,Development of an educational virtual transmission electron microscope laboratory,VRS - Virtual Reality,B,"In recent years, many high-tech instruments have been developed to investigate the nanostructures of materials. For example, the transmission electron microscope (TEM) is capable of imaging at a significantly higher resolution than light microscopes, enabling scientists to examine the very fine detail of materials even as small as a single column of atoms. It can also be used as a diffractometer to obtain the scattering pattern of a material for analysis of its 3D crystal structure. However, the TEM is a very expensive apparatus such that many students do not have a chance to operate it personally, thus causing the difficulty of understanding its principle and operating procedure. In this study, a virtual TEM laboratory has been developed to help students learn how to analyze the 3D crystal structures of different materials such as diamond, graphite, and anatase according to their diffraction patterns. A teaching experiment was conducted to study the learning effectiveness of using the virtual TEM laboratory for educational applications. The results showed that the virtual TEM laboratory could improve students’ learning motivation and achievement in nanostructure analysis. © 2014, Springer-Verlag London.",3D crystal structure; Diffraction pattern; Nanostructure; Transmission electron microscope; Virtual reality,Keywords,True,
Scopus,journalPaper,2016,A cost-effective interactive 3D virtual reality system applied to military live firing training,VRS - Virtual Reality,B,"The goal of the present study was to develop a cost-effective, man–machine digital interface, to improve students’ real-world firing range training, results, and achievement scores. A serious game-based learning environment was developed, integrating invisible laser infrared technology, 1:1 real-scale rifle guns with recoil effects, as well as 3D interactive virtual reality (VR) military training digital information content, to train students in military live firing. To evaluate the effectiveness of the proposed design, students’ performance, in terms of their learning achievement and learning motivation, was examined. One hundred and sixty high school students from Taiwan were divided into four individual groups of 40 students each, with one control group and three experimental groups (EG1, EG2, and EG3). The data were analyzed by one-way analysis of variance. The results of this cost-effective 3D VR showed significantly better learning motivation, learning outcomes, and positive impacts on users’ actual live firing achievement scores. © 2016, Springer-Verlag London.",Learning motivation; Learning outcome; Military training; Serious games; Virtual reality (VR),Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,Augmented reality-based remote coaching for fast-paced physical task,VRS - Virtual Reality,B,"One popular application of augmented reality (AR) is the real-time guidance and training in which the AR user receives useful information by a remote expert. For relatively fast-paced tasks, presentation of such guidance in a way that the recipient can make immediate recognition and quick understanding can be an especially challenging problem. In this paper, we present an AR-based tele-coaching system applied to the game of tennis, called the AR coach, and explore for interface design guidelines through a user study. We have evaluated the player’s performance for instruction understanding when the coaching instruction was presented in four different modalities: (1) Visual—visual only, (2) Sound—aural only/mono, (3) 3D Sound—aural only/3D and (4) Multimodal—both visual and aural/mono. Results from the experiment suggested that, among the three, the visual-only augmentation was the most effective and least distracting for the given pace of information transfer (e.g., under every 3 s). We attribute such a result to the characteristic of the visual modality to encode and present a lot of information at once and the human’s limited capability in handling and fusing multimodal information at a relatively fast rate. © 2017, Springer-Verlag London.",Augmented reality; Multimodal feedback; Pre-attentive recognition; Tele-coaching,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,Individual differences in virtual reality: Are spatial presence and spatial ability linked?,VRS - Virtual Reality,B,"One aim of virtual reality technology is to immerse the user in a digital environment that is distinct from physical reality. Feeling spatially located in this digital environment is central to the experience and is more formally known as spatial presence. Experiences of spatial presence differ between individuals; prominent theories assume that these differences may, in part, be explained by differences in more general spatial abilities. Whilst there is some support for this claim with desktop systems, there is currently no direct empirical evidence to support this with more immersive technologies such as head-mounted displays (HMDs). In this study, participants completed three different measures of spatial ability before experiencing two virtual environments. These measures included a self-report of visuospatial imagery; the mental rotations test; and a test of topographical memory. After completing the measures, participants briefly experienced a virtual city and a virtual train ride through a HMD. The user’s head movements were tracked, and visual displays were updated to give the sense of a full 360° environment. After each experience, the participants reported how present they felt and the extent to which they had a mental model of the environment. Self-reports of imagery were positively correlated with reports of spatial presence, consistent with the previous literature. However, spatial presence was not related to performance on either of the more objective tests. Whilst this provides confirmatory evidence that self-reports of imagery can predict presence, it is still unclear which more basic spatial abilities, if any, could underlie this relationship. © 2016, Springer-Verlag London.",Cognition; Head-mounted display; Immersive virtual reality; Presence; Spatial cognition; Spatial presence,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Reusing heterogeneous data for the conceptual design of shapes in virtual environments,VRS - Virtual Reality,B,"Today, digital data such as 2D images, 3D meshes and 3D point clouds are widely used to design virtual environments (VE). Most of the time, only one type of those multimodal data is used to describe and specify the shapes of the objects. However, a single object can be seen as a combination of components linked with constraints specifying the relationships and the rigid transformations defining their arrangement. Thus, the definition of new methods able to combine any kind of multimodal data in an easy way would allow non-experts of VE to rapidly mock up objects and scenes. In this paper, we propose a new shape description model together with its associated constraints toolbox enabling the description of complex shapes from multimodal data. Not only rigid transformations are considered but also scale modifications according to the specified context of the constraint setting. The heterogeneous virtual objects (i.e., composed by scalable multimodal components) then result from the resolution of a constraint satisfaction problem through an optimization approach. The proposed approach is illustrated and validated with examples obtained using our prototype software. © 2016, Springer-Verlag London.",Conceptual design; Constraint satisfaction problem; Heterogeneous data; Shape and object description; Virtual reality,Keywords,True,
Scopus,journalPaper,2017,Using augmented reality technology in storytelling activities: examining elementary students’ narrative skill and creativity,VRS - Virtual Reality,B,"The aim of the study was to examine the effects of augmented reality technology on stories in terms of narrative skill, story length and creativity and also to examine correlations between these variables. Posttest-only design with a nonequivalent group model was used. In this study, the sample consisted of 100 fifth-grade elementary students, comprising 46 boys and 54 girls. Purposive and convenience sampling methods were applied. For purposive sampling, the group’s ages, education levels, and experiences in storytelling activities were gathered, and for convenience sampling, easy access to schools was considered. As data collection tools, a suitable narrative scale was used which was found in the literature and creative story form was developed by the researcher. According to the findings, mean scores for all variables for the experimental group were higher than those for the control group. Also, a statistically significant mean difference was found between the experimental and control groups with regard to narrative skill, length of stories, and creativity in stories. In fact, a positive correlation was found between all variables. It is important to recognize when a technology is found to contribute positively to narrative skill and creativity in telling stories, and to ensure this technology is used. Determining correlation between these variables may provide a contribution to studies about evaluating the effect of the new technologies. © 2016, Springer-Verlag London.",Augmented reality; Interactive learning environments; Media in education; Storytelling; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,Comparing methods for numerical input in immersive virtual environments,VRS - Virtual Reality,B,"Numeric input is a sub-case of alphanumeric input where a user wishes to communicate a numerical value to the computer. While this is a straightforward task for windows-based environments, this is not the case for immersive virtual environments. In this paper we design, implement, and evaluate different methods based on gestures or hand-held devices such as wireless numeric keyboards and gamepads. The assessment showed that hand-held devices can be easier to use and faster for data input. A numeric keyboard can offer fast input for positive, small numbers, while the gamepad-based method can offer uniform performance. Although the gesture-based method did not perform as well, there is still scope for its use when a designer needs a method that leaves both user’s hands free during interaction. © 2017, Springer-Verlag London.",Immersive virtual environments; Numerical input; Virtual reality,Keywords,True,
Scopus,journalPaper,2016,Virtual reality training system for maintenance and operation of high-voltage overhead power lines,VRS - Virtual Reality,B,"The maintenance of high-voltage overhead power lines involves high-risk procedures; the accidents involving live lines maintenance can be lethal. This paper presents the architecture and main features of a novel non-immersive virtual reality training system for maintenance of high-voltage overhead power lines. The general aim of this work was to provide electric utilities a suitable workforce training system to train and to certify operators working in complex and unsafe environments. The developed system has three components: the virtual warehouse, interactive 3D environments, and a learning management system. The workforce training system consists of thirty-one maintenance maneuvers, including the application of different techniques and equipment designed for various structures. Additionally, the system, using 3D animations, illustrates the safety conditions required before starting the maintenance procedures. To fit the worker’s different skill levels, the system has three operation modes: learning, practice, and evaluation, which can be accessed according to the trainee’s level of knowledge. The system is currently used to train thousands of overhead power lines operators of an electric utility in Mexico. The system has demonstrated to be a cost-effective tool for transferring skills and knowledge to new workers while reducing the time and money invested in their training. © 2016, Springer-Verlag London.",E-learning; High-voltage overhead power lines maintenance; Interactive training; Power distribution systems; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,Gesture-based interactive augmented reality content authoring system using HMD,VRS - Virtual Reality,B,"This paper proposes an augmented reality content authoring system that enables ordinary users who do not have programming capabilities to easily apply interactive features to virtual objects on a marker via gestures. The purpose of this system is to simplify augmented reality (AR) technology usage for ordinary users, especially parents and preschool children who are unfamiliar with AR technology. The system provides an immersive AR environment with a head-mounted display and recognizes users’ gestures via an RGB-D camera. Users can freely create the AR content that they will be using without any special programming ability simply by connecting virtual objects stored in a database to the system. Following recognition of the marker via the system’s RGB-D camera worn by the user, he/she can apply various interactive features to the marker-based AR content using simple gestures. Interactive features applied to AR content can enlarge, shrink, rotate, and transfer virtual objects with hand gestures. In addition to this gesture-interactive feature, the proposed system also allows for tangible interaction using markers. The AR content that the user edits is stored in a database, and is retrieved whenever the markers are recognized. The results of comparative experiments conducted indicate that the proposed system is easier to use and has a higher interaction satisfaction level than AR environments such as fixed-monitor and touch-based interaction on mobile screens. © 2016, Springer-Verlag London.",Augmented reality authoring; Gesture interaction; Immersive augmented reality; Tangible interaction,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,Virtual dives into the underwater archaeological treasures of South Italy,VRS - Virtual Reality,B,"The paper presents a virtual diving system based on a virtual reality (VR) application for the exploitation of the Underwater Cultural Heritage. The virtual diving experience has been designed to entertain users, but its added pedagogical value is explicitly emphasized too. In fact, the ludic activities, consisting in the simulation of a real diving session from the point of view of a scuba diver, are following a storyline described by a virtual diving companion who guides users during the exploration of the underwater archaeological site. The virtual diving system provides general and historical-cultural contents, but also information about the flora and fauna of the specific submerged site to the users. The results collected through user studies demonstrate that the proposed VR system is able to provide a playful learning experience, with a high emotional impact, and it has been well appreciated by a large variety of audiences, even by younger and inexperienced users. © 2017, Springer-Verlag London Ltd.",Serious games; Underwater archaeological sites; Underwater Cultural Heritage; Virtual diving system; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2015,The effect of virtual covert sensitization on reducing alcohol craving in heavy social drinkers,VRS - Virtual Reality,B,"Covert sensitization is an imagery-based aversive treatment for decreasing craving and inducing aversion toward abused substances. In the present study, we used virtual reality to enhance the effects of covert sensitization. The aim was to verify the effectiveness of virtual covert sensitization treatment in reducing heavy social drinkers’ alcohol craving. The explicit and implicit measurements included a self-report questionnaire, alcohol-Implicit Association Test, eye-tracking test, and alcohol-Stroop test. To determine the baseline, we measured the alcohol craving in heavy social drinkers (N = 20) and light drinkers (N = 20). Furthermore, virtual covert sensitization treatment was administered to each participant for 10 min. Afterward, the same measurements as at baseline were repeated. Despite the one-time nature of the administered treatment, our results confirm the effectiveness of virtual covert sensitization based on the participants’ changed implicit craving and explicit, self-reported craving. Therefore, virtual covert sensitization may be an effective intervention technique for alcohol addiction treatment. © 2015, Springer-Verlag London.",Alcohol craving; Covert sensitization; Eye movement; Implicit cognition; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2015,DSCVR: designing a commodity hybrid virtual reality system,VRS - Virtual Reality,B,"This paper presents the design considerations, specifications, and lessons learned while building DSCVR, a commodity hybrid reality environment. Consumer technology has enabled a reduced cost for both 3D tracking and screens, enabling a new means for the creation of immersive display environments. However, this technology also presents many challenges, which need to be designed for and around. We compare the DSCVR System to other existing VR environments to analyze the trade-offs being made. © 2014, Springer-Verlag London.",3D; Commodity hardware; Display wall; High resolution; Hybrid reality; Immersive systems; Passive stereo; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2018,Virtual reality software package for implementing motor learning and rehabilitation experiments,VRS - Virtual Reality,B,"Virtual reality games for rehabilitation are attracting increasing growth. In particular, there is a demand for games that allow therapists to identify an individual’s difficulties and customize the control of variables, such as speed, size, distance, as well as visual and auditory feedback. This study presents and describes a virtual reality software package (Bridge Games) to promote rehabilitation of individuals living with disabilities and highlights preliminary researches of its use for implementing motor learning and rehabilitation. First, the study presents seven games in the software package that can be chosen by the rehabilitation team, considering the patient’s needs. All game characteristics are described including name, function presentation, objective and valuable measurements for rehabilitation. Second, preliminary results illustrate some applications of two games, considering 343 people with various disabilities and health status. Based on the results, in the Coincident Timing game, there was a main effect of movement sensor type (in this instance the most functional device was the keyboard when compared with Kinect and touch screen) on average time reached by sample analyzed, F(2, 225) = 4.42, p < 0.05. Similarly, in the Challenge! game, a main effect was found for movement sensor type. However, in this case, touch screen provided better performance than Kinect and Leap Motion, F(2, 709) = 5.90, p < 0.01. Thus, Bridge Games is a possible software game to quantify motor learning. Moreover, the findings suggest that motor skills might be practiced differently depending on the environmental interface in which the game may be used. © 2017, Springer-Verlag London Ltd.",Man–machine interface; Rehabilitation games; Virtual reality rehabilitation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Using a preamble to increase presence in digital virtual environments,VRS - Virtual Reality,B,"Immersion in a digital virtual environment (DVE) increases the likelihood that individuals will feel present in the DVE and hence respond as they would in a similar physically grounded environment. Previous research utilizing high-fidelity technology has demonstrated that by starting a virtual experience in a virtual replica of the immediate physical environment, presence is increased. The purpose of this study was to determine whether utilizing such a transitional environment to increase presence could be replicated on a significantly less immersive system—a 2D desktop monitor with mouse and keyboard for navigation. Participants began their DVE experience either in a “preamble” DVE made to look like the surrounding physical laboratory space, or in a novel DVE (i.e., a house). Then, they were given verbal instructions to leave their respective environments and told to go up a set of stairs to explore a museum. Afterward, they reported levels of immersion and presence in the latter DVE. Results demonstrated that entering a target DVE via a familiar “preamble” environment increased perceptions of reality judgment of the virtual experience, perceptions of possibility to act, and levels of presence. These results suggest that incorporating a familiar digital preamble environment as a prelude to the target DVE enables DVE designers and enthusiasts to increase presence without having to invest in more expensive hardware, but it could also augment existing immersive technology. Their efficacy may be because they offer a gradual transition into the virtual world, such that the familiarity eases users into the novel experience. © 2017, Springer-Verlag London.",Environment; Familiarity; Immersion; Preamble; Presence; Transitional environment; Virtual reality,Keywords,True,
Scopus,journalPaper,2014,Aerial full spherical HDR imaging and display,VRS - Virtual Reality,B,"This paper describes a framework for aerial imaging of high dynamic range (HDR) scenes for use in virtual reality applications, such as immersive panorama applications and photorealistic superimposition of virtual objects using image-based lighting. We propose a complete and practical system to acquire full spherical HDR images from the sky, using two omnidirectional cameras mounted above and below an unmanned aircraft. The HDR images are generated by combining multiple omnidirectional images captured with different exposures controlled automatically. Our system consists of methods for image completion, alignment, and color correction, as well as a novel approach for automatic exposure control, which selects optimal exposure so as to avoid banding artifacts. Experimental results indicated that our system generated better spherical images compared to an ordinary spherical image completion system in terms of naturalness and accuracy. In addition to proposing an imaging method, we have carried out an experiment about display methods for aerial HDR immersive panoramas utilizing spherical images acquired by the proposed system. The experiment demonstrated HDR imaging is beneficial to immersive panorama using an HMD, in addition to ordinary uses of HDR images. © 2014, Springer-Verlag London.",High dynamic range image; Image-based lighting; Immersive panorama; Omnidirectional camera; Tone-mapping,Abstract,True,
Scopus,journalPaper,2018,A systematic review of the application of interactive virtual reality to sport,VRS - Virtual Reality,B,"Virtual reality (VR) technology is being increasingly used by athletes, coaches, and other sport-related professionals. The present systematic review aimed to document research on the application of VR to sport to better understand the outcomes that have emerged in this work. Research literature databases were searched, and the results screened to identify articles reporting applications of interactive VR to sport with healthy human participants. Twenty articles were identified and coded to document the study aims, research designs, participant characteristics, sport types, VR technology, measures, and key findings. From the review, it was shown that interactive VR applications have enhanced a range of performance, physiological, and psychological outcomes. The specific effects have been influenced by factors related to the athlete and the VR system, which comprise athlete factors, VR environment factors, task factors, and the non-VR environment factors. Important variables include the presence of others in the virtual environment, competitiveness, task autonomy, immersion, attentional focus, and feedback. The majority of research has been conducted on endurance sports, such as running, cycling, and rowing, and more research is required to examine the use of interactive VR in skill-based sports. Additional directions for future research and reporting standards for researchers are suggested. © 2017, Springer-Verlag London Ltd.",Exercise; Sport; Systematic review; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,SPH haptic interaction with multiple-fluid simulation,VRS - Virtual Reality,B,"Physics-based fluid interaction plays an important role in computer animation, with wide applications in virtual reality, computer games, digital entertainment, etc. For example, in virtual reality education and games, we often need fluid interactions like acting as an alchemist to create a potion by stirring fluid in a crucible. The traditional input devices such as a mouse and keyboard can basically input 2D information without feedback. In recent years, the continuous development of haptic device not only can achieve six degrees-of-freedom input, but also can calculate the force in virtual scenes and feedback to the user to make a better virtual experience. How to use haptic device in different kinds of virtual fluid scenarios to provide better experience is an important issue in the field of virtual reality. On the other hand, the researches on multiple-fluid interaction especially based on smoothed particle hydrodynamics (SPH) method are very lacking. Therefore, we study the key techniques of haptic interaction with SPH multiple-fluid to compensate this defect in computer graphics community. Different from the single-phase flow, interaction with multiple-fluid flow has difficulties in the realization of properties of different phases. After adding the multiple-fluid simulation, it is also important to keep haptic interaction real time. Our research is based on the mixture model. We guarantee the authenticity of multiple-fluid mixing effect while changing the drift velocity solver to improve efficiency. We employ a unified particle model to achieve rigid body–liquid coupling, and use FIR filter to smooth feedback force to the haptic device. Our novel multiple-fluid haptic simulation can provide an interactive experience for mixing liquid in virtual reality. © 2017, Springer-Verlag London.",Haptics; Multiple fluid; Real time; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2015,A seamless solution for 3D real-time interaction: design and evaluation,VRS - Virtual Reality,B,"This paper aims to propose and evaluate a markerless solution for capturing hand movements in real time to allow 3D interactions in virtual environments (VEs). Tools such as keyboard and mice are not enough for interacting in 3D VE; current motion capture systems are expensive and require wearing equipment. We developed a solution to allow more natural interactions with objects and VE for navigation and manipulation tasks. We conducted an experimental study involving 20 participants. The goal was to realize object manipulation (moving, orientation, scaling) and navigation tasks in VE. We compared our solution (Microsoft Kinect-based) with data gloves and magnetic sensors (3DGloves) regarding two criteria: performance and acceptability. Results demonstrate similar performance (precision, execution time) but a better overall acceptability for our solution. Preferences of participants are mostly in favor of the 3DCam, mainly for the criteria of comfort, freedom of movement, and handiness. Our solution can be considered as a real alternative to conventional systems for object manipulation in virtual reality. © 2014, Springer-Verlag London.",3D camera; 3D interaction; Hand tracking; Real time; Seamless solution; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2016,An evaluation of asymmetric interfaces for bimanual virtual assembly with haptics,VRS - Virtual Reality,B,"Immersive computing technology provides a human–computer interface to support natural human interaction with digital data and models. One application for this technology is product assembly methods planning and validation. This paper presents the results of a user study which explores the effectiveness of various bimanual interaction device configurations for virtual assembly tasks. Participants completed two assembly tasks with two device configurations in five randomized bimanual treatment conditions (within subjects). A Phantom Omni® with and without haptics enabled and a 5DT Data Glove were used. Participant performance, as measured by time to assemble, was the evaluation metric. The results revealed that there was no significant difference in performance between the five treatment conditions. However, half of the participants chose the 5DT Data Glove and the haptic-enabled Phantom Omni® as their preferred device configuration. In addition, qualitative comments support both the preference of haptics during the assembly process and comments confirming Guiard’s kinematic chain model. © 2016, Springer-Verlag London.",Bimanual interaction; Haptic devices; Human–computer interaction (HCI); Interaction devices; Interaction techniques; Virtual reality,Keywords,True,
Scopus,journalPaper,2017,Using smartphone technology to deliver a virtual pedestrian environment: usability and validation,VRS - Virtual Reality,B,"Various programs effectively teach children to cross streets more safely, but all are labor- and cost-intensive. Recent developments in mobile phone technology offer opportunity to deliver virtual reality pedestrian environments to mobile smartphone platforms. Such an environment may offer a cost- and labor-effective strategy to teach children to cross streets safely. This study evaluated usability, feasibility, and validity of a smartphone-based virtual pedestrian environment. A total of 68 adults completed 12 virtual crossings within each of two virtual pedestrian environments, one delivered by smartphone and the other a semi-immersive kiosk virtual environment. Participants completed self-report measures of perceived realism and simulator sickness experienced in each virtual environment, plus self-reported demographic and personality characteristics. All participants followed system instructions and used the smartphone-based virtual environment without difficulty. No significant simulator sickness was reported or observed. Users rated the smartphone virtual environment as highly realistic. Convergent validity was detected, with many aspects of pedestrian behavior in the smartphone-based virtual environment matching behavior in the kiosk virtual environment. Anticipated correlations between personality and kiosk virtual reality pedestrian behavior emerged for the smartphone-based system. A smartphone-based virtual environment can be usable and valid. Future research should develop and evaluate such a training system. © 2016, Springer-Verlag London.",Injury; Mobile smartphone; Pedestrian; Safety; Simulation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2015,On the validity of virtual reality-based auditory experiments: a case study about ratings of the overall listening experience,VRS - Virtual Reality,B,"In recent years, new developments have led to an increasing number of virtual reality (VR)-based experiments, but little is known about their validity compared to real-world experiments. To this end, an experiment was carried out which compares responses given in a real-world environment to responses given in a VR environment. In the experiment, thirty participants rated the overall listening experience of music excerpts while sitting in a cinema and a listening booth being in a real-world environment and in a VR environment. In addition, the VR system that was used to carry out the sessions in the VR environment is presented in detail. Results indicate that there are only minor statistically significant differences between the two environments when the overall listening experience is rated. Furthermore, in the real-world environment, the ratings given in the listening booth were slightly higher than in the cinema. © 2015, Springer-Verlag London.",Convolution engine; Oculus Rift; Overall listening experience; Virtual reality-based experiments,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,When simulated environments make the difference: the effectiveness of different types of training of car service procedures,VRS - Virtual Reality,B,"An empirical analysis was performed to compare the effectiveness of different approaches to training a set of procedural skills to a sample of novice trainees. Sixty-five participants were randomly assigned to one of the following three training groups: (1) learning-by-doing in a 3D desktop virtual environment, (2) learning-by-observing a video (show-and-tell) explanation of the procedures, and (3) trial-and-error. In each group, participants were trained on two car service procedures. Participants were recalled to perform a procedure either 2 or 4 weeks after the training. The results showed that: (1) participants trained through the virtual approach of learning-by-doing performed both procedures significantly better (i.e. p < .05 in terms of errors and time) than people of non-virtual groups, (2) the virtual training group, after a period of non-use, were more effective than non-virtual training (i.e. p < .05) in their ability to recover their skills, (3) after a (simulated) long period from the training—i.e. up to 12 weeks—people who experienced 3D environments consistently performed better than people who received other kinds of training. The results also suggested that independently from the training group, trainees’ visuospatial abilities were a predictor of performance, at least for the complex service procedure, adj R2 = .460, and that post-training performances of people trained through virtual learning-by-doing are not affected by learning styles. Finally, a strong relationship (p < .001, R2 = .441) was identified between usability and trust in the use of the virtual training tool—i.e. the more the system was perceived as usable, the more it was perceived as trustable to acquire the competences. © 2016, Springer-Verlag London.",Automotive; Car service maintenance; Training effectiveness; Training evaluation; Virtual reality,Keywords,True,
Scopus,journalPaper,2015,Sound localization on a horizontal surface: virtual and real sound source localization,VRS - Virtual Reality,B,"As the technology improves and their cost decreases, tabletop computers and their inherent ability to promote collaboration amongst users are gaining in popularity. Their use in virtual reality-based applications including virtual training environments and gaming where multi-user interactions are common is poised to grow. However, before tabletop computers become widely accepted, there are many questions with respect to spatial sound production and reception for these devices that need to be addressed. Previous work (Lam et al. in ACM Comput Entertain 12(2):4:1–4:19, 2014) has seen the development of loudspeaker-based amplitude panning spatial sound techniques to spatialize a sound to a position on a plane just above a tabletop computer’s (horizontal) surface. Although it has been established that the localization of these virtual sources is prone to error, there is a lack of ground truth (reference) data with which to compare these earlier results. Here, we present the results of an experiment that measured sound localization of an actual sound source on a horizontal surface, thus providing such ground truth data. This ground truth data were then compared with the results of previous amplitude panning-based spatial sound techniques for tabletop computing displays. Preliminary results reveal that no substantial differences exist between previous amplitude panning results and the ground truth data reported here, indicating that amplitude panning is a viable spatial sound technique for tabletop computing and horizontal displays in general. © 2015, Springer-Verlag London.",Amplitude panning; Spatial sound; Surface computer; Tabletop computer,Abstract,True,
Scopus,journalPaper,2018,Evaluation of visual feedback techniques for virtual grasping with bare hands using Leap Motion and Oculus Rift,VRS - Virtual Reality,B,"Bare hand interaction (BHI) allows users to use their hands and fingers to interact with digital content without any attached devices or accessories. For BHI to realize widespread adoption, interaction techniques for fundamental operations, like grasp-and-release, need to be identified and optimized. This paper presents a controlled usability evaluation of four common visual feedback techniques in grasp-and-release tasks using bare hand interaction (BHI). The techniques are ‘object coloring,’ ‘connecting line,’ ‘shadow’ and ‘object halo.’ The usability was examined in terms of task time, accuracy, errors and user satisfaction. A software test bed was developed for two interface configurations: using the Leap Motion controller alone (desktop configuration) and using the Leap with Oculus Rift (virtual reality (VR) configuration). Participants (n 32) performed four trials × five feedback techniques × two UI (user interface) configurations, i.e., a total of 1280 trials. The results can be summarized into: (a) user performance is significantly better in the VR configuration compared to the desktop; (b) coloring techniques for visual feedback (‘object coloring’ and ‘object halo’) are more usable than ‘connecting line’ regardless of UI; (c) in the VR, coloring techniques remain more usable, while in the desktop interface the ‘shadow’ technique is also usable and preferred by users, (d) the ‘connecting line’ technique often distracts users from grasp-and-release tasks on static targets. (e) Some visual feedback is always preferred by users than none in both VR and desktop. We discuss these findings in terms of design recommendations for bare hands interactions that involve grasp-and-release tasks. © 2017, Springer-Verlag London.",Bare hand interaction; Leap Motion; Oculus Rift; Usability evaluation; Virtual grasping; Visual feedback techniques,Abstract,True,
Scopus,journalPaper,2015,Development of a virtual butterfly ecological system based on augmented reality and mobile learning technologies,VRS - Virtual Reality,B,"A campus butterfly garden is a useful teaching resource for studying insect ecology because students can learn about a butterfly’s life cycle and become familiar with its habitual behavior by breeding and observation activities. However, it requires professional construction and maintenance for sustainable development, so very few schools can afford to own a butterfly garden. In this study, the augmented reality and mobile learning technologies have been used to develop a virtual butterfly ecological system by combining with campus host plants and virtual breeding activities. Students can use smart phones or tablet PCs to breed virtual butterflies on host plants and observe their life cycles at different growing stages. Using the available space in campus, a virtual butterfly garden can also be created as a greenhouse where students are able to observe different species of butterflies using the tracking telescope and catch a butterfly to obtain its information by touch-screen control. The virtual butterfly ecological system can increase the learning motivation and interest of students through virtual breeding and observation activities, so it is a suitable assistant tool for science education. A teaching experiment has been conducted to investigate students’ learning effectiveness and attitudes after using the system, and the results show that using the virtual butterfly ecological system can improve their learning effectively. © 2015, Springer-Verlag London.",Augmented reality; Butterfly ecology; Context awareness; Mobile learning,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,An objective measure for the visual fidelity of virtual reality and the risks of falls in a virtual environment,VRS - Virtual Reality,B,"Despite decades of development of virtual reality (VR) devices and VR’s recent renaissance, it has been difficult to measure these devices’ effectiveness in immersing the observer. Previously, VR devices have been evaluated using subjective measures of presence, but in this paper, we suggest that postural stability can be used to objectively assess visual fidelity of VR headsets. We validated this measure by testing known differences between the devices. This study also aimed to determine the stability of healthy participants, while in a stable virtual world, compared to eyes-open and eyes-closed conditions and therefore provide a standard of safety requirements for future experimentation. Participants’ ability to maintain a stable centre of pressure was measured using a Wii Balance Board, covered by a foam pad. Stability in eyes-open and eyes-closed conditions was compared with: (1) an iPod Touch in a simple Google cardboard style headset, (2) the Oculus Rift Development Kits (DK) DK1, DK2, with and without the tracking of linear head movements, and (3) the Samsung Gear VR. With a stable VR visual stimulus, the eyes-open condition allowed for significantly greater postural stability than the other conditions, which supports the validity of posturography as a measure of visual fidelity. Further, the iPod Touch, with its narrow field of view and rudimentary software, was significantly less effective at destabilising participants with visual perturbations than the other headsets, with their wider field of view and time warping. Unexpected results are discussed with respect to the possible limitations of the experimental design. © 2016, Springer-Verlag London.",Postural sway; Risk of falls; Safety; Virtual reality fidelity,Title_Abstract_Keywords,True,
Scopus,journalPaper,2015,Creation and calibration method of acoustical models for historic virtual reality auralizations,VRS - Virtual Reality,B,"Virtual reality provides the possibility for interactive visits to historic buildings and sites. The majority of current virtual reconstructions have focused on creating realistic virtual environments, by concentrating on the visual component. However, by incorporating more authentic acoustical properties into visual models, a more realistic rendering of the studied venue is achieved. In historic auralizations, calibration of the studied building’s room acoustic simulation model is often necessary to come to a realistic representation of its acoustical environment. This paper presents a methodical calibration procedure for geometrical acoustics models using room acoustics prediction programs based on geometrical acoustics to create realistic virtual audio realities, or auralizations. To develop this procedure, a small unfinished amphitheater was first chosen due to its general simplicity and considerable level of reverberation. A geometrical acoustics model was calibrated according to the results of acoustical measurements. Measures employed during the calibration of this model were analyzed to come to a methodical calibration procedure. The developed procedure was then applied to a more complex building, the abbey church Saint-Germain-des-Prés. A possible application of the presented procedure is to enable interactive acoustical visits of former configurations of buildings. A test case study was carried out for a typical seventeenth-century configuration of the Saint-Germain-des-Prés. © 2015, Springer-Verlag London.",Acoustic archeology; Auditory VR; Auralization; Calibration; Geometrical acoustics; Virtual heritage,Title_Abstract,True,
Scopus,journalPaper,2015,A feasible group testing framework for producing usable virtual reality learning applications,VRS - Virtual Reality,B,"Designing a usable learning application is one of the key factors in ensuring effective learning. This article introduces modified group usability testing (MGUT) as a feasible framework for evaluating the usability of non-immersive virtual reality (VR) learning applications. Conventionally, usability testing of such learning applications often employs the one-to-one approach in which an evaluator conducts testing with several individual participants. As opposed to the one-to-one approach, the group approach involves several-to-many participants performing tasks simultaneously, with several evaluators observing and interacting with participants. This article describes the complete step-by-step procedure for conducting MGUT to uncover usability problems of a VR learning application that aims to educate its users on fire safety and prevention. It also proposes methods to analyze these usability problems. The effectiveness and efficiency of MGUT was then compared with DGUT, the original group testing technique and cooperative evaluation (CE), which is a typical one-to-one approach. Results indicate that all three techniques are able to reveal usability problems of different usability factors and show similar capability to discover the most critical and serious problems. MGUT is more effective than DGUT as it can collect additional usability problems of various factors and of different levels of severity. MGUT is as effective as CE as both techniques can identify usability problems which are more or less comparable in terms of quantity and quality. As for efficiency, MGUT and DGUT are more efficient than CE as these group testing approaches require lesser testing time, lesser effort in terms of the intensive interaction with participants although with slight more effort in the preparation of the physical setting. In addition, it is also obvious that MGUT and DGUT involve richer participation than CE. MGUT is also more feasible than DGUT as it allows some flexibility in the computer arrangement setting. © 2015, Springer-Verlag London.",Group usability testing; Human–computer interface; Interactive learning environments; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2014,The design space of dynamic interactive virtual environments,VRS - Virtual Reality,B,"Virtual environments have become a key component of many fields and the critical component of virtual reality applications. Due to their virtual nature, they can accommodate an infinite number of possibilities. A theoretical work is presented, which decomposes those innumerous possibilities into concepts to help clarify the vast design space and provide insights into future applied research. We propose that what makes environments interesting and engaging is having worlds that are both active and reactive. This article explores the manifestations of those actions and reactions in what we term: dynamic components and interactions. We term worlds containing these dynamic interactive virtual environments (DIVE). An analysis of each component time was performed, with the purpose of providing a theoretical understanding of the respective design spaces. Initially, we collected the myriad possibilities of each component, e.g., the possible kinds of interactions. We point to examples throughout the field to ground and explain concepts presented. We then categorized of each area into taxonomies. The result of the analyses provides insights into the design space of virtual environments, exposes several avenues of research that are yet underexplored, and provides better understandings of ways in which DIVE creation can be supported. © 2013 Springer-Verlag London.",3D user interaction; Dynamic interactive VEs; Virtual environments; VR systems,Abstract,True,
Scopus,journalPaper,2015,"Erratum to: Using immersive virtual reality and anatomically correct computer-generated characters in the forensic assessment of deviant sexual preferences [Virtual Reality, (2014), 18, 37–47, DOI 10.1007/s10055-013-0235-8]",VRS - Virtual Reality,B,[No abstract available],,Title,True,
Scopus,journalPaper,2015,"Erratum to: New wireless connection between user and VE using speech processing [Virtual Reality, (2015), 18, (235-243), DOI 10.1007/s10055-014-0248-y]",VRS - Virtual Reality,B,[No abstract available],,Title,True,
Scopus,journalPaper,2014,Creation of a new set of dynamic virtual reality faces for the assessment and training of facial emotion recognition ability,VRS - Virtual Reality,B,"The ability to recognize facial emotions is target behaviour when treating people with social impairment. When assessing this ability, the most widely used facial stimuli are photographs. Although their use has been shown to be valid, photographs are unable to capture the dynamic aspects of human expressions. This limitation can be overcome by creating virtual agents with feasible expressed emotions. The main objective of the present study was to create a new set of dynamic virtual faces with high realism that could be integrated into a virtual reality (VR) cyberintervention to train people with schizophrenia in the full repertoire of social skills. A set of highly realistic virtual faces was created based on the Facial Action Coding System. Facial movement animation was also included so as to mimic the dynamism of human facial expressions. Consecutive healthy participants (n = 98) completed a facial emotion recognition task using both natural faces (photographs) and virtual agents expressing five basic emotions plus a neutral one. Repeated-measures ANOVA revealed no significant difference in participants' accuracy of recognition between the two presentation conditions. However, anger was better recognized in the VR images, and disgust was better recognized in photographs. Age, the participant's gender and reaction times were also explored. Implications of the use of virtual agents with realistic human expressions in cyberinterventions are discussed. © 2013 Springer-Verlag London.",Cyberintervention; Dynamism; Emotion recognition; Social skills; Virtual agents,Title_Abstract,True,
Scopus,journalPaper,2014,Using a virtual environment to assess cognition in the elderly,VRS - Virtual Reality,B,"Early diagnosis of Alzheimer’s disease (AD) is essential if treatments are to be administered at an earlier point in time before neurons degenerate to a stage beyond repair. In order for early detection to occur tools used to detect the disorder must be sensitive to the earliest of cognitive impairments. Virtual reality technology offers opportunities to provide products which attempt to mimic daily life situations, as much as is possible, within the computational environment. This may be useful for the detection of cognitive difficulties. We develop a virtual simulation designed to assess visuospatial memory in order to investigate cognitive function in a group of healthy elderly participants and those with a mild cognitive impairment (MCI). Participants were required to guide themselves along a virtual path to reach a virtual destination which they were required to remember. The preliminary results indicate that this virtual simulation has the potential to be used for detection of early AD since significant correlations of scores on the virtual environment with existing neuropsychological tests were found. Furthermore, the test discriminated between healthy elderly participants and those with a MCI. © 2014, Springer-Verlag London.",Mild cognitive impairment; Spatial memory; Spatial navigation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2017,Exposure to an unpleasant odour increases the sense of Presence in virtual reality,VRS - Virtual Reality,B,"While olfactory cues affect the everyday human experience in the physical world, few studies have empirically examined the effect they could have on the human experience in virtual reality (VR). This project’s goal was to determine whether the exposure to olfactory stimuli would affect the senses of Presence (primary measure), Reality and Realism (exploratory measures) in VR. In a virtual kitchen devoid of obvious visual cues linking the visual scene to an odour, three groups of 20 randomly assigned participants (12 females and 8 males per group), unaware of the potential exposure to olfactory stimuli, were exposed to either ambient air, a pleasant odour, or an unpleasant odour. The results reveal that the unpleasant odour had a statistically significant effect on the sense of Presence (as measured by repeated brief measures of Presence and the Independent Television Commission Sense of Presence Inventory), but the pleasant one did not. The lower perceived intensity of the pleasant odour may have contributed to its lower detection rate which, in turn, may have contributed to the pleasant odour’s lack of effect on the sense of Presence. Neither of the olfactory stimuli had an effect on either the sense of Reality or the sense of Realism. © 2016, Springer-Verlag London.",Odours; Olfaction; Presence; Realism; Reality; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,A questionnaire for measuring presence in virtual environments: factor analysis of the presence questionnaire and adaptation into Brazilian Portuguese,VRS - Virtual Reality,B,"The increasing use of virtual reality (VR) environments in different domains of research and psychotherapy offers advantages over traditional treatment approaches. However, in order to feel immersed and involved by the VR experience, participants require VR scenarios that promote the subjective feeling of “being there,” i.e., presence. The most utilized mean of operationalization of presence is through self-report scales and questionnaires. This article aims to report the translation and adaptation of the presence questionnaire (PQ) into Brazilian Portuguese, comparing the factorial distribution of the adapted version with the original PQ. Translation and back-translations were conducted by a team of Brazilian psychologists and computer science professionals with experience on the field. Participants (n = 100) answered the Brazilian version of the questionnaire after wearing a head-mounted display (HMD) and driving a virtual automobile in a VR scenario. The principal component analysis of the translated version generated factors consistently with the original study; however, items that had equivocal construct adequacy in the original PQ changed factors. The factor structure of the PQ is discussed. The growing use of VR environments requires instruments assessing the presence of immersed individuals, and the Brazilian Portuguese version of the PQ appears to be a viable option. © 2016, Springer-Verlag London.",Adaptation; Presence; Presence questionnaire; Translation; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2017,A hybrid optical–mechanical calibration procedure for the Scalable-SPIDAR haptic device,VRS - Virtual Reality,B,"In this research, a simple, yet, efficient calibration procedure is presented in order to improve the accuracy of the Scalable-SPIDAR haptic device. The two-stage procedure aims to reduce discrepancies between measured and actual values. First, we propose a new semi-automatic procedure for the initialization of the haptic device. To perform this initialization with a high level of accuracy, an infrared optical tracking device was used. Furthermore, audio and haptic cues were used to guide the user during the initialization process. Second, we developed two calibration methods based on regression techniques that effectively compensate for the errors in tracked position. Both neural networks and support vector regression methods were applied to calibrate the position errors present in the haptic device readings. A comparison between these two regression methods was carried out to show the underlying algorithm and to indicate the inherent advantages and limitations for each method. Initial evaluation of the proposed procedure indicated that it is possible to improve accuracy by reducing the Scalable-SPIDAR’s average absolute position error to about 6 mm within a 1 m × 1 m × 1 m workspace. © 2016, Springer-Verlag London.",Calibration; Neural networks; Scalable-SPIDAR; Support vector regression; Tracking; Virtual reality,Keywords,True,
Scopus,journalPaper,2017,Industry use of virtual reality in product design and manufacturing: a survey,VRS - Virtual Reality,B,"In 1999, Fred Brooks, virtual reality pioneer and Professor of Computer Science at the University of North Carolina at Chapel Hill, published a seminal paper describing the current state of virtual reality (VR) technologies and applications (Brooks in IEEE Comput Graph Appl 19(6):16, 1999). Through his extensive survey of industry, Brooks concluded that virtual reality had finally arrived and “barely works”. His report included a variety of industries which leveraged these technologies to support industry-level innovation. Virtual reality was being employed to empower decision making in design, evaluation, and training processes across multiple disciplines. Over the past two decades, both industrial and academic communities have contributed to a large knowledge base on numerous virtual reality topics. Technical advances have enabled designers and engineers to explore and interact with data in increasingly natural ways. Sixteen years have passed since Brooks original survey. Where are we now? The research presented here seeks to describe the current state of the art of virtual reality as it is used as a decision-making tool in product design, particularly in engineering-focused businesses. To this end, a survey of industry was conducted over several months spanning fall 2014 and spring 2015. Data on virtual reality applications across a variety of industries was gathered through a series of on-site visits. In total, on-site visits with 18 companies using virtual reality were conducted as well as remote conference calls with two others. The authors interviewed 62 people across numerous companies from varying disciplines and perspectives. Success stories and existing challenges were highlighted. While virtual reality hardware has made considerable strides, unique attention was given to applications and the associated decisions that they support. Results suggest that virtual reality has arrived: it works! It is mature, stable, and, most importantly, usable. VR is actively being used in a number of industries to support decision making and enable innovation. Insights from this survey can be leveraged to help guide future research directions in virtual reality technology and applications. © 2016, Springer-Verlag London.",,Title_Abstract,True,
Scopus,journalPaper,2018,Dense 3D facial reconstruction from a single depth image in unconstrained environment,VRS - Virtual Reality,B,"With the increasing demands of applications in virtual reality such as 3D films, virtual human–machine interactions and virtual agents, the analysis of 3D human face is considered to be more and more important as a fundamental step in these tasks. Due to information provided by the additional dimension, 3D facial reconstruction enables aforementioned tasks to be achieved with higher accuracy than those based on 2D facial analysis. The denser the 3D facial model is, the more information it could provide. However, most existing dense 3D facial reconstruction methods require complicated processing and high system cost. To this end, this paper presents a novel method that simplifies the process of dense 3D facial reconstruction by employing only one frame of depth data obtained with an off-the-shelf RGB-D sensor. The proposed method is composed of two main stages: (a) the acquisition of the initial 3D facial point cloud with automatically 3D facial region cropping, and (b) the generating of the dense facial point cloud with RBF-based adaptive 3D point interpolation. Experiments reported in this paper demonstrate the competitive results with real-world data. © 2017, Springer-Verlag London.",3D interpolation; Three-dimensional image acquisition; Three-dimensional sensing; Virtual face,Abstract,True,
Scopus,journalPaper,2014,"Dynamic learning, retrieval, and tracking to augment hundreds of photographs",VRS - Virtual Reality,B,"Tracking is a major issue of virtual and augmented reality applications. Single object tracking on monocular video streams is fairly well understood. However, when it comes to multiple objects, existing methods lack scalability and can recognize only a limited number of objects. Thanks to recent progress in feature matching, state-of-the-art image retrieval techniques can deal with millions of images. However, these methods do not focus on real-time video processing and cannot track retrieved objects. In this paper, we present a method that combines the speed and accuracy of tracking with the scalability of image retrieval. At the heart of our approach is a bi-layer clustering process that allows our system to index and retrieve objects based on tracks of features, thereby effectively summarizing the information available on multiple video frames. Dynamic learning of new viewpoints as the camera moves naturally yields the kind of robustness and reliability expected from an augmented reality engine. As a result, our system is able to track in real-time multiple objects, recognized with low delay from a database of more than 300 entries. We released the source code of our system in a package called Polyora. © 2013 Springer-Verlag London.",Augmented reality; Image retrieval; Multiple object tracking,Abstract_Keywords,True,
Scopus,journalPaper,2014,Learning disabilities and visual-motor skills; comparing assessment from a hapto-virtual reality tool and Bender-Gestalt test,VRS - Virtual Reality,B,"Previous investigations conducted on post-secondary adult students with learning disabilities (LD) suggest that deficits in visual-motor skills contribute to difficulties in written expression which impact academic achievement. Intervention strategies for individuals with LD include assistive computer-based technologies (ATs) to compensate for or maximize performance. However, research fails to assess the impact of ATs on performance, learning, and motivation of students with LD. Also, one of the limitations of ATs is that they cannot be used for assessment and training and there are very few methods to assess or train visual-motor skills in this population. The present study explores the usefulness of a hapto-visual virtual reality motor skills assessment (MSA) device for visual-motor functioning in adults with and without LD. This is a preliminary step of developing an intervention to improve impaired visual-motor skills in adults with LD. A sample of 22 male and female university students with and without LD had their visual-motor skills pretested using a standard paper-and-pencil Bender-Gestalt (BG) test and were compared according to their performance on the MSA tool. We hypothesized that our LD participants' performance would be significantly lower than our control participants on the VR task in terms of number of errors and speed. Results showed that participants without LD performed better and more rapidly on the VR task than participants with LD. There were no correlations between the BG and MSA performance. We did not find significant differences between the groups on the Bender-Gestalt scores, previous experience with video game, arousal, and mood. Our results suggest that a novel 3D virtual reality tool such as the MSA can potentially discriminate motor function of people with and without LD; however, the difference between both may also be due to a lack of problem-solving ability in LD. © 2014 Springer-Verlag London.",Bender-Gestalt test; Haptics; Learning disability; Virtual reality; Visual-motor skills,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,Emotional reactions to the 3D virtual body and future willingness: the effects of self-esteem and social physique anxiety,VRS - Virtual Reality,B,"This study examined how the participant’s self-esteem and social physique anxiety affected the emotional reactions to viewing their own virtual body and willingness to participate in the virtual experience in the future. Three-dimensional body scanning technology was used as a virtual reality tool. Ninety-three (51 males and 42 females) subjects participated in the experiment, who were 18+ years old, both genders, and had no history of musculoskeletal or mental problems. The experiment consisted of the three phases, including the pre-scanning survey, 3D body scanning, and post-scanning evaluation. The results verified causal relationships that led to certain types of emotions after viewing the 3D virtual body and the willingness to participate in a future session, within the domains of self-esteem and social physique anxiety. Specifically, self-confidence (positive dimension of self-esteem) was strongly associated with positive emotions. The “other-oriented” perspective of social physique anxiety exhibited positive correlations with negative emotions. The participants who showed positive emotions indicated a strong willingness to participate in another session of 3D body scanning in the future, but those with negative emotions also showed their positive willingness to participate in the future session. It signified that regardless of their emotional responses (positive or negative) to viewing their 3D virtual body, the participants were willing to experience their 3D virtual body in the future. The findings suggested that this virtual reality approach could be used as a potentially effective, clinical tool for patients with body image-related disorders. Study limitations and future research were also discussed. © 2017, Springer-Verlag London.",3D body scanning; Body image; Emotions; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2014,Computer-based virtual reality simulator for phacoemulsification cataract surgery training,VRS - Virtual Reality,B,"Recent research in virtual reality indicates that computer-based simulators are an effective technology to use for surgeons learning to improve their surgical skills in a controlled environment. This article presents the development of a virtual reality simulator for phacoemulsification cataract surgery training, which is the most common surgical technique currently being used to remove cataracts from the patient’s eyes. The procedure requires emulsifying the cloudy natural lens of the eye and restoring vision by implanting an artificial lens through a small incision. The four main procedures of cataract surgery, namely corneal incision, capsulorhexis, phacoemulsification, and intraocular lens implantation, are incorporated in the simulator for virtual surgical training by implementing several surgical techniques. The surgical activity that are applied on the anatomy of the human eye, such as incision, grasping, tearing, emulsification, rotation, and implantation, are simulated in the system by using different types of mesh modifications. A virtual reality surgical simulator is developed, and the main procedures of phacoemulsification cataract surgery are successfully simulated in the system. The simulation results of the training system show that the developed simulator is capable of generating a virtual surgical environment with faithful force feedback for medical residents and trainees to conduct their training lessons via the computer using a pair of force-feedback haptic devices. In addition, the successful simulation of the mesh modifications on the human eyeball with visual realism and faithful force feedback throughout the surgical operation shows that the developed simulator is able to serve as a virtual surgical platform for surgeons to train their surgical skills. © 2014, Springer-Verlag London.",Haptic device; Medical simulator; Phacoemulsification cataract surgery; Surgical training; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,A brush device with visual and haptic feedback for virtual painting of 3D virtual objects,VRS - Virtual Reality,B,"We have previously developed a mixed reality (MR) painting system with which a user could take a physical object in the real world and apply virtual paint to it. However, this system could not provide the sensation of painting on virtual objects in MR space. Therefore, we subsequently proposed and developed mechanisms that simulated the effect of touch and movement when a brush device was used to paint on a virtual canvas. In this paper, we use visual and haptic feedback to provide the sensation of painting on virtual three-dimensional objects using a new brush device called the MAI Painting Brush++. We evaluate and confirm its effectiveness through several user studies. © 2017, Springer-Verlag London.",Augmented reality; Input device; Mixed reality; Paintbrush; Painting system; Visual and haptic feedback,Abstract_Keywords,True,
Scopus,journalPaper,2018,Semantic framework for interactive animation generation and its application in virtual shadow play performance,VRS - Virtual Reality,B,"Designing and creating complex and interactive animation is still a challenge in the field of virtual reality, which has to handle various aspects of functional requirements (e.g. graphics, physics, AI, multimodal inputs and outputs, and massive data assets management). In this paper, a semantic framework is proposed to model the construction of interactive animation and promote animation assets reuse in a systematic and standardized way. As its ontological implementation, two domain-specific ontologies for the hand-gesture-based interaction and animation data repository have been developed in the context of Chinese traditional shadow play art. Finally, prototype of interactive Chinese shadow play performance system using deep motion sensor device is presented as the usage example. © 2018, The Author(s).",Animation data management; Animation generation; Chinese shadow play; Hand-gesture-based interaction; Ontology; Semantic framework; Virtual interactive,Abstract,True,
Scopus,journalPaper,2014,Natural and hybrid bimanual interaction for virtual assembly tasks,VRS - Virtual Reality,B,"This paper focuses on the simulation of bimanual assembly/disassembly operations for training or product design applications. Most assembly applications have been limited to simulate only unimanual tasks or bimanual tasks with one hand. However, recent research has introduced the use of two haptic devices for bimanual assembly. We propose a more natural and low-cost bimanual interaction than existing ones based on Markerless motion capture (Mocap) systems. Specifically, this paper presents two interactions based on a Markerless Mocap technology and one interaction based on combining Markerless Mocap technology with haptic technology. A set of experiments following a within-subjects design have been implemented to test the usability of the proposed interfaces. The Markerless Mocap-based interactions were validated with respect to two-haptic-based interactions, as the latter has been successfully integrated into bimanual assembly simulators. The pure Markerless Mocap interaction proved to be either the most or least efficient depending on the configuration (with 2D or 3D tracking, respectively). Usability results among the proposed interactions and the two-haptic based interaction showed no significant differences. These results suggest that Markerless Mocap or hybrid interactions are valid solutions for simulating bimanual assembly tasks when the precision of the motion is not critical. The decision on which technology to use should depend on the trade-off between the precision requested to simulate the task, the cost, and inner features of the technology. © 2013 Springer-Verlag London.",Assembly training; Bimanual assembly simulation; Haptics; Human-computer interaction; Markerless Mocap; Virtual reality,Keywords,True,
Scopus,journalPaper,2018,A game prototype for understanding the safety issues of a lifeboat launch,VRS - Virtual Reality,B,"Novel, advanced game techniques provide us with new possibilities to mimic a complicated training process, with the added benefit of enhanced safety. In this paper, we design and implement a 3D game with the support of virtual reality equipment which imitates the process of a lifeboat launch, involving both tractor manoeuvres and boat operations. It is a complex but vital process which can save lives at sea but also has many potential hazards. The primary objective of the game is to allow novices to better understand the sequence of the operations and manage the potential risks which may occur during the launch process. Additionally, the game has been promoted to the general public for educational purposes and to raise awareness of the safety issues involved. The key modules of the game are designed based on physical simulations to give the players enhanced plausible cognition and enjoyable interaction. We conducted two case studies for the two purposes of the games: one for training with volunteers without launching experience and the other for public awareness of the potential hazards with young children. The game is proven to be very promising for future professional training, and it serves the educational purpose of awareness of the safety issues for general public while being entertaining. © 2018, The Author(s).",Game-based training; General public; Lifeboat launch; Serious games; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2016,Evolution of music performance anxiety and quality of performance during virtual reality exposure training,VRS - Virtual Reality,B,"Virtual reality exposure is increasingly used as a method of treatment for anxiety disorders. This exploratory study examines a virtual reality exposure training (VRET) conceived for the treatment of music performance anxiety (MPA). The aim is to obtain first-level knowledge in the music field concerning VRET. This article analyzes how MPA, concentration and quality of performance evolve during VRET. Nine music students participated in six 1-h sessions of VRET spread out over 3 weeks. They were exposed to four different virtual environments representing typical audiences for musicians. The findings indicate a significant decrease in MPA between sessions. They also indicate a significant increase in performance quality within sessions and a positive correlation between absorption ability and level of anxiety at the beginning of the VRET. Further studies must be conducted to evaluate the generalizability potential of these results to real performance situations. © 2016, Springer-Verlag London.",Exposure; Immersion ability; Music; Performance anxiety; Treatment; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2015,Virtual reality 360 interactive panorama reproduction obstacles and issues,VRS - Virtual Reality,B,"The research studies obstacles and issues for spherical panorama image reproduction. Virtual reality 360 interactive panorama presentation involves accurately reproduced spherical panorama images which can deliver pre-produced image information of the real-world location that allows user-controlled interactivity in virtual reality digital platforms with up to three hundred and sixty degrees of visibility. Spherical panorama image is also useful in various mixed and augmented reality applications. However, the photographic reproduction of spherical panorama image may tolerate various obstacles and issues that can cause visual abnormality. These can include parallax error, nadir angle difficulty, inconsistent white balance, insufficient dynamic range in multiple angle images, ghosting effect when working with high dynamic range imaging, high amount of multiple angle source images to manage correctly and overall lengthy acquisition time. Biased reproduction of spherical panorama would be inadequate to record and report authentic visual information. This case study investigation provides an overview of the occurrence of potential obstacles and issues with the intention of acquiring high-fidelity spherical panorama photographic reproduction. © 2014, Springer-Verlag London.",Augmented reality; High dynamic range imaging; Image reproduction; Spherical panorama; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,A comparative evaluation of viewing metaphors on psychophysical skills education in an interactive virtual environment,VRS - Virtual Reality,B,"In an empirical evaluation, we examined the effect of viewing condition on psychophysical skills education in an interactive 3D simulation to train users in electrical circuitry. We compared an immersive head-mounted display (HMD)-based viewing metaphor versus a limited, desktop-based virtual reality (DVR) viewing metaphor with interaction using a spatial user interface. Psychophysical skills education involves the association of cognitive functions with motor functions to make the task autonomous with repeated practice. In electrical circuitry, this is demonstrated by the fine movements involved in handling and manipulating components on the electrical circuit, particularly while measuring electrical parameters. We created an interactive circuitry simulation (IBAS) where participants could learn about electrical measurement instruments such as the ammeter, voltmeter and multimeter, in a simulated breadboard VR system. Twenty-four participants utilized the simulation (12 in each condition), and the quantitative and qualitative aspects of psychophysical skills education with respect to the viewing metaphor were examined. Each viewing condition in IBAS was head-tracked and non-stereoscopic. Perspective correction was coupled with head-tracking in the DVR condition. The key quantitative measures were cognitive questionnaires addressing different levels of Bloom’s cognitive taxonomy and a real-world psychophysical task addressing various levels of Dave’s psychomotor taxonomy. The qualitative measures were the Witmer–Singer sense of presence questionnaire and self-report. Results suggest that there was a significant increase in cognition post-experiment in both DVR and HMD viewing conditions in levels of knowledge, application, analysis and evaluation. Results also revealed a significant learning benefit with respect to the higher level concepts pertaining to evaluation in the HMD condition as compared to DVR. Participants seem to have enjoyed a greater level of affordance in task performance and spent a larger amount of time to complete the simulated exercises as well as manually maneuvered to further distances in the HMD viewing condition as compared to DVR viewing. © 2016, Springer-Verlag London.",3D human–computer interaction; Education; HMD; Human factors,Abstract,True,
Scopus,journalPaper,2014,A framework to design 3D interaction assistance in constraints-based virtual environments,VRS - Virtual Reality,B,"The equilibrium of complex systems often depends on a set of constraints. Thus, credible virtual reality modeling of these systems must respect these constraints, in particular for 3D interactions. In this paper, we propose a generic framework for designing assistance to 3D user interaction in constraints-based virtual environment that associates constraints, interaction tasks and assistance tools, such as virtual fixtures (VFs). This framework is applied to design assistance tools for molecular biology analysis. Evaluation shows that VF designed using our framework improve effectiveness of the manipulation task. © 2014 Springer-Verlag London.",3D interaction; Assistance model; Complex environments; Constraints; Framework; Virtual fixtures; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2018,Increasing student engagement through virtual interactions: How?,VRS - Virtual Reality,B,"Our ongoing research is focusing on identifying and taxonomising the elements and the factors that affect learner engagement with virtual worlds when hybrid virtual learning models are used. Our main hypothesis links learner engagement with interactions, both in the virtual world and in the physical classroom. In order to examine this subject, there is an elaboration on and consideration of aspects such as the learners’ prior experiences in the use of virtual worlds, their preconceptions about using them as a learning tool and the impact that the instructional designers’ choices have on enhancing the opportunities for interactions. In this paper, we examine the impact that the orientation process has on university students who study computer science and have almost no experience in the use of virtual worlds. Our findings suggest that the orientation process contributed positively to students’ smooth induction and that resulted in having meaningful and engaging interactions. Furthermore, students’ simultaneous coexistence in both environments eliminated the drawbacks of each educational approach and broadened the network of interactions. © 2018, The Author(s).",Architectures for educational technology systems; Collaborative learning; Improving classroom teaching; Interactive learning environments; Virtual reality,Keywords,True,
Scopus,journalPaper,2017,When brands come to life: experimental research on the vividness effect of Virtual Reality in transformational marketing communications,VRS - Virtual Reality,B,"Mobile Virtual Reality provides a gateway for marketers to innovatively reach consumers. This study examines the impact of Virtual Reality in the context of transformational brand experience appeals, focussing specifically on the determining role of vividness. A three-dimensional conceptual framework is presented, offering a systematic review of the literature on vividness effects in marketing communications, revealing the major gap that most available studies only focus on informational messages. We conducted an experiment to address this gap and demonstrate in the context of a transformational ad that Virtual Reality generates higher perceptions of vividness and presence than a regular two-dimensional video, with vividness positively affecting attitude toward the ad, both directly and indirectly via presence. Our study also reveals that vividness in turn elicits a positive effect on brand attitudes which stimulates consumers’ purchase intentions. As such, the strategic potential of Virtual Reality for marketing communications is highlighted. © 2017, Springer-Verlag London.",Brand attitudes; Marketing communications; Presence; Virtual Reality; Vividness,Title_Abstract_Keywords,True,
Scopus,journalPaper,2014,New wireless connection between user and VE using speech processing,VRS - Virtual Reality,B,"This paper presents a novel speak-to-VR virtual-reality peripheral network (VRPN) server based on speech processing. The server uses a microphone array as a speech source and streams the results of the process through a Wi-Fi network. The proposed VRPN server provides a handy, portable and wireless human machine interface that can facilitate interaction in a variety interfaces and application domains including HMD- and CAVE-based virtual reality systems, flight and driving simulators and many others. The VRPN server is based on a speech processing software development kits and VRPN library in C++. Speak-to-VR VRPN works well even in the presence of background noise or the voices of other users in the vicinity. The speech processing algorithm is not sensitive to the user’s accent because it is trained while it is operating. Speech recognition parameters are trained by hidden Markov model in real time. The advantages and disadvantages of the speak-to-VR server are studied under different configurations. Then, the efficiency and the precision of the speak-to-VR server for a real application are validated via a formal user study with ten participants. Two experimental test setups are implemented on a CAVE system by using either Kinect Xbox or array microphone as input device. Each participant is asked to navigate in a virtual environment and manipulate an object. The experimental data analysis shows promising results and motivates additional research opportunities. © 2014, Springer-Verlag London.",Speak-to-VR; Speech processing; VRPN server; Wi-Fi network,Abstract,True,
Scopus,journalPaper,2014,Measuring virtual experience in a three-dimensional virtual reality interactive simulator environment: A structural equation modeling approach,VRS - Virtual Reality,B,"With the rapid development of the VR market, virtual experience has increasingly been the object of study in recent years. A growing number of studies have reported the positive effect that virtual experience can have on a user's mood and loyalty. However, few studies have investigated the influence of the mechanism of virtual experience on users' mood and loyalty. To compensate for this research gap, this study aims to evaluate consumers' virtual experience by examining the flow state in a virtual environment. A total of 368 valid questionnaires were collected, and a structural equation modeling approach was employed in the data analysis. The study reveals that forming flow involves many factors: the intrinsic characteristics of the mediated environment, the consumer's assumptions and perceptions prior to entering the flow state, the stage at which the customer enters the flow state, and the consequences of the flow experience. © 2014 Springer-Verlag London.",Flow; Interactivity; Telepresence; Virtual experience; Vividness,Title,True,
Scopus,journalPaper,2015,Posttraumatic stress disorder: possibilities for olfaction and virtual reality exposure therapy,VRS - Virtual Reality,B,"Visual and auditory information has dominated the field of virtual reality (VR). Evaluation of the role of sensory stimulation in VR has highlighted olfactory stimulation as a potentially powerful yet underutilized therapeutic tool. Early studies of immersive environments, which were run as experiments, incorporated smell in the virtual experience; however, olfaction in virtual environment design and development has arguably failed to maintain a position commensurate with its sensory capacity, exemplified by the paucity of research and possible application. A review of the literature suggests that olfaction as a component of virtual environment exposure therapy may be a useful addition in the treatment of posttraumatic stress disorder (PTSD) a mental health condition triggered by a terrifying event, either experiencing or witnessing it. Symptoms may include flashbacks, nightmares and anxiety, as well as uncontrollable thoughts about the event. However, to investigate the role of olfaction further research is required in the formulation, display, staging and customization of scent, coupled with an in-depth analysis of the role of olfaction in cognitive function, memory, emotion and creation of presence, particularly in the context of VR treatment of PTSD. Benefits of olfactory therapy may, however, be compromised by the fact that olfactory identification deficit has been noted as a component of PTSD. Investigation is required into causative or reactive mechanisms that may underlie olfactory deficits and into suitable VR therapeutic protocols that could be designed to address these deficits. Additionally, ongoing VR technological developments may deliver increasing affordability and portability in terms of VR treatment options, particularly regarding head-mounted display units. A cyberpsychological consideration of the problem of PTSD, that is, an inter-disciplinary approach combining technology and psychology learning’s may merit consideration. A review of findings suggests that research protocols focused on olfaction as a variable in a multi-sensory VR exposure therapeutic program may positively impact on treatment outcomes in PTSD population. © 2015, The Author(s).",Memory; Odor; Olfaction; Posttraumatic stress disorder; Virtual reality exposure therapy,Title_Abstract_Keywords,True,
Scopus,journalPaper,2015,An experimental study of spatial sound usefulness in searching and navigating through AR environments,VRS - Virtual Reality,B,"This paper presents an experimental study of spatial sound usefulness in searching and navigating through augmented reality environments. Participants were asked to find three objects hidden within no-sound and spatial sound AR environments. The experiment showed that the participants of the spatialized sound group performed faster and more efficiently than working in no-sound configuration. What is more, 3D sound was a valuable cue for navigation in AR environment. The collected data suggest that the use of spatial sound in AR environments can be a significant factor in searching and navigating for hidden objects within indoor AR scenes. To conduct the experiment, the CARE approach was applied, while its CARL language was extended with new elements responsible for controlling audio in 3D space. © 2015, The Author(s).",3D sound in AR; AR; Audio augmented reality; Augmented reality; CARE; CARL; HCI; Spatial sound,Abstract_Keywords,True,
Scopus,journalPaper,2016,Activities of daily living assessment in spinal cord injury using the virtual reality system Toyra®: functional and kinematic correlations,VRS - Virtual Reality,B,"The main objective of this study was to analyze the correlations between functional scales and kinematic data collected during the execution of upper limb (UL) basic activities of daily living in an immersive virtual reality (VR) environment. Fifteen people with tetraplegia participated in the study. Moreover, we also want to confirm if changes in UL functional performance detected by functional scales are also detected by the VR system Toyra®. Patients were assessed before and after 4 weeks of daily conventional rehabilitation treatment complemented with a training with the VR system. Significant positive correlations between kinematic and functional parameters were found in the post assessment, verifying that changes in UL functional performance detected by functional scales are also measured by the VR system Toyra®, concretely the related to shoulder movements. Additionally, a predefined Agility metric has been applied, showing inversely proportional results to the level of injury, as we expected. The self-care category of the Spinal Cord Independence Measure (SCIM III) and the ranges of motion (ROM) captured with the VR system were analyzed, showing statistical significance changes between pre-post evaluations, supporting the hypothesis that kinematic analysis complements clinical and functional assessments of patients with tetraplegia. © 2015, Springer-Verlag London.",Activities of daily living; Assessment; Correlations; Kinematic; Tetraplegia; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Hand posture and gesture recognition techniques for virtual reality applications: a survey,VRS - Virtual Reality,B,"Motion recognition is a topic in software engineering and dialect innovation with a goal of interpreting human signals through mathematical algorithm. Hand gesture is a strategy for nonverbal communication for individuals as it expresses more liberally than body parts. Hand gesture acknowledgment has more prominent significance in planning a proficient human computer interaction framework, utilizing signals as a characteristic interface favorable to circumstance of movements. Regardless, the distinguishing proof and acknowledgment of posture, gait, proxemics and human behaviors is furthermore the subject of motion to appreciate human nonverbal communication, thus building a richer bridge between machines and humans than primitive text user interfaces or even graphical user interfaces, which still limits the majority of input to electronics gadget. In this paper, a study on various motion recognition methodologies is given specific accentuation on available motions. A survey on hand posture and gesture is clarified with a detailed comparative analysis of hidden Markov model approach with other classifier techniques. Difficulties and future investigation bearing are also examined. © 2016, Springer-Verlag London.",Gesture; Graphical user interface (GUI); HMM; Human computer interaction (HCI); Posture,Title,True,
Scopus,journalPaper,2014,Toward a validation of cyber-interventions for stress disorders based on stress inoculation training: A systematic review,VRS - Virtual Reality,B,"New advanced technologies have recently emerged as a potentially effective way for delivering stress management techniques. Specifically, the stress inoculation training (SIT) represents a validated approach to manage stress in several settings, and research is growing related to this clinical protocol combined with advanced technologies. This review aims to outline the state of the art of cyber-interventions based on SIT methodology (cyber-SIT). In the current review, we deeply analyzed and discussed three aspects of the selected studies: (1) the type of technological devices used for delivering cyber-SIT; (2) the sampling strategies; (3) and the stress-related measures for assessing the effectiveness of cyber-SIT. The results of this systematic review suggest the potential efficacy of cyber-SIT for managing psychological stress in several settings. Considering cyber-SIT for psychological stress, controlled trials testing a greater number of participants are needed. Other future challenges include adopting better inclusion/exclusion criteria, standardized outcome measures, and different conditions for comparing the effect and/or the integration of different technological devices. In conclusion, as the cyber-SIT may play an important role in the future clinical psychology, it is crucial to enhance the validation of this approach from a methodological point of view. © 2013 Springer-Verlag London.",Cyber-interventions; Stress inoculation training; Stress management; Systematic review; Validation; Virtual reality,Keywords,True,
Scopus,journalPaper,2018,Simulator sickness in patients with neck pain and vestibular pathology during virtual reality tasks,VRS - Virtual Reality,B,"Immersion in virtual environments can cause simulator sickness (SS). Further, head and neck movement in interactive virtual reality (VR) assessment and training stimulates the vestibular and cervical afferent systems that can cause dizziness in subjects with neck pain and vestibular pathology. This cross-sectional, observational, study investigated SS and factors that may influence this between 20 neck pain, 14 vestibular pathology and 20 asymptomatic control subjects. Pre-VR questionnaires included a visual symptom scale and dizziness intensity. SS measures included the simulator sickness visual analogue scale and the simulator sickness questionnaire. Significantly greater incidence of any SS and higher values were found in the vestibular and neck pain groups compared to the control group in selected SS measures. No significant differences were found when comparing SS measures between the vestibular and neck pain groups. Significant mild-to-moderate correlations for the entire population were found between both SS measures to pre-VR visual symptoms and dizziness intensity. SS levels in neck pain and vestibular populations are comparable and higher than asymptomatic individuals. Dizziness and visual disturbances may be associated with an increase in severity of SS in these clinical populations. © 2017, Springer-Verlag London Ltd.",Dizziness; Simulator sickness; Vestibular; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2014,Evaluation of direct manipulation using finger tracking for complex tasks in an immersive cube,VRS - Virtual Reality,B,"A solution for interaction using finger tracking in a cubic immersive virtual reality system (or immersive cube) is presented. Rather than using a traditional wand device, users can manipulate objects with fingers of both hands in a close-to-natural manner for moderately complex, general purpose tasks. Our solution couples finger tracking with a real-time physics engine, combined with a heuristic approach for hand manipulation, which is robust to tracker noise and simulation instabilities. A first study has been performed to evaluate our interface, with tasks involving complex manipulations, such as balancing objects while walking in the cube. The user's finger-tracked manipulation was compared to manipulation with a 6 degree-of-freedom wand (or flystick), as well as with carrying out the same task in the real world. Users were also asked to perform a free task, allowing us to observe their perceived level of presence in the scene. Our results show that our approach provides a feasible interface for immersive cube environments and is perceived by users as being closer to the real experience compared to the wand. However, the wand outperforms direct manipulation in terms of speed and precision. We conclude with a discussion of the results and implications for further research. © 2014 Springer-Verlag London.",Direct manipulation; Finger tracking; Immersive cube; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2015,Sounding fire for immersive virtual reality,VRS - Virtual Reality,B,"Both visual information and sound are required in immersive virtual reality. This paper proposes a computational method for fast synthesis of plausible fire sound that is synchronized with physically based fire animations. We divide fire sound into two parts: low frequency and mid- to high frequency, and use two processes to separately synthesize these two parts. By simplifying calculations using a novel combustion sound model as well as leveraging GPU parallel computing in a marching-cube-like manner, our method speeds up the computation of low-frequency part by an order of magnitude. To run the time-stepping fire simulation at a relative low frequency rather than the audio rate, we add synchronized mid- and high-frequency wavelet details to low-frequency simulation contents with a post-process to generate complete fire sound. We validated our method with various experiments to build a solid physically based basis for real-time acoustic rendering that can be used for immersive virtual reality scenarios. © 2015, Springer-Verlag London.",Fire sound; GPU; Immersive virtual reality; Physically based simulation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2015,Designing presence for real locomotion in immersive virtual environments: an affordance-based experiential approach,VRS - Virtual Reality,B,"This paper describes a framework for designing systems for real locomotion in virtual environments (VEs) in order to achieve an intense sense of presence. The main outcome of the present research is a list of design features that the virtual reality technology should have in order to achieve such a goal. To identify these features, an approach based on the combination of two design strategies was followed. The first was based on the theory of affordances and was utilized to design a generic VE in which the affordances of the corresponding real environment could be evoked. The second was the experiential design applied to VEs and was utilized to create an experience of locomotion corresponding to that achievable in a real environment. These design strategies were chosen because of their potential to enhance the sense of presence. The proposed list of features can be utilized as an instrument that allows VE designers to evaluate the maturity of their systems and to pinpoint directions for future developments. A survey analysis was performed using the proposed framework, which involved three case studies to determine how many features of the proposed framework were present and their status. The result of such analysis represented a measure of the completeness of the systems design, of the affordances provided to the user, and a prediction of the sense of presence. © 2015, Springer-Verlag London.",Affordance; Experiential design; Locomotion interfaces; Presence; Virtual environments,Abstract,True,
Scopus,journalPaper,2018,Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality,VRS - Virtual Reality,B,"Augmented reality (AR) “augments” virtual information over the real-world medium and is emerging as an important type of an information visualization technique. As such, the visibility and readability of the augmented information must be as high as possible amidst the dynamically changing real-world surrounding and background. In this work, we present a technique based on image saliency analysis to improve the conspicuity of the foreground augmentation to the background real-world medium by adjusting the local brightness contrast. The proposed technique is implemented on a mobile platform considering the usage nature of AR. The saliency computation is carried out for the augmented object’s representative color rather than all the pixels, and searching and adjusting over only a discrete number of brightness levels to produce the highest contrast saliency, thereby making real-time computation possible. While the resulting imagery may not be optimal due to such a simplification, our tests showed that the visibility was still significantly improved without much difference to the “optimal” ground truth in terms of correctly perceiving and recognizing the augmented information. In addition, we also present another experiment that explores in what fashion the proposed algorithm can be applied in actual AR applications. The results suggested that the users clearly preferred the automatic contrast modulation upon large movements in the scenery. © 2017, Springer-Verlag London Ltd.",Augmented reality; Contrast; Human perception and performance; Saliency; See-through display,Title_Abstract_Keywords,True,Duplicate
Scopus,journalPaper,2015,Projection-based visualization of tangential deformation of nonrigid surface by deformation estimation using infrared texture,VRS - Virtual Reality,B,"In this paper, we propose a projection-based mixed reality system that visualizes the tangential deformation of a nonrigid surface by superimposing graphics directly onto the surface by projected imagery. The superimposed graphics are deformed according to the surface deformation. To achieve this goal, we develop a computer vision technique that estimates the tangential deformation by measuring the frame-by-frame movement of an infrared (IR) texture on the surface. IR ink, which can be captured by an IR camera under IR light, but is invisible to the human eye, is used to provide the surface texture. Consequently, the texture does not degrade the image quality of the augmented graphics. The proposed technique measures individually the surface motion between two successive frames. Therefore, it does not suffer from occlusions caused by interactions and allows touching, pushing, pulling, and pinching, etc. The moving least squares technique interpolates the measured result to estimate denser surface deformation. The proposed method relies only on the apparent motion measurement; thus, it is not limited to a specific deformation characteristic, but is flexible for multiple deformable materials, such as viscoelastic and elastic materials. Experiments confirm that, with the proposed method, we can visualize the surface deformation of various materials by projected illumination, even when the user’s hand occludes the surface from the camera. © 2014, Springer-Verlag London.",Deformable surface; Projection-based mixed reality; User interaction,Abstract_Keywords,True,
Scopus,journalPaper,2017,A virtual reality keyboard with realistic haptic feedback in a fully immersive virtual environment,VRS - Virtual Reality,B,"This study presents a 3D virtual reality (VR) keyboard system with realistic haptic feedback. The system uses two five-fingered data gloves to track finger positions and postures, uses micro-speakers to create simulated vibrations, and uses a head-mounted display (HMD) for 3D display. When users press a virtual key in the VR environment, the system can provide realistic simulated key click haptic feedback to users. The results of this study show that the advantages of the haptic VR keyboard are that users can use it when wearing HMDs (users do not need to remove HMDs to use the VR keyboard), the haptic VR keyboard can pop-up display at any location in the VR environments (users do not need to go to a specific location to use an actual physical keyboard), and the haptic VR keyboard can be used to provide realistic key click haptic feedback (which other studies have shown enhances user performance). The results also show that the haptic VR keyboard system can be used to create complex vibrations that simulate measured vibrations from a real keyboard and enhance keyboard interaction in a fully immersive VR environment. © 2016, Springer-Verlag London.",Realistic haptic feedback; Simulated vibrations; Virtual reality haptic keyboard,Title_Abstract_Keywords,True,
Scopus,journalPaper,2014,Real-time infinite horizon tracking with data fusion for augmented reality in a maritime operations context,VRS - Virtual Reality,B,"In this paper, we propose a method for real-time horizon tracking (i.e., separation line between the sky and the sea) in a maritime operations context. We present the fusion of an image processing algorithm with the data obtained from the inertial measurement unit (IMU). The initial aim is to filter out environmental conditions using inertial information in order to combine a video stream with onboard electronic charts. This is achieved by the detection of the horizon with an image processing algorithm in an area defined by the IMU. We then present an evaluation of the algorithm with regard to the rate of detection of the horizon and the impact of the image resolution on the computational time. The purpose of developing this method is to create an augmented reality maritime operations application. We combine the video stream with electronic charts in a single display. We use the position of the horizon in the image to split the display into different areas. Then, we use transparency to display the video, the electronic charts or both. © 2013 Springer-Verlag London.",Augmented reality; Data fusion; Electronic chart system; Geographical information system; Image processing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,Remote collaboration in virtual reality: asymmetrical effects of task distribution on spatial processing and mental workload,VRS - Virtual Reality,B,"In the context of a remote collaboration task in virtual reality, this study aimed to analyze the effects of task distribution on the processing of spatial information and mental workload in spatial dialogs. Pairs of distant participants with specific roles (a guide and a manipulator) had to collaboratively move a virtual object in a plane factory mock-up. The displays allowed the participants to be immersed together in the virtual environment. We analyzed the dialogs that took place according to the frames of reference and the mental transformations required to produce the spatial statements. We also measured the associated mental workload. Results showed that when participants took a perspective, the manipulator’s point of view was preferred. Perspective-taking only yielded a moderate increase in mental rotations, which may explain a specifically high mental demand score for the guides’ NASA-TLX. Overall, this is in accordance with the least collaborative effort principle. This study reinforces the idea that, in collaboration, operators do not need the same aids as each other. Thus, it is not necessary to develop symmetrical tools, i.e., the same tools for all co-workers; instead, the needs of each operator should be taken into account, according to the task he has to perform. In our case, the guides would be helped with perspective-taking aids, while the manipulators would be helped with action-oriented tools. © 2016, Springer-Verlag London.",Mental workload; Remote collaboration; Spatial cognition; Spatial common frame of reference; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2015,AR image generation using view-dependent geometry modification and texture mapping,VRS - Virtual Reality,B,"Augmented reality (AR) applications often require virtualized real objects, i.e., virtual objects that are built based on real objects and rendered from an arbitrary viewpoint. In this paper, we propose a method for real object virtualization and AR image generation based on view-dependent geometry modification and texture mapping. The proposed method is a hybrid of model- and image-based rendering techniques that uses multiple input images of the real object as well as the object’s three-dimensional (3D) model obtained by an automatic 3D reconstruction technique. Even with state-of-the-art technology, the reconstructed 3D model’s accuracy can be insufficient, resulting in such visual artifacts as false object boundaries. The proposed method generates a depth map from a 3D model of a virtualized real object and expands its region in the depth map to remove the false object boundaries. Since such expansion reveals the background pixels in the input images, which is particularly undesirable for AR applications, we preliminarily extract object regions and use them for texture mapping. With our GPU implementation for real-time AR image generation, we experimentally demonstrated that using expanded geometry reduces the number of required input images and maintains visual quality. © 2015, Springer-Verlag London.",Augmented reality; Free-viewpoint image generation; View-dependent geometry modification; View-dependent texture mapping,Abstract_Keywords,True,
Scopus,journalPaper,2015,Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks,VRS - Virtual Reality,B,"This paper studies a new sensorial approach to improving communication between partners during collaborative tasks taking place in abstract and non-visual virtual reality environments. The sensorial approach was investigated in the context of the search and identification of targets in a simplified 2D environment. It consists in finding a spatial configuration corresponding to a defined criterion such as a maximum, minimum, or defined score. During the collaborative search, users need to be aware of their results and also the results of their partners. In addition, they need to compare examined scores (e.g., docking score or physical value) with other results to make decisions. To support these features, an audio–haptic display was developed employing binaural audio with an intermodal stimuli synthesis design to improve the collaborative search. This rendering tool allows for simultaneous use of the audio and haptic channels which enables an efficient individual search and comparison of results. In addition, it improves the communication and activity coordination between the partners. An experiment was carried out to evaluate the contribution of the tool to improve the collaborative search of targets in a 2D non-visual environment. The results clearly show a significant improvement in performance and working efficiency with the audio–haptic display as compared to a traditional haptic-only condition. Moreover, we observed a reduction in the need for verbal communication during some steps of the search process. However, this approach introduces some communication conflicts during the steps presenting high-level interactions between partners which reduce the working efficiency of some groups. © 2015, Springer-Verlag London.",Audio–haptic interaction; Collaborative virtual environment; Search of targets; Sonification,Abstract,True,
Scopus,journalPaper,2016,Effectiveness of a multidevice 3D virtual environment application to train car service maintenance procedures,VRS - Virtual Reality,B,"This paper reports a study which demonstrates the advantages of using virtual-reality-based systems for training automotive assembly tasks. Sixty participants were randomly assigned to one of the following three training experiences to learn a car service procedure: (1) observational training through video instruction; (2) an experiential virtual training and trial in a CAVE; and (3) an experiential virtual training and trial through a portable 3D interactive table. Results show that virtual trained participants, after the training, can remember significantly better (p < .05) the correct execution of the steps compared to video-trained trainees. No significant differences were identified between the experiential groups neither in terms of post-training performances nor in terms of proficiency, despite differences in the interaction devices. The relevance of the outcomes for the automotive fields and for the designers of virtual training applications are discussed in light of the outcomes, particularly that virtual training experienced through a portable device such as the interactive table can be effective, as can training performed in a CAVE. This suggests the possibility for automotive industries to invest in advanced portable hardware to deliver effectively long-distance programs of training for car service operators placed all over the world. © 2016, Springer-Verlag London.",Automotive; Effectiveness of training; Turst; Usability; Virtual reality,Keywords,True,
Scopus,journalPaper,2018,Self-reported discomfort when using commercially targeted virtual reality equipment in discomfort distraction,VRS - Virtual Reality,B,"Commercially targeted virtual reality (VR) equipment is gaining popularity and might be a viable tool for pain distraction. This experimental research aimed to discover whether active distraction techniques (such as commercially targeted VR and video games) result in reduced subjective discomfort relative to passive distraction techniques. The study examined a healthy adult population who experienced an experimentally induced discomfort task. Participants were 27 adults, 14 females and 13 males. Participants completed four tasks, a baseline measure of physical discomfort, video clip distraction (passive distraction), video game distraction (active distraction) and exploring a VR world using an Oculus Rift head-mounted display (active distraction). In all four test conditions, participants were asked to sit on a chair holding their non-dominant leg at a height of approximately 30 cm from the floor, up to a maximum of 5 min. Counterbalancing of task order was conducted to reduce effects of participant fatigue. The participants indicated significantly reduced self-reported discomfort in the active distraction tasks when compared to the passive distraction tasks. While the findings demonstrate the effectiveness of a commercially targeted VR technology in increasing pain tolerance, the relative benefits of this technology over non-immersive video games are not apparent. © 2017, Springer-Verlag London Ltd., part of Springer Nature.",Active distraction; Gaming; Pain distraction; Passive distraction; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,Control mapping in virtual reality: effects on spatial presence and controller naturalness,VRS - Virtual Reality,B,"This study explores how a video game player’s sense of being in a game world (i.e., spatial presence) is impacted by the use of a virtual reality head-mounted display (VR HMD). Research focused on VR (as realized with the use of HMDs) has fallen by the wayside since the early 1990s due to the limitations in the technology. With modern reimagining of VR HMDs, there is now an opportunity to reexamine the impact it has on gaming experience. This article explores the results of an experiment in which university students played video games using either a VR HMD or a standard monitor while playing a first-person shooter video game. Control interface was also manipulated between incomplete tangible mapped devices (Razer Hydra) and directionally mapped devices (mouse and keyboard). Results indicated that VR HMDs have a positive impact on a players’ level of spatial presence and feelings of controller naturalness. Controller naturalness also impacted spatial presence regardless of display condition. © 2017, Springer-Verlag London.",Controller naturalness; Natural mapping; Spatial presence; Video games; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,Augmented reality-based block piling game with superimposed collapse prediction,VRS - Virtual Reality,B,"Understanding what cannot be seen is difficult. Physical behavior can be explained on the basis of physical theories even if the behavior cannot be observed. Explanation of what is physically happening in the real world would become easy, however, if annotations were superimposed on the real objects. Herein, the authors demonstrate how an understanding of a physical event can be facilitated by overlapping a real-world situation with a simulation that predicts a future state. This idea is demonstrated in a game application in which a player stacks blocks into a pile until it collapses. In general, it is easy to estimate whether a block on the edge of a table will fall or not. However, it is more difficult to predict whether a stack of many blocks will collapse, and in what manner the stack will collapse. Even though previous research has demonstrated that the problem of how two-dimensionally stacked blocks collapse can be reduced to solving a sequence of convex quadratic programs, algorithms for convex quadratic programs require massive computational resources. Hence, the authors developed a fast and new algorithm based on a linear program. The proposed algorithm realizes real-time simulation based on physics that superimposes predicted collapse. The block that is predicted to fall is superimposed on the real block with a lit background projection. The system was evaluated in an experiment, and superimposed augmented reality annotation was observed to be efficient. The system was also demonstrated in game contests and received positive feedback and comments. © 2012 Springer-Verlag London.",Augmented reality; Block collapse; Linear program; Overhang problem; Physical simulation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,Fast and stable simulation of virtual water scenes with interactions,VRS - Virtual Reality,B,"Simulation of large-scale water interacting with objects is essential in virtual reality, with wide applications in games, movie special effects, etc. As it involves much physical computation, how to achieve fast rendering is still a challenge. This paper proposed a novel Graphics Processing Unit-based method for rapid simulation of water interacting with objects. The interactions between dynamic objects and the surrounding environment were realized with a specially designed simulation grid. Perfectly Matched Layers method was introduced to ensure the continued stability of the simulation grid's boundary fluctuations. To model the rigid body efficiently, a pre-rigid body method was proposed to achieve plausible visual results at higher rendering rates. Various experiment results showed the validity of our method. © 2013 Springer-Verlag London.",GPU; Interactions; Perfectly matched layers; Rigid body; Water,Abstract,True,
Scopus,journalPaper,2013,Structure and motion in urban environments using upright panoramas,VRS - Virtual Reality,B,"Image-based modeling of urban environments is a key component of enabling outdoor, vision-based augmented reality applications. The images used for modeling may come from off-line efforts, or online user contributions. Panoramas have been used extensively in mapping cities and can be captured quickly by an end-user with a mobile phone. In this paper, we describe and evaluate a reconstruction pipeline for upright panoramas taken in an urban environment. We first describe how panoramas can be aligned to a common vertical orientation using vertical vanishing point detection, which we show to be robust for a range of inputs. The orientation sensors in modern cameras can also be used to correct the vertical orientation. Secondly, we introduce a pose estimation algorithm, which uses knowledge of a common vertical orientation as a simplifying constraint. This procedure is shown to reduce pose estimation error in comparison with the state of the art. Finally, we evaluate our reconstruction pipeline with several real-world examples. © 2012 Springer-Verlag London Limited.",Panoramas; Structure and motion; Urban environments,Abstract,True,
Scopus,journalPaper,2014,Using immersive virtual reality and anatomically correct computer-generated characters in the forensic assessment of deviant sexual preferences,VRS - Virtual Reality,B,"Penile plethysmography (PPG) is the gold standard for the assessment of sexual interests, especially among sex offenders of children. Nonetheless, this method faces some ethical limitations inherent to the nature of its stimuli and could benefit from the improvement of its ecological validity. The use of computer-generated characters (CGC) in virtual immersion for PPG assessment might help address these issues. A new application developed to design made-to-measure anatomically correct virtual characters compatible with the Tanner developmental stages is presented. The main purpose of this study was to determine how the virtual reality (VR) modality compares to the standard auditory modality on their capacity to generate sexual arousal profiles and deviance differentials indicative of sexual interests. The erectile responses of 22 sex offenders of children and 42 non-deviant adult males were recorded. While both stimulus modalities generated significantly different genital arousal profiles for sex offenders of children and non-deviant males, deviance differentials calculated from the VR modality allowed for significantly higher classification accuracy. Performing receiver operating characteristic analyses further assessed discriminant potential. Auditory modality yielded an area under the curve (AUC) of 0.79 (SE = 0.059) while CGC in VR yielded an AUC of 0.90 (SE = 0.052). Overall, results suggest that the VR modality allows significantly better group classification accuracy and discriminant validity than audio stimuli, which provide empirical support for the use of this new method for PPG assessment. Additionally, the potential use of VR in interventions pertaining to self-regulation of sexual offending is addressed in conclusion. © 2013 Springer-Verlag London.",Immersive virtual reality; Made-to-measure virtual characters; Pedophilia; Penile plethysmography; Sexual self-regulation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,Reducing the gap between Augmented Reality and 3D modeling with real-time depth imaging,VRS - Virtual Reality,B,"Whereas 3D surface models are often used for augmented reality (e. g., for occlusion handling or model-based camera tracking), the creation and the use of such dense 3D models in augmented reality applications usually are two separated processes. The 3D surface models are often created in offline preparation steps, which makes it difficult to detect changes and to adapt the 3D model to these changes. This work presents a 3D change detection and model adjustment framework that combines AR techniques with real-time depth imaging to close the loop between dense 3D modeling and augmented reality. The proposed method detects the differences between a scene and a 3D model of the scene in real time. Then, the detected geometric differences are used to update the 3D model, thus bringing AR and 3D modeling closer together. The accuracy of the geometric difference detection depends on the depth measurement accuracy as well as on the accuracy of the intrinsic and extrinsic parameters. To evaluate the influence of these parameters, several experiments were conducted with simulated ground truth data. Furthermore, the evaluation shows the applicability of AR and depth image-based 3D modeling for model-based camera tracking. © 2011 Springer-Verlag London Limited.",3D modeling; Analysis-by-synthesis; Augmented Reality; Computer vision; Depth imaging; Tracking,Title_Abstract_Keywords,True,
Scopus,journalPaper,2014,Immersive front-projection analysis using a radiosity-based simulation method,VRS - Virtual Reality,B,"Video projectors are designed to project onto flat white diffuse screens. Over the last few years, projector-based systems have been used, in virtual reality applications, to light non-specific environments such as the walls of a room. However, in these situations, the images seen by the user are affected by several radiometric disturbances, such as interreflection. Radiometric compensation methods have been proposed to reduce the disturbance caused by interreflection, but nothing has been proposed for evaluating the phenomenon itself and the effectiveness of compensation methods. In this paper, we propose a radiosity-based method to simulate light transfer in immersive environments, from a projector to a camera (the camera gives the image a user would see in a real room). This enables us to evaluate the disturbances resulting from interreflection. We also consider the effectiveness of interreflection compensation and study the influence of several parameters (projected image, projection onto a small or large part of the room, reflectivity of the walls). Our results show that radiometric compensation can reduce the influence of interreflection but is severely limited if we project onto a large part of the walls around the user, or if all the walls are bright. © 2013 Springer-Verlag London.",Immersive environments; Radiometric compensation; Radiosity; Video projection,Abstract,True,
Scopus,journalPaper,2013,Perceiving affordances in virtual reality: Influence of person and environmental properties in perception of standing on virtual grounds,VRS - Virtual Reality,B,"We evaluated the perception of affordances in virtual environments (VE). In our work, we considered the affordances for standing on a virtual slanted surface. Participants were asked to judge whether a virtual slanted surface supported upright stance. The objective was to evaluate whether this perception was possible in virtual reality (VR) and comparable to previous works conducted in real environments. We found that the perception of affordances for standing on a slanted surface in virtual reality is possible and comparable (with an underestimation) to previous studies conducted in real environments. We also found that participants were able to extract and to use virtual information about friction in order to judge whether a slanted surface supported an upright stance. Finally, results revealed that the person's position on the slanted surface is involved in the perception of affordances for standing on virtual grounds. Taken together, our results show quantitatively that the perception of affordances can be effective in virtual environments and influenced by both environmental and person properties. Such a perceptual evaluation of affordances in VR could guide VE designers to improve their designs and to better understand the effect of these designs on VE users. © 2012 Springer-Verlag London.",Head-mounted display; Perception of affordances; Posture; Slanted surface; Virtual environments; Visual perception,Title_Abstract,True,
Scopus,journalPaper,2014,"Why, when and how to use augmented reality agents (AuRAs)",VRS - Virtual Reality,B,"Over the last number of years, multiple research projects have begun to create augmented reality (AR) applications that use augmented reality agents, or AuRAs, as their principle interaction and development paradigm. This paper aims to address this new and distinct field of AuRAs by asking three questions: why should AuRAs be researched, when are they a useful paradigm, and how can they be developed? The first question explores the motivation behind applying AuRAs to AR. Specifically, it investigates whether AuRAs are purely an interaction paradigm, or whether they can also serve as a development paradigm, by outlining in which circumstances it is appropriate for a project to use AuRAs and where their addition would only add unnecessary complexity. A navigational experiment, performed in simulated AR, explores the second question of when AuRAs can be a useful concept in AR applications. Results from this experiment suggest that an embodied virtual character allows for faster navigation along a shorter route than directional arrows or marking the target with an AR ""bubble"". An exploration of the limitations of the simulated AR environment illuminates how faithfully the experiment recreated the environmental challenges that AuRAs can help to address. Finally, the question of how to develop such applications is addressed through the introduction of the agent factory augmented reality toolkit that allows the rapid prototyping of such applications. Results from a usability study on the toolkit are also presented. © 2013 Springer-Verlag London.",AR simulation; Augmented reality; Interaction techniques; Multi-agent systems; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,Augmenting moving planar surfaces robustly with video projection and direct image alignment,VRS - Virtual Reality,B,"Augmented reality applications based on video projection, to be effective, must track moving targets and make sure that the display remains aligned even when they move, but the projection can severely alter their appearances to the point where traditional computer vision algorithms fail. Current solutions consider the displayed content as interference and largely depend on channels orthogonal to visible light. They cannot directly align projector images with real-world surfaces, even though this may be the actual goal. We propose instead to model the light emitted by projectors and reflected into cameras and to consider the displayed content as additional information useful for direct alignment. Using a color camera, our implemented software successfully tracks with subpixel accuracy a planar surface of diffuse reflectance properties at an average of eight frames per second on commodity hardware, providing a solid base for future enhancements. © 2012 Springer-Verlag London Limited.",Augmented reality; Image alignment; Projector-camera systems; Video projection; Vision-based tracking,Abstract_Keywords,True,
Scopus,journalPaper,2013,Human perception of a conversational virtual human: An empirical study on the effect of emotion and culture,VRS - Virtual Reality,B,"Virtual reality applications with virtual humans, such as virtual reality exposure therapy, health coaches and negotiation simulators, are developed for different contexts and usually for users from different countries. The emphasis on a virtual human's emotional expression depends on the application; some virtual reality applications need an emotional expression of the virtual human during the speaking phase, some during the listening phase and some during both speaking and listening phases. Although studies have investigated how humans perceive a virtual human's emotion during each phase separately, few studies carried out a parallel comparison between the two phases. This study aims to fill this gap, and on top of that, includes an investigation of the cultural interpretation of the virtual human's emotion, especially with respect to the emotion's valence. The experiment was conducted with both Chinese and non-Chinese participants. These participants were asked to rate the valence of seven different emotional expressions (ranging from negative to neutral to positive during speaking and listening) of a Chinese virtual lady. The results showed that there was a high correlation in valence rating between both groups of participants, which indicated that the valence of the emotional expressions was as easily recognized by people from a different cultural background as the virtual human. In addition, participants tended to perceive the virtual human's expressed valence as more intense in the speaking phase than in the listening phase. The additional vocal emotional expression in the speaking phase is put forward as a likely cause for this phenomenon. © 2013 Springer-Verlag London.",Affective computing; Culture; Emotion; Virtual human; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2013,Experiences in mixed reality-based collocated after action review,VRS - Virtual Reality,B,"After action review (AAR) is a widely used training practice in which trainees and trainers review past training experiences and performance for the purpose of learning. AAR has often been conducted with video-based systems whereby a video of the action is reviewed afterward, usually at another location. This paper proposes collocated AAR of training experiences through mixed reality (MR). Collocated AAR allows users to review past training experiences in situ with the user's current, real-world experience, i.e., the AAR is conducted at the same location where the action being reviewed occurred. MR enables a user-controlled egocentric viewpoint, augmentation such as a visual overlay of virtual information like conceptual visualizations, and playback of recorded training experiences collocated with the user's current experience or that of an expert. Collocated AAR presents novel challenges for MR, such as collocating time, interactions, and visualizations of previous and current experiences. We created a collocated AAR system for anesthesia education, the augmented anesthesia machine visualization, and interactive debriefing system. The system enables collocated AAR in two applications related to anesthesia training: anesthesia machine operation training and skin disinfection training with a mannequin patient simulator. Collocated AAR was evaluated in two informal pilot studies by students (n = 19) and an educator (n = 1) not directly affiliated with the project. We review the anecdotal data collected from the studies and point toward ways to refine and improve collocated AAR. © 2013 Springer-Verlag London.",After action review; Anesthesia machine; Human patient simulator; Mixed reality; Skin prepping; User studies,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,A usability study of multimodal input in an augmented reality environment,VRS - Virtual Reality,B,"In this paper, we describe a user study evaluating the usability of an augmented reality (AR) multimodal interface (MMI). We have developed an AR MMI that combines free-hand gesture and speech input in a natural way using a multimodal fusion architecture. We describe the system architecture and present a study exploring the usability of the AR MMI compared with speech-only and 3D-hand-gesture-only interaction conditions. The interface was used in an AR application for selecting 3D virtual objects and changing their shape and color. For each interface condition, we measured task completion time, the number of user and system errors, and user satisfactions. We found that the MMI was more usable than the gesture-only interface conditions, and users felt that the MMI was more satisfying to use than the speech-only interface conditions; however, it was neither more effective nor more efficient than the speech-only interface. We discuss the implications of this research for designing AR MMI and outline directions for future work. The findings could also be used to help develop MMIs for a wider range of AR applications, for example, in AR navigation tasks, mobile AR interfaces, or AR game applications. © 2013 Springer-Verlag London.",Augmented reality; Effectiveness; Efficiency; Multimodal interface; Satisfaction; Usability,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,In-Situ interactive image-based model building for Augmented Reality from a handheld device,VRS - Virtual Reality,B,"Three-dimensional models of objects and their creation process are central for a variety of applications in Augmented Reality. In this article, we present a system that is designed for in-situ modeling using interactive techniques for two generic versions of handheld devices equipped with cameras. The system allows online building of 3D wireframe models through a combination of user interaction and automated methods. In particular, we concentrate in rigorous evaluation of the two devices and interaction methods in the context of 3D feature selection. We present the key components of our system, discuss our findings and results and identify design recommendations. © 2011 Springer-Verlag London Limited.",Augmented Reality; Handheld device; Model building,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,Spectrum-based synthesis of vibrotactile stimuli: Active footstep display for crinkle of fragile structures,VRS - Virtual Reality,B,"When a human crinkles or scrunches a fragile object, for which the yield force is very small that it is hardly perceived, they identify the material of the object based on tactile stimuli delivered to the skin. In addition, humans are able to recognize materials even when they are crinkled at different speeds. In order to realize these human recognition features of the crinkle of a fragile object, we develop a vibrotactile synthesis method. This method synthesizes the vibrotactile acceleration stimuli in response to a crinkle speed based on the preliminarily measured acceleration spectra. Using this method, we develop an active footstep display that presents a virtual crinkle of fragile structures made of different materials to its users. Experimental participants could identify three of the four types of virtual structure materials at rates significantly higher than the chance level. The four materials were copy and typing paper, aluminum foil, and polypropylene film. Furthermore, the trends of answer ratios exhibit good correspondence with those for the real cylindrical fragile objects. We conclude that the developed method is valid for the virtual crinkle of fragile structures and will enhance the validity of virtual reality systems, such as a virtual walkthrough system. © 2013 Springer-Verlag London.",Amplitude spectrum; Haptic interface; Virtual material,Abstract,True,
Scopus,journalPaper,2013,Haptic modules for palpatory diagnosis training of medical students,VRS - Virtual Reality,B,"We have developed and evaluated a novel tool based on haptics and virtual reality technology for augmenting the teaching of palpatory diagnosis. This novel tool can act as an automated expert and an animated textbook to illuminate palpatory diagnosis concepts by touch on a laptop PC and by using affordable haptic interfaces that can be housed in a medical student resource library. It can be used for unlimited student practice for improving skills from Osteopathic Manipulative Medicine laboratory and also as a repeatable and objective measure of palpatory skill to track student progress. The system was evaluated by 22 osteopathic medical students (16 first- and 6 second-year). The majority of the participating students (>90.9 %) thought that future practice with the system may help them develop their palpatory skills. The majority (>77.3 %) of the students also thought that the instructions on the module screens were clear. When the students were asked about the user interface, most of the students (>86.4 %) responded that it was clear and easy to interpret. Evaluation results also showed that when the students were asked whether they would like to use the modules in the future for training at least 90. 9 % of them answered ""Yes"" or ""Maybe."" The achievement of purpose ratings for individual modules changed between 6. 27 and 8. 82 on a 10-point scale. This project has the potential to be extended from osteopathic medicine to allopathic medicine, veterinary medicine, physical therapy, massage therapy, and chiropractic schools. © 2013 Springer-Verlag London.",Haptic medical simulation; Haptic modules; Medical education; Osteopathic medicine; Palpatory diagnosis,Abstract,True,
Scopus,journalPaper,2023,"19th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2023",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 206 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: “The Relief is Amazing”: An In-situ Short Field Evaluation of a Personal Voice Assistive Technology for a User Living with Dementia; towards an Automatic Easy-to-Read Adaptation of Morphological Features in Spanish Texts; Challenges Faced by the Employed Indian DHH Community; haptic Auditory Feedback for Enhanced Image Description: A Study of User Preferences and Performance; using Colour and Brightness for Sound Zone Feedback; common Objects for Programming Workshops in Non-Formal Learning Contexts; engaging a Project Consortium in Ethics-Aware Design and Research; exploring Emotions: Study of Five Design Workshops for Generating Ideas for Emotional Self-report Interfaces; moving Away from the Blocks: Evaluating the Usability of EduBlocks for Supporting Children to Transition from Block-Based Programming; Point- and Volume-Based Multi-object Acquisition in VR; dark Finance: Exploring Deceptive Design in Investment Apps; elements that Influence Transparency in Artificial Intelligent Systems - A Survey; empowering Users: Leveraging Interface Cues to Enhance Password Security; friendly Folk Advice: Exploring Cybersecurity Information Sharing in Nigeria; trust in Facial Recognition Systems: A Perspective from the Users; comparing Screen-Based Version Control to Augmented Artifact Version Control for Physical Objects; emoClock: Communicating Real-Time Emotional States Through Data Physicalizations; extending User Interaction with Mixed Reality Through a Smartphone-Based Controller; fitts’ Throughput Vs Empirical Throughput: A Comparative Study; Developing and Evaluating a Novel Gamified Virtual Learning Environment for ASL; using Mid-Air Haptics to Guide Mid-Air Interactions; effects of Moving Speed and Phone Location on Eyes-Free Gesture Input with Mobile Devices; hap2Gest: An Eyes-Free Interaction Concept with Smartphones Using Gestures and Haptic Feedback; user-Centered Evaluation of Different Configurations of a Touchless Gestural Interface for Interactive Displays; assignment of a Vibration to a Graphical Object Induced by Resonant Frequency; mid-air Haptic Cursor for Physical Objects.,,Abstract,True,
Scopus,journalPaper,2023,"Eyes on Teleporting: Comparing Locomotion Techniques in Virtual Reality with Respect to Presence, Sickness and Spatial Orientation",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This work compares three locomotion techniques for an immersive VR environment: two different types of teleporting (with and without animation) and a manual (joystick-based) technique. We tested the effect of these techniques on visual motion sickness, spatial awareness, presence, subjective pleasantness, and perceived difficulty of operating the navigation. We collected eye tracking and head and body orientation data to investigate the relationships between motion, vection, and sickness. Our study confirms some results already discussed in the literature regarding the reduced invasiveness and the high usability of instant teleport while increasing the evidence against the hypothesis of reduced spatial awareness induced by this technique. We reinforce the evidence about the issues of extending teleporting with animation. Furthermore, we offer some new evidence of a benefit to the user experience of the manual technique and the correlation of the sickness felt in this condition with head movements. The findings of this study contribute to the ongoing debate on the development of guidelines on navigation interfaces in specific VR environments. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",locomotion in virtual environment; teleporting; virtual reality,Title_Keywords,True,
Scopus,journalPaper,2023,Comparing Perceived Restorativeness and Stress Reduction in Virtual Reality Environments Using Abstract Fractal Geometries Versus Realistic Natural Landscapes,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Stress and its related mental conditions are an increasing concern in modern societies. Natural settings have been shown to reduce stress and anxiety levels and help restore lost physical-psychological resources. The Perceptual Fluency Account associates this restorative potential with nature’s fractal characteristics, which facilitate their visual processing. While many studies have shown the value of Virtual Reality Nature for stress reduction and even treatment, no study of Virtual Reality fractal abstract worlds for restoration has been found. We question whether an abstract fractal-based environment can have similar restorative effects to a realistic nature-based environment in Virtual Reality. A total of 39 participants took part in two studies. In the first one, two groups (N = 19) of participants performed a collecting task in a fractal- or nature-based environment. The results showed that both environments were perceived as restorative and significantly reduced stress. To infer how the existence of a task modulated the results, in the second study, with 20 participants split into two groups, participants were exposed to the same environments, this time without a task. The results showed that the condition was significantly more restorative for the fractal-based environment when no task was performed. To conclude, fractal-based abstract environments show potential to be used for restoration purposes, but the extent to which having a task influences restorativeness needs further research. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",abstract fractals; restoration; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Explorative Study of Perceived Social Loafing in VR Group Discussion: A Comparison Between the Poster Presentation Environment and the Typical Conference Environment,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Social loafing is a phenomenon in which members of a group reduce individual motivation and effort. We explored the difference between social loafing perceived by the loafer himself/herself (Self Perceived Social Loafing; SPSL) and social loafing perceived by other group members in VR group discussion (Others Perceived Social Loafing; OPSL). We also investigated how this difference changes in two types of group discussion: the poster presentation environment and the typical conference environment. An experiment with a between-participant design was conducted, and participants conducted a desert survival task through VR group discussion. The results showed that, overall, there was only a weak positive correlation and not much agreement between SPSL and OPSL. The results also suggested that there were significant positive correlations between the indicators relating to conversation behavior and OPSL in the typical conference environment but not in the poster presentation environment. In addition, an analysis by Lasso was conducted to examine the relationship between OPSL and these indicators and found that three indicators relating to participants’ conversation behavior were selected in the typical conference environment, but none were selected in the poster presentation environment. Our study suggested that, in the typical conference environment, people judged the other people’s social loafing through their conversation behavior; on the other hand, people’s conversation behavior may not be used as significant indicators for social loafing in the poster presentation environment. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Metaverse; Perceived Social Loafing; Social Loafing; Social VR; VR Group Discussion,Keywords,True,
Scopus,journalPaper,2023,Skillab - A Multimodal Augmented Reality Environment for Learning Manual Tasks,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The paper investigates the usage of AR-based systems in teaching manual skills. We propose Skillab, a novel AR-based scaffolding system. It aids in the learning of manual work and functions as a multimodal immersive tool for feedback, including muscle actuation. As our initial investigation, we made a floor lamination tutorial. We evaluated our system’s performance and user experience in comparison to traditional paper instructions. With 20 participants, we conducted a between-group user study and obtained both qualitative and quantitative data. In terms of task performance, learnability, and overall user experience, Skillab significantly outperformed conventional paper instructions. In contrast to learning from paper instructions, Skillab training demonstrated a significant improvement in the systematic rating on the quality of the performed task. We believe that by demonstrating the potential of immersive multi-modal feedback technology for skill-building, researchers would be motivated to explore this area further. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Augmented Reality; Immersiveness; Muscle Actuation; Skill Building,Title_Keywords,True,
Scopus,journalPaper,2023,AllyChat: Developing a VR Conversational AI Agent Using Few-Shot Learning to Support Individuals with Intellectual Disabilities,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual Reality (VR) environments have been used for training, education, and entertainment due to the interactive and embodied experiences the technology provides. Studies have shown that VR can be used to help support individuals with intellectual disabilities in various aspects of their lives. Likewise, conversational agents such as chatbots can be used to bolster competence training and well-being management for this user population. This paper addresses the need for inclusive job interview practice for individuals with intellectual disabilities by discussing the development of a VR application, AllyChat. A two-part development phase is presented with pilot testing included for each phase. First, a conversational AI chatbot is tested and with positive feedback iterated upon to develop an immersive mock job interview experience in VR. A second pilot study is conducted with university students to test the functionality of a high-fidelity prototype. Future work will include improvement upon the developed VR application and further testing. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Intellectual Disabilities; Large Language Model; Virtual Reality Job Interview,Abstract_Keywords,True,
Scopus,journalPaper,2023,"Co-designing Immersive Virtual and Extended Reality Systems for Remote and Unsupervised Interaction, Intervention, Training and Research",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We propose a one-day transdisciplinary workshop in the broad area of HCI focused on co-designing immersive virtual reality (IVR) for remote and unsupervised interaction, intervention, training and research. The development and deployment of such systems is a significant and important challenge. While remote and unsupervised systems are more accessible to a wider user-base, their design, implementation and deployment poses unique challenges, related to the need to involve truly transdisciplinary design teams, co-designing solutions with users, providing step-by-step interaction scenarios, and retaining user motivation and engagement over longer periods of time. Moreover, there are multiple ethical considerations related to both the inclusivity and accessibility of such systems and the security of data collected. Therefore, to facilitate the use of IVR systems in various contexts, ranging from unique interactions and research, through psychological interventions, to education and training, we propose to formulate a set of best practices. Taking into account the diverse aspects involved, we will formulate actionable guidelines for co-designing such solutions with users based on review of extant literature, expert knowledge, case studies and insights from the workshop. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",co-design; human-computer interaction; immersive virtual and extended reality systems; participatory design; remote and unsupervised interaction; transdisciplinary collaboration,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Ingá Telikit: A Virtual Reality Game for Learning Penan’s Hunting Techniques,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this demo paper, we present Inga’ Telikit, a virtual reality game co-designed with the Penan community of Long Lamai to preserve their traditional hunting techniques. The game is based on the mythological story of Aka and Gugak, two Penan legends who are inextricably connected with the Penan hunting skills and practices of nomadic life. The game was developed using a community-based co-design approach, involving five youth and two elder community members in the design and development process. In our hands-on demonstration, attendees will learn the Inga’ Telikit story and also play the Penan hunting game against a non-player character. Our demo addresses the call of the track which welcomes technical and innovative solutions to address the current issues. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Co-Design; Inga’ Telikit; Malaysian; Penan; Virtual Reality; Visual Sovereignty,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Supporting Resilience Through Virtual Reality: Design and Preliminary Evaluation of a VR Experience Based on Viktor Frankl’s Logotherapy,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Promoting resilience is crucial to support people in the face of traumatic experiences caused by existential crises. Virtual Reality (VR) can support resilience as it allows for embodied experiences and experiential learning. We present the design and initial user evaluation of an immersive VR experience for strengthening resilience, inspired by Viktor Frankl’s psychotherapy, ‘logotherapy and existential analysis’ (LTEA). The prototype immerses users in two experiences related to guilt or suffering and guides them through an interactive reflection, encouraging them to consider their potential for finding meaning even in adverse circumstances. Although the self-reported resilience measures did not indicate increased resilience, qualitative data suggest that the users were able to use the prototype to reflect on meaning in life. This paper contributes to the field of VR for well-being by introducing the under-explored approach of LTEA to facilitate resilience. We discuss aspects of resilience support in VR by addressing the relevance of identifying and utilising technology-specific affordances to enhance reflective practice and the potential of peer support for promoting resilience. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Existential HCI; Logotherapy and Existential Analysis; Reflection; Resilience; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"Sick in the Car, Sick in VR? Understanding How Real-World Susceptibility to Dizziness, Nausea, and Eye Strain Influences VR Motion Sickness",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"A substantial number of Virtual Reality (VR) users (studies report 30–80%) suffer from cyber sickness, a negative experience caused by a sensory mismatch of real and virtual stimuli. Prior research proposed different mitigation strategies. Yet, it remains unclear how effectively they work, considering users’ real-world susceptibility to motion sickness. We present a lab experiment, in which we assessed 146 users’ real-world susceptibility to nausea, dizziness, and eye strain before exposing them to a roller coaster ride with low or high visual resolution. We found that nausea is significantly lower for higher resolution but real-world motion susceptibility has a much stronger effect on dizziness, nausea, and eye strain. Our work points towards a need for research investigating the effectiveness of approaches to mitigate motion sickness so as not to include them from VR use and access to the metaverse. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",motion sickness; resolution; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Design and Development of an Immersive Virtual Reality Application to Reduce Anxiety in Young Adults,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Young adults are a critical population where mental health disorders, especially anxiety, are commonly manifested [1, 2]. Alongside the genetic and social factors affecting each young person, transitional phases such as leaving home or starting work increase anxiogenic symptoms [3]. For this reason, it is necessary to provide young adults with effective tools for mitigating anxiety symptoms, such as relaxation techniques [4–6]. The integration of new technologies into psychological therapies can attract the attention of young people. Moreover, these technologies have demonstrated significant potential in teaching anxiety regulation strategies [7]. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,Title,True,
Scopus,journalPaper,2023,"19th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2023",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 206 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: “The Relief is Amazing”: An In-situ Short Field Evaluation of a Personal Voice Assistive Technology for a User Living with Dementia; towards an Automatic Easy-to-Read Adaptation of Morphological Features in Spanish Texts; Challenges Faced by the Employed Indian DHH Community; haptic Auditory Feedback for Enhanced Image Description: A Study of User Preferences and Performance; using Colour and Brightness for Sound Zone Feedback; common Objects for Programming Workshops in Non-Formal Learning Contexts; engaging a Project Consortium in Ethics-Aware Design and Research; exploring Emotions: Study of Five Design Workshops for Generating Ideas for Emotional Self-report Interfaces; moving Away from the Blocks: Evaluating the Usability of EduBlocks for Supporting Children to Transition from Block-Based Programming; Point- and Volume-Based Multi-object Acquisition in VR; dark Finance: Exploring Deceptive Design in Investment Apps; elements that Influence Transparency in Artificial Intelligent Systems - A Survey; empowering Users: Leveraging Interface Cues to Enhance Password Security; friendly Folk Advice: Exploring Cybersecurity Information Sharing in Nigeria; trust in Facial Recognition Systems: A Perspective from the Users; comparing Screen-Based Version Control to Augmented Artifact Version Control for Physical Objects; emoClock: Communicating Real-Time Emotional States Through Data Physicalizations; extending User Interaction with Mixed Reality Through a Smartphone-Based Controller; fitts’ Throughput Vs Empirical Throughput: A Comparative Study; Developing and Evaluating a Novel Gamified Virtual Learning Environment for ASL; using Mid-Air Haptics to Guide Mid-Air Interactions; effects of Moving Speed and Phone Location on Eyes-Free Gesture Input with Mobile Devices; hap2Gest: An Eyes-Free Interaction Concept with Smartphones Using Gestures and Haptic Feedback; user-Centered Evaluation of Different Configurations of a Touchless Gestural Interface for Interactive Displays; assignment of a Vibration to a Graphical Object Induced by Resonant Frequency; mid-air Haptic Cursor for Physical Objects.,,Abstract,True,Duplicate
Scopus,journalPaper,2023,VR Accessibility in Distance Adult Education,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"As virtual reality (VR) technology becomes more pervasive, it continues to find multiple new uses beyond research laboratories. One of them is distance adult education—the potential of VR to provide valuable education experiences is massive, despite the current barriers to its widespread application. Nevertheless, recent trends demonstrate clearly that VR is on the rise in education settings, and VR-only courses are becoming more popular across the globe. This trend will continue as more affordable VR solutions are released commercially, increasing the number of education institutions that benefit from the technology. No accessibility guidelines exist at present that are created specifically for the design, development, and use of VR hardware and software in distance education. The purpose of this workshop is to address this niche. It gathers researchers and practitioners who are interested in education and intend to work together to formulate a set of practical guidelines for the use of VR in distance adult education to make it accessible to a wider range of people. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Accessibility; Case studies; Distance education; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Pedestrian Interaction with a Snow Clearing Robot,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper, we investigate pedestrian interaction with a large autonomous robot that clears snow from the sidewalk. Through a virtual reality (VR) based user study, simulating different robot behaviors, we report on perceptions of encountering a potentially dangerous robot on the sidewalk. Overall, participants considered their actions in VR to be representative of their real-world actions. However, we note that VR headsets are not able to reproduce the high dynamic range required to realistically reproduce the high-intensity warning lights and sounds associated with close proximity to a large industrial machine. Participants expressed concern about interrupting the robot’s work, and that the robot’s safety-driven behavior should not delay passing by it on the sidewalk. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",digital twin; robots; user experience; user studies; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,How to Bring Diversity into Industry: Industrial Experiences in Public Transport Repair and Maintenance,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This paper on industrial experience reports on two prominent public transport companies who decided to incorporate diversity more substantially into their corporate culture. To achieve the integration, an initiative was started and a project together with Fraunhofer Austria Research (Fraunhofer) and Technische Universität Wien (TU Wien) was launched. The project aims to support the companies in preparing their strategy for upcoming challenges such as the retirement of a large corpus of workers, recruiting new trainees, and finding new jobs for people within the company, whose working environment has changed drastically, e.g., due to the loss of bus or train driving licences because of health restrictions. The desired integration to strengthen diversity in their corporate culture started with shop inspections, expert interviews, diversity & future workshops, and a user study on a prototypical work system. The results were incorporated into diversity guidelines considering various drivers of change. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Diversity in the Workspace; Industrial Repair and Maintenance; Spatial Augmented Reality,Keywords,True,
Scopus,journalPaper,2023,Spatial Augmented Reality in the Factory: Can In-Situ Projections Be Used to Communicate Dangers and Health Risks?,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In the context of industrial settings, extensive research of in-situ projections has proven their benefits for task performance. However, to date, these projections have not explicitly addressed policies designed to mitigate the dangers and health risks that are just as important, if not more than task performance considerations in such settings. We developed in-situ projections for three different use cases: (1) assembly support at a workbench, (2) ergonomic lifting, (3) restricted areas, which we studied with 15 representative target users. We found the expected benefits of the task-supporting projection (use case 1), increasing task performance and causing minimal cognitive load. However, our data also suggest that the other projections (use case 2 and 3) did not improve policy compliance. Our findings indicate that in-situ projections are not the most suitable solution to nudge workers to policy compliance in an industrial assembly setting, as most participants ignored the policy after evaluating the dangers themselves. Furthermore, based on our limitations and findings, we reflect on how current study practices can be improved for ubiquitous systems, especially when aiding policy compliance. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Cognitive Load; Collaborative Robot Safety Zone Awareness; Ergonomic Notifications; Human Machine Interaction; Industrial Assembly; Learnability; Manual Assembly Assistance; Pose Estimation; Spatial Augmented Reality,Title_Keywords,True,
Scopus,journalPaper,2023,Building Teamwork: Mixed Reality Game for Developing Trust and Communication,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Cultivating teamwork and a sense of empathy is an important part of boosting productivity and student and employee satisfaction and well-being. We present the initial development of a mixed-reality game using a LEGO block digitization system. Two participants collaborate, with one in real life and the other in VR. Our initial results indicate a positive impact on empathy and user involvement. Moving forward, we aim to create various teamwork experiences using different VR interactions with digitized LEGO builds both for children’s educational purposes and adult teamwork. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",3D digitization; communication; LEGO blocks; teamwork; VR,Title,True,
Scopus,journalPaper,2023,Using Virtual Reality to Investigate the Emergence of Gaze Conventions in Interpersonal Coordination,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Gaze plays a central role in regulating turn-taking, but it is currently unclear whether the turn-taking signals of eye gaze are static and fixed, or whether they can be negotiated by participants during interaction. To address this question, participants play a novel collaborative task, in virtual reality. The task is played by 3 participants, and is inspired by games such as Guitar hero, Rock Band, Beat Saber, and Dance-Dance Revolution. Crucially, the participants are not allowed to use natural language – they may only communicate by looking at each other. Solving the task requires that participants bootstrap a communication system, solely through using their gaze patterns. The results show that participants rapidly conventionalise idiosyncratic routines for coordinating the timing and sequencing of their gaze patterns. This suggests that the turn-taking function of eye-gaze can be flexibly negotiated by interlocutors during interaction. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Dialogue; Eye-gaze; Transformed Social Interaction; Turn-taking,Title_Abstract,True,
Scopus,journalPaper,2023,Together Porting: Multi-user Locomotion in Social Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We present a design prototype that demonstrates new locomotion techniques for social virtual reality experiences. Specifically, we showcase three teleportation-based techniques which, together, support users as they join other groups of people, and help them move together in virtual environments. The new interactions include a technique to join a group with option for adjacency, the ability to “reverse teleport"" a user to your own group, and finding a consensus on where to teleport together. We argue that our new locomotion techniques maintain the simplicity of current teleportation-based techniques while supporting new interactions with people. We present our design prototype so that we can explore how new teleportation-based techniques can be designed to better support prosocial behaviour, but also highlight their potential to disrupt social experiences in social VR. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Locomotion; Multi-User; SocialVR; Teleportation; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Exploring the Potential of Metaverse Apps for Real-World Applications: A Case Study with CALEND_AR,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This paper presents CALEND_AR, a hybrid calendar application that uses augmented reality technologies to blend physical and digital calendars. The study investigates user preferences and motivations for using physical or digital calendars and explores the need for a hybrid solution that incorporates both. It comprehensively describes the application’s implementation process, including technological background, design considerations, and a user study. Evaluation results show that the hybrid approach combining the benefits of paper-based and digital calendars was appreciated by participants, despite limitations such as inconsistent accuracy of text recognition and mixed user reception of the augmented reality feature. The contributions of the work include the development of a unique and innovative hybrid approach to calendar management and demonstrating the potential of metaverse apps to address real-world problems and enhance people’s lives. Future research could explore ways to improve text recognition accuracy and make the augmented reality feature more user-friendly. Overall, CALEND_AR represents an important step forward in designing augmented and mixed reality applications. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Augmented Reality; Calendar Management; Digital Calendar; Hybrid Calendar; Paper-Based Calendar; Text Recognition; User Evaluation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Understanding HCI Approaches for the Metaverse in Education Applications for the Global South,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The recent adoption of the Metaverse in various sectors indicates its potential for digital transformation. Technologies like XR, where X = Augmented/ Virtual/ Mixed Reality, will be its key enablers and can be powerful tools for developing nations’ digital educational transformation. These developing nations are often categorized under the Global South. This workshop presents a first step toward the socio-technical Human-Computer Interaction (HCI) aspects of the Metaverse to understand its impact on education in the Global South. The socio-technical approach helps cover human, social, and organizational factors, leading to more acceptable systems for end users and stakeholders. With the Metaverse, students, and teachers can log in via different immersive devices and experience different virtual environments fabricated to their individual needs, learning, and teaching styles. It will open up possibilities to explore new situations and access facilities that might be impossible due to physical world constraints. Two key concepts will be covered – (i) A socio-technical HCI approach to the Metaverse and (ii) an Interaction Design perspective on Avatar representation and understanding of human work in the Metaverse. This workshop will initiate dialogs on questions: (a) What are the socio-technical issues of the Metaverse for education in the global south? (b) Are there any cross-cultural usability and interaction design concerns regarding digital avatar representation? (c) What considerations will be required for educational Metaverse human-work interaction design? © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Education; Global South; Metaverse,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Mind the Heart: Designing a Stress Dashboard Based on Physiological Data for Training Highly Stressful Situations in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual Reality is becoming increasingly popular to serve as training ground for challenging environments, which often involve making decisions in cognitive demanding and stressful situations. It has become common practice to analyze bio-signals to determine our current physiological status and fitness. However, we continue to rely on subjective feedback obtained through self-rated questionnaires, when it comes to assessing cognitive states. In this paper we describe the user-centered design process of building a stress dashboard prototype and testing it in a field trial to fully understand its potential. We report on mixed-method studies exploring the interplay between the trainer and the VR system. Our findings demonstrate that integrating a stress dashboard, based on objective bio-signal data, can enhance the VR training process, providing trainers with actionable insights that have the potential to shape trainee behavior and learning outcomes. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Biofeedback; Challenging Environments; Contextual experience; High stress; Immersive technologies; Police training; Training experience; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Museum Visitor Experiences Based on Hyperspectral Image Data,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Hyper- and multispectral imaging allows to collect data from specific wavelength ranges or across the electromagnetic spectrum, including frequencies that are imperceivable for humans. As non-invasive imaging techniques, it has been used in the field of art conservation and art history extensively in the past. In these areas application of hyperspectral imaging include for example conservation monitoring or pigment identification. In the context of museum exhibits, hyperspectral data of artworks offers a unique opportunity to enhance visitor experiences by providing new ways of engaging with artefacts, artworks, and cultural heritage. This paper presents design concepts for creating immersive and meaningful experiences using hyperspectral data. We used an expert led design workshop to explore the possibilities of museum experiences with such data, including considerations such as suited technologies, visitor types and visitor experience. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Augmented Reality; Hyperspectral Imaging; Multispectral Imaging; Museum Experiences; Virtual Reality,Keywords,True,
Scopus,journalPaper,2023,Electroencephalographic (EEG) Correlates of Visually Induced Motion Sickness (VIMS) in the Virtual Reality (VR) Based Simulations,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In the presented study, the possibility of detecting Visually Induced Motion Sickness (VIMS) in Virtual Reality (VR) simulations using electroencephalographic (EEG) recordings was investigated. 31 adult participants were tested with VR and EEG in both VIMS and non-VIMS conditions. A correlation between EEG signals and the Simulation Sickness Questionnaire (SSQ) scores was shown, indicating that VIMS can be detected through EEG recordings. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",EEG; Electroencephalography; simulations; VIMS; Virtual Reality; Visually Induced Motion Sickness; VR,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"“Do I Run Away?”: Proximity, Stress and Discomfort in Human-Drone Interaction in Real and Virtual Environments",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Social drones are autonomous flying machines designed to operate in inhabited environments. Yet, little is known about how their proximity might impact people’s well-being. This knowledge is critical as drones are often perceived as potential threats due to their design (e.g., visible propellers, unpleasant noise) and capabilities (e.g., moving at high speed, surveillance). In parallel, Virtual Reality (VR) is a promising tool to study human–drone interactions. However, important questions remain as to whether VR is ecologically valid for exploring human–drone interactions. Here, we present a between-within subjects user study (N = 42) showing that participants’ stress significantly differs between different drone states and locations. They felt more comfortable when the drone retreated from their personal space. Discomfort and stress were strongly correlated with the perceived drone’s threat level. Similar findings were found across real and virtual environments. We demonstrate that drones’ behaviour and proximity can threaten peoples’ well-being and comfort, and propose evidence-based guidelines to mitigate these impacts. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Proxemics; Social Drones; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2023,Comparing Screen-Based Version Control to Augmented Artifact Version Control for Physical Objects,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Besides referring to digital twins, the iterative development of physical objects cannot be easily managed in version control systems. However, physical content also could benefit from versioning for structured work and collaborative uses, thereby increasing equality between digital and physical design. Hence, it needs to be investigated what kind of system is most suitable for supporting a physical object version control. Focusing on the visualization of differences between states of a physical artifact, two systems were compared against each other in a lab study: a screen-based solution optimized for 3D models as baseline and an approach that augments a physical artifact with digital information as hypothesis. Our results indicate that the Augmented Artifact system is superior in task completion time but scores a lower usability rating than the baseline. Based on the results, we further provide design considerations for building a physical object version control system. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Augmented Reality; Physical Object; Version Control,Keywords,True,
Scopus,journalPaper,2023,PeriFocus - Training Peripheral Color- and Shape Detection in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Peripheral vision plays an important role in monitoring and detection of peripheral events not only in our daily life but also in complex and time-demanding situations. For example athletes in team sports such as Football or Handball, that require to not only keep track of one’s own team but also the opponents around oneself, benefit greatly from good peripheral vision. This paper examines the possibilities of training peripheral color and shape detection in virtual reality (VR). To this end we created an application called PeriFocus, a simple shooter game in VR that requires users to use their peripheral vision for object identification of the features color, size and shape. Our initial evaluation with six participants over 12 days revealed a statistical significant increase in peripheral color detection measured by an analog visual acuity test. This not only validates prior work conducted in a desktop setting but also highlights potential for future VR applications. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Eye-Tracking; Peripheral Vision; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Exploring Indigenous Knowledge Through Virtual Reality: A Co-Design Approach with the Penan Community of Long Lamai,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This study aims to explore the design process of a virtual reality (VR) application to preserve indigenous knowledge (IK). Leveraging the community’s preference for visual means of communication, a VR application was designed. The process involved working with the local Penan community in the forests of Borneo to co-design a VR application for a traditional hunting game called Nelikit and the mythological origin story associated with the game. Our methodology ensures that the application accurately represents the traditional game by community-led ownership of the design process and the final result. The findings of this study confirms that the use of VR can be an effective tool for cultural preservation and education, and that collaboration with the local indigenous community and designers is essential in the design process. Future research will further investigate the development, deployment and testing of the VR application. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Co-design; Indigenous Knowledge; Penan; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"19th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2023",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 206 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: “The Relief is Amazing”: An In-situ Short Field Evaluation of a Personal Voice Assistive Technology for a User Living with Dementia; towards an Automatic Easy-to-Read Adaptation of Morphological Features in Spanish Texts; Challenges Faced by the Employed Indian DHH Community; haptic Auditory Feedback for Enhanced Image Description: A Study of User Preferences and Performance; using Colour and Brightness for Sound Zone Feedback; common Objects for Programming Workshops in Non-Formal Learning Contexts; engaging a Project Consortium in Ethics-Aware Design and Research; exploring Emotions: Study of Five Design Workshops for Generating Ideas for Emotional Self-report Interfaces; moving Away from the Blocks: Evaluating the Usability of EduBlocks for Supporting Children to Transition from Block-Based Programming; Point- and Volume-Based Multi-object Acquisition in VR; dark Finance: Exploring Deceptive Design in Investment Apps; elements that Influence Transparency in Artificial Intelligent Systems - A Survey; empowering Users: Leveraging Interface Cues to Enhance Password Security; friendly Folk Advice: Exploring Cybersecurity Information Sharing in Nigeria; trust in Facial Recognition Systems: A Perspective from the Users; comparing Screen-Based Version Control to Augmented Artifact Version Control for Physical Objects; emoClock: Communicating Real-Time Emotional States Through Data Physicalizations; extending User Interaction with Mixed Reality Through a Smartphone-Based Controller; fitts’ Throughput Vs Empirical Throughput: A Comparative Study; Developing and Evaluating a Novel Gamified Virtual Learning Environment for ASL; using Mid-Air Haptics to Guide Mid-Air Interactions; effects of Moving Speed and Phone Location on Eyes-Free Gesture Input with Mobile Devices; hap2Gest: An Eyes-Free Interaction Concept with Smartphones Using Gestures and Haptic Feedback; user-Centered Evaluation of Different Configurations of a Touchless Gestural Interface for Interactive Displays; assignment of a Vibration to a Graphical Object Induced by Resonant Frequency; mid-air Haptic Cursor for Physical Objects.,,Abstract,True,Duplicate
Scopus,journalPaper,2023,Supporting Construction and Architectural Visualization Through BIM and AR/VR: A Systematic Literature Review,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The Architecture, Engineering, Construction, and Facility Management (AEC/FM) industry deals with the design, construction, and operation of complex buildings. Today, Building Information Modeling (BIM) is used to represent information about a building in a single, non-redundant representation. Here, Augmented Reality (AR) and Virtual Reality (VR) can improve the visualization and interaction with the resulting model by augmenting the real world with information from the BIM model or allowing a user to immerse in a virtual world generated from the BIM model. This can improve the design, construction, and operation of buildings. While an increasing number of studies in HCI, construction, or engineering have shown the potential of using AR and VR technology together with BIM, often research remains focused on individual explorations and key design strategies. In addition to that, a systematic overview and discussion of recent works combining AR/VR with BIM are not yet fully covered. Therefore, this paper systematically reviews recent approaches combining AR/VR with BIM and categorizes the literature by the building’s lifecycle phase while systematically describing relevant use cases. In total, 32 out of 447 papers between 2017 and 2022 were categorized. The categorization shows that most approaches focus on the construction phase and the use case of review and quality assurance. In the design phase, most approaches use VR, while in the construction and operation phases, AR is prevalent. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",AR; BIM; Construction; Design; Operation; VR,Abstract,True,
Scopus,journalPaper,2023,Mapping Virtual Reality Controls to Inform Design of Accessible User Experiences,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"A lack of accessible controls remains a barrier to disabled users engaging in virtual reality experiences. This paper presents a modified cognitive walkthrough of 120 virtual reality applications to identify 2,284 pairs of operant and resultant actions and creates an inventory of domain objects and their operant and resultant actions in the virtual space. This inventory captures both the form and prevalence of interactions that are expected of users in current virtual reality design. An analysis of this inventory reveals that while many barriers could be addressed by existing solutions, those options currently are not often present in current designs. Further, there are a set of barriers related to embodied controls that represent opportunities and challenges for new and innovative designs in virtual reality. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",accessibility; controls; input; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Design Paradigms of 3D User Interfaces for VR Exhibitions,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual reality (VR) technology capably complements exhibitions. Despite the considerable attention to the development of VR exhibitions, corresponding 3D user interface (UIs) design is in strong need. In this paper, we focus on the design of 3D UIs for VR exhibitions based on head-mounted displays (HMDs). The paper presents two workshops: one explores UI design paradigms that improve the user’s perceived usability and sense of immersion in the VR space, while the other examines ergonomic arrangements of the UI to address spatial layout issues. We aim to assist designers and developers by suggesting design choices, thus saving them time and avoiding the need to start from scratch. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",3D user interfaces; Design paradigms; VR exhibitions,Abstract,True,
Scopus,journalPaper,2023,"19th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2023",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 206 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: “The Relief is Amazing”: An In-situ Short Field Evaluation of a Personal Voice Assistive Technology for a User Living with Dementia; towards an Automatic Easy-to-Read Adaptation of Morphological Features in Spanish Texts; Challenges Faced by the Employed Indian DHH Community; haptic Auditory Feedback for Enhanced Image Description: A Study of User Preferences and Performance; using Colour and Brightness for Sound Zone Feedback; common Objects for Programming Workshops in Non-Formal Learning Contexts; engaging a Project Consortium in Ethics-Aware Design and Research; exploring Emotions: Study of Five Design Workshops for Generating Ideas for Emotional Self-report Interfaces; moving Away from the Blocks: Evaluating the Usability of EduBlocks for Supporting Children to Transition from Block-Based Programming; Point- and Volume-Based Multi-object Acquisition in VR; dark Finance: Exploring Deceptive Design in Investment Apps; elements that Influence Transparency in Artificial Intelligent Systems - A Survey; empowering Users: Leveraging Interface Cues to Enhance Password Security; friendly Folk Advice: Exploring Cybersecurity Information Sharing in Nigeria; trust in Facial Recognition Systems: A Perspective from the Users; comparing Screen-Based Version Control to Augmented Artifact Version Control for Physical Objects; emoClock: Communicating Real-Time Emotional States Through Data Physicalizations; extending User Interaction with Mixed Reality Through a Smartphone-Based Controller; fitts’ Throughput Vs Empirical Throughput: A Comparative Study; Developing and Evaluating a Novel Gamified Virtual Learning Environment for ASL; using Mid-Air Haptics to Guide Mid-Air Interactions; effects of Moving Speed and Phone Location on Eyes-Free Gesture Input with Mobile Devices; hap2Gest: An Eyes-Free Interaction Concept with Smartphones Using Gestures and Haptic Feedback; user-Centered Evaluation of Different Configurations of a Touchless Gestural Interface for Interactive Displays; assignment of a Vibration to a Graphical Object Induced by Resonant Frequency; mid-air Haptic Cursor for Physical Objects.,,Abstract,True,Duplicate
Scopus,journalPaper,2023,Point- and Volume-Based Multi-object Acquisition in VR,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Multi-object acquisition is indispensable for many VR applications. Commonly, users select a group of objects of interest to perform further transformation or analysis. In this paper, we present three multi-object selection techniques that were derived based on a two-dimensional design space. The primary design dimension concerns whether a technique acquires targets through point-based methods (selecting one object at a time) or volume-based methods (selecting a set of objects within a selection volume). The secondary design dimension examines the mechanisms of selection and deselection (cancel the selection of unwanted objects). We compared these techniques through a user study, emphasizing on scenarios with more randomly distributed objects. We discovered, for example, that the point-based technique was more efficient and robust than the volume-based techniques in environments where the targets did not follow a specific layout. We also found that users applied the deselection mechanism mostly for error correction. We provide an in-depth discussion of our findings and further distill design implications for future applications that leverage multi-object acquisition techniques in VR. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Multiple targets; Object selection; Virtual Reality,Keywords,True,
Scopus,journalPaper,2023,Stress Embodied: Developing Multi-sensory Experiences for VR Police Training,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"VR applications primarily rely on audio-visual stimuli, limiting the sense of immersion. Multi-sensory stimuli show promise in enhancing presence, realistic behavior, and overall experience. Existing approaches are either stationary or wearable, and movement-intensive. Multi-user VR police training requires a mobile device for intensive multi-sensory stimuli. This paper presents the design and development of a mobile platform for multi-sensory feedback, introducing heat, wind, mist, and pain to improve immersion. Preliminary evaluations indicate promising effects on stress in VR. The paper concludes with lessons learned for designing multi-sensory experiences in police VR training. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Multi-Sensory; Virtual Reality; VR Training,Keywords,True,
Scopus,journalPaper,2023,Embodied PointCloud: Combining Embodied Avatars with Point Clouds to Represent Users in VR Remote Meetings,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Avatar realism is crucial for high-quality virtual reality remote meetings. In this work, we introduce Embodied PointCloud, a user representation technique that combines point clouds’ realism with the increased embodiment of 3D humanoid avatars for improved interaction. After we conducted a user study to compare this approach to full-body tracked humanoid avatars, we found that Embodied PointCloud lowered perceived workload but had no significant effect on presence and social presence. We believe that Embodied PointCloud could help personalize user representation while ensuring high self-embodiment levels. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Embodiment; Point Clouds; Social Presence; Social VR,Abstract,True,
Scopus,journalPaper,2023,"Asymmetric Communication in Virtual Reality: Designing for Presence, Effectiveness, and Enjoyment",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This paper investigates the design of multi-user virtual reality (VR) communication and collaboration, focusing on asymmetric VR where one user is immersed in the virtual environment while the other interacts from the external world. Through an exploratory user study (n = 16), we examine how users experience different asymmetric VR communication methods and how these experiences inform the design of effective communication systems. We identify key factors that influence the effectiveness of different styles of asymmetric VR communication and highlight the benefits and limitations of these styles. Our thematic analysis of the participants’ responses and experiences sheds light on the importance of considering communication methods and their impact on users’ experiences in asymmetric virtual environments. Our study provides insights into how communication methods can be designed to enhance presence, social presence, communication effectiveness, and enjoyment. These findings can inform the design and development of more engaging and effective asymmetric communication methods for multi-user collaboration. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Asymmetric virtual reality; Collaboration; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,A Comparative Analysis of Multi-Object Animation with Motion Paths in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual reality has become a valuable tool for training and problem-solving, and creating animations in VR provides several advantages over traditional tools by increasing sense of presence within the scene. This paper presents an animation tool that allows novice animators to create multi-object animations using motion paths in VR. The proposed prototype is compared to the animation tool in an existing VR application. While previous studies have proposed VR animation tools, they are often targeted at experts or are missing key features such as animation with multiple objects. The comparative analysis evaluates the perceived usability and workload based on quantitative questionnaires as well as a qualitative interview. The results show the proposed tool to be superior in terms of perceived usability, however, we also found that having several motion paths could clutter the view and that future work is required in the visualization of these. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Multiple objects; Object animation; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Extending User Interaction with Mixed Reality Through a Smartphone-Based Controller,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"A major concern in mixed-reality (MR) environments is to support intuitive and precise user interaction. Various modalities have been proposed and used, including gesture, gaze, voice, hand-recognition or even special devices, i.e. external controllers. However, these modalities may often feel unfamiliar and physically demanding to the end-user, leading to difficulties and fatigue. One possible solution worth investigating further is to use an everyday object, like a smartphone, as an external device for interacting with MR. In this paper, we present the design of a framework for developing an external smartphone controller to extend user input in MR applications, which we further utilize to implement a new interaction modality, a tap on the phone. We also report on findings of a user study (n=24) in which we examine performance and user experience of the suggested input modality through a comparative user evaluation task. The findings suggest that incorporating a smartphone as an external controller shows potential for enhancing user interaction in MR tasks requiring high precision, as well as pinpointing the value of providing alternative means of user input in MR applications depending on a given task and personalization aspects of an end-user. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Cross-Device Interaction; Head-Mounted-Displays; Mixed-Reality; Usability; User Study,Title,True,
Scopus,journalPaper,2023,A Case Study Using Virtual Reality to Prime Knowledge for Procedural Medical Training,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Procedural training within medical education relies heavily on skill practice. This training requires developing a cognitive understanding of a procedure to prime learners before motor skill trials. With the high demand and costs of specialist equipment, virtual reality (VR) is poised to provide accessible content to develop cognitive understanding, and bridge the gap between knowledge and practice outside of dedicated training centres. Previous work in this field has focused on knowledge transfer, which is important yet insufficient to understand the interplay of instruction, usability, presence, and experience. All of which could impact learning outcomes and frequency of use. To have a more nuanced view of VR medical training beyond its knowledge transfer capability, we integrate HCI & games perspectives into our evaluation approach appraising the VR Bronchoscope Assembly (VR-Bronch) training. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Design Process; Education; Edutainment; Training; Usability Testing; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Playing with Data: An Augmented Reality Approach to Interact with Visualizations of Industrial Process Tomography,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Industrial process tomography (IPT) is a specialized imaging technique widely used in industrial scenarios for process supervision and control. Today, augmented/mixed reality (AR/MR) is increasingly being adopted in many industrial occasions, even though there is still an obvious gap when it comes to IPT. To bridge this gap, we propose the first systematic AR approach using optical see-through (OST) head mounted displays (HMDs) with comparative evaluation for domain users towards IPT visualization analysis. The proof-of-concept was demonstrated by a within-subject user study (n=20) with counterbalancing design. Both qualitative and quantitative measurements were investigated. The results showed that our AR approach outperformed conventional settings for IPT data visualization analysis in bringing higher understandability, reduced task completion time, lower error rates for domain tasks, increased usability with enhanced user experience, and a better recommendation level. We summarize the findings and suggest future research directions for benefiting IPT users with AR/MR. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Augmented Reality; Industrial Process Tomography; Optical See-through Head Mounted Display; User Study,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Developing and Evaluating a Novel Gamified Virtual Learning Environment for ASL,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The use of sign language is a highly effective way of communicating with individuals who experience hearing loss. Despite extensive research, many learners find traditional methods of learning sign language, such as web-based question-answer methods, to be unengaging. This has led to the development of new techniques, such as the use of virtual reality (VR) and gamification, which have shown promising results. In this paper, we describe a gamified immersive American Sign Language (ASL) learning environment that uses the latest VR technology to gradually guide learners from numeric to alphabetic ASL. Our hypothesis is that such an environment would be more engaging than traditional web-based methods. An initial user study showed that our system scored highly in some aspects, especially the hedonic factor of novelty. However, there is room for improvement, particularly in the pragmatic factor of dependability. Overall, our findings suggest that the use of VR and gamification can significantly improve engagement in ASL learning. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",ASL Learning; Human Computer Interaction; VR,Abstract,True,
Scopus,journalPaper,2023,The Effect of Teleporting Versus Room-Scale Walking for Interacting with Immersive Visualizations,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The use of different locomotion techniques such as walking or teleportation affects the way people explore and build an understanding of complex data in immersive visualizations. We report results from a quantitative user study (14 participants) comparing the effect of room-scale real walking versus teleportation on information search and task performance in a word-cloud immersive visualization. Participants performed an ordering task and we measured performance, post-task recall, workload and flow. Results suggest that room-scale real walking favors interaction and short-term recall but could imply higher workload. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",data visualization; information foraging; information search; locomotion; virtual reality,Keywords,True,
Scopus,journalPaper,2023,VR for HR – A Case Study of Human Resource Development Professionals Using Virtual Reality for Social Skills Training in the Workplace,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The Human Resource (HR) area has made little use of innovative technologies to develop its processes, routines and education. However, we believe that digital tools such as Virtual Reality (VR) can play an important role in developing social aspects of work. We have investigated Human Resource Development Professionals’ (HRD-Ps’) perception of using a VR-prototype for training of social skills in the workplace. A digital three-dimensional world was designed for the study participants, in which they interacted with agents to train social skills in the workplace. Study participants explored a VR-prototype through the usage of head-mounted devices (HMD). We collected the designer’s description of the intended design element of the VR prototype and pre- and post-intervention questionnaire from the study participants and conducted a top-down thematic analysis. The three intended design elements 1) focus on the training experience, 2) learning-depth through emotional response for engagement and motivation, and 3) perspective-taking enabled by game design, were confirmed and reflected upon by the HRD-Ps’. Additionally, using VR for social skills training in the workplace was recognized as innovative, and could have the capacity to position an organization as being in the forefront of digitalization. The conclusion is that VR has a potential to create engagement and provide insights in HR matters, but further studies are needed to show the full power and potential in using VR for HR matters. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Digitalization; Human Resource Development; Master Suppression Techniques; Organization; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Designing AR Applications for People Living with Dementia,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Given the extensive use of Augmented Reality (AR) for learning and training in different domains, this technology could support traditional therapies for people with dementia. However, to devise useful experiences, it is crucial to involve specialists and patients to fully understand the potential and limitations of ad-hoc AR applications considering the patients’ medical conditions as well as their expectations about the technology. In the context of a project aimed at improving healthcare service centers, following an action research approach, we codesigned two AR applications, tablet- and glass-based, to practice specific cognitive competencies (memory, recognition, and association) involving two specialists in psychiatry and two patients diagnosed with mild cognitive impairment. In an initial phase, we identified key issues about how the patients interact with the two devices and the interfaces in the doctors’ room that helped to improve the final prototypes. These prototypes were tested in a clinical study with ten patients diagnosed with mild cognitive impairment in the presence of their therapist. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Action Research; Augmented Reality; Cognitive Impairment,Abstract_Keywords,True,
Scopus,journalPaper,2023,MetaCUX: Social Interaction and Collaboration in the Metaverse,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this poster, we present a Virtual Reality environment, named MetaCUX (Collaborative User eXperience in the Metaverse), where people may collaborate in a virtual workplace and interact with intelligent objects for simulating real tasks. Several multi-user workplace scenarios were reproduced in the MetaCUX, on which an experimental study has been carried out involving 15 participants. To analyze remote collaboration small group of participants performed a task consisting in putting out a fire. Self-reported surveys and qualitative measures are adopted to investigate the effectiveness of social interaction during the collaborative activity. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Collaborative environment; Metaverse; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,VRSketch: Investigating 2D Sketching in Virtual Reality with Different Levels of Hand and Pen Transparency,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Sketching is a vital step in design processes. While analog sketching on pen and paper is the defacto standard, Virtual Reality (VR) seems promising for improving the sketching experience. It provides myriads of new opportunities to express creative ideas. In contrast to reality, possible drawbacks of pen and paper drawing can be tackled by altering the virtual environment. In this work, we investigate how hand and pen transparency impacts users’ 2D sketching abilities. We conducted a lab study (N= 20 ) investigating different combinations of hand and pen transparency. Our results show that a more transparent pen helps one sketch more quickly, while a transparent hand slows down. Further, we found that transparency improves sketching accuracy while drawing in the direction that is occupied by the user’s hand. © 2021, IFIP International Federation for Information Processing.",Occlusion; Sketching; Transparency; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"18th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2021",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 247 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Emotion Elicitation Techniques in Virtual Reality; expanding Affective Computing Paradigms Through Animistic Design Principles; preface; Human-Centered AI: A New Synthesis; AI-Based Clinical Decision Support Tool on Mobile Devices for Neurodegenerative Diseases; COBO: A Card-Based Toolkit for Co-Designing Smart Outdoor Experiences with People with Intellectual Disability; digital Producers with Cognitive Disabilities: Participatory Video Tutorials as a Strategy for Supporting Digital Abilities and Aspirations; dot-to-Dot: Pre-reading Assessment of Literacy Risk via a Visual-Motor Mechanism on Touchscreen Devices; exploring the Acceptability of Graphical Passwords for People with Dyslexia; intraVox: A Personalized Human Voice to Support Users with Complex Needs in Smart Homes; LIFT: An eLearning Introduction to Web Search for Young Adults with Intellectual Disability in Sri Lanka; social Robots in Learning Experiences of Adults with Intellectual Disability: An Exploratory Study; co-designing Tangible Break Reminders with People with Repetitive Strain Injury; exploring User Requirements for an Exoskeleton Arm Insights from a User-Centered Study with People Living with Severe Paralysis; multisensory Experiences: Where the Senses Meet Technology; interactive Modular Tactile Maps of Rooms for Older Adults with Vision Impairments; recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study; VR Invite: A Project-Independent Smartphone App for VR Observation and Interactivity; “Honestly I Never Really Thought About Adding a Description”: Why Highly Engaged Tweets Are Inaccessible; ‘Did You See That!?’ Enhancing the Experience of Sports Media Broadcast for Blind People.,,Abstract,True,
Scopus,journalPaper,2021,Usability Evaluation of a VibroTactile API for Web-Based Virtual Reality Experiences,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This paper presents the Vibrotactile Editor System for designing and programming Vibrotactile (VT) feedback for Virtual Reality (VR) experiences. We describe the system and a usability evaluation of the programmatic component for the A-Frame web-based VR framework through a set of programming tasks. The resolution of the tasks showed some difficulty in understanding the state of the component when developing without the VT hardware present. The results show a number of potential improvements that we discuss. © 2021, IFIP International Federation for Information Processing.",API; Usability evaluation; Vibrotactile feedback; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Stress Out: Translating Real-World Stressors into Audio-Visual Stress Cues in VR for Police Training,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual Reality (VR) training has become increasingly important for police first responders in recent years. Improving the training experience in such complex contexts requires ecological validity of virtual training. To achieve this, VR systems need to be capable of simulating the complex experiences of police officers ‘in the field.’ One way to do this is to add stressors into training simulations to induce stress similar to the stress experienced in real-life situations, particularly in situations where this is difficult (e.g., dangerous or resource-intensive) to achieve with traditional training. To include stressors in VR, this paper thus presents the concept of so-called ‘stress cues’ for operationalizing stressors to augment training in VR simulations for the context of police work. Considering the level of complexity of police work and training, a co-creation process that allows for creative collaboration and mitigation of power imbalances was chosen to access the police officers’ knowledge and experience. We assert that stress cues can improve the training experience from the trainer’s perspective as they provide novel interaction design possibilities for trainers to control the training experience. E.g., by actively intervening in training and dynamically changing the interaction space for trainees which also improves the trainee’s experience. Stress cues can also improve the trainee’s experience by enabling personalizable and customizable training based on real-time stress measurements and supplementing information for improved training feedback. © 2021, The Author(s).",Co-creation; Contextual experience; High stress; Police training; Stress cue interaction; Stress cues; Training experience; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Cooking in the Dark: Exploring Spatial Audio as MR Assistive Technology for the Visually Impaired,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In the context of raising awareness to assistive technologies, we propose a gaming experience that allows users to embody having a visual impairment. By occluding the user’s vision and providing spatialized audio and passive haptic feedback, allied with a speech recognition digital assistant, our goal is to present a multi-sensory experience that offers the user a sense of embodiment inside a mixed reality blindness simulation. Inside the game environment, the player is required to cook a meal completely in the dark. Being aided solely by their remaining senses and a digital assistant with spatialized audio capabilities. © 2021, IFIP International Federation for Information Processing.",Accessibility; Assistive technologies; Embodiment; Mixed and Augmented reality; Passive haptics; Spatial audio,Abstract_Keywords,True,
Scopus,journalPaper,2023,AHO-Guide: Automatically Guiding the Head Orientation of a Local User in Augmented Reality to Realign the Field of View with Remote Users,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Augmented Reality (AR) offer significant benefits for remote collaboration scenarios. However, when using a Head-Mounted Display (HMD), remote users do not always see exactly what local users are looking at. This happens when there is a spatial offset between the center of the Field of View (FoV) of the HMD’s cameras and the center of the FoV of the user. Such an offset can limit the ability of remote users to see objects of interest, creating confusion and impeding the collaboration. To address this issue, we propose the AHO-Guide techniques. AHO-Guide techniques are Automated Head Orientation Guidance techniques in AR with a HMD. Their goal is to encourage a local HMD user to adjust their head orientation to let remote users have the appropriate FoV of the scene. This paper presents the conception and evaluation of the AHO-Guide techniques. We then propose a set of recommendations from the encouraging results of our experimental study. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Augmented Reality; Field-of-View; Guidance; Mixed Reality; Remote collaboration,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"18th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2021",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 247 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Emotion Elicitation Techniques in Virtual Reality; expanding Affective Computing Paradigms Through Animistic Design Principles; preface; Human-Centered AI: A New Synthesis; AI-Based Clinical Decision Support Tool on Mobile Devices for Neurodegenerative Diseases; COBO: A Card-Based Toolkit for Co-Designing Smart Outdoor Experiences with People with Intellectual Disability; digital Producers with Cognitive Disabilities: Participatory Video Tutorials as a Strategy for Supporting Digital Abilities and Aspirations; dot-to-Dot: Pre-reading Assessment of Literacy Risk via a Visual-Motor Mechanism on Touchscreen Devices; exploring the Acceptability of Graphical Passwords for People with Dyslexia; intraVox: A Personalized Human Voice to Support Users with Complex Needs in Smart Homes; LIFT: An eLearning Introduction to Web Search for Young Adults with Intellectual Disability in Sri Lanka; social Robots in Learning Experiences of Adults with Intellectual Disability: An Exploratory Study; co-designing Tangible Break Reminders with People with Repetitive Strain Injury; exploring User Requirements for an Exoskeleton Arm Insights from a User-Centered Study with People Living with Severe Paralysis; multisensory Experiences: Where the Senses Meet Technology; interactive Modular Tactile Maps of Rooms for Older Adults with Vision Impairments; recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study; VR Invite: A Project-Independent Smartphone App for VR Observation and Interactivity; “Honestly I Never Really Thought About Adding a Description”: Why Highly Engaged Tweets Are Inaccessible; ‘Did You See That!?’ Enhancing the Experience of Sports Media Broadcast for Blind People.,,Abstract,True,Duplicate
Scopus,journalPaper,2021,Diegetic and Non-diegetic Health Interfaces in VR Shooter Games,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The player’s health is one of the most pervasive components in computer games. However, in virtual reality games, it is unclear how different representations of player health function compared to traditional flat-screen games. Because the viewpoint changes based on the player’s head movement, non-diegetic UI elements may not be ideal. Also, the sense of embodiment in VR provides opportunities to experiment with diegetic ways of communicating the player’s health. To investigate different implementations of player health in VR games, we developed three health interfaces and evaluated them in a shooter game. The health interfaces included: 1) A non-diegetic health bar, visible on the screen at all times, 2) A diegetic health value on a virtual wristwatch, and 3) A diegetic physical interface, where lost health results in trembling and slower movement. 37 participants played the game using all three health interfaces and provided feedback. We found that all three interfaces had their own strengths. The non-diegetic health bar was seen as suitable for multi-player games, while the wristwatch was seen as suitable for single-player, story-driven games. The physical interface was liked for its impact on gameplay, and was also seen as suitable for story-driven games. © 2021, IFIP International Federation for Information Processing.",Diegetic interfaces; First-person shooters; Game design; Games; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,A Lens-Based Extension of Raycasting for Accurate Selection in Dense 3D Environments,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In mixed environments, the selection of distant 3D objects is commonly based on raycasting. To address the limitations of raycasting for selecting small targets in dense environments, we present RayLens an extended raycasting technique. RayLens is a bimanual selection technique, which combines raycasting with a virtual 2D magnification lens that can be remotely moved in 3D space using the non-dominant hand. We experimentally compared RayLens with a standard raycasting technique as well as with RaySlider an extension of raycasting based on a target expansion mechanism whose design is akin to RayLens. RayLens is considerably more accurate and more than 1.3 × faster than raycasting for selecting small targets. Furthermore, RayLens is more than 1.6 × faster than RaySlider in dense environments. Qualitatively, RayLens is easy-to-learn and the preferred technique making it a good candidate technique for general public usage. © 2021, IFIP International Federation for Information Processing.",Augmented Reality; HMD; Lens; Pointing technique,Keywords,True,
Scopus,journalPaper,2021,Real vs Simulated Foveated Rendering to Reduce Visual Discomfort in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper, a study aimed at investigating the effects of real (using eye tracking to determine the fixation) and simulated foveated blurring in immersive Virtual Reality is presented. Techniques to reduce the optical flow perceived at the visual field margins are often employed in immersive Virtual Reality environments to alleviate discomfort experienced when the visual motion perception does not correspond to the body’s acceleration. Although still preliminary, our results suggest that for participants with higher self-declared sensitivity to sickness, there might be an improvement for nausea when using blurring. The (perceived) difficulty of the task seems to improve when the real foveated method is used. © 2021, IFIP International Federation for Information Processing.",Foveated rendering; Motion sickness; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Towards Identifying Augmented Reality Unique Attributes to Facilitate Chemistry Learning,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Augmented Reality (AR) applications have the potential to improve students’ Chemistry learning performance. By identifying the unique features and affordances of this technology, we can design more effective tools to facilitate the learning process of abstract concepts. We developed Periodic Fable in the Wild, an AR serious game as an instrument to conduct design-based research. The game aims to facilitate the learning of abstract concepts related to the Periodic Table by children (9 to 13 years old). We intend to optimize our game by continuing our research with our target audience, analysing their feedback, making refinements and continuing testing. © 2021, IFIP International Federation for Information Processing.",Augmented Reality; Chemistry; Serious games; Spatial skills,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Intention Recognition in Human Robot Interaction Based on Eye Tracking,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In human robot interaction any input that might help the robot to understand the human behaviour is valuable, and the eyes and their movement undoubtedly hold valuable information. In this paper we propose a novel algorithm for intention recognition using eye tracking in human robot collaboration. We first explore how the Cascade Effect hypothesis and a LSTM-based machine learning model perform to classify intent from gaze. Second, an algorithm is proposed, which can be used in a real time interaction to infer intention from the human user with a small uncertainty. A data collection with 30 participants was conducted in virtual reality to train and test the algorithm. The algorithm allows to detect the user intention up to two seconds before any user action with a success rate of up to 75%. These results open the possibility to study human robot interaction, where the robot can take the initiative based on the intention recognition. © 2021, IFIP International Federation for Information Processing.",Eye tracking; Human-robot interaction; Intention recognition,Abstract,True,
Scopus,journalPaper,2021,"Global Scene Filtering, Exploration, and Pointing in Occluded Virtual Space",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Target acquisition in an occluded environment is challenging given the omni-directional and first-person view in virtual reality (VR). We propose Solar-Casting, a global scene filtering technique to manage occlusion in VR. To improve target search, users control a reference sphere centered at their head through varied occlusion management modes: Hide, SemiT (Semi-Transparent), Rotate. In a preliminary study, we find SemiT to be better suited for understanding the context without sacrificing performance by applying semi-transparency to targets within the controlled sphere. We then compare Solar-Casting to highly efficient selection techniques to acquire targets in a dense and occluded VR environment. We find that Solar-Casting performs competitively to other techniques in known environments, where the target location information is revealed. However, in unknown environments, requiring target search, Solar-Casting outperforms existing approaches. We conclude with scenarios demonstrating how Solar-Casting can be applied to crowded and occluded environments in VR applications. © 2021, IFIP International Federation for Information Processing.",,Abstract,True,
Scopus,journalPaper,2021,A Flexible and Adaptable Workflow to Develop and Visualise Industrial Digital Twins,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"A workflow for building a Digital Twin with a 3D interface for VR and desktop output was developed by focusing on the interaction with the virtual model, an easy deployment of the framework and a straightforward reusability. The case study was conceived for supercomputing datacenters, but its main strength lies in flexibility and adaptability. © 2021, IFIP International Federation for Information Processing.",3DWeb; Digital twin; Industry 4.0; Virtual reality,Keywords,True,
Scopus,journalPaper,2021,"18th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2021",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 247 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Emotion Elicitation Techniques in Virtual Reality; expanding Affective Computing Paradigms Through Animistic Design Principles; preface; Human-Centered AI: A New Synthesis; AI-Based Clinical Decision Support Tool on Mobile Devices for Neurodegenerative Diseases; COBO: A Card-Based Toolkit for Co-Designing Smart Outdoor Experiences with People with Intellectual Disability; digital Producers with Cognitive Disabilities: Participatory Video Tutorials as a Strategy for Supporting Digital Abilities and Aspirations; dot-to-Dot: Pre-reading Assessment of Literacy Risk via a Visual-Motor Mechanism on Touchscreen Devices; exploring the Acceptability of Graphical Passwords for People with Dyslexia; intraVox: A Personalized Human Voice to Support Users with Complex Needs in Smart Homes; LIFT: An eLearning Introduction to Web Search for Young Adults with Intellectual Disability in Sri Lanka; social Robots in Learning Experiences of Adults with Intellectual Disability: An Exploratory Study; co-designing Tangible Break Reminders with People with Repetitive Strain Injury; exploring User Requirements for an Exoskeleton Arm Insights from a User-Centered Study with People Living with Severe Paralysis; multisensory Experiences: Where the Senses Meet Technology; interactive Modular Tactile Maps of Rooms for Older Adults with Vision Impairments; recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study; VR Invite: A Project-Independent Smartphone App for VR Observation and Interactivity; “Honestly I Never Really Thought About Adding a Description”: Why Highly Engaged Tweets Are Inaccessible; ‘Did You See That!?’ Enhancing the Experience of Sports Media Broadcast for Blind People.,,Abstract,True,Duplicate
Scopus,journalPaper,2021,Experiencing Contemporary Art at a Distance,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This panel wants to start a discussion about the importance of designing new ways of Contemporary Art digitization and digitalization to foster the creation of successful user experiences for its remote fruition. © 2021, IFIP International Federation for Information Processing.",Contemporary art; Cross-disciplinary collaboration; Digital job skills; Digitalization; Digitization; Extended reality,Keywords,True,
Scopus,journalPaper,2021,A Proposal for Discreet Auxiliary Figures for Reducing VR Sickness and for Not Obstructing FOV,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We aim to reduce the virtual reality (VR) sickness by superimposing discreet auxiliary figures: “gazing point” on the center of the user’s field of view, “dots” on the four corners of the field of view, “user’s horizontal line” links a line to the user’s head movement, and “real world’s horizontal line” links to the real-world horizontal line. We conducted an experiment to evaluate the degree to which these figures could reduce VR sickness in a VR environment on a head-mounted display (HMD) by using Simulator Sickness Questionnaire (SSQ) and skin conductance (SC) on the user’s palm. The results show that the VR sickness tended to reduce by the superimposition of “dots.” © 2021, IFIP International Federation for Information Processing.",Auxiliary figure; Simulator Sickness Questionnaire; Skin conductance; Virtual reality sickness,Abstract_Keywords,True,
Scopus,journalPaper,2021,Field of View Limitation-Driven Design of a Mixed Reality Game for Heritage Sites,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this work, we describe the design of a customized user interface (UI) for a mixed reality application in a heritage site. The visuals, widgets and spatial interaction techniques have been customized to improve the user experience without diverting the user’s attention by minimizing the effect of the limited field of view (FOV) of the see-through head-mounted display (HMD) used, Microsoft HoloLens v1. The approach consists in using diegetic elements that are coherent with the narrative of the heritage site, and widgets and augmented layers always entirely included in the FOV of the see-through display. © 2021, IFIP International Federation for Information Processing.",Limited FOV; Mixed reality; User interface design,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,When Friends Become Strangers: Understanding the Influence of Avatar Gender on Interpersonal Distance in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper, we investigate how mismatches between biological gender and avatar gender affect interpersonal distance (IPD) in virtual reality (VR). An increasing number of VR experiences and online platforms like Rec Room and VRChat allow users to assume other genders through customized avatars. While the effects of acquaintanceship and gender have been studied with regard to proxemic behavior, the effect of changed genders remains largely unexplored. We conducted a user study (N = 40, friends = 20, strangers = 20) where users played a two-player collaborative game in Rec Room using both male and female avatars. We found that with swapped avatar genders, the preferred distance increased between friends but not between strangers. We discuss how our results can inform researchers and designers in the domain of multi-user VR. © 2021, IFIP International Federation for Information Processing.",,Title_Abstract,True,
Scopus,journalPaper,2021,"18th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2021",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 247 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Emotion Elicitation Techniques in Virtual Reality; expanding Affective Computing Paradigms Through Animistic Design Principles; preface; Human-Centered AI: A New Synthesis; AI-Based Clinical Decision Support Tool on Mobile Devices for Neurodegenerative Diseases; COBO: A Card-Based Toolkit for Co-Designing Smart Outdoor Experiences with People with Intellectual Disability; digital Producers with Cognitive Disabilities: Participatory Video Tutorials as a Strategy for Supporting Digital Abilities and Aspirations; dot-to-Dot: Pre-reading Assessment of Literacy Risk via a Visual-Motor Mechanism on Touchscreen Devices; exploring the Acceptability of Graphical Passwords for People with Dyslexia; intraVox: A Personalized Human Voice to Support Users with Complex Needs in Smart Homes; LIFT: An eLearning Introduction to Web Search for Young Adults with Intellectual Disability in Sri Lanka; social Robots in Learning Experiences of Adults with Intellectual Disability: An Exploratory Study; co-designing Tangible Break Reminders with People with Repetitive Strain Injury; exploring User Requirements for an Exoskeleton Arm Insights from a User-Centered Study with People Living with Severe Paralysis; multisensory Experiences: Where the Senses Meet Technology; interactive Modular Tactile Maps of Rooms for Older Adults with Vision Impairments; recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study; VR Invite: A Project-Independent Smartphone App for VR Observation and Interactivity; “Honestly I Never Really Thought About Adding a Description”: Why Highly Engaged Tweets Are Inaccessible; ‘Did You See That!?’ Enhancing the Experience of Sports Media Broadcast for Blind People.,,Abstract,True,Duplicate
Scopus,journalPaper,2021,An Immersive Approach Based on Two Levels of Interaction for Exploring Multiple Coordinated 3D Views,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Multiple coordinated views have often been used for visual analytics purposes over the last years. In this context, if the exploration of 2D visualizations is not an obstacle, adding an extra dimension can be an issue. The interaction with multiple 3D visualizations in 2D conventional displays lacks usability and does not guarantee the usefulness the extra dimension would provide. Immersive visualization techniques can potentially fulfill these gaps by providing 3D visualizations and novel 3D interactions simultaneously. In this paper, we propose a new approach for interacting with composite and multiple coordinated visualizations in immersive virtual environments. We use a 3D-WIMP-like concept, i.e., virtual cubes (Spaces), for encapsulating views, which the user can freely control in the virtual environment. Moreover, operations like “cloning” and “coordinated interactions” features provide a way for performing composed tasks. We compared our approach with a desktop version to evaluate its performance when dealing with composed tasks. A user study with 19 participants was conducted, and the results show that the immersive approach has advantages over the corresponding desktop version regarding interaction with multiple coordinated 3D views. © 2021, IFIP International Federation for Information Processing.",Immersive analytics; Multiple coordinated views; Virtual reality,Keywords,True,
Scopus,journalPaper,2021,VRQUEST: Designing and Evaluating a Virtual Reality System for Factory Training,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Training is vital in factories to ensure the quality of workers’ technical expertise but can be costly due to various physicality constrains. The emergence of virtual reality (VR) opens the opportunities to address this issue by providing affordable solutions where workers can freely learn by doing without compromising safety or risking damage. This paper presents an industrial research project carried out in a company, focusing on designing and evaluating a VR application for training a procedural task in factories. Insights from the evaluation suggest multiple design considerations that need to be taken into account in designing future VR interfaces for factory training in the company. © 2021, IFIP International Federation for Information Processing.",Factory training; Immersive environment; VR,Title_Abstract,True,
Scopus,journalPaper,2021,A Trajectory Model for Desktop-Scale Hand Redirection in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In Virtual Reality, visuo-haptic illusions such as hand redirection introduce a discrepancy between the user’s hand and its virtual avatar. This visual shift can be used, for instance, to provide multiple virtual haptic objects through a single physical proxy object. This low-cost approach improves the sense of presence, however, it is unclear how these illusions impact the hand trajectory and if there is a relationship between trajectory and the detection of illusion. In this paper, we present an empirical model predicting the hand trajectory as a function of the redirection. It relies on a cubic Bézier curve with 4 control points. We conduct a two alternative forced choice (2AFC) experiment to calibrate and validate our model. Results show that (1) our model predicts well the hand trajectory of each individual using a single parameter; (2) the hand trajectory better explains the detection of the illusion than the amplitude of the redirection alone; (3) a user specific calibration allows to predict per-user redirected trajectories and detection probabilities. Our findings provide a better understanding of visuo-haptic illusions and how they impact the user’s movements. As such they may provide foundations to design novel interaction techniques, e.g. interacting in a scene with multiple physical obstacles. © 2021, IFIP International Federation for Information Processing.",Detection threshold; Hand redirection; Trajectory estimation; Visuo-haptic illusion,Title_Abstract,True,
Scopus,journalPaper,2021,Gesture Interaction in Virtual Reality: A Low-Cost Machine Learning System and a Qualitative Assessment of Effectiveness of Selected Gestures vs. Gaze and Controller Interaction,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We explore gestures as interaction methods in virtual reality (VR). We detect hand and body gestures using human pose estimation based on off-the-shelf optical camera images using machine learning, and obtain reliable gesture recognition without additional sensors. We then employ an avatar to prompt users to learn and use gestures to communicate. Finally, to understand how well gestures serve as interaction methods, we compare the studied gesture-based interaction methods with baseline common interaction modalities in VR (controllers, gaze interaction) in a pilot study including usability testing. © 2021, IFIP International Federation for Information Processing.",Gestures; Interaction; Usability; VR,Title_Abstract,True,
Scopus,journalPaper,2021,Typing on Midair Virtual Keyboards: Exploring Visual Designs and Interaction Styles,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We investigate typing on a QWERTY keyboard rendered in virtual reality. Our system tracks users’ hands in the virtual environment via a Leap Motion mounted on the front of a head mounted display. This allows typing on an auto-correcting midair keyboard without the need for auxiliary input devices such as gloves or handheld controllers. It supports input via the index fingers of one or both hands. We compare two keyboard designs: a normal QWERTY layout and a split layout. We found users typed at around 16 words-per-minute using one or both index fingers on the normal layout, and about 15 words-per-minute using both index fingers on the split layout. Users had a corrected error rate below 2% in all cases. To explore midair typing with limited or no visual feedback, we had users type on an invisible keyboard. Users typed on this keyboard at 11 words-per-minute at an error rate of 3.3% despite the keyboard providing almost no visual feedback. © 2021, IFIP International Federation for Information Processing.",Head mounted display (HMD); Text entry; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,Empirical Evaluation of Moving Target Selection in Virtual Reality Using Egocentric Metaphors,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual hand or pointer metaphors are among the key approaches for target selection in immersive environments. However, targeting moving objects is complicated by factors including target speed, direction, and depth, such that a basic implementation of these techniques might fail to optimize user performance. We present results of two empirical studies comparing characteristics of virtual hand and pointer metaphors for moving target acquisition. Through a first study, we examine the impact of depth on users’ performance when targets move beyond and within arms’ reach. We find that movement in depth has a great impact on both metaphors. In a follow-up study, we design a reach-bounded Go-Go (rbGo-Go) technique to address challenges of virtual hand and compare it to Ray-Casting. We find that target width and speed are significant determinants of user performance and we highlight the pros and cons for each of the techniques in the given context. Our results inform the UI design for immersive selection of moving targets. © 2021, IFIP International Federation for Information Processing.",,Title,True,
Scopus,journalPaper,2021,An Empirical Study of Picture Password Composition on Smartwatches,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Recent research works suggest that human cognitive differences affect security and usability of picture passwords within a variety of interaction contexts, such as conventional desktops, smartphones, and extended reality. However, the interplay of human cognition towards users’ interaction behavior and security of picture passwords on smartwatch devices has not been investigated so far. In this paper, we report on such a research attempt that embraced a between-subjects in-lab user study (n = 50) in which users were classified according to their cognitive processing characteristics (i.e., Field Dependence-Independence cognitive differences), and further composed a picture password on a smartwatch device. Analysis of results reveal that already known effects of human cognition towards interaction behavior and security of picture passwords within conventional interaction contexts, do not necessarily replicate when these are deployed on smartwatch devices. Findings point towards the need to design for diversity and device-aware picture password schemes. © 2021, IFIP International Federation for Information Processing.",Efficiency; Graphical authentication; Human cognition; Security,Abstract,True,
Scopus,journalPaper,2021,Exploring How Saliency Affects Attention in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper, we investigate how changes in the saliency of the Virtual Environment (VE) affect our visual attention during different tasks. We investigate if users are attracted to the most salient regions in the VE. This knowledge will help researchers design optimal VR environments, purposefully direct the attention of users, and avoid unintentional distractions. We conducted a user study (N = 30) where participants performed tasks (video watching, object stacking, visual search, waiting) with two different saliency conditions in the virtual environment. Our findings suggest that while participants notice the differences in saliency, their visual attention is not diverted towards the salient regions when they are performing tasks. © 2021, IFIP International Federation for Information Processing.",Saliency; Virtual Reality; Visual attention,Title_Keywords,True,
Scopus,journalPaper,2021,VRTactileDraw: A Virtual Reality Tactile Pattern Designer for Complex Spatial Arrangements of Actuators,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Creating tactile patterns on the body via a spatial arrangement of many tactile actuators offers many opportunities and presents a challenge, as the design space is enormous. This paper presents a VR interface that enables designers to rapidly prototype complex tactile interfaces. It allows for painting strokes on a modeled body part and translates these strokes into continuous tactile patterns using an interpolation algorithm. The presented VR approach avoids several problems of traditional 2D editors. It realizes spatial 3D input using VR controllers with natural mapping and intuitive spatial movements. To evaluate this approach in detail, we conducted a user study and iteratively improved the system. The study participants gave predominantly positive feedback on the presented VR interface (SUS score 79.7, AttrakDiff “desirable”). The final system is released alongside this paper as an open-source Unity project for various tactile hardware. © 2021, IFIP International Federation for Information Processing.",Design interface; Design tool; Spatial input; Tactile feedback; Tactile pattern design; Tactile patterns; VR tool,Title,True,
Scopus,journalPaper,2021,Contemporary Art Digitalization: An Opportunity for Designing New Experiences,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"CLOUDART is a cross-disciplinary working group that joins the forces of three specific domain expert communities: Contemporary Art, Human-Computer Interaction, and Multimedia Software Engineering, all experienced in socio-technical creative projects. CLOUDART initiative aims to investigate how to design new hybrid user experiences for the online fruition of Contemporary Art. © 2021, IFIP International Federation for Information Processing.",Art digitalization; Extended reality; Human-computer interaction; Interaction design; User experience,Keywords,True,
Scopus,journalPaper,2021,Visualization of User’s Behavior in Indoor Virtual Environments Through Interactive Heatmaps,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Three-dimensional virtual environments (VEs), such as those used in video games and virtual reality experiences, pose new challenges to the study of user’s behavior. This paper proposes a system based on the combination of two heatmaps for the analysis of user’s movement in the VE and of the areas looked at by him/her. It also describes a pilot study aimed at assessing the efficacy of the system. Results of the study indicate that the system can effectively support analysts in identifying user’s look-at behaviors as well as navigation strategies, patterns, and coverage of specific areas during movement. © 2021, IFIP International Federation for Information Processing.",Heatmap; Information visualization; User behavior; Virtual environment; Visual analytics,Abstract,True,
Scopus,journalPaper,2021,Multisensory Augmented Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Multisensory augmented reality enables content developers to generate more realistic, sensory rich user experiences. Applications and research related to multisensory augmented reality expands through several areas such as education, medicine, human-machine interactions, human-food interactions, marketing, and neuroscience. Aim of this workshop is to gather researchers and industry representatives who are involved in multisensory augmented reality research to discuss the current state-of-the-art in the field, define future research directions, form new collaborations, and come up with future publication plans. We believe that this workshop would enhance the participants’ experience of the Interact 2021 and encourage more participants to attend the main conference. © 2021, IFIP International Federation for Information Processing.",HCI; Multimodal interfaces; Multisensory augmented reality; Multisensory internet; Multisensory user experiences,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Emotion Elicitation Techniques in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper, we explore how state-of-the-art methods of emotion elicitation can be adapted in virtual reality (VR). We envision that emotion research could be conducted in VR for various benefits, such as switching study conditions and settings on the fly, and conducting studies using stimuli that are not easily accessible in the real world such as to induce fear. To this end, we conducted a user study (N = 39) where we measured how different emotion elicitation methods (audio, video, image, autobiographical memory recall) perform in VR compared to the real world. We found that elicitation methods produce largely comparable results between the virtual and real world, but overall participants experience slightly stronger valence and arousal in VR. Emotions faded over time following the same pattern in both worlds. Our findings are beneficial to researchers and practitioners studying or using emotional user interfaces in VR. © 2021, IFIP International Federation for Information Processing.",Elicitation methods; Emotions; User studies; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Character Input in Augmented Reality: An Evaluation of Keyboard Position and Interaction Visualisation for Head-Mounted Displays,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Character input in immersive environments is a non trivial task that has attracted much attention in recent years. This paper presents an evaluation of keyboard position, orientation and interaction together with the influence of visual interaction feedback in a controlled character input task with 27 participants in Augmented Reality (AR). It presents 5 different keyboard locations (3 bounded to the headset and 2 bounded to the non-dominant hand of the user) and 3 visual interaction feedback methods (finger raycast, fingertip glow and both combined). Objective (completion time, accuracy, Key per Minute (KPM)) and subjective (After Scenario Questionnaire (ASQ)) metrics are presented. Results showed that keyboard placement had an effect on accuracy, KPM metrics and subjective preference, with keyboard visualisation parallel and bounded to the headset position and orientation outperforming other keyboard locations. © 2021, IFIP International Federation for Information Processing.",Augmented reality; Text input; Usability; User evaluation; Visualisation,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Simplifying Robot Programming Using Augmented Reality and End-User Development,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Robots are widespread across diverse application contexts. Teaching robots to perform tasks, in their respective contexts, demands a high domain and programming expertise. However, robot programming faces high entry barriers due to the complexity of robot programming itself. Even for experts robot programming is a cumbersome and error-prone task where faulty robot programs can be created, causing damage when being executed on a real robot. To simplify the process of robot programming, we combine Augmented Reality (AR) with principles of end-user development. By combining them, the real environment is extended with useful virtual artifacts that can enable experts as well as non-professionals to perform complex robot programming tasks. Therefore, Simple Programming Environment in Augmented Reality with Enhanced Debugging (SPEARED) was developed as a prototype for an AR-assisted robot programming environment. SPEARED makes use of AR to project a robot as well as a programming environment onto the target working space. To evaluate our approach, expert interviews with domain experts from the area of industrial automation, robotics, and AR were performed. The experts agreed that SPEARED has the potential to enrich and ease current robot programming processes. © 2021, IFIP International Federation for Information Processing.",Augmented Reality; Robot programming; Usability,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"Exploring the Visual Space to Improve Depth Perception in Robot Teleoperation Using Augmented Reality: The Role of Distance and Target’s Pose in Time, Success, and Certainty",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Accurate depth perception in co-located teleoperation has the potential to improve task performance in manipulation and grasping tasks. We thus explore the operator’s visual space and design visual cues using augmented reality. Our goal is to facilitate the positioning of the gripper above a target object before attempting to grasp it. The designs we propose include a virtual circle (Circle), virtual extensions (Extensions) from the gripper’s fingers, and a color matching design using a real colormap with matching colored virtual circles (Colors). We conducted an experiment to evaluate these designs and the influence of distance from the operator to the workspace and the target object’s pose. We report on time, success, and perceived certainty in a grasping task. Our results show that a shorter distance leads to higher success, faster grasping time, and higher certainty. Concerning the target object’s pose, a clear pose leads to higher success and certainty but interestingly slower task times. Regarding the design of cues, our results reveal that the simplicity of the Circle cue leads to the highest success and outperforms the most complex cue Colors also for task time, while the level of certainty seems to be depending more on the distance than the type of cue. We consider that our results can serve as an initial analysis to further explore these factors both when designing to improve depth perception and within the context of co-located teleoperation. © 2021, IFIP International Federation for Information Processing.",Augmented reality; Certainty; Depth perception; Human-robot interaction; Robot teleoperation; Visual cues,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Projection Grid Cues: An Efficient Way to Perceive the Depths of Underground Objects in Augmented Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Augmented Reality is increasingly used for visualizing underground networks. However, standard visual cues for depth perception have never been thoroughly evaluated via user experiments in a context involving physical occlusions (e.g., ground) of virtual objects (e.g., elements of a buried network). We therefore evaluate the benefits and drawbacks of two techniques based on combinations of two well-known depth cues: grid and shadow anchors. More specifically, we explore how each combination contributes to positioning and depth perception. We demonstrate that when using shadow anchors alone or shadow anchors combined with a grid, users generate 2.7 times fewer errors and have a 2.5 times lower perceived workload than when only a grid or no visual cues are used. Our investigation shows that these two techniques are effective for visualizing underground objects. We also recommend the use of one technique or another depending on the situation. © 2021, IFIP International Federation for Information Processing.",Augmented reality; Depth cues; Projection techniques; Underground objects; Visualization,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,VR Invite: A Project-Independent Smartphone App for VR Observation and Interactivity,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual Reality (VR) is a promising immersive technology, which provides users with place and plausibility illusions in a virtual environment (VE). However, current immersive experiences are often limited to those users wearing a VR head-mounted display (HMD). In this paper we present VR Invite, a project-independent smartphone app, which allows multiple non-immersive bystanders to observe and interact with the VE and the HMD users. Our system renders multiple view ports of the scene on a host computer, and transmits the data via wireless local network to the mobile devices. Furthermore, the position and orientation of the smartphones is tracked to change the viewpoints accordingly. We conducted a user study with 26 participants in the context of rehabilitation for older adults in retirement homes, with a focus on bystander integration. In the study, a VR user had to play multiple rounds of a memory game, while a bystander provided support. We compared VR Invite with a TV-gamepad-combination as interaction medium for the support role regarding sense of presence, social presence, workload and usability, both with purely verbal and active assistance capabilities. The results indicate that the opportunity for direct interaction positively influences the bystander’s sense of presence in the VE and the reported usability of the Smartphone app. However, social presence was rated higher in passive conditions in which the real person was the center of attention, as opposed to the avatar on the screen. Furthermore, users valued the comfort of sitting down over active participation and agency with room-scale movement. © 2021, IFIP International Federation for Information Processing.",Collaborative Virtual Environments; Multi-user mixed reality interactions; Smartphones; Social VR; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2021,S.A.M.I.R.: Supporting Tele-Maintenance with Integrated Interaction Using Natural Language and Augmented Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Remote maintenance is becoming a crucial aspect in the industrial life cycle, especially in context where different countries, languages, and time zones are involved. Supplying on-site operators with smart tools to rapidly and easily request support can boost maintenance procedure execution times and solving of unexpected problems, can reduce the number of interventions and can speed up novice training, resulting in cost reduction and equipment use maximization. Authoring tools are needed to properly create new multimedia content and to process, organize and index legacy company knowledge, regardless of type. In this paper the SAMIR solution is presented, it defines a SaaS platform integrating Augmented Reality and Natural Language Processing to supply an effective support during maintenance intervention together with authoring tools to create multimedia content and procedures. © 2021, IFIP International Federation for Information Processing.",Architecture for AR services; Augmented reality; Middleware; Natural language interaction; Natural language processing; Tele-maintenance,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Encouraging Chemistry Learning Through an Augmented Reality Magic Game,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Chemistry is often labelled as a difficult subject, which challenges come mostly from topics that require abstract thinking, leading to a negative attitude by the learners. Studies show that augmented reality (AR) is potentially useful for education purposes, particularly in the representation of abstract concepts by depicting them through virtual objects. This paper presents an AR educational mobile game, named “Magic Elements”, targeting children from 9 to 12 years old. The game was designed to promote a positive attitude towards chemistry, teaching children basic concepts while creating climate change awareness. As climate change is having a profound impact on Earth and urgent actions need to be taken, the game also addresses this topic relating chemistry with the real world and promoting sustainable habits among children from an early age. Tangibles and storytelling are explored to engage and sensitize children. Our findings show a positive outcome, as well as a considerable increase in the acquired chemistry knowledge and awareness towards climate change. © 2021, IFIP International Federation for Information Processing.",Augmented reality; Chemistry education; Climate change; Learning; Mobile game; Storytelling; Tangible interfaces,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,The Effect of Rhythm in Mid-air Gestures on the User Experience in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this work, we examine the effect of mid-air gesture rhythm on user experience in Virtual Reality. In particular, we investigate gesture regularity, speed and highlighting with a sound guide. We measure the effect of these components on the perceived fatigue, presence, difficulty, success and helpfulness. Our findings indicate that an irregular and slow rhythm leads to a lower arm fatigue. We also find that such an irregular rhythm could increase the user perceived difficulty of the task and the absence of a sound guide could decrease the sense of presence. © 2021, IFIP International Federation for Information Processing.",Mid-air gestures; Muscular fatigue; Presence; Rhythm; Sound guide; User experience; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Looking for Info: Evaluation of Gaze Based Information Retrieval in Augmented Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This paper presents the results of an empirical study and a real-world deployment of a gaze-adaptive UI for Augmented Reality (AR). AR introduces an attention dilemma between focusing on the reality vs. on AR content. Past work suggested eye gaze as a technique to open information interfaces, however there is only little empirical work. We present an empirical study comparing gaze-adaptive to an always-on interface in tasks that vary focus between reality and virtual content. Across tasks, we find most participants prefer the gaze-adaptive UI and find it less distracting. When focusing on reality, the gaze UI is faster, perceived as easier and more intuitive. When focusing on virtual content, always-on is faster but user preferences are split. We conclude with the design and deployment of an interactive application in a public museum, demonstrating the promising potential in the real world. © 2021, IFIP International Federation for Information Processing.",Adaptive UI; AR; Eye-tracking; Gaze interaction,Title_Abstract,True,
Scopus,journalPaper,2021,On the Use of Handheld Augmented Reality for Inventory Tasks: A Study with Magazine Retailers,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper we investigate if handheld augmented reality, in the form of an application running on a mainstream smartphone, can serve as a practical and effective tool for inventory tasks. Taking magazine retail as an example, we have applied a user-centered design process to research, design, implement and evaluate a handheld AR application prototype. We conducted a qualitative user study at magazine retail stores, where staff responsible for magazines were interviewed (n= 8 ) and their primary magazine handling tasks observed. After an analysis of the study findings, we selected a key task as the basis for the design, implementation and test of an AR app prototype. The task consisted of collecting and registering a list of magazines for return to the distributor. We evaluated the AR app prototype in a user study (n= 22 ), where participants used it to perform the selected task. They also performed the task using the paper list currently in use, and a second, simplified app prototype, without AR features. Task performance was measured based on time and error rate. The participant’s subjective experience was also captured in the form of a post-task survey and interview. Our findings suggest that handheld AR can prove effective when used for specific, focused tasks, rather than more open-ended ones. © 2021, IFIP International Federation for Information Processing.",Augmented reality; Handheld devices; User studies; User-centered design,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Head Mounted Display Interaction Evaluation: Manipulating Virtual Objects in Augmented Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Augmented Reality (AR) is getting close to real use cases, which is driving the creation of innovative applications and the unprecedented growth of Head-Mounted Display (HMD) devices in consumer availability. However, at present there is a lack of guidelines, common form factors and standard interaction paradigms between devices, which has resulted in each HMD manufacturer creating their own specifications. This paper presents the first experimental evaluation of two AR HMDs evaluating their interaction paradigms, namely we used the HoloLens v1 (metaphoric interaction) and Meta2 (isomorphic interaction). We report on precision, interactivity and usability metrics in an object manipulation task-based user study. 20 participants took part in this study and significant differences were found between interaction paradigms for translation tasks, where the isomorphic mapped interaction outperformed the metaphoric mapped interaction in both time to completion and accuracy, while the contrary was found for the resize task. From an interaction perspective, the isomorphic mapped interaction (using the Meta2) was perceived as more natural and usable with a significantly higher usability score and a significantly lower task-load index. However, when task accuracy and time to completion is key mixed interaction paradigms need to be considered. © 2019, IFIP International Federation for Information Processing.",Augmented Reality; Hand interaction; Natural interaction,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Same same but different: Exploring the effects of the stroop color word test in virtual reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual reality (VR) is used for different trainings e.g. for pilots, athletes, and surgeons. Dangerous and difficult situations are often focused in such simulations in VR, targeting to learn how to perform well under stress. However, there has been little work on understanding stress perception in VR compared to the real-world situation. In this paper we present an investigation of how users experience a stressful task in VR compared to in a classic office environment. Specifically, we investigate the subjective stress experience and physiological arousal with 15 participants performing the Stroop color word test either on a regular desktop screen, in VR, or in VR requiring head movements. Our findings suggest that stressful tasks are perceived less stressful when being performed in VR compared to the real environment as long as there is no additional stress factor, such as head movement involved. Our work indicates that it may be valuable to transfer stressful tasks, currently done in traditional office environments into VR. © 2019, IFIP International Federation for Information Processing.",Human-computer interaction; Stress; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"18th IFIP TC 13 International Conference on Human-Computer Interaction, INTERACT 2021",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,The proceedings contain 247 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Emotion Elicitation Techniques in Virtual Reality; expanding Affective Computing Paradigms Through Animistic Design Principles; preface; Human-Centered AI: A New Synthesis; AI-Based Clinical Decision Support Tool on Mobile Devices for Neurodegenerative Diseases; COBO: A Card-Based Toolkit for Co-Designing Smart Outdoor Experiences with People with Intellectual Disability; digital Producers with Cognitive Disabilities: Participatory Video Tutorials as a Strategy for Supporting Digital Abilities and Aspirations; dot-to-Dot: Pre-reading Assessment of Literacy Risk via a Visual-Motor Mechanism on Touchscreen Devices; exploring the Acceptability of Graphical Passwords for People with Dyslexia; intraVox: A Personalized Human Voice to Support Users with Complex Needs in Smart Homes; LIFT: An eLearning Introduction to Web Search for Young Adults with Intellectual Disability in Sri Lanka; social Robots in Learning Experiences of Adults with Intellectual Disability: An Exploratory Study; co-designing Tangible Break Reminders with People with Repetitive Strain Injury; exploring User Requirements for an Exoskeleton Arm Insights from a User-Centered Study with People Living with Severe Paralysis; multisensory Experiences: Where the Senses Meet Technology; interactive Modular Tactile Maps of Rooms for Older Adults with Vision Impairments; recommendations for the Development of a Robotic Drinking and Eating Aid - An Ethnographic Study; VR Invite: A Project-Independent Smartphone App for VR Observation and Interactivity; “Honestly I Never Really Thought About Adding a Description”: Why Highly Engaged Tweets Are Inaccessible; ‘Did You See That!?’ Enhancing the Experience of Sports Media Broadcast for Blind People.,,Abstract,True,Duplicate
Scopus,journalPaper,2019,"17th IFIP TC13 International Conference on Human-Computer Interaction, INTERACT 2019",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The proceedings contain 213 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Experiencing Materialized Reading: Individuals’ Encounters with Books; “I Kept Browsing and Browsing, But Still Couldn’t Find the One”: Salient Factors and Challenges in Online Typeface Selection; integrating a Binaural Beat into the Soundscape for the Alleviation of Feelings; what Is Beautiful Continues to Be Good: People Images and Algorithmic Inferences on Physical Attractiveness; design and Evaluation of Three Interaction Models for Manipulating Internet of Things (IoT) Devices in Virtual Reality; head Mounted Display Interaction Evaluation: Manipulating Virtual Objects in Augmented Reality; on the Use of Persistent Spatial Points for Deploying Path Navigation in Augmented Reality: An Evaluation Study; User Experience Guidelines for Designing HMD Extended Reality Applications; am I Moving Along a Curve? A Study on Bicycle Traveling-In-Place Techniques in Virtual Environments; analysis of Utilization in the Message Card Production by Use of Fusion Character of Handwriting and Typeface; design and Evaluation of an Augmented Reality App for Learning Geometric Shapes in 3D; Enhance Engine Room Diagnostics Through Audio-Focused VR Simulation; Head-Controlled Menu in Mixed Reality with a HMD; VR Interaction Modalities for the Evaluation of Technical Device Prototypes; combining Tablets with Smartphones for Data Analytics; COMMONS: A Board Game for Enhancing Interdisciplinary Collaboration When Developing Health and Activity-Related Wearable Devices; on-Body Tangible Interaction: Using the Body to Support Tangible Manipulations for Immersive Environments; splitSlider: A Tangible Interface to Input Uncertainty; introduction to Automation and to Its Potential for Interactive Systems Design.",,Abstract,True,
Scopus,journalPaper,2021,Co-watching 360-Films in Nursing Homes,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This work investigates experiences and practical aspects of co-located and co-watched 360-videos in head mounted displays by groups of older people at nursing homes. In a study involving 19 residents at two different nursing homes, co-watching screenings were arranged with 360-videos produced in the local area by filmmakers. Data was collected through non-participant observation and semi-structured interviews with the participants. Input from nurses and facilitators were also collected. We found this to be a much appreciated, feasible, and enjoyable immersive experience improving short-term well-being, expressed through (e.g.) new conversations, pride in participation, and spontaneous movements. However, the value of co-watching was mainly captured for residents who already knew each other, and we found limited indications of virtual co-presence. We further recognized the value of the videos themselves and the desire for new 360-video experiences. But also, a need for better headsets suitable for older people and shared use at nursing homes to avoid social isolation due to the introduction of VR technology. © 2021, IFIP International Federation for Information Processing.",360-films; Co-watching; Nursing homes; Virtual reality,Keywords,True,
Scopus,journalPaper,2021,The Role of Mobile and Virtual Reality Applications to Support Well-Being: An Expert View and Systematic App Review,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Interactive technologies for autonomous mental health management are on the rise due to limited therapy access and stigma. However, most commercial mental health apps are neither theory-based nor clinically tested, and psychological theories are not easily accessible to app designers. Thus, it remains unclear if current mobile and VR mental health apps meet therapists’ expectations. To address this gap, we conducted interviews (N= 11 ) to build an understanding about current therapeutic practices with a focus on emotion regulation and their applicability to mobile apps. We then conducted a systematic app review of 60 mental-health-related mobile and VR apps applying the themes identified in our interviews as an understanding lens. We draw upon the identified discrepancies to pinpoint design implications for better embedding lived therapeutic practice into mental health apps. We contribute by providing a common grounding between therapists and developers on the features and properties of well-being mobile and VR apps. © 2021, IFIP International Federation for Information Processing.",Emotions; Feelings; Mental health; Mobile apps; Mood; Therapy; Virtual reality; Well-being,Title_Keywords,True,
Scopus,journalPaper,2021,Acceptance of an AR-Based In-Store Shopping Advisor - the Impact of Psychological User Characteristics,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We present a study on the acceptance of augmented reality-based product comparison and recommending in a physical store context. An online study was performed, in which a working prototype for head-mounted displays, developed in previous research, was used to showcase the concept. The survey included questionnaires to assess shopping behaviour, decision styles and propensity to adopt new technologies of the participants. A cluster analysis of these psychological traits reveals the existence of different types of customers, who also differ on their assessment of the system. While the technology adoption propensity index is the better predictor of the acceptance of an augmented reality shopping advisor, the results suggest that factors such as the user’s previous experience, a high experiential chronic shopping orientation, or an intuitive decision style have a significant impact on it as well. Thus, predicting user acceptance solely based on one of the investigated psychological traits may be unreliable, and studying them in conjunction can provide a more accurate estimation. © 2021, IFIP International Federation for Information Processing.",Augmented reality; Retailing; Shopping advisors; Technology acceptance,Abstract_Keywords,True,
Scopus,journalPaper,2021,Placement of Teleported Co-users in AR,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Teleportation and conversations with virtual representations of remote people have been made possible by recent developments in augmented reality (AR) technology. This paper aims at understanding how such AR telecommunication systems should be implemented by asking where to display 3D scans of potential remote users. As the perfect interaction design solution may be different while walking versus while staying in one place, we conducted a user study comparing both. We also varied the placement of the remote user in the co-user’s field of view (FoV) and where the coordinate system in which the 3D scan is visualized has its origin. We found that remote users we talk to should, in general, be visualized in AR in front of us, but in situations in which the physical world requires attention a visualization in the periphery is better. Re-placing the co-user through gestures is not desired, but the ability to look away from them should be supported, which strongly supports placing virtual co-users in AR relatively to the user’s body. © 2021, IFIP International Federation for Information Processing.",Augmented reality; Co-user; Hologram; Remote user visualization; Teleportation,Abstract_Keywords,True,
Scopus,journalPaper,2019,Design and Evaluation of an Augmented Reality App for Learning Geometric Shapes in 3D,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Augmented Reality (AR) is increasingly used as an educational tool in a range of domains with the goal of enhancing students’ performance as well as their learning experience, thanks to the interactivity and visual appeal of AR objects. While some attempts, albeit limited, have been undertaken to prove these beneficial effects in learning 3D geometry, the results remain inconclusive and there are some methodological issues such as under-evaluation of user experience. With the aim to enrich the applied body of knowledge on this specific topic, we developed an AR application that allows its users to learn about cross-sectional shapes and variables in 3D geometry. We compared the AR-based approach with the traditional chalk-and-board approach by involving sixty 12–16 year-olds from two schools. The AR class showed a significantly stronger learning effect than the traditional class, especially for the more complex geometric concepts. The AR class found the application engaging, regardless of their level of knowledge gain, which bore no significant relation with their intention to use it. The methodological challenge for implementing control groups and the practical challenge for affordable emerging educational tools should be tackled in future research. © 2019, IFIP International Federation for Information Processing.",Augmented Reality; Education; Geometry; User engagement; UX,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,VR Interaction Modalities for the Evaluation of Technical Device Prototypes,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Virtual Reality (VR) systems have reached the consumer market which offers new application areas. In our paper, we examine to what extent current VR technology can be used to perform local or remote usability tests for technical devices, e.g., coffee machines. For this, we put virtual prototypes of the technical devices into VR and let users interact with them. We implemented four interaction modalities that are suitable for low-level smartphone-based VR systems up to high fidelity, tethered systems with room-scale tracking. The modalities range from a simple gaze pointer to interaction with virtual hands. Our case study and its counter evaluation show that we are able to detect usability issues of technical devices based on their virtual prototypes in VR. However, the quality of the results depends on the matching between the task, the technical device, the prototype level, and the interaction modality. © 2019, IFIP International Federation for Information Processing.",Device assessment; Interaction pattern; Usability; Usability testing; Virtual prototype; Virtual Reality,Abstract_Keywords,True,
Scopus,journalPaper,2019,Am I Moving Along a Curve? A Study on Bicycle Traveling-In-Place Techniques in Virtual Environments,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"There are many techniques for locomotion and navigation that can support the exploration of large virtual environments in a limited physical area. Previous studies focused on measuring curvature gains and bending gains applied to the walking direction in the real world. However, the effects of different moving techniques and their relationship with shapes and patterns of virtually moving paths have not been studied extensively before. In this study, we present our experimental results on how users perceive two different traveling-in-place techniques with different bending gains of moving paths using a hybrid electric bike simulator. Moreover, the impact of different factors including road textures, road widths, and road curve directions and their relationships with the techniques are investigated. Generally, users could travel along a curve without noticing with a point of subjective equality (PSE) at bending angle and a just-noticeable difference (JND) of 0.75 for a movement at around 20 km/h during 5 s. In addition, movement technique, curve direction, and future travel path significantly affected how they perceived the curvature of their travel path. © 2019, IFIP International Federation for Information Processing.",Curve perception; Human perception; Locomotion; Redirected walking; Traveling-in-place; Virtual reality,Keywords,True,
Scopus,journalPaper,2019,Using Virtual Reality to Enable Individuals with Severe Visual Disabilities to Read Books,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this work, we present a bespoke assistive tool for people with severe visual disabilities. We are able to download text from books and present these books to our users in a virtual reality environment. This gives them specific capabilities to manipulate the text and factors such as brightness, size and contrast, in order for them to gain a comfortable reading experience. © 2019, IFIP International Federation for Information Processing.",Reading; Virtual Reality; Visual disabilities,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Design and Evaluation of Three Interaction Models for Manipulating Internet of Things (IoT) Devices in Virtual Reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"More and more things are getting connected to the internet, including lights, speakers, and refrigerators. These connected things are an example of what a smart home system that is part of the Internet of Things (IoT) can incorporate. IoT enables advanced services by interconnecting physical and virtual things. But, building interactive prototypes for smart home systems can be difficult and costly, since it involves a number of different devices and systems with varying technological readiness level. Virtual reality (VR) is a technology that can create computer-generated environments and has been used as a design tool in many different domains, such as architecture, city planning, and industrial design. However, the focus has traditionally been on visualizing design proposals rather than letting the intended users directly interact with them. Recently, we have seen an intensified development of VR headsets such as HTC Vive and Oculus Rift. These headsets come with relatively well-developed hand controllers, which can be used to interact with the virtual environment. This opens up opportunities to develop and evaluate interactive virtual smart home systems. This paper presents three interaction models developed and evaluated using the new generation of VR technology. The interaction models were then compared in a user study with 18 participants. Some statistically significant differences and subjective preferences could be observed in the quantitative and qualitative data respectively. The main contribution of this paper is to elucidate knowledge about using VR as a prototyping tool to explore IoT interaction. Moreover, this study implies that you can collect and analyze data for statistical analysis using VR. © 2019, IFIP International Federation for Information Processing.",Interaction; Internet of Things; Method; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Reducing Anxiety for Dental Visits,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This work addresses children’s (under 6 years of age) fear and apprehension to visit dental clinics. We present a bespoke interactive Virtual Reality reproduction of the physical dental clinic, augmented with virtual characters and enriched with gamification style information for a richer user experience. The experience allows the user to navigate and familiarise themselves with the location and the procedures they will undertake before visiting the clinic. The experience is now being piloted at the Dental Public Health at the University Hospital of Wales. © 2019, IFIP International Federation for Information Processing.",Children; Dental fear; Virtual Reality; Virtual tour,Abstract_Keywords,True,
Scopus,journalPaper,2019,On the Use of Persistent Spatial Points for Deploying Path Navigation in Augmented Reality: An Evaluation Study,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"People use various techniques and tools to perform spatial navigation tasks, such as asking a local person for instructions in order to reach a destination or creating a detailed route plan for their trip through technology-mediated tools. Novel technologies, such as augmented reality, have been introduced to improve the performance and enhance the experience of the end-users. To provide navigation experiences with improved accuracy and decent stability, the developers of such tools can use persistent spatial points, which are stationary points in the real world that an augmented-reality system should keep track of over time. However, the use of persistent spatial points can dramatically increase the development effort, as it requires additional and time-consuming actions to be made by the developers. In this paper, we investigate the use of persistent spatial points for navigation in an AR environment from a developer and an end-user perspective, aiming to understand the trade-off between the development effort, the user performance, and the user experience. We report an empirical study in which a software engineer developed two versions of an augmented-reality navigation application (one with persistent spatial points and one without) which were used by twenty-eight individuals to navigate. Our study results revealed a trade-off between the development effort, the user performance, and the user experience, which depends on the length of the navigation path. The shorter the path is, the less the need for persistent spatial points is, while, on the other hand, the longer the path is, the more critical it is to use persisting spatial points. Based on the results, we discuss ways of mitigating the development effort while maintaining high user performance and experience. © 2019, IFIP International Federation for Information Processing.",Augmented reality; Empirical study; Evaluation; Microsoft HoloLens; Persistent spatial points; Spatial anchors; Spatial navigation; User study,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Head-Controlled Menu in Mixed Reality with a HMD,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We present a design-space and three new techniques for head-based interaction with menus in Mixed Reality (MR) with a Head-Mounted Display (HMD). Usual input modalities such as hand gestures and voice commands are not suitable in noisy MR contexts where the users have both hands occupied as in augmented surgery and machine maintenance. To address the two issues of noisy MR contexts and hand-free interaction, we systematically explore the design space of head-controlled menu interaction by considering two design factors: (1) head-controlled menu versus head-controlled cursor (2) virtual targets versus mixed targets anchored on physical objects. Based on the design space, we present three novel menu techniques that we compared with a baseline head-controlled cursor technique. Experimental results suggest that head-controlled menu and head-controlled cursor techniques offer similar performance. In addition, the study found that mixed targets do not impact ultimate user performance when users are trained enough, but improve the learning phase. When using virtual targets, users still progressed after the training phase by reducing their mean selection time by 0.84 s. When using mixed targets, the improvement was limited to 0.3 s. © 2019, IFIP International Federation for Information Processing.",HMD; Menu technique; Mixed Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,"Memories of Carvalhal’s Palace: Haunted Encounters, a Museum Experience to Engage Teenagers",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"While museums are making great efforts in updating their communication and engagement strategies to include a wide variety of publics, teenagers (15–19) are still identified as an audience group that is often excluded from a museum’s curatorial strategies. As consequence, this audience appears to be generally disinterested in what museums might offer. Our installation, deployed at the Natural History Museum of Funchal (NHMF), in Portugal, makes use of mobile interactive technologies and gaming strategies to promote more engaging museum experiences for teenage visitors. Memories of Carvalhal’s Palace: Haunted Encounters is a location-based game that prompts teenagers to uncover the science in the museum, through investigating the site, which is presented as haunted. In order to complete the quest, the audience needs to find and collect scientific information about specific exhibits while interacting with their Augmented Reality (AR) three-dimensional (3D) models. The audience’s interactions with the museum exhibits are rewarded with pieces of a map, which when completed, will guide them to the hidden scientific library where they can finally unlock the mysteries they have been trying to solve. © 2019, IFIP International Federation for Information Processing.",3D models; Augmented Reality; Gaming; Museums; Teenagers,Abstract_Keywords,True,
Scopus,journalPaper,2019,Multi-level Engagement in Augmented Reality Children’s Picture Books,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We demonstrate our AR enhanced picture book that provides multiple levels of interaction and engagement. Holding the camera at a range of heights facilitates reader exploration of layered book features and content. These multi-level enhancements extend the traditional learning possibilities of books while providing increased opportunities for both shared and individual reading. © 2019, IFIP International Federation for Information Processing.",Augmented reality; Children’s books; Reading; Shared reading,Title_Keywords,True,
Scopus,journalPaper,2019,User Experience Guidelines for Designing HMD Extended Reality Applications,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"With the rise of Extended Reality (XR) technologies, such as head mounted displays (HMD) for Virtual Reality (VR), Mixed Reality (MR), and Augmented Reality (AR), designers are presented with many unique challenges and opportunities when creating applications. Publications can be found from research and industry that offer insights and ideas surrounding user experience (UX) for XR applications. However, these publications often vary in format and content. Based on a thorough analysis of 68 different resources from research, industry, and 2D design, we present a set of eleven UX guidelines for designing XR applications. Our work serves as a reference to the literature for understanding what others have tried and discovered and provides an integrated set of guidelines. Furthermore, our guidelines offer guidance to a software developer to aid in the design of XR applications for HMD devices. © 2019, IFIP International Federation for Information Processing.",Augmented Reality; Design guidelines; Extended Reality; Mixed Reality; User experience; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Influences of mixed reality and human cognition on picture passwords: An eye tracking study,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Recent research revealed that individual cognitive differences affect visual behavior and task performance of picture passwords within conventional interaction realms such as desktops and tablets. Bearing in mind that mixed reality environments necessitate from end-users to perceive, process and comprehend visually-enriched content, this paper further investigates whether this new interaction realm amplifies existing observed effects of individual cognitive differences towards user interactions in picture passwords. For this purpose, we conducted a comparative eye tracking study (N = 50) in which users performed a picture password composition task within a conventional interaction context vs. a mixed reality context. For interpreting the derived results, we adopted an accredited human cognition theory that highlights cognitive differences in visual perception and search. Analysis of results revealed that new technology realms like mixed reality extend and, in some cases, amplify the effect of human cognitive differences towards users’ visual and interaction behavior in picture passwords. Findings can be of value for improving future implementations of picture passwords by considering human cognitive differences as a personalization factor for the design of user-adaptive graphical passwords in mixed reality. © 2019, IFIP International Federation for Information Processing.",Eye tracking; Human cognition; Mixed reality; Picture passwords; Security; Usability; Visual behavior,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,"17th IFIP TC13 International Conference on Human-Computer Interaction, INTERACT 2019",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The proceedings contain 213 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Balance talking and doing! Using Google design sprint to enhance an intensive UCD course; How do typically developing children and children with ASD play a tangible game?; kahaniyan - Designing for acquisition of Urdu as a second language; on making, tinkering, coding and play for learning: A review of current research; rexy, a configurable application for building virtual teaching assistants; two-way gaze sharing in remote teaching; designing interactions with intention-aware gaze-enabled artificial agents; gazeLens: Guiding attention to improve gaze interpretation in hub-satellite collaboration; influences of mixed reality and human cognition on picture passwords: An eye tracking study; does the pop-out make an effect in the product selection of signage vending machine?; scaffoMapping: Assisting concept mapping for video learners; Shared gaze while driving: How drivers can be supported by an LED-visualization of the front-seat passenger’s gaze; TEXTile: Eyes-free text input on smart glasses using touch enabled textile on the forearm; “I don’t fit into a single type”: A trait model and scale of game playing preferences; comparing the applicability of pressure as a game metric to self-assessment and physiological metrics; fall-prevention exergames using balance board systems; dhana Labha: A financial management application to underbanked communities in Rural Sri Lanka; training non-designers in co-design methods through an active assisted living interactive workshop; mobile mapmaking: A field study of gamification and cartographic editing; understanding the digital and non-digital participation by the gaming youth; foreword.",,Abstract,True,Duplicate
Scopus,journalPaper,2019,Augmented reality technology for displaying close-proximity sub-surface positions,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"When designing human-system collaboration to assist in strenuous manual tasks we need to develop methods of communication between the system and the human. In this paper we are evaluating augmented reality (AR) technologies for displaying task-relevant information when the target is on a work surface for a typically standing work operation. In this case we are testing AR interfaces for displaying sub-surface positions. To do this we compare four types of AR interfaces, a head-mounted see-through display, a mounted see-through display, top-down surface projection and graphical overlays on a static monitor. We performed the experiment with 48 participants. Data analyses show significant difference between the AR interfaces in terms of task completion times and user satisfaction with the projection-based display being the fastest and most satisfying to the participants. © 2019, IFIP International Federation for Information Processing.",Augmented reality; Usability testing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Improved Memory Elicitation in Virtual Reality: New Experimental Results and Insights,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Eliciting accurate and complete knowledge from individuals is a non-trivial challenge. In this paper, we present the evaluation of a virtual-world based approach, informed by situated cognition theory, which aims to assist with knowledge elicitation. In this approach, we place users into 3D virtual worlds which represent real-world locations and ask users to describe information related to tasks completed in those locations. Through an empirical A/B evaluation of 62 users, we explore the differences in recall ability and behaviour of those viewing the virtual world via a virtual reality headset and those viewing the virtual world on a monitor. Previous results suggest that the use of a virtual reality headset was able to meaningfully improve memory recall ability within the given scenario. In this study, we adjust experiment protocol to explore the potential confounds of time taken and tool usability. After controlling for these possible confounds, we once again found that those given a virtual reality headset were able to recall more information about the given task than those viewing the virtual world on a monitor. © 2017, IFIP International Federation for Information Processing.",,Title_Abstract,True,
Scopus,journalPaper,2019,"Welcome, Computer! How Do Participants Introduce a Collaborative Application During Face-to-Face Interaction?",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"During cooperative interaction, participants introduce materials, artefacts, and other individuals into the ongoing interaction. Depending on how this introduction unfolds, the participants may embrace the new element in an easy way or not. If the new element is a collaborative application of interactive software designed to support the interaction, it may or may not improve the collaboration because of how it was introduced. Therefore, understanding and designing the initial interaction is key for unleashing the positive impact of collaborative systems. The literature has identified the fact that humans employ a specific range of behaviors when introducing an element into an ongoing interaction. Those introduction rituals are determined by whether the new element is a human or a material artefact. Introduction rituals involving interactive elements are still underexplored: How do participants introduce and initiate interaction with them? This manuscript explores the introduction behaviors emerging when an augmented-reality collaborative application is being introduced into a financial advisory service. It shows that the participants employ a wider range of introduction rituals during the introduction of this application than they do when they introduce a brochure. Notably, many of the observed behaviors resemble familiar opening rituals typically used when introducing and greeting humans. This supports the computers-are-social-actors argument and provides evidence that introducing a collaborative application has a social rather than a material character. © 2019, IFIP International Federation for Information Processing.",Advisory service scenario; Augmented reality; Collaborative applications; Computers-are-social-actors; Mixed reality; Rituals,Keywords,True,
Scopus,journalPaper,2019,FriendGroupVR: Design Concepts Using Virtual Reality to Organize Social Network Friends,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Creating friend lists offers social network users the ability to select a fine-grained audience for their posts, thereby reducing the amount of unwanted disclosures. However, research has shown that the user burden involved in creating and managing friend lists leads to the fact that this functionality is rarely used, despite its advantages. In this paper, we propose two design concepts using virtual reality to allow the user to create and organize her friend lists. Whereas the first “pragmatic” concept is targeted towards usability and practicability using a metaphor similar to card sorting, the second “playful” concept has the goal to achieve a high user experience score by offering a VR game to sort and organize the friends. In a lab study, we compared the two concepts with the Facebook interface in terms of usability, user experience and error rate (like missing friends in a group or friends placed in the wrong group). We were able to show that both designs significantly outperform the Facebook interface in both usability and user experience. The playful interface is experienced as more interesting and stimulating than its pragmatic counterpart, at the cost of an increased error rate. © 2019, IFIP International Federation for Information Processing.",,Title_Abstract,True,
Scopus,journalPaper,2019,Cinévoqué: Design of a Passively Responsive Framework for Seamless Evolution of Experiences in Immersive Live-Action Movies,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper, we present a passively responsive framework for immersive movies, called Cinévoqué. The framework seamlessly alters the narrative and visual elements within an immersive live-action movie, based on real-time passive data, such as gaze direction and system time, obtained during the experience. The paper primarily focuses on the design and storytelling aspects of Cinévoqué, such as possible narrative structures and the design challenges involved in creating responsive experiences. We further examine the potential of this framework through two prototypes of varying complexity and responsive features, and the insights from them are used to suggest approaches that can lead to effective seamless narrative experiences. © IFIP International Federation for Information Processing 2019.",Presence; Responsive narrative; Virtual reality; VR storytelling,Keywords,True,
Scopus,journalPaper,2017,Pragati - A mobile based virtual reality (VR) platform to train and educate community health workers,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Accredited Social Health Activists (ASHAs) are essential link to healthy communities in resource-constrained environments. However, they are insufficiently trained to solve community health challenges. In this paper, we present Pragati - a mobile-based Virtual Reality (VR) platform to train and educate ASHAs in rural Assam, India. Mobile based VR platform was chosen due to its ability to increase focus, attention and learnability among users. We developed 3 modules on maternal and child healthcare. Modules were presented via audio-visual interface in local Assamesse language. This paper presents the design of Pragati, user interactions, technology implementations and future directions of our study. © IFIP International Federation for Information Processing 2017.",Community health workers; ICT in social development - interaction design for developing regions; Medical training; Technology in healthcare; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,"Exploring the Effects of Replicating Shape, Weight and Recoil Effects on VR Shooting Controllers",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Commercial Virtual Reality (VR) controllers with realistic force feedback are becoming available, to increase the realism and immersion of first-person shooting (FPS) games in VR. These controllers attempt to mimic not only the shape and weight of real guns but also their recoil effects (linear force feedback parallel to the barrel, when the gun is shot). As these controllers become more popular and affordable, this paper investigates the actual effects that these properties (shape, weight, and especially directional force feedback) have on performance for general VR users (e.g. users with no marksmanship experience), drawing conclusions for both consumers and device manufacturers. We created a prototype replicating the properties exploited by commercial VR controllers (i.e. shape, weight and adjustable force feedback) and used it to assess the effect of these parameters in user performance, across a series of user studies. We first analysed the benefits on user performance of adding weight and shape vs a conventional controller (e.g. Vive controller). We then explore the implications of adding linear force feedback (LFF), as well as replicating the shape and weight. Our studies show negligible effects on the immediate shooting performance with some improvements in subjective appreciation, which are already present with low levels of LFF. While higher levels of LFF do not increase subjective appreciations any further, they lead users to reach their maximum distance skillset more quickly. This indicates that while adding low levels of LFF can be enough to influence user’s immersion/engagement for gaming contexts, controllers with higher levels of LFF might be better suited for training environments and/or when dealing with particularly demanding aiming tasks. © IFIP International Federation for Information Processing 2019.",First person shooters; Force feedback; Virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2019,Evaluating Mixed Reality Notifications to Support Excavator Operator Awareness,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Operating heavy vehicles, for instance an excavator, requires a high level of attention to the operation done using the vehicle and awareness of the surroundings. Digital transformation in heavy vehicles aims to improve productivity and user experience, but it can also increase the operators mental load because of a higher demand of attention to instrumentation and controls, subsequently leading to reduced situation awareness. One way to mitigate this, is to display information within the operators’ field of view, which enhances information detectability through quick glances, using mixed reality interfaces. This work explores two types of mixed reality visualizations and compares them to a traditional display setup in a simulated excavator environment. We have utilized eye-tracking glasses to study users’ attention to the task, surrounding awareness, and interfaces, followed by a NASA-RTLX questionnaire to evaluate the users’ reported mental workload. The results indicate benefits for the mixed reality approaches, with lower workload ratings together with an improved rate in detection of presented information. © IFIP International Federation for Information Processing 2019.",Excavator; Head-up display; Heavy-vehicles; Human-vehicle interaction; Mixed reality; Situational awareness,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Design Challenges for Mobile and Wearable Systems to Support Learning on-the-move at Outdoor Cultural Heritage Sites,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This paper presents a novel set of design challenges for the development of mobile and wearable applications to be used at cultural heritage sites. These challenges were drawn out based on a user study that was carried out to evaluate a mobile application prototype, SmartC. SmartC was designed for supporting people in taking learning opportunities at sites whenever they need informally while they are on the move. Augmented reality and wearable computing, i.e. smart eye glasses, were used in this research with the aim of bringing the past to life, as well as enhancing visitors’ engagement. SmartC was evaluated by 26 participants, potential end-users, in the field. The evaluation study mainly focused on the interaction and usability aspects, which contribute to the field of HCI. The paper outlines several issues and challenges that were identified based on the evaluation study, summarised as: (1) interaction design related; (2) wearable computing related; (3) surroundings and environment related; (4) learner related; (5) context of use; and (6) technical issues. This paper also identifies aspects that relate to methods to be used and applied to such cases for evaluation studies. © 2019, IFIP International Federation for Information Processing.",Augmented reality; Mobile location-based services; Outdoors cultural heritage; Ubiquitous learning; Wearable computing,Abstract_Keywords,True,
Scopus,journalPaper,2017,Increasing presence in virtual reality with a vibrotactile grid around the head,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"A high level of presence is an important aspect of immersive virtual reality applications. However, presence is difficult to achieve as it depends on the individual user, immersion capabilities of the system (visual, auditory, and tactile) and the concrete application. We use a vibrotactile grid around the head in order to further increase the level of presence users feel in virtual reality scenes. In a between-groups comparison study the vibrotactile group scored significantly higher in a standardized presence questionnaire compared to the baseline of no tactile feedback. This suggests the proposed prototype as an additional tool to increase the level of presence users feel in virtual reality scenes. © IFIP International Federation for Information Processing 2017.",Immersion; Presence; Vibrotactile feedback; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Experience probes: Immersion and reflection between reality and virtuality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This research addresses the issue of the memory-experience gap, the disconnect between momentary perceptions and post experience reporting as relates to HCI research methodologies and the study of immersive technology-mediated experiences in particular. The paper presents an overview of contemporary understanding of immersion and examines HCI methods that investigate participant experiences. We introduce Experience Probes, an integrated design and evaluation methodology that affords momentary reporting by blending states of reflection and immersion in a structured activity situated within the immersive experience. A pilot study is presented that examines an immersive soundscape installation and an Experience Probe enacted through participant-authored sound maps. The maps provide data for thematic analysis, and are coded for signs of self-perception and a sense of place to evaluate participants’ sensations of presence and immersion. Preliminary results are discussed in relation to the reality-virtuality continuum and suggest that the reflective act of reporting, and the experience of immersion within the soundscape installation are not mutually exclusive. This research seeks to extend HCI methods by overcoming the memory-experience gap in the evaluation of technology-mediated experiences. © IFIP International Federation for Information Processing 2017.",HCI evaluation methods; Immersive experiences; Mixed reality environments; Momentary assessment,Keywords,True,
Scopus,journalPaper,2017,Guidelines for designing interactive omnidirectional video applications,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Interactive omnidirectional videos (iODV) can offer informative, entertaining, and immersive experiences, especially when combined with novel platforms such as head-mounted displays. However, omnidirectional videos, and interaction with them, present many unique challenges. In the absence of existing guidelines that accommodate for these challenges, we present dos and don’ts for designing and producing interactive omnidirectional videos. We base these guidelines on numerous interactive systems that we have produced in the recent years. Our work offers useful guidance for those working with omnidirectional videos, especially when designing interactivity and navigation within such systems. © IFIP International Federation for Information Processing 2017.",Head-mounted displays; Omnidirectional videos; Virtual reality,Keywords,True,
Scopus,journalPaper,2017,How real is unreal?: Virtual reality and the impact of visual imagery on the experience of exercise-induced pain,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"As a consequence of prolonged muscle contraction, acute pain arises during exercise due to a build-up of noxious biochemicals in and around the muscle. Specific visual cues, e.g., the size of the object in weight lifting exercises, may reduce acute pain experienced during exercise. In this study, we examined how Virtual Reality (VR) can facilitate this “material-weight illusion”, influencing perception of task difficulty, which may reduce perceived pain. We found that when vision understated the real weight, the time to exhaustion was 2 min longer. Furthermore, participants’ heart rate was significantly lower by 5-7 bpm in the understated session. We concluded that visual-proprioceptive information modulated the individual’s willingness to continue to exercise for longer, primarily by reducing the intensity of negative perceptions of pain and effort associated with exercise. This result could inform the design of VR aimed at increasing the level of physical activity and thus a healthier lifestyle. © IFIP International Federation for Information Processing 2017.",Body representation; Exercise; Material-Weight illusions; Pain; Virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Leveraging virtual trips in google expeditions to elevate students’ social exploration,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This paper reports on an exploratory case study on the use of Google Expeditions in the context of an intensive Greek language course for specific academic purposes. Google Expeditions are collections of linked virtual reality content that can enable teachers to bring students on virtual trips to places like museums, human anatomy, surgical processes etc. Thematic analysis of instructors’ field notes, students’ reflections, interviews and focus group was employed aiming at identifying the potential of Google Expeditions for extending the language classroom through virtual trips. The use of Google Expeditions enabled students to extend the borders of the classroom by making virtual walkthroughs in places that would normally be unreachable and trigger social exploration through inter- and extra-VR communication, sharing of ideas, concepts and experiences. This study acts as a pilot with an eye to inform larger-scale investigation of Google Expeditions in the future. © IFIP International Federation for Information Processing 2017.",Computer-assisted language learning; Constructionism; Social constructionism; Wearables,Abstract,True,
Scopus,journalPaper,2017,"16th IFIP TC13 International Conference on Human-Computer Interaction, INTERACT 2017",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"The proceedings contain 161 papers. The special focus in this conference is on Human-Computer Interaction. The topics include: Coping with accessibility challenges for security - a user study with blind smartphone users; effects of uncertainty and cognitive load on user trust in predictive decision making; towards understanding the influence of personality on mobile app permission settings; towards understanding the influence of personality on mobile app permission settings; exploring offline context and consciousness in everyday social media use; the design of alipay and wechat wallet for mobile payment practices in china; active involvement of software developers in usability engineering; adoption of UX evaluation in practice; empowering project managers in enterprises - a design thinking approach to manage commercial projects; learning HCI across institutions, disciplines and countries; UX professionals' definitions of usability and UX - a comparison between turkey, finland, denmark, france and malaysia; estimating visual discomfort in head-mounted displays using electroencephalography; immersion and reflection between reality and virtuality; guidelines for designing interactive omnidirectional video applications; increasing presence in virtual reality with a vibrotactile grid around the head; user experience and immersion of interactive omnidirectional videos in CAVE systems and head-mounted displays; a digital employability marketplace; adoption of structural analysis capabilities in an IOT based scenario for connected assets; design and development of a location-based social networking mobile application; designing interactive spatiotemporal visualizations to enhance movie browsing; designing and assessing interactive systems using task models.",,Abstract,True,Duplicate
Scopus,journalPaper,2017,Practice in Reality for Virtual Reality Games: Making Players Familiar and Confident with a Game,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Game designers include training levels in video games to prepare players so that they can enjoy the game. The training levels of virtual reality (VR) games are typically assumed to be within the virtual world of the game. New players must learn about a new game in such an unfamiliar virtual world. A tutorial in the real world offers a potential way to enable players to learn about a new game and to practice the skills in a familiar world. To explore any effects of a real-world tutorial in VR games, an experiment was conducted, the results of which show that a real-world tutorial is effective in helping new players feel confident about and familiar with a VR game before playing it. However, it is not as effective as virtual-world tutorial in increasing game performance. © 2017, IFIP International Federation for Information Processing.",,Title_Abstract,True,
Scopus,journalPaper,2013,"RelicPad: A hands-on, mobile approach to collaborative exploration of virtual museum artifacts",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In an ideal world, physical museum artefacts could be touched, handled, examined and passed between interested viewers by hand. Unfortunately, this is not always possible - artefacts may be too fragile to handle or pass around, or groups of people with mutual interests in objects may not be in the same location. This can be problematic when attempting to explain or make sense of the physical properties of artefacts. To address these problems, we propose that direct manipulation of 3D content based on real-world interaction metaphors can help collaborators (both co and remotely located) to construct personal and mutual physical and spatial awareness of artefacts, while networked communication and collaboration allow for ideas and knowledge to be exchanged and shared. We present our interpretations from two studies of RelicPad, a tablet-based application that allows users to manually manipulate museum artefacts and to 'point out' areas of interest to each other using 3D annotations, facilitating a mutual awareness of spatial properties and referencing during discussion. © 2013 IFIP International Federation for Information Processing.",3D interaction techniques; Museum artefacts; remote collaboration; tablet interfaces; virtual reality,Keywords,True,
Scopus,journalPaper,2017,Estimating visual discomfort in head-mounted displays using electroencephalography,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Head-Mounted displays, while providing unprecedented immersiveness and engagement in interaction, can substantially add mental workload and visual strain on users. Being a novel technology, users often do not know what to expect and therefore accept visual stress as being state of the art. Assessing visual discomfort is currently possible through questionnaires and interviews that interrupt the interaction and provide only subjective feedback. Electroencephalography (EEG) can provide insights about the visual discomfort and workload of HMDs. We evaluate the use of a consumer-grade Brain Computer Interface for estimating visual discomfort in HMD usage in a study with 24 participants. Our results show that the usage of a BCI to detect uncomfortable viewing conditions is possible with a certainty of 83% in our study. Further the results give insights on the usage of BCIs in order to increase the detection certainty by reducing costs for the hardware. This can pave the way for designing adaptive virtual reality experiences that consider user visual fatigue without disrupting immersiveness. © IFIP International Federation for Information Processing 2017.",Brain-computer interface; Electroencephalography; Head-mounted displays; Virtual reality; Visual fatigue,Abstract_Keywords,True,
Scopus,journalPaper,2017,Could people with stereo-deficiencies have a rich 3D experience using HMDs?,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"People with stereo-deficiencies usually have problems for the perception of depth using stereo devices. This paper presents a study that involves participants who did not have stereopsis and participants who had stereopsis. The two groups of participants were exposed to a maze navigation task in a 3D environment in two conditions, using a HMD and a large stereo screen. Fifty-nine adults participated in our study. From the results, there were no statistically significant differences for the performance on the task between the participants with stereopsis and those without stereopsis. We found statistically significant differences between the two conditions in favor of the HMD for the two groups of participants. The participants who did not have stereopsis and could not perceive 3D when looking at the Lang 1 Stereotest did have the illusion of depth perception using the HMD. The study suggests that for the people who did not have stereopsis, the head tracking largely influences the 3D experience. © 2017, IFIP International Federation for Information Processing.",3D experience; Head tracking; HMD; Large stereo screen; Stereo-deficiency; Stereoblindness; Stereopsis; Virtual reality,Keywords,True,
Scopus,journalPaper,2017,Investigating control of virtual reality snowboarding simulator using a Wii FiT board,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"This work presents a virtual reality snowboarding application which uses a Nintendo Wii balance board for richer interaction modalities. We present the application and test the prototype with 7 participants to investigate immersion, enjoyability and to an extent performance. The outcomes from the study will be used to start forming research directions and questions to indicate likely research and development directions for future research. © IFIP International Federation for Information Processing 2017.",HMD; Snowboarding; Virtual reality; Wii FiT board,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,MeViTa: Interactive visualizations to help older adults with their medication intake using a camera-projector system,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In this paper, we investigate whether augmented reality visualization techniques can empower older adults to explore and understand medication information in an effective and timely manner. Through a user-centered design process involving older adults and health professionals we developed an interactive camera-projector system called MeViTa (Medication Visualization Table) that projects medication information surrounding medication boxes laid on a table. Six designs were iteratively developed. In total 26 older adults, with a mean age of 71 (±7), participated in the user studies. Although no time benefits were observed, participants perceived MeViTa as an effective means to explore and understand medication information, and as more engaging than the traditional patient information leaflet. Furthermore, by visualizing medication information, our approach provides qualitative findings of the relative ease and difficulty for older adults to learn more about medication information. © 2017, IFIP International Federation for Information Processing.",Camera-Projector; Medication; Older adults; User-Centered,Abstract,True,
Scopus,journalPaper,2013,"Funneling and saltation effects for tactile interaction with ""detached"" out of the body virtual objects",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"In a previous work, we confirmed the existing effects of ""Out of the Body"" tactile illusion for virtual and augmented objects through funneling and saltation. However, it required a virtual imagery to be attached to the user for directly extending one's body. This paper aims at investigating similar phantom tactile sensations exist when the virtual object is visually detached from the user's body. Two usability experiments were conducted to verify the hypothesized phantom tactile effects: one for funneling and the other, saltation. Our results have shown that in addition to the perception of the phantom sensations with the ""detached"" visual feedback, the interaction experience was significantly enriched (vs. when without explicit visual feedback). We also discovered for the first time that for funneling, phantom sensations can be elicited without any visual feedback at all. The findings can be applied to the tactile interaction design using minimal number of actuators on a variety of media platforms including the mobile, holography and augmented reality. © 2013 IFIP International Federation for Information Processing.",Funneling; Illusory feedback; Multimodal feedback; Phantom sensation; Saltation; Vibro-tactile feedback,Abstract,True,
Scopus,journalPaper,2013,Assessing the impact of automatic vs. controlled rotations on spatial transfer with a joystick and a walking interface in VR,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We present a user study assessing spatial transfer in a 3D navigation task, with two different motor activities: a minimal (joystick) and an extensive motor activity (walking Interface), with rotations of the viewpoint either controlled by the user, or automatically managed by the system. The task consisted in learning a virtual path of a 3D model of a real city, with either one of these four conditions: Joystick / Treadmill Vs Manual Rotation / Automatic Rotation. We assessed spatial knowledge with six spatial restitution tasks. To assess the interfaces used, we analyzed also the interaction data acquired during the learning path. Our results show that the direct control of rotations has different effects, depending on the motor activity required by the input modality. The quality of spatial representation increases with the Treadmill when rotations are enabled. With the Joystick, controlling the rotations affect spatial representations. We discuss our findings in terms of cognitive, sensorimotor processes and human computer interaction issues. © 2013 IFIP International Federation for Information Processing.",Body-based Information; Human Factors; Human Machine Interaction; Interfaces; Joystick; Motor Activity; Navigation; Rotation; Spatial Cognition; Treadmill; User Study; Vestibular Information; Virtual Reality,Keywords,True,
Scopus,journalPaper,2013,Precise pointing techniques for handheld augmented reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"We propose two techniques that improve accuracy of pointing at physical objects for handheld Augmented Reality (AR). In handheld AR, pointing accuracy is limited by both touch input and camera viewpoint instability due to hand jitter. The design of our techniques is based on the relationship between the touch input space and two visual reference frames for on-screen content, namely the screen and the physical object that one is pointing at. The first technique is based on Shift, a touch-based pointing technique, and video freeze, in order to combine the two reference frames for precise pointing. Contrastingly -without freezing the video-, the second technique offers a precise mode with a cursor that is stabilized on the physical object and controlled with relative touch inputs on the screen. Our experimental results show that our techniques are more accurate than the baseline techniques, namely direct touch on the video and screen-centered crosshair pointing. © 2013 IFIP International Federation for Information Processing.",Handheld Augmented Reality; Interaction Techniques; Pointing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,HandsIn3D: Supporting remote guidance with immersive virtual environments,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"A collaboration scenario involving a remote helper guiding in real time a local worker in performing a task on physical objects is common in a wide range of industries including health, mining and manufacturing. An established ICT approach to supporting this type of collaboration is to provide a shared visual space and some form of remote gesture. The shared space and remote gesture are generally presented in a 2D video form. Recent research in tele-presence has indicated that technologies that support co-presence and immersion not only improve the process of collaboration but also improve spatial awareness of the remote participant. We therefore propose a novel approach to developing a 3D system based on a 3D shared space and 3D hand gestures. A proof of concept system for remote guidance called HandsIn3D has been developed. This system uses a head tracked stereoscopic HMD that allows the helper to be immersed in the virtual 3D space of the worker's workspace. The system captures in 3D the hands of the helper and fuses the hands into the shared workspace. This paper introduces HandsIn3D and presents a user study to demonstrate the feasibility of our approach. © 2013 IFIP International Federation for Information Processing.",hand gesture; mixed reality; remote collaboration; co-presence; shared visual space,Keywords,True,
Scopus,journalPaper,2013,User-defined gestures for augmented reality,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Recently there has been an increase in research towards using hand gestures for interaction in the field of Augmented Reality (AR). These works have primarily focused on researcher designed gestures, while little is known about user preference and behavior for gestures in AR. In this paper, we present our guessability study for hand gestures in AR in which 800 gestures were elicited for 40 selected tasks from 20 participants. Using the agreement found among gestures, a user-defined gesture set was created to guide designers to achieve consistent user-centered gestures in AR. Wobbrock's surface taxonomy has been extended to cover dimensionalities in AR and with it, characteristics of collected gestures have been derived. Common motifs which arose from the empirical findings were applied to obtain a better understanding of users' thought and behavior. This work aims to lead to consistent user-centered designed gestures in AR. © 2013 IFIP International Federation for Information Processing.",Augmented reality; gestures; guessability,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,"Truly useful 3D drawing system for professional designer by ""life-sized and operable"" feature and new interaction",INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"""Media"" is an artifact that expands our creativity and intelligence. We have been studying the use of ""Rich Media"" to support creative and intelligent human activities. Specifically, for over ten years we have focused on the 3D space as one of ""Rich Media"" and developed many 3D sketch systems that support the design of 3D objects. However, their long-term evaluation has revealed that they are not used by designers in real fields. Even worse, they are treated as if they were just mere attractions in an amusement park. The fundamental problem is the lack of the indispensable function for 3D space. In this paper, we propose new design principles, ""life-size and operability"", which make the 3D sketch system truly valuable for the designer. The new 3D sketch system is designed on the basis of ""life-size and operability"", developed, and evaluated successfully. © 2013 IFIP International Federation for Information Processing.",3D Sketch; Life-size; Mixed reality; Operability; Professional Designer,Keywords,True,
Scopus,journalPaper,2013,Interacting with augmented reality: How does location-based AR enhance learning?,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"Augmented Reality (AR) can provide additional information about mediated events, but can it enhance our learning and comprehension? We performed a user study of a location-based AR application in order to answer this question. A 2-condition (AR modality vs. non-AR digital book modality) between-subjects experiment with 36 dyads of secondary school students in Singapore was conducted to examine how the use of AR modality in an educational context impacts students' learning performance. Data from the experiment showed that location-based AR improved students' learning performance by catching their attention and enhancing their ability to elaboratively process the information they encountered. Theoretical and practical implications are discussed. © 2013 IFIP International Federation for Information Processing.",learning; Location-based AR; modality; transportation,Title_Abstract,True,
Scopus,journalPaper,2013,Tracking eyes in service prototyping,INTERACT - IFIP TC 13 International Conference on Human-Computer Interaction,B,"A mobile eye tracker was used to collect viewing behavior in a mixed reality immersive Cave Automatic Virtual Environment (CAVE) environment to evaluate a design concept of a tourist information office. The synthetic office consists of physical artifacts and virtual contents projected onto three walls of a room-sized cube. A Think Aloud study was conducted with both a goal-oriented condition and a free-browsing condition while subjects wearing the eye-tracker. Multiple Augmented Reality markers were used to reconstruct gaze positions in the coordinate system of the real environment. Gaze points were later aggregated to create heat maps, which were used as textures for a computer 3D model replication of the synthetic tourist office. The interactive visualization of the 3D heat map showcases different viewing patterns for different conditions. The insights suggest the combination of eye-tracking and mixed reality environment to be a valuable tool for prototyping service design of similar kinds. © 2013 Springer-Verlag.",Experience Prototyping; Eye-Tracking; Heat Map; Service Design,Abstract,True,
Scopus,journalPaper,2013,Playing it real again: a repeated evaluation of magic lens and static peephole interfaces in public space,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We repeated a study on the usage of a magic lens and a static peephole interface for playing a find-and-select game in a public space. While we reproduced the study setup and procedure the task was conducted in a public transportation stop with different characteristics. The results on usage duration and user preference were significantly different from those reported for previous conditions. We investigate possible causes, specifically the differences in the spatial characteristics and the social contexts in which the study took place.",augmented reality; field trial; in-the-wild; magic lens; place; proxemics; social context; space; static peephole,Keywords,True,
Scopus,journalPaper,2013,Moving beyond the map: automated landmark based pedestrian guidance using street level panoramas,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"In the past people have used very different forms of directions depending on how those directions were acquired. If a person is giving another person directions in a familiar area, he will frequently use landmarks to describe the route [10]. If the person gets the route from a personal navigation system though, it will be displayed on a map and make use of street names for the directions.In this paper we present a system to automatically give landmark based navigation to pedestrians by using panoramic imagery to both find salient landmarks along a route automatically, and to present those landmarks to a pedestrian navigator in an immersive and intuitive manner. Our system primarily uses automatically detected business signs as landmarks, and currently works in a half dozen cities around the world. We have also evaluated our system and found that people can effectively navigate solely using landmark enhanced panoramas of decision points along the route.",mixed reality; natural guidance; navigation,Keywords,True,
Scopus,journalPaper,2013,An experimentation environment for mobile 3D and virtual reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Unlike desktop screens, mobile devices can be moved around freely. This allows us to create different experiences when exploring 3D data and virtual environments on such handhelds. Yet, this additional degree of freedom not only introduces exciting new possibilities, but also potential issues when used in actual applications. We present a demo environment that enables users to explore different kinds of 3D visualizations on smartphones and tablets, experiment with various characteristics and implementation options, and experience the advantages and disadvantages of these different realizations.",interactive 3d; mobile 3d; mobile virtual reality,Title_Keywords,True,
Scopus,journalPaper,2013,Creating a stereoscopic magic-lens to improve depth perception in handheld augmented reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Handheld Augmented Reality (AR) is often presented using the magic-lens paradigm where the handheld device is portrayed as if it was transparent. Such a virtual transparency is usually implemented using video captured by a single camera rendered on the device's screen. This removes binocular-disparity, which may undermine user's ability to correctly estimate depth when seeing the world through the magic-lens. To confirm such an assumption this paper presents a qualitative user study that compares a magic-lens implemented on a mobile phone and a transparent glass replica. Observational results and questionnaire analysis indicate that binocular-disparity may play a significant role in participants' depth perception. These promising results led to the subsequent implementation of a stereoscopic magic-lens prototype on a commercially available mobile device.",binocular disparity; depth perception; handheld; mobile; parallax; stereoscopic rendering; user study; virtual transparency,Title_Abstract,True,
Scopus,journalPaper,2013,Enhanced virtual transparency in handheld ar: digital magnifying glass,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Handheld Augmented Reality (AR) is often presented using the magic-lens paradigm in which a magic-lens is a transparent interface. Such transparency is usually implemented by rendering camera captured video on the device's screen. The transparency quality is limited by the video stream quality which may be affected by: unfocused camera lens, poor lighting conditions and limited video stream resolution. All these factors may reduce the readability of the AR scene. To address quality of rendering and increase scene readability, this paper presents an enhanced virtual transparency solution where segments of the scene are replaced by high definition digital content. The proposed enhanced virtual transparency isdemonstrated through the design of a digital magnifying glass which has been implemented on of-the-shelf mobile phone.",augmented reality; handheld; magnifying glass; mobile; rendering; virtual transparency,Abstract_Keywords,True,
Scopus,journalPaper,2013,Interaction with services using an augmented reality user interface,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"This paper introduces a new concept of interacting with mobile devices while on the move. The concept is using a combination of interaction techniques, like AR, touch, tilting and rotating the mobile device. The validation is done using some basic scenarios in which the users interact with various services available on the street. The user interface elements are represented by AR anchored objects (still pictures) and floating objects that are giving visual cues to where services are available. The users are aided in their actions by freezing the user interface when the device is down tilted so that they could easily manipulate the objects.",3d user interface; augmented reality; mobile touch screen device; sensors,Title_Keywords,True,
Scopus,journalPaper,2013,Mobile augmented reality: exploring content in natural and controlled settings using 3d tracking,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The advent of the mobile device has propelled education, adoption and implementation of Augmented Reality on a massive scale. The success and evolution of this event and the underlying technology over the last 5 years is nearly indisputable proof that the industry is progressing, though incredible examples of useful augmented reality applications are yet to be explored. The ability to recognize images, markers and 3D objects is one of the most important aspects of Augmented Reality. With present work we are proposing three application examples, which make recognition and visual search more intuitive, natural and accessible.",3d tracking; augmented reality; controlled settings; information visualization; natural settings; poi; slam,Title_Abstract_Keywords,True,
Scopus,journalPaper,2013,Representing and interpreting reformation in the wild,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,A mobile educational app combining the strengths of sound pedagogical frameworks Generic Learning Outcomes and of interactive technologies Augmented Reality (AR) and Quick Response (QR) code to deliver the historical information of the Howard family in the Reformation is being developed by an interdisciplinary team. Visiting the relatively small archaeological site – Thetford Priory- with the new technologies may contribute to resolving an age-old puzzle through interpretations from multiple perspectives and to fostering the sustainability of such a site.,archaeological sites; augmented reality; cultural heritage; generic learning outcomes; qr code; reformation,Abstract_Keywords,True,
Scopus,journalPaper,2013,WozARd: a wizard of oz tool for mobile AR,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Wizard of Oz methodology is useful when conducting user studies of a system that is in early development. It is essential to be able to simulate part of the system and to collect feedback from potential users. Using a human to act as the system is one way to do this.The Wizard of Oz tool presented here is called WozARd and it aims at offering a set of tools that help the test leader control the visual, tactile and auditive output that is presented to the test participant. Additionally, it is suitable for using in an augmented reality environment where images are overlaid on the phone's camera view or on glasses.The main features that were identified as necessary include presentation of media such as images, video and sound, navigation and location based triggering, automatically taking photos, capability to log test results and visual feedback, and the integration of Sony SmartWatch for interaction possibilities.",augmented reality; multimodal interaction; multimodal user interface; wizard of oz,Abstract_Keywords,True,
Scopus,journalPaper,2013,Designing mobile augmented reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The development of mobile Augmented Reality application became increasingly popular over the last few years. However, many of the existing solutions build on the reuse of available standard metaphors for visualization and interaction without considering the manifold contextual factors of their use. Within this workshop we want to discuss theoretical design approaches and practical tools which should help developers to make more informed choices when exploring the design space of Augmented Reality interfaces in mobile contexts.",adaptive user interfaces; augmented reality; design; mobile,Title_Abstract_Keywords,True,
Scopus,journalPaper,2014,AudioTorch: using a smartphone as directional microphone in virtual audio spaces,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Mobile audio augmented reality systems can be used in a series of applications, e.g., as a navigational aid for visually impaired or as audio guide in museums. The implementation of such systems usually relies on head orientation data, requiring additional hardware in form of a digital compass in the headphones. As an alternative we propose AudioTorch, a system that turns a smartphone into a virtual directional microphone. This metaphor, where users move the device to detect virtual sound sources, allows quick orientation and easy discrimination between proximate sources, even with simplified rendering algorithms. We compare the navigation performance of head orientation measurement to AudioTorch. A lab study with 18 users showed the rate of correctly recognized sources to be significantly higher with AudioTorch than with head-tracking, while task completion times did not differ significantly. The presence in the virtual environment received similar ratings for both conditions.",audio augmented reality; mobile devices; navigation; virtual audio spaces,Abstract_Keywords,True,
Scopus,journalPaper,2014,Time tremors: developing transmedia gaming for children,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Time Tremors is a transmedia experience for children aged 8-14 that crosses television, web, locative media, and mobile apps. Time Tremors is a collection game in which players search for objects from history supposedly scattered throughout time and space, hidden, invisible to the human eye but detectable and collectable using a variety of mobile and online broadband technologies. Extending the game into locative augmented reality and mobile play was an applied research challenge that required narrative continuity while ensuring safe play.",augmented reality; children's entertainment; coppa; cross-platform; engagement; locative; transmedia; wearable,Abstract_Keywords,True,
Scopus,journalPaper,2016,In situ CAD capture,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We present an interactive system to capture CAD-like 3D models of indoor scenes, on a mobile device. To overcome sensory and computational limitations of the mobile platform, we employ an in situ, semi-automated approach and harness the user's high-level knowledge of the scene to assist the reconstruction and modeling algorithms. The modeling proceeds in two stages: (1) The user captures the 3D shape and dimensions of the room. (2) The user then uses voice commands and an augmented reality sketching interface to insert objects of interest, such as furniture, artwork, doors and windows. Our system recognizes the sketches and add a corresponding 3D model into the scene at the appropriate location. The key contributions of this work are the design of a multi-modal user interface to effectively capture the user's semantic understanding of the scene and the underlying algorithms that process the input to produce useful reconstructions.",algorithms; design; human factors; interactive 3D modeling; mobile augmented reality; sketching,Abstract_Keywords,True,
Scopus,journalPaper,2016,Where are we? evaluating the current rendering fidelity of mobile audio augmented reality systems,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Mobile audio augmented reality systems (MAARS) simulate virtual audio sources in a physical space via headphones. While 20 years ago, these required expensive sensing and rendering equipment, the necessary technology has become widely available. Smartphones have become capable to run high-fidelity spatial audio rendering algorithms, and modern sensors can provide rich data to the rendering process. Combined, these constitute an inexpensive, powerful platform for audio augmented reality.We evaluated the practical limitations of currently available off-the-shelf hardware using a voice sample in a lab experiment. State of the art motion sensors provide multiple degrees of freedom, including pitch and roll angles instead of yaw only. Since our rendering algorithm is also capable of including this richer sensor data in terms of source elevation, we also measured its impact on sound localization. Results show that mobile audio augmented reality systems achieve the same horizontal resolution as stationary systems. We found that including pitch and roll angles did not significantly improve the users' localization performance.",audio augmented reality; mobile devices; navigation; spatial audio; virtual audio spaces,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,Playing on AREEF: evaluation of an underwater augmented reality game for kids,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"This paper reports on a study of AREEF, a multi-player Underwater Augmented Reality (UWAR) experience for swimming pools. Using off-the-shelf components combined with a custom made waterproof case and an innovative game concept, AREEF puts computer game technology to use for recreational and educational purposes in and under water. After an experience overview, we present evidence gained from a user-centred design-process including a pilot study with 3 kids and a final evaluation with 36 kids. Our discussion covers technical findings regarding marker placement, tracking, and device handling, as well as design related issues like virtual object placement and the need for extremely obvious user interaction and feedback when staging a mobile underwater experience.",augmented reality; exertion; games; mobile; underwater; virtual environments,Title_Abstract_Keywords,True,
Scopus,journalPaper,2016,Changing the camera-to-screen angle to improve AR browser usage,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Mobile devices are currently the most commonly used platform to experience Augmented Reality (AR). Nevertheless, they typically provide a less than ideal ergonomic experience, requiring the user to operate them with arms raised. In this paper we evaluate how to improve the ergonomics of AR experiences by modifying the angle between the mobile device's camera and its display. Whereas current mobile device cameras point out vertically from the back cover, we modify the camera angle to be 0, 45 and 90 degrees. In addition, we also investigate the use of the smartwatch as an AR browser form factor. Key findings are, that whilst the current approximately see-through configuration provides the fastest task completion times, a camera offset angle of 45° provides reduced task load and was preferred by users. When comparing different form factors and screen sizes, the smartwatch format was found to be unsuitable for AR browsing use.",augmented reality; augmented reality browsers; magic lens interaction; mobile devices; smartwatches,Abstract_Keywords,True,
Scopus,journalPaper,2017,FrontFace: facilitating communication between HMD users and outsiders using front-facing-screen HMDs,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"A head-mounted display (HMD) immerses users in a virtual world, but separates them from outsiders in the real world. We present FrontFace, which is a novel HMD that combines an eye-tracker with a front-facing screen, to lower the communication barrier between HMD users and outsiders. The front-facing screen reveals user attention (e.g., the users eye motions) and user presence in the virtual or real world by displaying the scene in the virtual world or a skin background respectively, enabling eye-contact interactions between the HMD user and the outsiders. FrontFace has the following benefits. Firstly, it communicates the presence of the HMD user to outsiders; secondly, it reveals the player's visual attention by introducing the HMD users originally occluded eye motions, enabling outsiders to make sense of the HMD user's reaction in the virtual world or the real world. Three interactive techniques for the outsiders to initiate communication to HMD users are proposed: they are tap-trigger, hand-gesture trigger, and voice-trigger interactions. A small focus group provided feedback.",spectator experience; virtual reality,Keywords,True,
Scopus,journalPaper,2017,Using nature elements in mobile AR for education with children,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We present a concept, prototype and in-the-wild evaluation of a mobile augmented reality (AR) application in which physical items from nature are used as AR markers. By blending the physical and digital, AR technology has the potential to create an enhanced learning experience compared to paper-based solutions and conventional mobile applications. Our prototype, an application running on a tablet computer, uses natural markers such as leaves and pinecones in a game-like nature quiz. The system was evaluated using interviews with and observations of 6- to 12-year-old children (n=11) who played the game as well as focus group discussions with play club counsellors (n=4) and primary school teachers (n=7). Our salient findings suggest that the concept has sound potential in its mixture of physical activity and educational elements in an outdoor context. In particular, teachers found the use of natural objects to be an appealing approach and a factor contributing to the learning experience.",children; education; field studies; mobile augmented reality; visual markers,Abstract_Keywords,True,
Scopus,journalPaper,2017,EatAR tango: portion estimation on mobile devices with a depth sensor,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The accurate assessment of nutrition information is a challenging task, but crucial for people with certain diseases, such as diabetes. An important part of the assessment of nutrition information is portion estimation, i.e. volume estimation. Given the volume and the food type, the nutrition information can be computed on the basis of the food type specific nutrition density. Recently mobile devices with depth sensors have been made available for the public (Google's project tango platform). In this work, an app for mobile devices with a depth sensor is presented which assists users in portion estimation. Furthermore, we present the design of a user study for the app and preliminary results.",augmented reality; mobile; portion estimation,Keywords,True,
Scopus,journalPaper,2017,PeriMR: a prototyping tool for head-mounted peripheral light displays in mixed reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Nowadays, Mixed and Virtual Reality devices suffer from a field of view that is too small compared to human visual perception. Although a larger field of view is useful (e.g., conveying peripheral information or improving situation awareness), technical limitations prevent the extension of the field-of-view. A way to overcome these limitations is to extend the field-of-view with peripheral light displays. However, there are no tools to support the design of peripheral light displays for Mixed or Virtual Reality devices. Therefore, we present our prototyping tool PeriMR that allows researchers to develop new peripheral head-mounted light displays for Mixed and Virtual Reality.",augmented reality; head-mounted; mixed reality; peripheral light displays; prototyping; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Study on manual palletization of inhomogeneous boxes with the help of different interfaces to assess specific factors of ergonomic impact,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The pursued project concerns the general topic of optimizing the packaging of pallets. To be more precise, it analyzes the execution of the work task of palletizing from the point of view of the user. Thus, the project seeks to provide a guidance for an ergonomic task execution for the worker as well as to ensure an economical pallet-packaging pattern concerning the work task of packing a pallet.",augmented reality; head-mounted displays; mobile 3D; natural interaction,Keywords,True,
Scopus,journalPaper,2017,Visualizing out-of-view objects in head-mounted augmented reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Various off-screen visualization techniques that point to off-screen objects have been developed for small screen devices. A similar problem arises with head-mounted Augmented Reality (AR) with respect to the human field-of-view, where objects may be out of view. Being able to detect so-called out-of-view objects is useful for certain scenarios (e.g., situation monitoring during ship docking). To augment existing AR with this capability, we adapted and tested well-known 2D off-screen object visualization techniques (Arrow, Halo, Wedge) for head-mounted AR. We found that Halo resulted in the lowest error for direction estimation while Wedge was subjectively perceived as best. We discuss future directions of how to best visualize out-of-view objects in head-mounted AR.",augmented reality; head-mounted; off-screen; out-of-view objects; peripheral awareness; visualization techniques,Title_Abstract_Keywords,True,
Scopus,journalPaper,2017,Creating community fountains by (re-)designing the digital layer of way-finding pillars,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Way-finding pillars for tourists aid them in navigating an unknown area. The pillars show nearby points of interest, offer information about public transport and provide a scale for the neighbourhood. Through a series of studies with tourists and locals, we establish their different needs. In this space, we developed Mappy, a mobile application which augments and enhances way-finding pillars with an added digital layer. Mappy opens up opportunities for reappropriation of, and engagement with, the pillars. Seeing the pillars beyond their initial use case by involving a diverse range of people let us develop the digital layer and subsequently overall meaning of way-finding pillars further: as ""community fountains"" they engage locals and tourists alike and can provoke encounters between them.",augmented reality; digital layers; reappropriation; smart city; urban spaces; way-finding,Keywords,True,
Scopus,journalPaper,2017,Interacting with large maps using HMDs in VR settings,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Location based services are a common application scenario in mobile and ubiquitous computing use. A typical issue with cartographic applications in this domain is the limited size of the displayed map, which makes interaction and visualization a difficult problem to solve. With the increasing popularity of head mounted displays for VR and AR systems, an opportunity is presented for map-based applications to overcome the limitation of the small display size, as the user's information visualization space can extend to his entire surroundings. In this paper we present a preliminary investigation into how interaction with such very large display maps can take place, using a virtual reality headset as the sole input and interaction method.",augmented reality; digital maps; interaction; map-based applications; virtual reality,Abstract_Keywords,True,
Scopus,journalPaper,2018,Hand range interface: information always at hand with a body-centric mid-air input surface,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Most interfaces of our interactive devices such as phones and laptops are flat and are built as external devices in our environment, disconnected from our bodies. Therefore, we need to carry them with us in our pocket or in a bag and accommodate our bodies to their design by sitting at a desk or holding the device in our hand. We propose Hand Range Interface, an input surface that is always at our fingertips. This body-centric interface is a semi-sphere attached to a user's wrist, with a radius the same as the distance from the wrist to the index finger. We prototyped the concept in virtual reality and conducted a user study with a pointing task. The input surface can be designed as rotating with the wrist or fixed relative to the wrist. We evaluated and compared participants' subjective physical comfort level, pointing speed and pointing accuracy on the interface that was divided into 64 regions. We found that the interface whose orientation was fixed had a much better performance, with 41.2% higher average comfort score, 40.6% shorter average pointing time and 34.5% lower average error. Our results revealed interesting insights on user performance and preference of different regions on the interface. We concluded with a set of guidelines for future designers and developers on how to develop this type of new body-centric input surface.",body-centric interaction; interface evaluation; mid-air input surface,Abstract,True,
Scopus,journalPaper,2018,Workaholistic: on balancing typing- and handover-performance in automated driving,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Automated driving eliminates the permanent need for vehicle control and allows to engage in non-driving related tasks. As literature identifies office work as one potential activity, we estimate that advanced input devices will shortly appear in automated vehicles. To address this matter, we mounted a keyboard on the steering wheel, aiming to provide an exemplary safe and productive working environment. In a driving simulator study (n=20), we evaluated two feedback mechanisms (heads-up augmentation on a windshield, conventional heads-down display) and assessed both typing effort and driving performance in handover situations. Results indicate that the windshield alternative positively influences handovers, while heads-down feedback results in better typing performance. Text difficulty (two levels) showed no significant impact on handover time. We conclude that for a widespread acceptance of specialized interfaces for automated vehicles, a balance between safety aspects and productivity must be found in order to attract customers while retaining driving safety.",augmented reality; automated driving; handover/take-over requests; non-driving related tasks; typing; windshield displays,Keywords,True,
Scopus,journalPaper,2018,RadialLight: exploring radial peripheral LEDs for directional cues in head-mounted displays,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Current head-mounted displays (HMDs) for Virtual Reality (VR) and Augmented Reality (AR) have a limited field-of-view (FOV). This limited FOV further decreases the already restricted human visual range and amplifies the problem of objects going out of view. Therefore, we explore the utility of augmenting HMDs with RadialLight, a peripheral light display implemented as 18 radially positioned LEDs around each eye to cue direction towards out-of-view objects. We first investigated direction estimation accuracy of multi-colored cues presented on one versus two eyes. We then evaluated direction estimation accuracy and search time performance for locating out-of-view objects in two representative 360° video VR scenarios. Key findings show that participants could not distinguish between LED cues presented to one or both eyes simultaneously, participants estimated LED cue direction within a maximum 11.8° average deviation, and out-of-view objects in less distracting scenarios were selected faster. Furthermore, we provide implications for building peripheral HMDs.",augmented reality; directional cues; head-mounted displays; LEDs; peripheral display; virtual reality; wide field-of-view,Abstract_Keywords,True,
Scopus,journalPaper,2018,Beyond Halo and Wedge: visualizing out-of-view objects on head-mounted virtual and augmented reality devices,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Head-mounted devices (HMDs) for Virtual and Augmented Reality (VR/AR) enable us to alter our visual perception of the world. However, current devices suffer from a limited field of view (FOV), which becomes problematic when users need to locate out of view objects (e.g., locating points-of-interest during sightseeing). To address this, we developed and evaluated in two studies HaloVR, WedgeVR, HaloAR and WedgeAR, which are inspired by usable 2D off-screen object visualization techniques (Halo, Wedge). While our techniques resulted in overall high usability, we found the choice of AR or VR impacts mean search time (VR: 2.25s, AR: 3.92s) and mean direction estimation error (VR: 21.85°, AR: 32.91°). Moreover, while adding more out-of-view objects significantly affects search time across VR and AR, direction estimation performance remains unaffected. We provide implications and discuss the challenges of designing for VR and AR HMDs.",augmented reality; Halo; head-mounted; off-screen; out-of-view; virtual reality; visualization technique; Wedge,Title_Abstract_Keywords,True,
Scopus,journalPaper,2018,ARPilot: designing and investigating AR shooting interfaces on mobile devices for drone videography,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Drones offer camera angles that are not possible with traditional cameras and are becoming increasingly popular for videography. However, flying a drone and controlling its camera simultaneously requires manipulating 5-6 degrees of freedom (DOF) that needs significant training. We present ARPilot, a direct-manipulation interface that lets users plan an aerial video by physically moving their mobile devices around a miniature 3D model of the scene, shown via Augmented Reality (AR). The mobile devices act as the viewfinder, making them intuitive to explore and frame the shots. We leveraged AR technology to explore three 6DOF video-shooting interfaces on mobile devices: AR keyframe, AR continuous, and AR hybrid, and compared against a traditional touch interface in a user study. The results show that AR hybrid is the most preferred by the participants and expends the least effort among all the techniques, while the users' feedback suggests that AR continuous empowers more creative shots. We discuss several distinct usage patterns and report insights for further design.",augmented reality; human-drone interaction; interaction techniques; mobile device; tangible; virtual camera control,Abstract_Keywords,True,
Scopus,journalPaper,2019,SeeingHaptics: Visualizations for Communicating Haptic Designs,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Rendering haptic feedback in virtual reality is a common approach to enhancing the immersion of virtual reality content. However, current editing tools allow developers to access the haptic feedback only through physical contact with the actuators, making it difficult to fast iterate haptic interaction designs. This paper introduces SeeingHaptics, an authoring tool which visualizes haptic properties in 3D scenes. The active area of certain feedback is simulated with mesh shapes, while the 2D icons allow for indicating the type of haptic sensation. Our evaluation showed that SeeingHaptics helps developers rapidly create haptic feedbacks after a short training session.",haptics design; virtual reality; visualization,Abstract_Keywords,True,
Scopus,journalPaper,2019,Investigating Smartphone-based Pan and Zoom in 3D Data Spaces in Augmented Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"In this paper, we investigate mobile devices as interactive controllers to support the exploration of 3D data spaces in head-mounted Augmented Reality (AR). In future mobile contexts, applications such as immersive analysis or ubiquitous information retrieval will involve large 3D data sets, which must be visualized in limited physical space. This necessitates efficient interaction techniques for 3D panning and zooming. Smartphones as additional input devices are promising because they are familiar and widely available in mobile usage contexts. They also allow more casual and discreet interaction compared to free-hand gestures or voice input. We introduce smartphone-based pan &amp; zoom techniques for 3D data spaces and present a user study comparing five techniques. Our results show that spatial device gestures can outperform both touch-based techniques and hand gestures in terms of task completion times and user preference. We discuss our findings in detail and suggest suitable techniques for specific AR navigation tasks.",3D Data Exploration; 3D Navigation; Augmented Reality; Immersive Visualization; Interaction Techniques; Pan &amp; Zoom,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Cake Cam: Take Your Photo and Be in it Too,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Tourists often turn to strangers when they need a photographer while traveling; however, they do so at a cost. Strangers are not typically trained photographers, nor are they telepathically intuiting what composition the tourist wants. Existing smartphone camera interfaces do not communicate the desired framing to the stranger, and prior work in mobile photography guidance does not manage the 3D movement required when composing the tourist's ideal photo. We offer a new kind of mobile interaction for communicating the intended photo to a stranger without instructions. In our methodology, the tourist first composes a photo with the desired framing. The app, Cake Cam, then stores the camera position and orientation. Finally, 3D augmented reality markers guide the stranger to retake the photo with the tourist now standing in the frame. Our study resulted in more accurate camera placements and required fewer additional instructions than the traditional tourist photography method (n=40).",Augmented reality; Collaborative photography,Abstract_Keywords,True,
Scopus,journalPaper,2019,Investigating the Influence of External Car Displays on Pedestrians' Crossing Behavior in Virtual Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Focusing on pedestrian safety in the era of automated vehicles, we investigate the interaction between pedestrians and automated cars. In particular, we investigate the influence of external car displays (ECDs) on pedestrians' crossing behavior, and the time needed to make a crossing decision. We present a study in a high-immersion VR environment comparing three alternative car-situated visualizations: a smiling grille, a traffic light style indicator, and a gesturing robotic driver. Crossing at non-designated crossing points on a straight road and at a junction, where vehicles turn towards the pedestrian, are explored. We report that ECDs significantly reduce pedestrians' decision time, and argue that ECDs support comfort, trust and acceptance in automated vehicles. We believe ECDs might become a valuable addition for future vehicles.",Autonomous vehicles; External car displays; Pedestrian-autonomous vehicle interaction; Traffic safety; Virtual reality,Title_Keywords,True,
Scopus,journalPaper,2019,Exploring the Potential of Augmented Reality in Domestic Environments,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"While Augmented Reality (AR) technologies are becoming increasingly available, our understanding of AR is primarily limited to controlled experiments which address use at work or for entertainment. Little is known about how it could enhance everyday interaction from a user's perspective. Personal use of AR at home may improve how users' interface with information on a daily basis. Through an online survey, we investigated attitudes towards domestic AR. We further explored the opportunities for AR at home in a technology probe. We first introduced the users to AR by offering an AR experience presented through mixed reality smart glasses. We then used a tailor-made tablet application to elicit photos illustrating how users imagine future AR experiences. Finally, we conducted semi-structured interviews based on elicited photos. Our results show that users are eager to benefit from on-demand information, assistance, enhanced sensory perception, and play offered by AR across many locations at home. We contribute insights for future AR systems designed for domestic environments.",augmented reality; domestication; survey; technology probe,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,CryptoAR Wallet: A Blockchain Cryptocurrency Wallet Application that Uses Augmented Reality for On-chain User Data Display,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Blockchain technology has recently become popular and its use in business and industry has increased, especially in finance and technology. A blockchain wallet plays a vital role in blockchain industry, but it is difficult to understand and get started, as well as hard to learn how to use blockchain wallet. From the aspect of Interaction Design, we proposed a blockchain cryptocurrency wallet that combine with augmented reality and crypto technology, that is, an Augmented Crypto Wallet (CryptoAR Wallet). We except that browsing and viewing virtual information (On-Chain User Data) through augmented reality, will shorten the distance between user and blockchain wallet services. Though our preliminary design, development and user testing, this in-development application shows its potential on increasing the level of trust and satisfactions, with more comprehensive user experience.",Augmented Reality; Blockchain; Cryptocurrency; User Trust; Virtual Information,Title_Abstract_Keywords,True,
Scopus,journalPaper,2019,Fostering Virtual Guide in Exhibitions,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Museums are essential to make culture accessible to the mass audience. Human museum guides are important to explain the presented artifacts to the visitors. Recently, museums started to experiment with enhancing exhibitions through mixed reality. It enables cultural exhibitors to provide each visitor with an individualized virtual guide that adapts to the visitor's interests. The effect of the presence and appearance of a virtual museum guide is, however, unclear. In this paper, we compare a real-world guide with a realistic, an abstract, and an audio-only representation of the virtual guide. Participants followed four multimodal presentations while we investigated the effect on comprehension and perceived co-presence. We found that a realistic representation of a virtual guide increases the perceived co-presence and does not adversely affect the comprehension of learning content in mixed reality exhibitions. Insights from our study inform the design of virtual guides for real-world exhibitions.",co-presence; exhibition; Mixed reality; virtual avatar,Abstract_Keywords,True,
Scopus,journalPaper,2020,Augmenting Public Bookcases to Support Book Sharing,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Public bookcases offer the opportunity to serendipitously discover books and to anonymously share books with others. The set of available books as well as the sharing patterns are highly dynamic, as anybody can freely take or donate books. This makes it difficult for users to see what is available or of interest to them. To support book sharing via public bookcases we developed a mobile AR application that highlights relevant books in the camera viewfinder and that facilitates searching for specific books. The application recognizes books via text and color features on the spine. In a lab study with 15&nbsp;participants we evaluated our book recognition algorithm and found that it outperforms unaided visual search. We interviewed users of public bookcases and analyzed the bookcases’ setup and rate of change. A subsequent field evaluation of the AR application on nine public bookcases found a recognition accuracy of 80&nbsp;% for 450 books under different conditions. The proposed approach provides the basis for effectively sharing books via public bookcases.",Augmented Reality; Computer Vision; Design for Sharing; Mobile Interaction; Sharing Community; Sharing Economy,Keywords,True,
Scopus,journalPaper,2020,Distance-Dependent Barcodes for&nbsp;Context-Aware&nbsp;Mobile&nbsp;Applications,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"This article introduces the novel concept of distance-dependent barcodes, which provide users with different data based on their scanning distance. These barcodes employ color blending as the key technique to achieve distance-dependence. A simple yet robust encoding scheme is devised accordingly to distinguish between near and far users. Through several experimental results, the proposed technique is shown to be effective (in terms of clear separation between near and far scanners), reliable (as to successful scans), and practical (it can be used in off-the-shelf smartphones). A few representative use cases are then presented to establish distance-dependent barcodes as an enabling technology for context-aware mobile applications. They include casual interactions with public displays, where the role of users is determined based on their distance from a screen, and augmented reality in retail, where distance-dependent barcodes provide information on available goods with different granularities. Finally, distance-dependent barcodes are shown to be user-friendly and effective through a user study.",augmented reality; barcode; color blending; context awareness; distance-dependent encoding; mobile computing; public displays; smartphone,Abstract_Keywords,True,
Scopus,journalPaper,2020,Evaluating Menu Techniques for Handheld AR with a Smartphone &amp; Mid-Air Pen,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Adding a mid-air pen to Handheld Augmented Reality creates a new kind of bimanual interaction for which many fundamental interaction design questions have not been answered yet. In particular, menus are an essential component in most visual interfaces, but it is unclear how to best interact with them in this setting: using the pen in mid-air or on a surface, using the touchscreen, or by moving the smartphone itself. We compared basic menus for these methods by analyzing success rates, selection times, device movement, and subjective ratings. Our results indicate that interacting with a mid-air menu using the pen, and operating a menu with the hand holding the smartphone, are sufficiently competitive to the current standard of two-handed touchscreen interaction, so that interaction designers can freely choose among them based on the interaction context of their application.",3D; AR; ARPen; Augmented Reality; Bimanual; Handheld; Interaction; Menu techniques; Mid-air; Mobile; Smartphone,Abstract_Keywords,True,
Scopus,journalPaper,2021,Towards Transparent Behavior of Automated Vehicles: Design and Evaluation of HUD Concepts to Support System Predictability Through Motion Intent Communication,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"In automated vehicles, it is essential to feedforward motion intentions to users so that they understand the vehicle’s actions. Otherwise, non-transparency limits situation awareness and leads to mistrust. In this work, we are communicating the vehicle’s actions to the user either by displaying icons (planar HUD) or through augmented reality (contact-analog HUD) to increase transparency. We developed both concepts in a user-centered design process. Further, we evaluated them in two subsequent user studies (N = 27). In the first study, we focused on UX and trust in higher automation levels (cf. SAE level 3-5). In the second study, we focused on safety and error prevention in lower automation levels (cf. SAE levels 1-2). Our results show that both visualizations increase UX and trust in an automated system. Nevertheless, the AR approach outperforms the icon-based approach by achieving higher user experience as well as faster and less error-prone take-overs of participants.",Augmented Reality; Automated Vehicles; Autonomous Driving; Head-up Displays; Intention Communication; Safety; Situation Awareness; Transparency; Trust; UX; Virtual Windshields,Abstract_Keywords,True,
Scopus,journalPaper,2021,Increasing Pedestrian Safety Using External Communication of Autonomous Vehicles for Signalling Hazards,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Pedestrians are very vulnerable road users. Autonomous vehicles driving on the street are expected to reduce the risks for pedestrians. However, traffic will include manually driven vehicles for the next decades; therefore, the risk remains and can even increase due to pedestrians’ overreliance on technology. Thus, we propose to use autonomous vehicles parked at the side of the road to continually survey its surroundings and to issue warnings to pedestrians in potentially risky situations. Findings of a Virtual Reality (N=20) study show that participants preferred the communication on the vehicle compared to a communication on the sidewalk, that the visualization grabbed participants’ attention, and that overall crossing time was not significantly affected. This concept highlights the potential autonomous vehicles could have in making traffic safer, even while being parked.",Autonomous vehicles; external communication.; self-driving vehicles,Abstract,True,
Scopus,journalPaper,2021,Understanding Drone Landing on the Human Body,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We envision the human body as a platform for fast take-off and landing of drones in entertainment and professional uses such as medical emergencies, rescue missions, or supporting police units. This new interaction modality challenges our knowledge of human-drone experiences, in which interaction usually occurs at a distance from the body. This work explores important factors for understanding the interplay between drones and humans. We first investigated the suitability of various body locations for landing in an online study (N = 159). Our results, presented as body maps, show that the hand and upper back are particularly well-suited body locations. We further tested these findings in a follow-up study (N = 12), in which participants experienced drones landing on their bodies through carefully designed and pre-recorded 360°videos. This immersion into the landing scenarios helped us to identify common themes and research approaches for different body parts. Taken together, the findings provide first insights into location preferences and themes for drones landing on the human body.",Drone; Human-Drone Interaction; Mechanical Turk; Micro Air Vehicle; On-Body Landing; UAV; Virtual Reality Study,Keywords,True,
Scopus,journalPaper,2021,"Grab the Frog: Comparing Intuitive Use and User Experience of a Smartphone-only, AR-only, and Tangible AR Learning Environment",MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The integration of Augmented Reality (AR) in teaching concepts allows for the visualization of complex learning contents and can simultaneously enhance the learning motivation. By providing Tangible Augmented Reality (TAR), an AR learning environment receives a haptic aspect and allows for a direct manipulation of augmented learning materials. However, manipulating tangible objects while using handheld AR might reduce the intuitive use and hence user experience. Users need to simultaneously control the application and manipulate the tangible object. Therefore, we compare the differences in intuitive use and user experience evoked by varied technologies of knowledge presentation in an educational context. In particular, we compare a TAR learning environment targeting the learning of the anatomy of vertebrates to its smartphone-only and AR-only versions. The three versions of the learning environment only differ in their method of knowledge presentation. The three versions show a similar perceived intuitive use. The TAR version, however, yielded a significantly higher attractiveness and stimulation than AR-only and smartphone-only. This suggests a positive effect of TAR learning environments on the overall learning experience.",augmented reality; education; gamification; serious games; tangible user interfaces,Abstract_Keywords,True,
Scopus,journalPaper,2021,AR-enhanced Widgets for Smartphone-centric Interaction,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We contribute a detailed investigation of AR-enhanced widgets for smartphones, where AR technology is not only used to offload widgets from the phone to the air around it, but to give users more control on input precision as well. Such widgets have the obvious benefit of freeing up screen real-estate on the phone, but their other potential benefits remain largely theoretical. Their limitations are not well understood, most particularly in terms of input performance. We compare different AR-enhanced widget designs against their state-of-the-art touch-only counterparts with a series of exploratory studies in which participants had to perform three tasks: command trigger, parameter value adjustment, and precise 2D selection. We then derive guidelines from our empirical observations.",augmented reality; mobile phone; multi-device mobile setup; widget control,Keywords,True,
Scopus,journalPaper,2021,Mobile Screen-based User Interface Design Guideline for Panoramic VR in Shopping Scene,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The pandemic has shifted people’s shopping behavior towards online resources. Mobile screen-based panoramic virtual reality (VR) appears to be a promising form for future online shopping. However, the existing user interface (UI) design of panoramic VR shopping needs more natural interactions based on a rational standard design guideline. In this article, we first conducted a heuristic study to compare six representative virtual store apps. This revealed a gap between users’ requirements and the current design attempts. Thus, a UI design guideline for panoramic VR in the shopping scene was presented. Then, we conducted a verification study with a demo optimized according to our design guideline. Both user experience and interactive efficiency improved as a result of the design guideline. Our design guidelines and discoveries derived from user studies provided references for the design, development, and potential use of dynamic panoramic VR in shopping scene.",Human-mobile interaction; Panoramic VR; Shopping,Abstract,True,
Scopus,journalPaper,2021,Understanding Bystanders’ Tendency to Shoulder Surf Smartphones Using 360-degree Videos in Virtual Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Shoulder surfing is an omnipresent risk for smartphone users. However, investigating these attacks in the wild is difficult because of either privacy concerns, lack of consent, or the fact that asking for consent would influence people’s behavior (e.g., they could try to avoid looking at smartphones). Thus, we propose utilizing 360-degree videos in Virtual Reality (VR), recorded in staged real-life situations on public transport. Despite differences between perceiving videos in VR and experiencing real-world situations, we believe this approach to allow novel insights on observers’ tendency to shoulder surf another person’s phone authentication and interaction to be gained. By conducting a study (N=16), we demonstrate that a better understanding of shoulder surfers’ behavior can be obtained by analyzing gaze data during video watching and comparing it to post-hoc interview responses. On average, participants looked at the phone for about 11% of the time it was visible and could remember half of the applications used.",Eye Tracking; Omnidirectional Videos; Shoulder Surfing; Virtual Reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,"Bus Stop Spotting: a Field Experiment Comparing 2D Maps, Augmented Reality and 3D Maps",MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Urban environments can be challenging to navigate due to the often ambiguous, sometimes even repetitive cues, combined with the cognitive pressure of busy surroundings. The last step of navigation, reaching the goal, may still be frustrating for an unfamiliar target. We pursue a visualization for instantaneous identification of a proximal but ambiguous target, freeing us from turn-by-turn guidance. We present a field experiment for bus stop spotting in a central public transport hub using a 2D map, augmented reality (AR), and realistic 3D maps at oblique bird’s eye view and at ground level. 3D Bird’s eye view provided fastest responses and highest success rates. AR and 2D map provided slowest responses and lowest success rates. 3D ground level view performed between these, but without a significant difference to AR and 2D map. 3D Bird’s Eye view was considered the most useful modality by subjects, and was also selected as the preferred modality.",3D maps; augmented reality; field experiments; navigation; Spatial interfaces,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Evaluating Prototype Augmented and Adaptive guidance system to support Industrial Plant Maintenance,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We evaluate AR for Plant maintenance by measuring how a prototype guidance system, tested under representative conditions, impacts performance. We are motivated to determine the cost-benefit of interactive guidance for hazardous, repetitive tasks and we observe an improvement of 21% efficiency, 50% accuracy and 19% reduced task load. AR has already been shown to deliver improvements in task performance, however, there is limited research exploring the integration of AR into complete task routines which presents a barrier to adoption. We apply mixed reality guidance via two within-group experiments. We measure efficiency and accuracy over a complete routine conducted under simulated conditions. Results compare AR versus Static and AR versus Adaptive. We conclude AR is best suited to demanding spatial translation and completion under pressure. We suggest AR offers potential in similar routines and propose further work to integrate in a live setting.",Industrial Augmented Reality; Mixed / Augmented Reality,Abstract_Keywords,True,
Scopus,journalPaper,2022,Investigating the Effects of External Communication and Automation Behavior on Manual Drivers at Intersections,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Automated vehicles are expected to substitute or even improve driver-driver communication, for example, via LED strips or displays. Numerous situations exist where ambiguities have to be resolved via gestures or implicit communication (i.e., movement). An already demanding situation is the unsignalized four-way intersection. Additionally, Vehicle-To-Everything technology enables automated vehicles to perform maneuvers impossible before such as blocking an intersection to safely let an emergency vehicle pass. Therefore, we report the results of a within-subject Virtual Reality study (N=17) evaluating these two scenarios. Results show that external communication increased perceived safety and reduced mental workload, and also that the novel behavior confused participants. Our work helps better to integrate external communication in settings with manual drivers.",automated vehicles; external communication; self-driving vehicles; virtual reality.,Abstract_Keywords,True,
Scopus,journalPaper,2022,AR4CAD: Creation and Exploration of a Taxonomy of Augmented Reality Visualization for Connected Automated Driving,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Infrastructure-mounted sensors that monitor roads can provide essential information for manual drivers and automated vehicles, e.g., positions of other vehicles occluded by buildings. However, human drivers and passengers have to trust and accept their use. This raises the question of how trust can be increased in such a scenario. One important factor for this is understanding the available information, including its quality and, for passengers of automated vehicles, the actions planned based on it. For this, augmented reality is a promising visualization technology because it can present the relevant information integrated into the physical world. Thus, this work develops a taxonomy of augmented reality visualizations for connected automated and manual driving. It is intended to classify and compare existing visualizations, identify novel visualizations, and provide a common language for discussions. The use case infrastructure-supported automated driving is explored by suggesting augmented reality visualizations to inform passengers of automated vehicles and are intended to increase trust. They present information available from infrastructure and onboard sensors as well as the driving decisions based on it. Finally, we evaluated the visualizations' influence on trust in an automated vehicle by conducting a driving simulator study (N=18). Results indicate a high dependency of trust on presenting driving decisions and information on road users but less on location-specific information.",augmented reality; automated vehicles; connected driving; interface design.,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,A Systematic Evaluation of Solutions for the Final 100m Challenge of Highly Automated Vehicles,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Automated vehicles will change the interaction with the user drastically. While freeing the user of the driving task for most of the journey, the ""final 100 meters problem”, directing the vehicle to the final parking spot, could require human intervention. Therefore, we present a classification of interaction concepts for automated vehicles based on modality and interaction mode. In a subsequent Virtual Reality study (N=16), we evaluated sixteen interaction concepts. We found that the medially abstracted interaction mode was consistently rated most usable over all modalities (joystick, speech, gaze, gesture, and tablet). While the steering wheel was still preferred, our findings indicate that other interaction concepts are usable if the steering wheel were unavailable.",automated vehicles.; interaction modalities; systematic comparison,Abstract,True,
Scopus,journalPaper,2022,OsciHead: Simulating Versatile Force Feedback on an HMD by Rendering Various Types of Oscillation,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Current haptic devices are usually designed to provide one type of force feedback; however, most VR scenarios require versatile force feedback, which may require the integration of different devices to provide various types of forces. In addition, besides the main haptic effects caused by the forces, multiple types of oscillation may also commonly accompany them, which are crucial for improving VR realism and immersion. Therefore, we simulate versatile force feedback by rendering the corresponding types of oscillation as the effects caused by those forces. We take inertia and impact forces as examples in this paper, and achieve versatility using the proposed device, OsciHead, on a head-mounted display (HMD), instead of integrating different devices. By controlling elastic bands' elasticity and stored power, OsciHead uses two rotatable oscillators on both sides of the HMD, in order to render various multilevel and multidimensional oscillation feedback in 2D translation and 2D rotation directions on a head. In an exploratory study, we explored different scenarios in which multiple types of oscillation could be simulated by OsciHead. We then observed oscillation level distinguishability in two just-noticeable difference (JND) studies, and evaluated the oscillation type recognition rates in a recognition study. Based on the results, we performed a VR study, which verified that the inertia and impact feedback simulated by OsciHead enhances realism and achieves versatility.",haptic feedback; hmd prototype; oscillation; virtual reality.,Keywords,True,
Scopus,journalPaper,2022,Designing Mobile MR Workspaces: Effects of Reality Degree and Spatial Configuration During Passenger Productivity in HMDs,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Virtual Reality (VR) is increasingly used in everyday contexts for a variety of tasks. We particularly look at the confined space for passengers inside cars, where head-mounted displays (HMDs) could complement the prevalent use of mobile devices for work. In a field study (N=19), we tested three mobile workspace setups along the reality-virtuality continuum (Mounted Tablet, Augmented Focus Bubble, and Virtual Office) and let users re-position the virtual keyboard and display while typing on a physical keyboard in a parked car. The results revealed that using HMDs lowered users' awareness of their real surroundings but increased their perceived workload with a performance impairment of text entry rate compared to just using a tablet. Letting users customize their workspace layout improved their perceived performance and decreased pitch-axis head movements for switching between the virtual display and keyboard. This paper discusses challenges and strategies for future work regarding dynamic incorporation of productivity tools, adaptive mixed reality (MR) work environment designs, and optimizing upper thresholds of physical discomfort in mobile MR workspaces.",in-car mixed reality; mobile workspace; typing task,Abstract_Keywords,True,
Scopus,journalPaper,2022,ARm Haptics: 3D-Printed Wearable Haptics for Mobile Augmented Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Augmented Reality (AR) technology enables users to superpose virtual content onto their environments. However, interacting with virtual content while mobile often requires users to perform interactions in mid-air, resulting in a lack of haptic feedback. Hence, in this work, we present the ARm Haptics system, which is worn on the user's forearm and provides 3D-printed input modules, each representing well-known interaction components such as buttons, sliders, and rotary knobs. These modules can be changed quickly, thus allowing users to adapt them to their current use case. After an iterative development of our system, which involved a focus group with HCI researchers, we conducted a user study to compare the ARm Haptics system to hand-tracking-based interaction in mid-air (baseline). Our findings show that using our system results in significantly lower error rates for slider and rotary input. Moreover, use of the ARm Haptics system results in significantly higher pragmatic quality and lower effort, frustration, and physical demand. Following our findings, we discuss opportunities for haptics worn on the forearm.",augmented reality; empirical evaluation; haptics; mobile; wearable,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,AR Sightseeing: Comparing Information Placements at Outdoor Historical Heritage Sites using Augmented Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Augmented Reality (AR) has influenced the presentation of historical information to tourists and museum visitors by making the information more immersive and engaging. Since smartphones and AR glasses are the primary devices to present AR information to users, it is essential to understand how the information about a historical site can be presented effectively and what type of device is best suited for information placements. In this paper, we investigate the placement of two types of content, historical images and informational text, for smartphones and AR glasses in the context of outdoor historical sites. For this, we explore three types of placements: (1) on-body, (2) world, and (3) overlay. To evaluate all nine combinations of text and image placements for smartphone and AR glasses, we conducted a controlled experiment (N = 18) at outdoor historical landmarks. We discovered that on-body image and text placements were the most convenient compared to overlay and world for both devices. Furthermore, participants found themselves more successful in exploring historical sites using a smartphone than AR glasses. Although interaction with a smartphone was more convenient, participants found exploring AR content using AR glasses more fun.",augmented reality; historical heritage; information placement; sightseeing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,What Is Happening Behind The Wall? Towards a Better Understanding of a Hidden Robot's Intent By Multimodal Cues,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Research in human-robot collaboration explores aspects of using interaction modalities and their effect on human perception. Particular attention is paid to intent communication, which is essential for successful interaction and collaboration. This work investigates the effect of using audio, visual, and haptic feedback on intent communication in a human-robot collaboration task where the collaborators do not share a direct line of sight. A user study was conducted in virtual reality with 20 participants. Qualitative and quantitative feedback was collected from all participants. When compared with a baseline of no feedback given to the participants, results show that using visual feedback had a significant impact on task efficiency, user experience, and cognitive load. Audio feedback was slightly less impactful, while haptic feedback had a divisive effect. Multimodal feedback combining the three modalities showed the highest impact compared to the individual modalities, leading to the highest task efficiency and user experience, and the lowest cognitive load.",human-robot collaboration; human-robot interaction; robotics; user studies,Abstract,True,
Scopus,journalPaper,2022,NotiBike: Assessing Target Selection Techniques for Cyclist Notifications in Augmented Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Cyclists' attention is often compromised when interacting with notifications in traffic, hence increasing the likelihood of road accidents. To address this issue, we evaluate three notification interaction modalities and investigate their impact on the interaction performance while cycling: gaze-based Dwell Time, Gestures, and Manual And Gaze Input Cascaded (MAGIC) Pointing. In a user study (N=18), participants confirmed notifications in Augmented Reality (AR) using the three interaction modalities in a simulated biking scenario. We assessed the efficiency regarding reaction times, error rates, and perceived task load. Our results show significantly faster response times for MAGIC Pointing compared to Dwell Time and Gestures, while Dwell Time led to a significantly lower error rate compared to Gestures. Participants favored the MAGIC Pointing approach, supporting cyclists in AR selection tasks. Our research sets the boundaries for more comfortable and easier interaction with notifications and discusses implications for target selections in AR while cycling.",augmented reality; cycling; notifications; selection,Title_Abstract_Keywords,True,
Scopus,journalPaper,2022,A Study into the Effect of Mobile Device Configurations on Co-Located Collaboration using AR,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The increasing availability of portable handheld mobile Augmented Reality technology is revolutionising the way digital information is embedded into the real world. As this data is embedded, it enables new forms of cross-device collaborative work. However, despite the widespread availability of handheld AR, little is known about the role that device configurations and size play on collaboration. This paper presents a study that examines how completing tasks using a simple mobile AR interface on different device sizes and configurations impacts key factors of collaboration such as collaboration strategy, behaviour, and efficacy. Our results show subtle differences between device size and configurations that have a direct influence on the way people approach tasks and interact with virtual models. We highlight key observations and strategies that people employ across different device sizes and configurations.",augmented reality; co-located collaboration; collaboration; mobile augmented reality; mobile interaction,Abstract_Keywords,True,
Scopus,journalPaper,2023,A First Exploration on the Use of Head-Mounted Augmented Reality in the Context of the Portuguese Military,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"In this paper, we present the design and implementation of a first iteration of an augmented reality (AR) system for dismounted soldiers in the Portuguese military. We started the work via a survey of 86 members of the military to better understand their experience, needs, and preferences with current Command &amp; Control (C2) systems. We then assessed the effects of our prototype on the performance, situational awareness, and perceived usability and workload of 13 participants from a local Commando Regiment. We compared our results to a representative baseline using a paper map and radio in a hostage extraction simulation and found that our first AR iteration, despite a short practice session, increased the quality of the information available and decreased the complexity, temporal demands, and effort required to complete the study tasks; leading to an overall decrease in perceived workload. Overall, participants described the AR experience as more user-friendly. We conclude our case study with research ideas for further iterations of our prototype.",augmented reality; extended reality; head-mounted displays; military; situational awareness,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,"A Mobile Augmented Reality App for Creating, Controlling, Recommending Automations in Smart Homes",MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Automations in the context of smart homes have been adopted more and more frequently; thus, users should be able to control them and create automations most suitable to their needs. Current solutions for this purpose are based on visual apps with conceptual representations of possible automation elements. However, they tend to be static, abstract, and detached from the user's real context. In this paper, we propose a novel solution based on mobile augmented reality, which provides situated, dynamic representations associated with the physical objects available in the current users' context while they are freely moving about. It allows direct interaction with the objects of interest, monitoring nearby objects' automations while moving, and creating new automations or modifying existing ones. It also supports users with recommendations of object and service configurations relevant to complete the editing of the new automations. The paper also reports on a user test, which provided positive feedback.",augmented reality; end-user development; internet of things; recommender systems; trigger-action programming,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Adapting Visual Complexity Based on Electrodermal Activity Improves Working Memory Performance in Virtual Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Biocybernetic loops encompass users' state detection and system adaptation based on physiological signals. Current adaptive systems limit the adaptation to task features such as task difficulty or multitasking demands. However, virtual reality allows the manipulation of task-irrelevant elements in the environment. We present a physiologically adaptive system that adjusts the virtual environment based on physiological arousal, i.e., electrodermal activity. We conducted a user study with our adaptive system in social virtual reality to verify improved performance. Here, participants completed an n-back task, and we adapted the visual complexity of the environment by changing the number of non-player characters. Our results show that an adaptive virtual reality can control users' comfort, performance, and workload by adapting the visual complexity based on physiological arousal. Thus, our physiologically adaptive system improves task performance and perceived workload. Finally, we embed our findings in physiological computing and discuss applications in various scenarios.",adaptive systems; electrodermal activity; physiological computing; virtual reality; visual complexity; working memory,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,An Asymmetric Multiplayer Learning Environment for Room-Scale Virtual Reality and a Handheld Device,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Many different digital learning environments are currently in use. In combination with virtual reality (VR) technologies, these allow the creation of engaging hands-on experiences. While VR environments can deeply immerse the person wearing the headset, spectators are often not actively involved or are not even considered in the design phase. This is an issue for learning environments, as learning often takes place in pairs or groups. We propose a novel system that enables more than one person to join the VR world in a co-located space to overcome this problem. In addition to the classic VR headset, the asymmetric VR system features a position-tracked tablet. To evaluate this asymmetric VR concept, we conducted a study with 14 students to explore the user experience and motivation, the social presence, and possible further fields of application. The results indicate that users in both perspectives feel that they can control the virtual world.",asymmetric multiplayer; collaborative learning; STEM education; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,BeeAR: Augmented Reality Beeline Navigation for Spatial Knowledge Acquisition,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Navigation assistance systems have become integral to our daily routines, helping us to find our way through unfamiliar environments. However, their use may come at a price, as empirical evidence suggests a potentially harmful impact of these systems on our spatial abilities, including the acquisition of spatial knowledge. This could be remedied by giving users more freedom and involving them in the decision-making process. Therefore, we present a navigation system that combines augmented reality and Beeline Navigation (BeeAR). Here, the location of the destination is overlaid with a digital landmark and permanently displayed to the user via a visual, translucent AR display (without a map). Since the digital content is integrated into the real world, no mapping between the device and reality is required, potentially lowering the workload. Making one's own decisions along the route is expected to increase engagement with the environment, leading to increased acquisition of spatial knowledge. We compare BeeAR with findings from a previous study comparing Free Choice Navigation (FCN) and Turn-by-Turn (TBT) navigation conducted along the same routes on the outskirts of Vienna, Austria. Although BeeAR and FCN do not provide users with a map, BeeAR users could better retrace the walked route and remembered more points of interest along the route than FCN users. Participants of all three navigation conditions achieved a high configuration similarity between drawn points of interest and their true locations, albeit only one navigation condition included a map.",augmented reality; beeline navigation; spatial knowledge acquisition,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Drawing Connections: Designing Situated Links for Immersive Maps,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We explore the design of situated visual links in outdoor augmented reality (AR) for connecting miniature buildings on a virtual map to their real-world counterparts. We first distill design criteria from prior work, then conduct two user studies to evaluate a set of proposed link designs to better understand users' preferences for different design choices of the links. In two user studies we evaluated, respectively, a set of link geometries in a virtual environment and a refined AR prototype in two different outdoor environments. The studies reveal that links help in identifying buildings in the environments. Participants prefer straight rather than curved links, simple and thin links to avoid information occlusion, and links and maps aligned with their direction of view. We recommend using a consistent color with a strong contrast to the background color for all links in a scene. To improve visibility, the diameter of links should grow with distance to the viewer and optional animated stripes can be placed on links. The findings of this study have the potential to bolster the development of various situated visualization applications, such as those used in urban planning, tourism, smart agriculture, and other fields.",augmented reality; immersive visualization; mixed reality; situated visualization; visual links,Abstract_Keywords,True,
Scopus,journalPaper,2023,DrivingVibe: Enhancing VR Driving Experience using Inertia-based Vibrotactile Feedback around the Head,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"We present DrivingVibe, which explores vibrotactile feedback designs around the head to enhance VR driving motion experiences. We propose two approaches that use a 360-degree vibrotactile headband: 1) mirroring and 2) 3D inertia-based. The mirroring approach extends the vibrotactile patterns of handheld controllers to actuate the entire headband uniformly. The 3D inertia-based approach uses the acceleration telemetry data that driving games/simulators export to motion platforms to generate directional vibration patterns, including: i) centrifugal forces, ii) horizontal acceleration/deceleration, and iii) vertical motion due to rough terrain. The two approaches are complementary as the mirroring approach supports all driving games because it does not require telemetry data, while the 3D inertia-based approach provides higher feedback fidelity for games that provide such data. We conducted a 24-person user experience evaluation in both passive passenger mode and active driving mode. Study results showed that both DrivingVibe designs significantly improved realism, immersion, and enjoyment (p&lt;.01) with large effect sizes for the VR driving experiences. For overall preference, 88% (21/24) of participants preferred DrivingVibe, with a 2:1 preference for 3D inertia-based vs. mirroring designs (14 vs. 7 participants). For immersion and enjoyment, 96% (23/24) of participants preferred DrivingVibe, with nearly a 3:1 preference (17 vs. 6 participants) for the 3D inertia-based design.",games/play; haptic; head vibrations; motion simulators; sensorimotor contingency; virtual reality,Keywords,True,
Scopus,journalPaper,2023,Effects of Urgency and Cognitive Load on Interaction in Highly Automated Vehicles,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"In highly automated vehicles, passengers can engage in non-driving-related activities. Additionally, the technical advancement allows for novel interaction possibilities such as voice, gesture, gaze, touch, or multimodal interaction, both to refer to in-vehicle and outside objects (e.g., thermostat or restaurant). This interaction can be characterized by levels of urgency (e.g., based on late detection of objects) and cognitive load (e.g., because of watching a movie or working). Therefore, we implemented a Virtual Reality simulation and conducted a within-subjects study with N=11 participants evaluating the effects of urgency and cognitive load on modality usage in automated vehicles. We found that while all modalities were possible to use, participants relied on touch the most. This was followed by gaze, especially for external referencing. This work helps to further understand multimodal interaction and the requirements this poses on natural interaction in (automated) vehicles.",automated vehicles; interaction design; multimodal,Abstract,True,
Scopus,journalPaper,2023,VR-Hiking: Physical Exertion Benefits Mindfulness and Positive Emotions in Virtual Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Exploring the great outdoors offers physical and mental health benefits. Hiking is healthy, provides a sense of accomplishment, and offers an opportunity to relax. However, a nature trip is not always possible, and there is a lack of evidence showing how these beneficial experiences can be replicated in Virtual Reality (VR). In response, we recruited (N=24) participants to explore a virtual mountain landscape in a within-subjects study with different levels of exertion: walking, using a chairlift, and teleporting. We found that physical exertion when walking produced significantly more positive emotions and mindfulness than other conditions. Our research shows that physically demanding outdoor activities in VR can be beneficial for the user and that the achievement of hiking up a virtual mountain on a treadmill positively impacts wellbeing. We demonstrate how physical exertion can be used to add mindfulness and positive affect to VR experiences and discuss consequences for VR designers.",accomplishment; haptics; presence; virtual reality; wellbeing,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,PAWS: Personalized Arm and Wrist Movements With Sensitivity Mappings for Controller-Free Locomotion in Virtual Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Virtual Reality (VR) headsets equipped with multiple cameras enable hands-only teleportation techniques without requiring any physical controller. Hands-only teleportation is an effective alternative to controllers for navigation tasks in virtual reality - allowing users to move from one point to another instantaneously. However, the current implementation of hands-only techniques does not consider users' physical attributes (e.g., arm's reach). Thus, a hands-only teleportation technique can lead to different user experiences based on physical attributes. We propose PAWS, a personalized arm and wrist-based teleportation technique that incorporates users' physical attributes for improved teleportation experiences. We first evaluate different degrees of teleportation personalization with no-, partial, and full personalization. We find that full personalization offers faster locomotion - but at the cost of degraded performances with distant targets due to increased sensitivity. We hence further explore different combinations of mapping functions (e.g., sigmoid, quadratic) to personalize motor movements and find that asymmetric functions result in improved performance. Overall, our results show that PAWS helps users to navigate quickly in virtual environments.",arm; locomotion; personalization; teleportation; virtual reality; wrist,Title_Abstract_Keywords,True,
Scopus,journalPaper,2023,Reality Anchors: Bringing Cues from Reality to Increase Acceptance of Immersive Technologies in Transit,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Immersive technologies allow us to control and customise how we experience reality, but are not widely used in transit due to safety, social acceptability, and comfort barriers. We propose that cues from reality can create reference points in virtuality, which we call Reality Anchors, will reduce these barriers. We used simulated public transportation journeys in a lab setting to explore Reality Anchors using speculative methods in two studies. Our first study (N=20) explored how elements of reality like objects, furniture, and people could be used as anchors, demonstrating that visibility of other passengers and personal belongings could reduce barriers. Our second study (N=19) focused on journey types that emerged from the first study - self-managed vs. externally managed journeys - revealing that self-managed journeys increased the need for anchors. We conclude that Reality Anchors can reduce concerns associated with immersive technology use in transit, especially for self-managed journeys.",augmented reality; immersive technologies; mixed reality; passengers; public context; public transport; reality awareness; virtual reality,Keywords,True,
Scopus,journalPaper,2024,An Examination of Ultrasound Mid-air Haptics for Enhanced Material and Temperature Perception in Virtual Environments,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Rendering realistic tactile sensations of virtual objects remains a challenge in VR. While haptic interfaces have advanced, particularly with phased arrays, their ability to create realistic object properties like state and temperature remains unclear. This study investigates the potential of Ultrasound Mid-air Haptics (UMH) for enhancing the perceived congruency of virtual objects. In a user study with 30 participants, we assessed how UMH impacts the perceived material state and temperature of virtual objects. We also analyzed EEG data to understand how participants integrate UMH information physiologically. Our results reveal that UMH significantly enhances the perceived congruency of virtual objects, particularly for solid objects, reducing the feeling of mismatch between visual and tactile feedback. Additionally, UMH consistently increases the perceived temperature of virtual objects. These findings offer valuable insights for haptic designers, demonstrating UMH's potential for creating more immersive tactile experiences in VR by addressing key limitations in current haptic technologies.",haptic perception; ultrasonic mid-air haptics; virtual reality; VR rendering,Keywords,True,
Scopus,journalPaper,2024,Augmented Reality on the Move: A Systematic Literature Review for Vulnerable Road Users,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Due to the continuous improvement of Augmented Reality (AR) head-mounted displays (HMDs), these devices are bound to be increasingly integrated into our daily routines. So far, a major focus of AR research has been on indoor usage and deployment. However, since seamlessly supporting users in their activities while being on-the-move in various outdoor contexts becomes increasingly important, there is a need to investigate the current state-of-the-art of AR technologies while people are in motion outdoors. Therefore, we conducted a systematic literature review of pertinent HCI publications, specifically looking into applications concerning vulnerable road users. We identify the contexts in which such technologies have been researched, prevailing challenges in the field, and applied methodological approaches. Our findings show that most contributions address pedestrians, a shift towards HMDs, and a prevalence of lab studies due to technology limitations. Based on our findings, we discuss trends, existing gaps and opportunities for future research.",augmented reality; literature review; mobility; on the move; VRUs,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Body-Based Augmented Reality Feedback During Conversations,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Engaging with our devices as we engage with each other is problematic as it distracts us and diminishes our social interactions. Subtle interactions have been presented as an approach to reconcile personal and computing interactions, through less disrupting technology. Along those lines, we investigate showing information right on and next to the people we are engaging with. Body-based data visualization allows us to maintain our attention with others, but to also receive information at the same time. We explore potential designs of such body-based and especially on-face visualizations and create a set of five prototype visualizations in a Snapchat lens. We use these prototypes in a video call study with 16 participants to evaluate how body-based visualizations affect actual conversations.",augmented reality; body-based visualization; notifications,Title_Keywords,True,
Scopus,journalPaper,2024,Experience from Designing Augmented Reality Browsing Interfaces for Real-world Walking Scenarios,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"Mobile phones have enabled users to browse information in varying mobility contexts. For high-mobility settings such as walking, however, phones pose several usability challenges, particularly safety and limited screen sizes. While Augmented Reality (AR) has been proposed to address these issues, prior work has yet to investigate AR interface design in real-world walking conditions beyond text readability and notification design. This paper presents the first exploration of AR browsing interface design and extended usage while walking in the wild. We first conducted design sessions with 12 UI designers while walking in varied environments to design the window size, distance, opacity, anchor type, and placement for three categories of apps: text, video, and mixed content. Results show that traffic level significantly affects the designed window size, whereas content type significantly affects window size, distance, opacity, and vertical placement. To gain further insights from real-world usage, we conducted a multi-day observational study with 5 participants and observed that participants on average switched among window layouts every 3.3 minutes, for reasons such as safety and the level of extended visual attention.",augmented reality; user interface design; walking user interface,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Exploring Redirection and Shifting Techniques to Mask Hand Movements from Shoulder-Surfing Attacks during PIN Authentication in Virtual Reality,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The proliferation of mobile Virtual Reality (VR) headsets shifts our interaction with virtual worlds beyond our living rooms into shared spaces. Consequently, we are entrusting more and more personal data to these devices, calling for strong security measures and authentication. However, the standard authentication method of such devices - entering PINs via virtual keyboards - is vulnerable to shoulder-surfing, as movements to enter keys can be monitored by an unnoticed observer. To address this, we evaluated masking techniques to obscure VR users' input during PIN authentication by diverting their hand movements. Through two experimental studies, we demonstrate that these methods increase users' security against shoulder-surfing attacks from observers without excessively impacting their experience and performance. With these discoveries, we aim to enhance the security of future VR authentication without disrupting the virtual experience or necessitating additional hardware or training of users.",hand redirection; shoulder-surfing; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Medusa3D: The Watchful Eye Freezing Illegitimate Users in Virtual Reality Interactions,MobileHCI - International Conference on Human-Computer Interaction with Mobile Devices and Services,B,"The remarkable growth of Virtual Reality (VR) in recent years has extended its applications beyond entertainment to sectors including education, e-commerce, and remote communication. Since VR devices contain user's private information, user authentication becomes increasingly important. Current authentication systems in VR, such as password-based or static biometric-based methods, are either cumbersome to use or vulnerable to attacks such as shoulder surfing. To address these limitations, we propose Medusa3D, a challenge-response authentication system for VR based on reflexive eye responses. Unlike existing methods, reflexive eye responses are involuntary and effortless, offering a secure and user-friendly credential for authentication. We implement Medusa3D on an off-the-shelf VR and conduct evaluations with 25 participants. The evaluation results show that Medusa3D achieves 0.21% FAR and 0.13% FRR, demonstrating high security under various ocular conditions and resilience against attacks such as zero-effort attack, replay attack, and mimicry attack. A user study indicates that Medusa3D is user-friendly and well-adopted among participants.",gaze; user authentication; vr,Title_Abstract,True,
Scopus,conferencePaper,2023,Visual Gaze Labeling for Augmented Reality Studies,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"Augmented Reality (AR) provides new ways for situated visualization and human-computer interaction in physical environments. Current evaluation procedures for AR applications rely primarily on questionnaires and interviews, providing qualitative means to assess usability and task solution strategies. Eye tracking extends these existing evaluation methodologies by providing indicators for visual attention to virtual and real elements in the environment. However, the analysis of viewing behavior, especially the comparison of multiple participants, is difficult to achieve in AR. Specifically, the definition of areas of interest (AOIs), which is often a prerequisite for such analysis, is cumbersome and tedious with existing approaches. To address this issue, we present a new visualization approach to define AOIs, label fixations, and investigate the resulting annotated scanpaths. Our approach utilizes automatic annotation of gaze on virtual objects and an image-based approach that also considers spatial context for the manual annotation of objects in the real world. Our results show, that with our approach, eye tracking data from AR scenes can be annotated and analyzed flexibly with respect to data aspects and annotation strategies. © 2023 The Authors. Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.",CCS Concepts; • Human-centered computing → Visualization,Title_Abstract,True,
Scopus,conferencePaper,2022,Hybrid Touch/Tangible Spatial Selection in Augmented Reality,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"We study tangible touch tablets combined with Augmented Reality Head-Mounted Displays (AR-HMDs) to perform spatial 3D selections. We are primarily interested in the exploration of 3D unstructured datasets such as cloud points or volumetric datasets. AR-HMDs immerse users by showing datasets stereoscopically, and tablets provide a set of 2D exploration tools. Because AR-HMDs merge the visualization, interaction, and the users' physical spaces, users can also use the tablets as tangible objects in their 3D space. Nonetheless, the tablets' touch displays provide their own visualization and interaction spaces, separated from those of the AR-HMD. This raises several research questions compared to traditional setups. In this paper, we theorize, discuss, and study different available mappings for manual spatial selections using a tangible tablet within an AR-HMD space. We then study the use of this tablet within a 3D AR environment, compared to its use with a 2D external screen. © 2022 The Author(s) Computer Graphics Forum © 2022 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.",CCS Concepts; Scientific visualization; Touch screens; • Human-centered computing → Mixed / augmented reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2023,Evaluating View Management for Situated Visualization in Web-based Handheld AR,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"As visualization makes the leap to mobile and situated settings, where data is increasingly integrated with the physical world using mixed reality, there is a corresponding need for effectively managing the immersed user's view of situated visualizations. In this paper we present an analysis of view management techniques for situated 3D visualizations in handheld augmented reality: a shadowbox, a world-in-miniature metaphor, and an interactive tour. We validate these view management solutions through a concrete implementation of all techniques within a situated visualization framework built using a web-based augmented reality visualization toolkit, and present results from a user study in augmented reality accessed using handheld mobile devices. © 2023 The Authors. Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.","CCS Concepts; Visualization systems and tools; Visualization theory, concepts and paradigms; • Human-centered computing → User interface management systems",Abstract,True,
Scopus,conferencePaper,2018,VirtualDesk: A Comfortable and Efficient Immersive Information Visualization Approach,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"3D representations are potentially useful under many circumstances, but suffer from long known perception and interaction challenges. Current immersive technologies, which combine stereoscopic displays and natural interaction, are being progressively seen as an opportunity to tackle this issue, but new guidelines and studies are still needed, especially regarding information visualization. Many proposed approaches are impractical for actual usage, resulting in user discomfort or requiring too much time or space. In this work, we implement and evaluate an alternative data exploration metaphor where the user remains seated and viewpoint change is only realisable through physical movements. All manipulation is done directly by natural mid-air gestures, with the data being rendered at arm's reach. The virtual reproduction of the analyst's desk aims to increase immersion and enable tangible interaction with controls and two dimensional associated information. A comparative user study was carried out against a desktop-based equivalent, exploring a set of 9 perception and interaction tasks based on previous literature and a multidimensional projection use case. We demonstrate that our prototype setup, named VirtualDesk, presents excellent results regarding user comfort and immersion, and performs equally or better in all analytical tasks, while adding minimal or no time overhead and amplifying user subjective perceptions of efficiency and engagement. Results are also contrasted to a previous experiment employing artificial flying navigation, with significant observed improvements. © 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.",CCS Concepts; Human-centered computing → Empirical studies in visualization; Virtual reality,Keywords,True,
Scopus,conferencePaper,2018,Maps and Globes in Virtual Reality,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"This paper explores different ways to render world-wide geographic maps in virtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's viewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c) an egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved map, created by projecting the map onto a section of a sphere which curves around the user. In all four visualisations the geographic centre can be smoothly adjusted with a standard handheld VR controller and the user, through a head-tracked headset, can physically move around the visualisation. For distance comparison exocentric globe is more accurate than egocentric globe and flat map. For area comparison more time is required with exocentric and egocentric globes than with flat and curved maps. For direction estimation, the exocentric globe is more accurate and faster than the other visual presentations. Our study participants had a weak preference for the exocentric globe. Generally the curved map had benefits over the flat map. In almost all cases the egocentric globe was found to be the least effective visualisation. Overall, our results provide support for the use of exocentric globes for geographic visualisation in mixed-reality. © 2018 The Author(s) Computer Graphics Forum © 2018 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.",CCS Concepts; Empirical studies in HCI; Geographic visualization; Human-centered computing → Virtual reality,Title_Abstract_Keywords,True,
Scopus,conferencePaper,2017,Stardust: Accessible and Transparent GPU Support for Information Visualization Rendering,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"Web-based visualization libraries are in wide use, but performance bottlenecks occur when rendering, and especially animating, a large number of graphical marks. While GPU-based rendering can drastically improve performance, that paradigm has a steep learning curve, usually requiring expertise in the computer graphics pipeline and shader programming. In addition, the recent growth of virtual and augmented reality poses a challenge for supporting multiple display environments beyond regular canvases, such as a Head Mounted Display (HMD) and Cave Automatic Virtual Environment (CAVE). In this paper, we introduce a new web-based visualization library called Stardust, which provides a familiar API while leveraging GPU's processing power. Stardust also enables developers to create both 2D and 3D visualizations for diverse display environments using a uniform API. To demonstrate Stardust's expressiveness and portability, we present five example visualizations and a coding playground for four display environments. We also evaluate its performance by comparing it against the standard HTML5 Canvas, D3, and Vega. © 2017 The Author(s) Computer Graphics Forum © 2017 The Eurographics Association and John Wiley & Sons Ltd. Published by John Wiley & Sons Ltd.",Categories and Subject Descriptors (according to ACM CCS); D.2.2 [Computer Graphics]: Software Engineering—Design Tools and Techniques,Abstract,True,
Scopus,conferencePaper,2024,Visual Highlighting for Situated Brushing and Linking,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"Brushing and linking is widely used for visual analytics in desktop environments. However, using this approach to link many data items between situated (e.g., a virtual screen with data) and embedded views (e.g., highlighted objects in the physical environment) is largely unexplored. To this end, we study the effectiveness of visual highlighting techniques in helping users identify and link physical referents to brushed data marks in a situated scatterplot. In an exploratory virtual reality user study (N=20), we evaluated four highlighting techniques under different physical layouts and tasks. We discuss the effectiveness of these techniques, as well as implications for the design of brushing and linking operations in situated analytics. © 2024 The Authors. Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd.",CCS Concepts; Empirical studies in HCI; Information visualization; • Human-centered computing → Empirical studies in visualization,Abstract,True,
Scopus,conferencePaper,2013,Multiple light source estimation in a single image,EuroVis - Eurographics/IEEE Symposium on Visualization,B,"Many high-level image processing tasks require an estimate of the positions, directions and relative intensities of the light sources that illuminated the depicted scene. In image-based rendering, augmented reality and computer vision, such tasks include matching image contents based on illumination, inserting rendered synthetic objects into a natural image, intrinsic images, shape from shading and image relighting. Yet, accurate and robust illumination estimation, particularly from a single image, is a highly ill-posed problem. In this paper, we present a new method to estimate the illumination in a single image as a combination of achromatic lights with their 3D directions and relative intensities. In contrast to previous methods, we base our azimuth angle estimation on curve fitting and recursive refinement of the number of light sources. Similarly, we present a novel surface normal approximation using an osculating arc for the estimation of zenith angles. By means of a new data set of ground-truth data and images, we demonstrate that our approach produces more robust and accurate results, and show its versatility through novel applications such as image compositing and analysis. Many high-level image processing tasks require an estimate of the positions, directions and relative intensities of the light sources that illuminated the depicted scene. In image-based rendering, augmented reality and computer vision, such tasks include matching image contents based on illumination, inserting rendered synthetic objects into a natural image, intrinsic images, shape from shading and image relighting. Yet, accurate and robust illumination estimation, particularly from a single image, is a highly ill-posed problem. In this paper, we present a new method to estimate the illumination in a single image as a combination of achromatic lights with their 3D directions and relative intensities. In contrast to previous methods, we base our azimuth angle estimation on curve fitting and recursive refinement of the number of light sources. Likewise, we present a novel surface normal approximation using an osculating arc for the estimation of zenith angles. © 2013 The Authors Computer Graphics Forum © 2013 The Eurographics Association and John Wiley & Sons Ltd.",computational photography; I.4.8 [Image Processing and Computer Vision]: Scene Analysis - Photometry; image/video editing; light source estimation,Abstract,True,
Scopus,journalPaper,2024,Lightweight 3-D Convolutional Occupancy Networks for Virtual Object Reconstruction,CG&A - Computer Graphics and Applications,B,"The increasing demand for edge devices causes the necessity for recent technologies to be adaptable to nonspecialized hardware. In particular, in the context of augmented, virtual reality, and computer graphics, the 3-D object reconstruction task from a sparse point cloud is highly computationally demanding and for this reason, it is difficult to accomplish on embedded devices. In addition, the majority of earlier works have focused on mesh quality at the expense of speeding up the creation process. In order to find the best balance between time for mesh generation and mesh quality, we aim to tackle the object reconstruction process by developing a lightweight implicit representation. To achieve this goal, we leverage the use of convolutional occupancy networks. We show the effectiveness of the proposed approach through extensive experiments on the ShapeNet dataset using systems with different resources such as GPU, CPU, and an embedded device.  © 1981-2012 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,The Mixed Tangible Catalog: Toward Tangible and Sustainable B2B Metaverse Fashion Showrooms,CG&A - Computer Graphics and Applications,B,"The metaverse, driven by mixed reality (MR), is positioned as the future market, revolutionizing product exploration in virtual space. Existing literature on this subject mainly focuses on business-to-consumer perspectives, leaving a gap in understanding business-to-business (B2B) applications, particularly in the fashion industry. This article introduces a ""Mixed Tangible Catalog""(MTC) for B2B that combines a physical, foldable cardboard booth with an MR application linked to a head-mounted display. Targeting the fashion sector's need for high standards in material evaluation, the MTC allows retailers and distributors to browse garments, customize material attributes, and receive visual and tangible feedback. Evaluation through a focus group of 10 industry experts revealed positive feedback. The MTC maintains the tangibility of traditional B2B showrooms and reduces the environmental impact by minimizing transportation, samples, and waste. This innovative approach offers an efficient and sustainable alternative to conventional physical showrooms, enhancing both economic and ecological aspects.  © 1981-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,@theSource: Welcome,CG&A - Computer Graphics and Applications,B,"This inaugural article sets the stage and scope for a new department in IEEE Computer Graphics and Applications: @theSource. In this department, we set out to address the questions, How have open source projects and open Standards driven graphics innovations and applications' and What can we learn from them? Thus, we are broadly concerned with how open communities and ecosystems have (and are) impacting computer graphics. The intent is to highlight: open source software (such as architectures, engines, frameworks, libraries, services); open Standards and open source data and models; and applications as well as the impacts of open graphics technologies. We also consider historical and summative reviews on the cultural and economic aspects of open source and open Standards graphics ecosystems, such as visualization and mixed reality. © 1981-2012 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,XR4ED: An Extended Reality Platform for Education,CG&A - Computer Graphics and Applications,B,"Recent developments in extended reality (XR) are already demonstrating the benefits of this technology in the educational sector. Unfortunately, educators may not be familiar with XR technology and may find it difficult to adopt this technology in their classrooms. This article presents the overall architecture and objectives of an EU-funded project dedicated to XR for education, called Extended Reality for Education (XR4ED). The goal of the project is to provide a platform, where educators will be able to build XR teaching experiences without the need to have programming or 3-D modeling expertise. The platform will provide the users with a marketplace to obtain, for example, 3-D models, avatars, and scenarios; graphical user interfaces to author new teaching environments; and communication channels to allow for collaborative virtual reality (VR). This article describes the platform and focuses on a key aspect of collaborative and social XR, which is the use of avatars. We show initial results on a) a marketplace which is used for populating educational content into XR environments, b) an intelligent augmented reality assistant that communicates between nonplayer characters and learners, and c) self-avatars providing nonverbal communication in collaborative VR. © 1981-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,Design Principles and Challenges for Gaze + Pinch Interaction in XR,CG&A - Computer Graphics and Applications,B,"For Extended Reality (XR) headsets, a key aim is the natural interaction in 3-D space beyond what traditional methods of keyboard, mouse, and touchscreen can offer. With the release of the Apple Vision Pro, a novel interaction paradigm is now widely available where users seamlessly navigate content through the combined use of their eyes and hands. However, blending these modalities poses unique design challenges due to their dynamic nature and the absence of established principles and standards. In this article, we present five design principles and issues for the Gaze + Pinch interaction technique, informed by eye-hand research in the human-computer interaction field. The design principles encompass mechanisms like division of labor and minimalistic timing, which are crucial for usability, alongside enhancements for the manipulation of objects, indirect interactions, and drag & drop. Whether in design, technology, or research domains, this exploration offers valuable perspectives for navigating the evolving landscape of 3-D interaction.  © 1981-2012 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,Panoramic Ray Tracing for Interactive Mixed Reality Rendering Based on 360° RGBD Video,CG&A - Computer Graphics and Applications,B,"This article presents an interactive panoramic ray tracing method for rendering real-time realistic lighting and shadow effects when virtual objects are inserted in 360° RGBD videos. First, we approximate the geometry of the real scene. We propose a sparse sampling ray generation method to speed up the tracing process by reducing the number of rays that need to be emitted in ray tracing. After that, an irradiance estimation channel is introduced to generate noisy Monte Carlo images. Finally, the final result is smoothed and synthesized by interpolation, temporal filtering, and differential rendering. We tested our method in a number of natural and synthesized scenes and compared our method with results from ground truth and image-based illumination methods. The results show that our method can generate visually realistic frames for dynamic virtual objects in 360° RGBD videos in real time, making the rendering results more natural and believable.  © 1981-2012 IEEE.",,Title,True,
Scopus,journalPaper,2024,Enhancing Archaeological Research Through Immersive Virtual Reality,CG&A - Computer Graphics and Applications,B,"Virtual reality (VR) is increasingly employed in archaeology to showcase reconstructions of ancient sites to the general public, yet its utilization for professional purposes by archaeologists remains less common. To address this gap, we introduce a VR application specifically designed to streamline the storage and access of critical data for archaeological studies. This application provides experts with an immersive visualization of excavation sites and related information during the postexcavation analysis phase. The application interface facilitates direct interaction with 3-D models generated through photogrammetry and modeling techniques, enabling detailed examination of collected data and enhancing research activities. We applied this system to the case study of excavations at the Temple of Juno in Agrigento, Italy. In addition, we present the findings of a pilot user study involving archaeologists, which evaluates the effectiveness of immersive technologies for professionals in documenting, preserving, and exploring archaeological sites, while also driving potential future developments.  © 1981-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,Virtual Access to STEM Careers: In the Field Experiments,CG&A - Computer Graphics and Applications,B,"The Virtual Access to STEM Careers (VASC) project is an intertwined classroom and virtual reality (VR) curricular program for third through fourth graders. Elementary school students learn about and take on the roles and responsibilities of STEM occupations through authentic, problem-based tasks with physical kits and immersive VR environments. This article reports on a round of curriculum and virtual environment development and in-classroom experimentation that was guided by preliminary results gathered from our initial VASC prototyping and testing. This specific iteration focuses on curriculum for learning about sea turtles and tasks regularly completed by park rangers and marine biologists who work with these creatures and a new backend data collection component to analyze participant behavior. Our results showed that educators were able to setup and integrate VASC into their classrooms with relative ease. Elementary school students were able to learn how to interface with our system quickly and enjoyed being in the environment, making a positive link to STEM education.  © 1981-2012 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,A Comparative Study Between a Large Screen and an HMD Using Wind Representations in Virtual Reality,CG&A - Computer Graphics and Applications,B,"In this article, we investigated the representation of wind in urban spaces through computational fluid dynamics simulations in virtual environments (VE). We compared wind perception (force and direction) as well as the sense of presence and embodiment in VE using different display technologies: head-mounted displays (HMD) and large screens, with or without an avatar. The tactile display was found to be most effective for detecting wind characteristics and enhancing presence and embodiment in virtual scenes, regardless of display type. Wind force and overall presence showed no significant differences between projection methods, but the perception of wind direction varied, which can be attributed to the head tracking of the HMD. In addition, gender differences emerged: females had a 7.42% higher presence on large screens, while males had a 23.13% higher presence with HMD (avatar present). These results highlight nuances in wind perception, the influence of technology, and gender differences in VE. © 1981-2012 IEEE.",,Title,True,
Scopus,journalPaper,2024,PerSiVal: On-Body AR Visualization of Biomechanical Arm Simulations,CG&A - Computer Graphics and Applications,B,"In this work, we explore different combinations of techniques for an interactive, on-body visualization in augmented reality (AR) of an upper arm muscle simulation model. In terms of data, we focus on a continuum-mechanical simulation model involving five different muscles of the human upper arm, with physiologically realistic geometry. In terms of use cases, we focus on the immersive illustration, education, and dissemination of such simulation models. We describe the process of developing six on-body visualization prototypes over the period of five years. For each prototype, we employed different types of motion capture, AR display technologies, and visual encoding approaches, and gathered feedback throughout outreach activities. We reflect on the development of the individual prototypes and summarize lessons learned of our exploration process into the design space of situated on-body visualization. © 1981-2012 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,2023 IEEE Scientific Visualization Contest Winner: VisAnywhere: Developing Multiplatform Scientific Visualization Applications,CG&A - Computer Graphics and Applications,B,"Scientists often explore and analyze large-scale scientific simulation data by leveraging 2-D and 3-D visualizations. The data and tasks can be complex and therefore best supported using myriad display technologies, from mobile devices to large high-resolution display walls to virtual reality headsets. Using a simulation of neuron connections in the human brain provided for the 2023 IEEE Scientific Visualization Contest, we present our work leveraging various web technologies to create a multiplatform scientific visualization application. Users can spread visualization and interaction across multiple devices to support flexible user interfaces and both colocated and remote collaboration. Drawing inspiration from responsive web design principles, this work demonstrates that a single codebase can be adapted to develop scientific visualization applications that operate everywhere. © 2024 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,Integrated Augmented and Virtual Reality Technologies for Realistic Fire Drill Training,CG&A - Computer Graphics and Applications,B,"In this article, we propose a novel fire drill training system designed specifically to integrate augmented reality (AR) and virtual reality (VR) technologies into a single head-mounted display device to provide realistic as well as safe and diverse experiences. Applying hybrid AR/VR technologies in fire drill training may be beneficial because they can overcome limitations such as space-time constraints, risk factors, training costs, and difficulties in real environments. The proposed system can improve training effectiveness by transforming arbitrary real spaces into real-time, realistic virtual fire situations, and by interacting with tangible training props. Moreover, the system can create intelligent and realistic fire effects in AR by estimating not only the object type but also its physical properties. Our user studies demonstrated the potential of integrated AR/VR for improving training and education.  © 1981-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,AlterVerse: A Framework for Interactive Virtual Altering of Physical Objects in Extended Reality Environments,CG&A - Computer Graphics and Applications,B,"This article presents AlterVerse, a framework that provides users with novel experiences through visual illusions of interactively altering physical objects in extended reality environments. In the AlterVerse framework, physical objects are seamlessly virtualized by aligning their 3-D virtual replicas with them. It enables users to manipulate (move or rotate) and reshape (deform the shapes or transform the styles of) the physical objects virtually. Simultaneously, the physical objects are visually removed from a physical environment, creating the perception that the altering occurs directly on the physical objects. As a promising application of the AlterVerse framework, the article demonstrates interactive virtual altering of home furniture in interior design or decoration scenarios without physically altering existing furniture.  © 1981-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,"Sitting or Standing in VR: About Comfort, Conflicts, and Hazards",CG&A - Computer Graphics and Applications,B,"This article examines the choices between sitting and standing in virtual reality (VR) experiences, addressing conflicts, challenges, and opportunities. It explores issues such as the risk of motion sickness in stationary users and virtual rotations, the formation of mental models, consistent authoring, affordances, and the integration of embodied interfaces for enhanced interactions. Furthermore, it delves into the significance of multisensory integration and the impact of postural mismatches on immersion and acceptance in VR. Ultimately, the article underscores the importance of aligning postural choices and embodied interfaces with the goals of VR applications, be it for entertainment or simulation, to enhance user experiences.  © 1981-2012 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,Is Native Naïve? Comparing Native Game Engines and WebXR as Immersive Analytics Development Platforms,CG&A - Computer Graphics and Applications,B,"Native game engines have long been the 3-D development platform of choice for research in mixed and augmented reality. For this reason, they have also been adopted in many immersive visualization and immersive analytics systems and toolkits. However, with the rapid improvements of WebXR and related open technologies, this choice may not always be optimal for future visualization research. In this article, we investigate common assumptions about native game engines versus WebXR and find that while native engines still have an advantage in many areas, WebXR is rapidly catching up and is superior for many immersive analytics applications.  © 1981-2012 IEEE.",,Abstract,True,
Scopus,journalPaper,2024,Multisensory Experiences in Extended Reality,CG&A - Computer Graphics and Applications,B,"Since the beginning, virtual reality (VR) has been envisioned as a technology that engages multiple senses. Examples include Morton Heiliga's Sensorama simulator and Ivan E. Sutherlanda's notion of the Ultimate Display described in the 1960s. In more than 60 years, technological development and scientific knowledge have predominantly focused virtual and extended reality (VR and XR) applications on the sense of sight and hearing. Despite their importance in everyday interaction, the other senses remain on the margins of VR and XR applications. The potential to include additional senses has been acknowledged and extensively explored in the literature. Those who deal with this topic encounter challenges in understanding the functioning mechanism of the human sensorial and perceptive components and creating interfaces capable of communicating through these senses. Last but not least, it is important to design case studies capable of exploiting the potential of this technology and encouraging investors to believe in its development and end users to adopt it once it is ready. These challenges drive the research activities included in this special issue.  © 1981-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2024,Integrating GPT as an Assistant for Low-Cost Virtual Reality Escape-Room Games,CG&A - Computer Graphics and Applications,B,"This work explores the integration of generative pretrained transformer (GPT), an AI language model developed by OpenAI, as an assistant in low-cost virtual escape games. The study focuses on the synergy between virtual reality (VR) and GPT, aiming to evaluate its performance in helping solve logical challenges within a specific context in the virtual environment while acting as a personalized assistant through voice interaction. The findings from user evaluations revealed both positive perceptions and limitations of GPT in addressing highly complex challenges, indicating its potential as a valuable tool for providing assistance and guidance in problem-solving situations. The study also identified areas for future improvement, including adjusting the difficulty of puzzles and enhancing GPT's contextual understanding. Overall, the research sheds light on the opportunities and challenges of integrating AI language models such as GPT in virtual gaming environments, offering insights for further advancements in this field. © 1981-2012 IEEE.",,Title_Abstract,True,
Scopus,journalPaper,2022,Multi-agent-based Modelling and Virtual Reality Simulation System of Major Accidents in Petrochemical Enterprise,JS - Journal of Software,B,"The major accidents usually cause massive property loss and casualties in petrochemical enterprise. The simulation system of major accidents plays an important role in safety management, risk assessment, accident prevention and emergency drill and others. In this paper, the Multi-Agent block model for simulating major accidents is based on Multi-Agent technology. A method for reconstructing major accidents is proposed based on virtual reality (VR) idea. Then, a VR simulation system (VRSS) of major accidents is developed by introducing the established methodologies. Finally, the VRSS is applied to simulate the occurrence and consequences of major accidents including fire, explosion, leakage and diffusion triggered by liquefied petroleum gas tank in the storage area of petrochemical enterprise. The application results show that the established methodologies are reasonable and stable. The advantages of VRSS include intelligence, real-time consequence assessment, high efficiency and immersive sense.",,Title_Abstract,True,
Scopus,journalPaper,2022,TCP/IP Model for Metaverse Networks and Some Potential Applications,JS - Journal of Software,B,"It is well known that internet will play a key role to the infrastructure of metaverse, while different immerging virtual worlds communicate with each other. However, the common TCP/IP model is not suitable enough for metaverse networks. To this end, we explore a preliminary frame of revised TCP/IP model. At first, a new layer called metaverse layer is defined. And then, this layer is placed as the top layer, i.e., the upper of the application layer in TCP/IP model. As a result, a novel network model having the six layers is obtained. Our simulated experiments indicate the feasibility of the new model. Furthermore, we explore several potential application scenes of metaverse networks in this paper.",,Title_Abstract,True,
Scopus,journalPaper,2021,Point Cloud Compression and Transmission for Remote Handling Applications,JS - Journal of Software,B,"Remote handling systems are commonly used for decommissioning and maintenance of hazardous environments, especially in the nuclear sector. The necessity for a more realistic and accurate user interaction with the remote environment has led research towards the usage of immersive technologies such as augmented and virtual reality. In order for this to succeed, the state of the remote environment needs to be known accurately at all times. Information gathered using RGB-D cameras can serve this purpose. The high accuracy and density of data retrieved by these devices provide an extraordinary insight of the remote environment but can represent a burden on the communication channels. This paper addresses two point cloud compression techniques based on kd-trees and octrees for point cloud data transmission within a Robot Operative System (ROS) communications middleware.",,Abstract,True,
Scopus,journalPaper,2021,Design and Implementation of Hardware in the Loop Simulation Operation Simulator for Complex Mechanical Equipment,JS - Journal of Software,B,"Hardware in the loop simulation technology is an important branch of system modeling and simulation. With the development of artificial intelligence, big data, virtual reality and other technologies, more and more advanced technologies are integrated into it, and the economic and military benefits are constantly highlighted, which attracts the attention of all parties. In view of the complex structure of modern large-scale mechanical equipment system and the high cost-effectiveness ratio of carrying out practical operation training, an operation simulator is developed by using hardware in the loop simulation and virtual reality technology. The simulator can complete the daily training of mechanical equipment and the interactive operation, maintenance training, assessment and operation data recording of other equipment, and can popularize and update equipment knowledge Improve the technical level and maintenance skills of operators and technical support personnel as soon as possible.",,Abstract,True,
Scopus,journalPaper,2017,Design and Implementation of AR Electronic Map System Based on IOS,JS - Journal of Software,B,"The rapid development of LBS mobile application has become an essential tool for people to travel and brought great convenience to people's life. Under such circumstances, the application of real technology in mobile ends has emerged as a new trend. However, the existing LBS applications do not do enough for the identification of spatial targets, not intuitive and guidance, and need a new way to identify space targets.Using augmented reality technology can greatly enhance the user experience index of LBS applications, and give people the feeling of being immersive.This paper developed a mobile electronic map information system based on IOS applications with Augmented Reality (AR) technology, catering to the requirement. Traditional functions of mobile electronic map, such as querying position, data updates within the system have been achieved. In addition, AR technology was applied to interact with the camera screen (real scene) for achieving a space target identification function.",,Abstract,True,
Scopus,journalPaper,2015,An Exploratory Study of 3D Interaction Techniques in Augmented Reality Environments,JS - Journal of Software,B,"Augmented Reality (AR) interfaces typically involve the overlay of virtual imagery onto the real world. 3D interaction with the virtual scene is a key feature to explore the full potential of AR. Despite the large number of interaction techniques that have been proposed to enable for intuitive 3D interaction in AR environments, little effort has been done to compare these techniques. In addition, many techniques have been arbitrarily used without formative evaluation or without considering the requirements of different AR applications. This paper discusses three interaction techniques with 3D content in AR environments and reports on an experiment that we conducted to compare them. These techniques are: 1) manipulation of hand-held fiducial markers, 2) free hand interaction, and 3) a keypad controller. 18 participants were recruited and asked to perform a series of tasks which involved the manipulation of a 3D object. The study sheds the light on the strengths and limitations of the studied techniques and when it is appropriate to use each of them. We believe that the reported results will help inform the design and customization of 3D interaction techniques for AR applications.",,Title_Abstract,True,
Scopus,journalPaper,2014,Virtual Reality Software Usage in an EFL Scenario: An Empirical Study,JS - Journal of Software,B,"Current software development and advancement have created many possibilities in both industry and education. Within the educational realm, software advancement in terms of virtual reality simulation has introduced an entire new arena in learning scenarios. In Taiwan, numerous scholars have ventured into this endeavor. However, empirical findings are quite limited. In light of this issue, this case study focuses on evaluating the effectiveness of virtual reality courseware in learning occupational English. A total of 120 freshmen and sophomore English as Foreign Language students participated in the study. Pre/post-tests were gathered and analyzed. Results show that students gradually increased their vocabulary competence. Insights are also provided on how the courseware was created.",,Title_Abstract,True,
Scopus,journalPaper,2013,Effectiveness Assessment on Urban Planning Analyzed in Virtual Reality Environments,JS - Journal of Software,B,"Public participation is very important to the success for urban planning projects. This study tries to establish two different systems, the 3-dimensional digital model and the 360 panoramic QTVR photos models, to simulate the feasibility and effectiveness of the participants' experiences feedback got from both environments, by the case study area of Hangzhou CBD, Zhejiang Province, China. This study used Logistic Model to calculate the parameters and their significance to understand if the participant would be interested or not. The results showed that different age/gender people would be impacted by different indicators. It is also found that both QTVR system and 3D system have advantages and disadvantages for representing the indicators' characteristics for participants.",,Title,True,
Scopus,journalPaper,2013,A Three-Dimensional Virtual Simulation System of Spinning Production Line,JS - Journal of Software,B,"This study takes a chemical fiber factory’s spinning production line as the prototype to realize a threedimensional virtual simulation system. Geometric threedimensional models are constructed by exploiting the imagebased three dimension reconstruction technology, in which Harris algorithm is utilized to detect corners of the modeling objects. 3DSMAX is used to give each model a highly imitative material, and deploy panoramic lighting. For virtual interaction, OGRE is utilized to realize the roaming and interaction of the virtual scene, thus finally completes a virtual simulation system for the spinning production line. Experimental results show that the system achieves a high degree of simulation of three-dimensional virtual scene and gives the user great sense of immersion, which meet the needs of virtual reality.",,Abstract,True,
Scopus,journalPaper,2013,Geometric and Kinematics Modeling of Tele-operated Virtual Construction Robot,JS - Journal of Software,B,"The tele-operated virtual construction robot is an important part of the tele-operated construction robot system based on virtual reality. To improve the realness of the virtual construction robot and reduce the difficulty of development, the hybrid modeling method of combining OpenGL function modeling with Solidworks software modeling, along with the Denavit-Hartenberg (D-H) method were investigated and applied to the geometric and kinematics modeling of the virtual construction robot. The simulation experimental results demonstrated that the motions of virtual construction robot and real construction robot are consistent, and the position and attitude of the mechanical gripper fingertips of both are the same and are consistent with the theoretical results. The modeling method is feasible and effective, which can provide some references to the construction robot modeling with similar structure.",,Abstract,True,
Scopus,journalPaper,2013,Animating Prairies Simulation with Shell Method in Real-time,JS - Journal of Software,B,"Real-time simulation of grassland, especially the dynamic grassland under the effect of external forces such as wind is an important research topic in photorealistic simulation of natural scenes. It is also a challenging problem due to the huge number of grass blades. In this paper we put forward a method to represent the large area of dynamic grass realistically in real-time. In the method, we implement the real-time grass representation by using shell strategy, and based on the strategy, we further realize the simulation of dynamic grassland in wind. In order to solve the over transparent problem in the silhouette area of layered textures representation, we use vertical slices strategy, with which the rendering effects are improved. The grass shift in wind is simulated by moving the multilayer vertices. The vertices movement is computed by vertex shader program in GPU to achieve high simulating speed. To improve the rendering effects, we utilize the fragment shader of GPU to compute the grass lighting model. The quadrilateral continuity issues is also considered in the generation of layered grass textures. Experimental results show that our method can simulate dynamic grassland realistically in real-time, making our method is valuable for the application in virtual reality and games.",,Abstract,True,
IEEE,journalPaper,2014,Next-generation mobile computing,IEEE Software,B,"This issue of IEEE Software discusses where the mobile computing has brought us today and where it could be taking us in the future. It provides a glimpse into the near future of mobile computing by focusing on proximate software challenges coupled with promising techniques, infrastructure, and research from academia, government, and industry. The first Web extra at http://youtu.be/iLnNHwp-H8E is a video demonstration of the Group Autonomy for Mobile Systems (GAMS) project, which is an extension of a research project called Self-governing Mobile Adhocs with Sensors and Handhelds (SMASH) that investigated human-in-the-loop autonomy at Carnegie Mellon University. The project created Android interfaces to a drone swarm that tried to autonomously search for survivors in a disaster scenario using the Parrot AR.Drone and custom GPS and thermal sensors. The second Web extra at http://youtu.be/lW1dqsrdRHU is a video demonstration of HD4AR mobile augmented reality technology that was commercialized through PAR Works. The third Web extra at http://youtu.be/ M4w5oPqrMRo is an audio interview in which James Edmondson talks with Suzanne Miller about autonomous systems, specifically as they relate to robotic systems. In particular, Edmondson's research focuses on partial autonomy with an aim of complementing human users and extending their reach and capabilities in mission-critical environments. From the SEI Podcast Series 'Conversations in Software Engineering.' © 1984-2012 IEEE.",mobile; mobile computing; smartphones,Abstract,True,
Scopus,journalPaper,2024,A review of the application of virtual and augmented reality in physical and occupational therapy,SPE  - Software - Practice and Experience,B,"This paper includes a research review in five bibliographic databases on using the application of virtual reality (VR) and augmented reality (AR) in physical and occupational therapy (POT). This literature review addresses five research questions and two sub-research questions. A total of 36 relevant studies were selected in the review based on the defined keywords and inclusion-exclusion criteria. The primary motivation for using the application of VR and AR in POT is that it is accurate, involves higher patient participation, and requires less therapy recovery time. The standard software tool used is the Unity 3D game engine, and the common device used is the Oculus Rift HMD. Various applications of VR and AR consist of different VR environments and AR contents used in POT. Post-stroke rehabilitation, rehabilitation exercises, pain management, mental and behavioral disorders, and autism in children are the main aspects addressed through the VR and AR environments. Literature review indicates that questionnaires, interviews, and observation are the primary metrics for measuring therapy's effectiveness. The study's findings show positive results such as reduced treatment time, nervousness, pain, hospitalization period, making therapy enjoyable and encouraging, improved quality of life, and focus on using the application of VR and AR in POT. This review will be relevant to researchers, VR and AR application designers, doctors, and patients using the application of VR and AR in POT. Further research addressing multiple participants with clinical trials, adding new VR environments and AR content in VR and AR applications, including follow-up sessions, and increasing training sessions while using the application of VR and AR in POT are recommended. © 2024 John Wiley & Sons Ltd.",augmented reality; occupational therapy; physical therapy; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,A cloud-edge service offloading method for the metaverse in smart manufacturing,SPE  - Software - Practice and Experience,B,"With the development of artificial intelligence, cloud-edge computing and virtual reality, the industrial design that originally depends on human imagination and computing power can be transitioned to metaverse applications in smart manufacturing, which offloads the services of metaverse to cloud and edge platforms for enhancing quality of service (QoS), considering inadequate computing power of terminal devices like industrial sensors and access points (APs). However, large overhead and privacy exposure occur during data transmission to cloud, while edge computing devices (ECDs) are at risk of overloading with redundant service requests and difficult central control. To address these challenges, this paper proposes a minority game (MG) based cloud-edge service offloading method named COM for metaverse manufacturing. Technically, MG possesses a distribution mechanism that can minimize reliance on centralized control, and gains its effectiveness in resource allocation. Besides, a dynamic control of cut-off value is supplemented on the basis of MG for better adaptability to network variations. Then, agents in COM (i.e., APs) leverage reinforcement learning (RL) to work on MG history, offloading decision, QoS mapping to state, action and reward, for further optimizing distributed offloading decision-making. Finally, COM is evaluated using a variety of real-world datasets of manufacturing. The results indicate that COM has 5.38% higher QoS and 8.58% higher privacy level comparing to benchmark method. © 2023 John Wiley & Sons Ltd.",cloud-edge computing; metaverse; minority game; reinforcement learning; service offloading,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,"Balancing performance and comfort in virtual reality: A study of FPS, latency, and batch values",SPE  - Software - Practice and Experience,B,"This manuscript investigates the relationships among various performance metrics in a virtual reality (VR), namely frames per second (FPS), latency, batches, and the number of triangles (tris) and vertices (verts). The study aims to uncover correlations and directional associations between these metrics, shedding light on their impact on VR performance. The findings reveal a significant correlation between FPS and latency, albeit in opposite directions. Higher FPS values are associated with reduced latency, indicating that a smoother visual experience is accompanied by shorter delays in the VR. Conversely, lower FPS values are linked to increased latency, suggesting a potential degradation in overall system responsiveness. Additionally, a strong correlation is observed between latency and batches processed. This finding implies that latency has a direct impact on the system's ability to efficiently process and render objects within VR. Furthermore, a positive correlation is identified between the number of batches and the values of tris and verts. This relationship suggests that higher batch counts are associated with larger quantities of triangles and vertices, reflecting a more complex scene rendering process. Consequently, the performance of VR may be influenced by the density and intricacy of the virtual environments, as indicated by these metrics. © 2024 The Author(s). Software: Practice and Experience published by John Wiley & Sons Ltd.",batches; frames per second; latency; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2024,Development of a second-screen system for sharing virtual reality information,SPE  - Software - Practice and Experience,B,"Since virtual reality technology can easily realize a particular environment, it has been applied to various training. This technology is also expected in education such as those dealing with three-dimensional figures because they can represent phenomena in three dimensions. One of the standard virtual reality technologies is wearing a head-mounted display to experience a virtual reality project. However, the virtual reality experience is limited to the person wearing the head-mounted display (in most cases) when considering the use of head-mounted display-based virtual reality in education, making its usage unsuitable for mass education. Therefore, in this study, we develop a second-screen system that can share virtual reality information using an Android device. Healthy first- and second-year high school students between the ages of 15 and 17 were asked to use the virtual reality educational materials that implemented the developed system, and a questionnaire survey was conducted. There were no negative comments in the survey on whether proposing second-screen system was necessary, demonstrating the usefulness of the proposed system. Therefore, the second-screen system may adapt VR technology to mass education. In addition, we developed a package to integrate our second-screen system into the pre-existing virtual reality projects. This package simplifies the procedure for implementing a second-screen system, thus helping to reuse the virtual reality projects that have been developed so far. © 2023 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.",reuse; second-screen system; virtual reality,Title_Abstract_Keywords,True,
Scopus,journalPaper,2021,Ajalon: Simplifying the authoring of wearable cognitive assistants,SPE  - Software - Practice and Experience,B,"Wearable Cognitive Assistance (WCA) amplifies human cognition in real time through a wearable device and low-latency wireless access to edge computing infrastructure. It is inspired by, and broadens, the metaphor of GPS navigation tools that provide real-time step-by-step guidance, with prompt error detection and correction. WCA applications are likely to be transformative in education, health care, industrial troubleshooting, manufacturing, assisted driving, and sports training. Today, WCA application development is difficult and slow, requiring skills in areas such as machine learning and computer vision that are not widespread among software developers. This paper describes Ajalon, an authoring toolchain for WCA applications that reduces the skill and effort needed at each step of the development pipeline. Our evaluation shows that Ajalon significantly reduces the effort needed to create new WCA applications. © 2021 The Authors. Software: Practice and Experience published by John Wiley & Sons Ltd.",artificial intelligence; augmented reality; cloudlets; computer vision; edge computing; Gabriel; machine learning; mobile computing; software productivity; wearables,Keywords,True,
Scopus,journalPaper,2021,EFFORT: Energy efficient framework for offload communication in mobile cloud computing,SPE  - Software - Practice and Experience,B,"There is an abundant expansion in the race of technology, specifically in the production of data, because of the smart devices, such as mobile phones, smart cards, sensors, and Internet of Things (IoT). Smart phones and devices have undergone an enormous evolution in a way that they can be used. More and more new applications, such as face recognition, augmented reality, online interactive gaming, and natural language processing are emerging and attracting the users. Such applications are generally data intensive or compute intensive, which demands high resource and energy consumption. Mobile devices are known for the resource scarcity, having limited computational power and battery life. The tension between compute/data intensive application and resource constrained mobile devices hinders the successful adaption of emerging paradigms. In the said perspective, the objective of this article is to study the role of computation offloading in mobile cloud computing to supplement mobile platforms ability in executing complex applications. This article proposes a systematic approach (EFFORT) for offload communication in the cloud. The proposed approach provides a promising solution to partially solve energy consumption issue for communication-intensive applications in a smartphone. The experimental study shows that our proposed approach outperforms its counterparts in terms of energy consumption and fast processing of smartphone devices. The battery consumption was reduced to 19% and the data usage was reduced to 16%. © 2020 The Authors. Software:Practice and Experience published by John Wiley & Sons, Ltd.",cloud computing; cloud service providers; energy consumption; Internet spoofing; IoT; offload communication,Abstract,True,
Scopus,journalPaper,2020,VR-Rides: An object-oriented application framework for immersive virtual reality exergames,SPE  - Software - Practice and Experience,B,"Exercise can improve health and well-being. With this in mind, immersive virtual reality (VR) games are being developed to promote physical activity, and are generally evaluated through user studies. However, building such applications is time consuming and expensive. This paper introduces VR-Rides, an object-oriented application framework focused on the development of experiment-oriented VR exergames. Following the modular programming pattern, this framework facilitates the integration of different hardware (such as VR devices, sensors, and physical activity devices) within immersive VR experiences that overlay game narratives on Google Street View panoramas. Combining software engineering and interaction patterns, modules of VR-Rides can be easily added and managed in the Unity game engine. We evaluate the code efficiency and development effort across our VR exergames developed using VR-Rides. The reliability, maintainability, and usability of our framework are also demonstrated via code metrics analysis and user studies. The results show that investing in a systematic approach to reusing code and design can be a worthwhile effort for researchers beyond software engineering. © 2020 John Wiley & Sons, Ltd.",code reuse; exergame; immersive virtual reality; object-oriented application framework; reusable component; Unity,Title_Abstract_Keywords,True,
Scopus,journalPaper,2020,A review on the computation offloading approaches in mobile edge computing: A game-theoretic perspective,SPE  - Software - Practice and Experience,B,"In recent years, novel mobile applications such as augmented reality, virtual reality, and three-dimensional gaming, running on handy mobile devices have been pervasively popular. With rapid developments of such mobile applications, decentralized mobile edge computing (MEC) as an emerging distributed computing paradigm is developed for serving them near the smart devices, usually in one hop, to meet their computation, and delay requirements. In the literature, offloading mechanisms are designed to execute such mobile applications in the MEC environments through transferring resource-intensive tasks to the MEC servers. On the other hand, due to the resource limitations, resource heterogeneity, dynamic nature, and unpredictable behavior of MEC environments, it is necessary to consider the computation offloading issues as the challenging problem in the MEC environment. However, to the best of our knowledge, despite its importance, there is not any systematic, comprehensive, and detailed survey in game theory (GT)-based computation offloading mechanisms in the MEC environment. In this article, we provide a systematic literature review on the GT-based computation offloading approaches in the MEC environment in the form of a classical taxonomy to recognize the state-of-the-art mechanisms on this important topic and to provide open issues as well. The proposed taxonomy is classified into four main fields: classical game mechanisms, auction theory, evolutionary game mechanisms, and hybrid-base game mechanisms. Next, these classes are compared with each other according to the important factors such as performance metrics, case studies, utilized techniques, and evaluation tools, and their advantages and disadvantages are discussed, as well. Finally, open issues and future uncovered or weakly covered research challenges are discussed and the survey is concluded. © 2020 John Wiley & Sons, Ltd.",game theory; mobile edge computing; offloading; systematic review,Abstract,True,
IEEE,conferencePaper,2024,Characterizing the Developer Groups for Metaverse Services in Roblox,SSE/SCC - IEEE International Conference on Software Services Engineering/Services Computing,B,"The Metaverse has experienced exponential growth in recent years. Most metaverse platforms enable users to create their own metaverse services to deliver immersive content via visualized no-code/low-code programming tools. Usually, users with similar interests form or join a developer group to create and maintain complex metaverse services. In this paper, we focus on one of the most successful metaverse platforms, Roblox, to reveal how developer groups perform to create metaverse services. We collected a snapshot of Roblox encompassing 960,000 developer groups and 18.73 million Roblox game services. This dataset allows us to analyze the development patterns of these groups and identify key factors influencing their creativity. Our observations reveal that developer groups have diverse roles and members, exhibiting remarkable creativity that offers valuable insights into effective creation modes within the Metaverse. To understand why certain groups create exceptional game services, we examined various features of users at different ranks within the groups and developed a model to assess group creativity. Our findings indicate that the organizational structure within groups significantly impacts group creativity. Notably, we discovered that middle-ranked users, rather than top-ranked ones, play a more critical role in fostering creativity, highlighting their pivotal position in the group dynamic.",Metaverse;Roblox;Creativity;Developer groups,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2016,XYZ Indoor Navigation through Augmented Reality: A Research in Progress,SSE/SCC - IEEE International Conference on Software Services Engineering/Services Computing,B,"We present an overall framework of services for indoor navigation, which includes Indoor Mapping, Indoor Positioning, Path Planning, and En-route Assistance. Within such framework we focus on an augmented reality (AR) solution for en-route assistance. AR assists the user walking in a multi-floor building by displaying a directional arrow under a camera view, thus freeing the user from knowing his/her position. Our AR solution relies on geomagnetic positioning and north-oriented space coordinates transformation. Therefore, it can work without infrastructure and without relying on GPS. The AR visual interface and the integration with magnetic positioning is the main novelty of our solution, which has been validated by experiments and shows a good performance.",Augmented Reality Indoor Navigation;Geomagnetic Sensor;Indoor Positioning;Coordinate Transform,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2015,Building the Computational Virtual Reality Environment for Anaesthetists' Training and Practice,SSE/SCC - IEEE International Conference on Software Services Engineering/Services Computing,B,"Understanding the real world based on visualisation and prediction is essential for the decision-maker. We build a computational virtual reality environment to improve visualisation, understanding and prediction of the physical world and to guide action. It develops a five-dimensional, computer-generated, computational Virtual Reality Environment for Anaesthesia (VREA). Our online prediction will be calculated based on the correlation and composition computing with respect to the three dimensions: horizontal, vertical and individual. The novel musical notes based anesthetic simulator is proposed to identify the abnormality and visualize the online medical time series. The experiments with the online ECG data will present a real-time case to show the effectiveness and efficiency of our proposed system and algorithms.",Computational Virtual Reality;Critical Event Training;Anesthetic Simulator;Five dimensional;ECG Online Prediction,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Exploring Collaborative Immersive Virtual Reality Serious Games for Enhancing Learning Motivation in Physics Education,COMPSAC - International Computer Software and Applications Conference,B,"Physics education posed challenges, with students' limited interest attributed to the subject's abstract nature as well as educators' struggles in conveying complex concepts effectively. To address these challenges, a collaborative-based, high-immersion virtual reality (IVR) game approach for physics education was proposed in this study. The focus of this study was to enhance student motivation and improving learning outcome through the integration of serious game elements. Eight distinct serious game elements were explored, incorporating narrative, autonomy, accomplishment, ownership, social interaction, challenge, and immersion into the game design. Students were assigned to complete three physics experiments within the game where they required to work collaboratively. Results indicated high ratings for integrated game features, with questionnaire responses revealing enhanced motivation among students who found physics less interesting. These findings highlighted the effectiveness of collaborative learning within IVR serious games, offering a valuable framework for future serious game design in physics education.",Extended reality;high-immersive;physics education;serious game;virtual reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Hybrid Teaching Based on a 4C Model-Based Framework,COMPSAC - International Computer Software and Applications Conference,B,"As an extension to a previous paper, a 4C model-based framework is presented in this short paper for hybrid teaching. The framework comprises three layers: model layer, platform layer and activity layer together with other supporting functions. Various methods are presented based on this framework, including 2D metaverse-based teaching, 3D metaverse-based teaching, teaching using a 360-degree camera with virtual reality support, meeting using a hybrid meeting camera and attending lectures using a telepresence robot. Furthermore, as a supporting function, online student eye movement can be analyzed using eye trackers. The aforementioned methods can provide valuable insights and also useful ideas for further research.",Hybrid teaching;Hybrid classroom;Online learning,Abstract,True,
IEEE,conferencePaper,2024,Uncovering the Metaverse within Everyday Environments: A Coarse-to-Fine ApproachBehaviors,COMPSAC - International Computer Software and Applications Conference,B,"The recent release of the Apple Vision Pro has reignited interest in the metaverse, showcasing the intensified efforts of technology giants in developing platforms and devices to facilitate its growth. As the metaverse continues to proliferate, it is foreseeable that everyday environments will become increasingly saturated with its presence. Consequently, uncovering links to these metaverse items will be a crucial first step to interacting with this new augmented world. In this paper, we address the problem of establishing connections with virtual worlds within everyday environments, especially those that are not readily discernible through direct visual inspection. We introduce a vision-based approach leveraging Artcode visual markers to uncover hidden metaverse links embedded in our ambient surroundings. This approach progressively localises the access points to the metaverse, transitioning from coarse to fine localisation, thus facilitating an exploratory interaction process. Detailed experiments are conducted to study the performance of the proposed approach, demonstrating its effectiveness in Artcode localisation and enabling new interaction opportunities.",Metaverse;Interaction;Coarse-to-fine;Artcode;Localisation,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Exploring Emotional Responses with Dynamic Difficulty Adjustment Adaptation in Immersive Virtual Reality Exergaming,COMPSAC - International Computer Software and Applications Conference,B,"Immersive Virtual Reality (IVR) exergaming presents a promising avenue to integrate physical exercise with engaging virtual experiences, potentially encouraging sustained physical activity. However, maintaining user motivation over extended periods poses a significant challenge. Recent research has introduced the Dynamic Difficulty Adjustment (DDA) mechanism, dynamically regulating exergame difficulty based on specific conditions to enhance user adaptation. While prior studies have predominantly focused on gaming performance to adjust difficulty, they often overlook the emotional impact on user motivation. This study investigates users' emotional responses to the game timer change (TC) as a DDA mechanism during IVR exergaming. Results indicate that subjects in the TC-implemented game displayed more neutral emotions, concomitant with improved gaming performance. Conversely, subjects in the game without TC exhibited a broader range of detected emotions (sad and happy), suggesting difficulties in adapting to in-game difficulty levels incongruent with their gaming abilities. Overall, this study establishes a foundation for future research in affective computing-based IVR exergaming, aiming to develop an intelligent autonomous DDA mechanism tailored to users' physical and mental conditions.",Immersive virtual reality;dynamic difficulty adjustment;affective computing,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,"Exploring Gaming Technologies, Digital Twins, and VR to Visualise Wireless Propagation Simulations",COMPSAC - International Computer Software and Applications Conference,B,"Over the years, the wireless communication industry and the research community investigated methods of creating accurate and efficient models for signal propagation. The recent advancements in wireless communications and its exponential usage through high mobility of numerous connected devices introduced challenges in simulating dynamic radio propagation. To address these challenges, specialized software have been developed, offering high-fidelity simulations. However, these solutions have expensive cost requirements and are largely dependent on offline computations, lacking flexibility and scalability. As a result, their wider use in scientific and industrial sectors is limited. In response to these limitations, this paper proposes an alternative solution leveraging the latest developments in Gaming technologies, GPU technology, and Virtual Reality through the concept of Digital Twins to develop a prototype for a deterministic channel simulator. The prototype utilise a game development engine, high-performance GPU, and commercial VR headsets to achieve a low cost, accessible and scalable method for visualizing wireless signal propagation in real time. This paper presents the work in progress, describing the system architecture, current state of development and intended functionalities.",Digital Twins;Ray tracing;Virtual Reality;Wireless Propagation Simulations,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Automatic Generation of Selective Disassembly Sequences for Augmented Reality-Guided Maintenance,COMPSAC - International Computer Software and Applications Conference,B,"Equipment maintenance is essential for ensuring the longevity and optimal performance throughout the product lifecycle. The augmented reality technology is used to replace traditional paper manuals for maintenance, leading to enhanced efficiency and accuracy. However, the current development of augmented reality maintenance procedures primarily depends on the product's process manual. This approach only reduces the user's operation time, but still requires developers to invest significant time and effort. In this article, we propose automating the planning of product disassembly sequences based on the product's CAD model in order to improve the automation process of augmented reality maintenance applications. We propose a novel selective disassembly sequences method for maintaining faulty components based on product and disassembly levels. We utilize this method as the input for our developed Guided Information Generation System (GIGS). We conduct a rapid maintenance process for experimental equipment to validate the feasibility and potential practical application of our method.",selective disassembly sequences;physics-based planning;augmented reality;equipment maintenance,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,A Real-Time Background Replacement Method Based on Machine Learning for AR Applications,COMPSAC - International Computer Software and Applications Conference,B,"Recent technological advances in Virtual Reality (VR) and Augmented Reality (AR) enable users to experience a high-quality virtual world. Using VR to experience the virtual world, the user's entire view becomes the virtual world, and the user's physical movement is generally limited because the user cannot see the surrounding situation in the real world. Using AR to experience the virtual world, we generally use special sensors such as LiDAR to detect the real space and superimpose the virtual world on the real space. However, it is difficult for devices without such special sensors to detect real space and superimpose a virtual world at an appropriate position. This study proposes two methods for replacing the background: a method using depth estimation and a method using semantic segmentation. This study also confirmed that the system can be used with sufficient removal accuracy and response time by using appropriate image size for the environment and that a safe and highly immersive virtual world experience can be achieved.",augmented reality;virtual reality;video processing;dynamic background replacement;mobile computing,Abstract_Keywords,True,
IEEE,conferencePaper,2024,An AR Visualization System of Near-Future Information to Avoid Dangerous Situations,COMPSAC - International Computer Software and Applications Conference,B,"In recent years, there has been an urgent need to improve safety and efficiency to avoid dangerous situations. In this paper, the authors proposes a visualization system of near-future information by augmented reality (AR) to avoid dangerous situations. The proposed system utilizes the position and speed information of the user and obstacles, predicts crashes, and recommends avoidance actions such as changing course or stopping. Experiments were conducted under different conditions by the implemented system to determine how many seconds into the future it would be safer for the user and easier to take avoidance action. The experiment results showed that the avoidance rate was almost the same for 2 seconds or longer, but the best rate was for 3 seconds ahead.",Augmented Reality (AR);location prediction;collision avoidance;safety improvement;near-future information,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Meeting Latency and Jitter Demands of Beyond 5G Networking Era: Are CNFs Up to the Challenge?,COMPSAC - International Computer Software and Applications Conference,B,"The introduction of Network Function Virtualization (NFV) has shifted network processing from specialized hardware to more flexible commodity servers, and this transition is still evolving. New industrial applications, the Internet of Things (IoT), and technologies like augmented, virtual, and mixed reality (AR/VR/MR) require networks that can handle event-based operations and middleware with very low and predictable latency. These requirements pose performance optimization challenges for packet processing in a layered infrastructure. In this study, we look deep into the challenges of implementing such network infrastructures using general-purpose hardware, a strategy motivated by its flexibility to realize telco-cloud and the potential to reduce electronic waste. Focusing on NFV with an emphasis on containerized network functions (CNFs), we investigate the performance limitations, particularly the high jitter and throughput variation observed in packet forwarding. We used a network function (NF) implemented using defacto industry standard user-space I/O architecture DPDK in bare-metal and containerized environments for performance evaluation. We conducted ten experiments in a 40 GbE environment to measure throughput, latency, and jitter across various packet sizes, traffic rates, and system configurations. The results indicate that adjusting CPU settings can significantly enhance throughput for CNFs despite a potential increase in jitter. We found that CNFs are feasible for latency-sensitive tasks, particularly under conditions of low traffic and specific packet sizes. With careful system-level configuration, CNFs can be used in beyond 5G cloud-native networking, offering promising potential for latency-sensitive applications.",6G;Network Function Virtualization (NFV);Containerized Network Functions (CNFs);COTS;Cloud Native;Jitter;Latency;DPDK,Abstract,True,
IEEE,conferencePaper,2024,Visualization of Crowd Contamination Simulations Using Immersive Virtual Reality,COMPSAC - International Computer Software and Applications Conference,B,"Smart cities can generate a lot of useful data for analyzing behavior and infrastructures, helping with urban planning. However, the applications found are often limited to desktop viewing only, which can hinder or restrict data analysis. This work proposes an immersive data visualization of the population of medical centers to assist in the analysis and development of new strategies to deal with overcrowding situations. The prototype allows the visualization of crowd data located in places of interest on top of a 3D map. The user can view the number of people present at the location at the current stage of the simulation or over time. To test the prototype, data from an infectious disease simulator was used, called LODUS. The results showed that the application has potential for visualizing this type of data.",smart cities;simulation;visualization;virtual reality,Title_Keywords,True,
IEEE,conferencePaper,2024,Exploring New Horizons in Dental Education: Leveraging AI and the Metaverse for Innovative Learning Strategies,COMPSAC - International Computer Software and Applications Conference,B,"The COVID-19 pandemic significantly disrupted dental education, hindering hands-on exposure to cutting-edge dental practices and advanced equipment worldwide. Amidst these challenges, the emergence of Metaverse-based learning has presented an innovative solution, fulfilling the growing need for remote educational opportunities in dentistry. Traditional online learning platforms like Zoom have proven inadequate for providing a comprehensive learning experience in dentistry, prompting a shift towards more engaging, immersive educational settings. This trend is especially evident in the dental education sector in the United Arab Emirates (UAE). This research delves into dental studentsâ€˜ perceptions of how Metaverse technology aids in achieving their educational objectives within the UAE. Our analysis focuses on crucial factors that influence technology adoption, particularly â€˜Perceived Valueâ€™ and â€˜Perceived Satisfactionâ€™. We collected a substantial dataset of 89 responses from the College of Dental Medicine (CDM) at the University of S harjah. To rigorously examine our research model, we applied Partial Least S quares-Structural Equation Modeling (PLS -S EM) and an advanced Machine Learning (ML) technique, based on data from our student survey. Our results highlight the Metaverse's critical role in guiding technology adoption decisions, strongly driven by â€˜Perceived Valueâ€™ and â€˜Perceived Satisfactionâ€™. Remarkably, the ML method demonstrated higher predictive accuracy in identifying the outcome variable compared to other analysis techniques. This research contributes to the scholarly conversation on artificial intelligence, especially its relationship with environmental sustain ability, offering valuable insights for industry stakeholders, policymakers, and AI developers. The findings lay the groundwork for de veloping AI-powered solutions aligned with user preferences and environmental co n si de rati o n s.",AI-Driven Learning Strategies;Dental Education Distance Learning Innovations;Metaverse;Technology Adoption,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Development of a Virtual Travel System to Enhance the Discovery of Aspiration and Pleasure,COMPSAC - International Computer Software and Applications Conference,B,"Systems providing various virtual experiences through virtual reality (VR) are being utilized in many fields such as sports, education, entertainment, and healthcare. Many of these systems offer unique and rare experiences that are not easily accessible in the real world, aimed at improving motor and cognitive skills, as well as enhancing enjoyment. However, from the perspective of information systems that improve user happiness and well-being quality, the field is still in a developmental state, and the specific design methods for virtual experiences remain unclear. Our aim is not to enhance athletic performance or skills, but rather to construct a VR system that supports individuals in discovering their own pleasures and aspirations. This paper reports on the implementation of a prototype system aligned with the concept of supporting the discovery of pleasures and aspirations, focusing on a travel experience system as the prototype for this VR system.",virtual reality;well-being;user adaptation;sense of agency;human-agent interaction,Abstract_Keywords,True,
IEEE,conferencePaper,2024,Advancing Arousal and Valence Prediction in Virtual Reality Through Sentiment Analysis Integration,COMPSAC - International Computer Software and Applications Conference,B,"This study investigated how integrating sentiment analysis derived from verbal responses to open-ended questions with physiological signals into support vector machine (SVM) models can enhance the prediction of arousal and valence in virtual reality (VR) gaming. Three VR Pong scenariosâ€“slow, fast, and lag-inducingâ€“were tested to observe the impact of game-play speed and responsiveness on sentiment scores. Participant responses were measured using the Self-Assessment Manikin for arousal and valence, and sentiment was analyzed from open-ended responses. Physiological measures, such as electrocardio-grams, galvanic skin responses, and electromyogram activity in forearm muscles, were also recorded. The results indicated that gameplay speed and responsiveness significantly affects player emotions, with faster games leading to higher sentiment scores. Additionally, incorporating these sentiment scores into SVM models improved the accuracy of valence and arousal predictions. This highlights the role of gameplay speed and responsiveness in influencing both emotional and physiological responses in VR and suggests that using diverse data types, including audio-derived sentiment analysis, could enhance the accuracy of affective computing models.",Virtual reality;Affective computing;Machine learning;Support vector machine;Sentiment analysis;Emotion;Physiology,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,The Impact of Anonymity on Communication in the Metaverse,COMPSAC - International Computer Software and Applications Conference,B,"The number of opportunities for activities that can be done from the comfort of one's home has increased considerably, and recently, activities such as events in the metaverse using virtual avatars have become more accessible. We believe that one of the main reasons for this is the high degree of anonymity of social networks which has led to a state of immersion. Unfortunately, problems may occur in a highly anonymous metaverse. In this experiment, we investi-gate the effect of immersion on communication by changing the anonymity of the person with whom one converses using a virtual avatar in the metaverse. The subjects were di-vided into four groups: anonymous-anonymous, anonymous-non-anonymous, non-anonymous-anonymous, and non-anonymous-non-anonymous. The results of the experiment showed that there were no significant differences between the groups, suggesting that the subjects' anonymity did not affect their immersive communication. We deduce that primitive stages of visual anonymity inhibit rude and violent behaviors in the metaverse. Certainly, a superficial status of primitive visual anonymity could be utilized for different applications. Nevertheless, high levels of anonymity are required to allow de-individualization to take effect in the metaverse.",Metaverse;Virtual avatars;Anonymity;de-individualization,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Investigating Radiological Diagnosis Through Smartphone-Based Virtual Reality Applications: A User Study,COMPSAC - International Computer Software and Applications Conference,B,"This study investigates the utilization of low-cost three-dimensional visualization devices., specifically virtual reality applications on smartphones., to enhance radiological diagnosis processes. Radiology., a critical field in medicine., often faces challenges such as external illumination and poor ergonomic conditions during diagnostic procedures. Virtual reality technology has emerged as a potential solution to address these issues. The research conducted user studies with radiologists and medical physicists to assess the effectiveness and usability of the virtual reality application. Feedback from participants indicated positive perceptions of the virtual environment., 3D model quality., and interaction with the application. The study also evaluated geometric transformations on the 3D model, highlighting the importance of user-friendly controls. Overall, the findings suggest that virtual reality technology on smartphones holds promise in supporting radiological diagnosis by providing an immersive and efficient tool for medical professionals.",virtual reality;radiology;smartphone;scrolling;windowing,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2023,Reputation-Based Trust Assessment of Transacting Service Components,COMPSAC - International Computer Software and Applications Conference,B,"We witness a paradigm shift where entities such as software applications, people, businesses and service providers increasingly interact in virtual rather than physical environments, such as social media, social commerce, and the metaverses. A key issue that emerges in such environments is how one entity can trust another. Here, the concept of trust is considered as a meta-requirement, that is, the level of belief a service requestor has that a service provider will provide the service in a way that meets the requestorâ€™s expectations. We refer to the service offering entities as service providers (SPs) and the service requesting entities as service clients (SCs). In this paper, we propose a technique that allows for evaluating trust and assigning reputation to various service providers by considering first their ability to fulfill their clientsâ€™ expectations or policies, and second the reputation other service clients have, when acting as recommenders for the aforementioned service providers. In this work, service clients and service providers are considered as virtual entities that coordinate with each other, and may include physical users, avatars, micro-services, software agents, smart contracts or any other distributed inter-networked resource, without making any assumptions as to what a service client or a service provider entity is as long as it participates in an interaction.",Distributed Components;Service-oriented Computing;Trust;Reputation Systems;Multi-agent Systems;Metaverse,Abstract_Keywords,True,
IEEE,conferencePaper,2023,A Metaverse Object Management Method Based on Visible Areas Using Geographical Overlay Networks,COMPSAC - International Computer Software and Applications Conference,B,"The metaverse, which has been the focus of much attention in recent years, has a load-related problem. We focused on the architecture (objects) in the metaverse that users acquire more than necessary as a cause of the load. Since managing all the objects in the metaverse on a userâ€™s terminal is difficult, a mechanism that can retain only the necessary objects is required. In this study, we propose a method to manage objects in a distributed manner to retrieve only the necessary objects. We propose a retrieval method that determines which objects to recover based on size and distance, considering the userâ€™s visible area, and a retrieval method that determines the shielding relationship calculated from size and location information and excludes unnecessary parts from the distributed objects. The proposed method was evaluated in simulations in two different environments, and a reduction in the number of communication targets was confirmed.",Virtual space;location-based service (LBS);distributed data management (DDM);range search,Title_Abstract,True,
IEEE,conferencePaper,2023,A Process Reduction Method for Spatial Information in Real-Time AR Snow Visualization Systems,COMPSAC - International Computer Software and Applications Conference,B,"In this paper, the authors propose a real-time AR visualization system that enables the user to visually grasp the future snow-covered situation at the current location, aiming to support residents and visitors in heavy snow areas. The proposed system generates snow-covered spatial information that reflects the snow-covered situation on spatial information in the real world sensed by 3D LiDAR, and composes it on the video image captured by a camera. This paper describes a lightweight method of spatial information processing by reducing the amount of spatial data transmission.",Augmented Reality;3D LiDAR;snow-covered;polygon mesh;process offloading,Keywords,True,
IEEE,conferencePaper,2023,LINE Metaverse for elderly people,COMPSAC - International Computer Software and Applications Conference,B,"With the popularization of smartphones, SNS, which allows easy message exchange, has become popular. On the other hand, a metaverse has attracted attention recently. In the metaverse, participants interact with each other through avatars in a virtual space. Therefore, if we can show participants the existing SNS space as the metaverse, we can provide participants with more intuitive use cases. This manner is beneficial for elderly people who are unfamiliar with using smartphones. In this paper, we propose LINE metaverse. LINE is the most popular messaging application in Japan. The LINE metaverse is characterized by replacing the bot mechanism used for marketing using SNS with a metaverse agent. In the LINE metaverse, elderly people can exchange messages via avatars on the metaverse.",Metaverse;SNS;Bot;Avatar;Message exchange services,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2023,Optimized NLP Models for Digital Twins in Metaverse,COMPSAC - International Computer Software and Applications Conference,B,"Digital Twins (DTs) in Metaverse face many challenges such as the lack of optimized AI models to allow the interaction between the user and the virtual environment. In this paper, we propose an optimized model for human language processing based on Convolutional Neural Networks (CCNs) and we present an input processing strategy to meet the real-time requirements of smart applications that integrate DTs oriented to speech-based functionalities for user interaction and Metaverse. In our solution, CNNs are applied for the processing and classification of the human voice, while structured data and MFCC coefficients are used to train the neural networks and generate interference in the models. Similarly, the MFCC algorithm is provided to extract the unique characteristics that specify each generated audio file and to reduce the complexity of the neural network model in order to obtain better performance. Starting from an approach to the problem available in the literature, we have optimized a specific CNN model for Natural Language Processing (NLP) in order to increase effective results. The proposed model has demonstrated excellent performance and can be used as a basis for the implementation of software that allows the interaction of DTs with voice commands issued by a user.",Digital Twins;Metaverse;Virtual Reality;Natural Language Processing;Automated Speech Recognition;Convolutional Neural Networks;MFCCs,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2023,Hanfu AR: Digital Twins of Traditional Chinese Costumes for Augmented Reality Try-On Systems,COMPSAC - International Computer Software and Applications Conference,B,"We present Hanfu AR, an Augmented Reality (AR) try-on system that presents digital twins of traditional Chinese costumes based on Kinect. The system allows users to virtually try on 3D clothing with real-time interactions and realistic cloth simulation. Specifically, we present an optimized framework that addresses four aspects of the development of digital twins of virtual clothing and try-on systems: calibration, cloth simulation, control, and configuration. Our work contributes to the development of realistic digital twins of virtual clothing and interactive try-on systems. The system can be applied in various areas and has great values in design, culture, education, and marketing. The proposed framework will benefit the future development of digital twins of virtual clothing for applications in the Metaverse.",Digital Twins;Augmented Reality;Virtual Try-On;Kinect,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2023,An Empathetic Approach to Human-Centric Requirements Engineering Using Virtual Reality,COMPSAC - International Computer Software and Applications Conference,B,"People who use software applications are different, including with significant cognitive differences such as neurodiversity. Capturing requirements for software that addresses these cognitive differences is hard for software engineers, especially when they do not have the same cognitive challenges. We wanted to explore the use of virtual reality (VR) in assisting software engineers to better understand the perspectives of the end user for the purpose of human-centric requirements elicitation, with a focus on users with attention-deficit/hyperactivity disorder (ADHD). We developed an immersive VR prototype using a virtual gym environment and fitness app as a concrete motivating example scenario. We carried out an evaluation of requirements identification for ADHD fitness app users by instructing participants to complete activities whilst under visual and auditory distractions similar to various documented symptoms of ADHD. Results indicated an increase in understanding the perspectives of someone with ADHD and an awareness of potential challenges with software not intentionally designed for ADHD users. Improved requirements for our target fitness app resulted that better take into account these diverse user needs.",Virtual Reality;Human-Centric Requirements Engineering;Attention Deficit Hyperactivity Disorder,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2022,eShadow+: Mixed Reality Storytelling Inspired by Traditional Shadow Theatre,COMPSAC - International Computer Software and Applications Conference,B,"eShadow is a digital storytelling platform inspired by traditional Shadow Theatre. It enables the creation of digital stories within a project-based approach that may start from scenario development and include the creation of digital puppets and sceneries, the set-up and recording of story scenes and the final assembly of a digital story. This paper presents how eShadow can be enhanced to solve the problem of creating mixed reality installations to offer rich learning experiences in informal learning settings. This enhanced version is eShadow+ and it is evaluated via two installations which are described and compared. The evaluation results demonstrate the effectiveness of the approach thus offering new learning opportunities that are aligned with current trends in the use of mixed reality technologies.",shadow theatre;digital storytelling;mixed reality;informal learning,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2022,Connecting Everyday Objects with the Metaverse: A Unified Recognition Framework,COMPSAC - International Computer Software and Applications Conference,B,"The recent Facebook rebranding to Meta has drawn renewed attention to the metaverse. Technology giants, amongst others, are increasingly embracing the vision and opportunities of a hybrid social experience that mixes physical and virtual interactions. As the metaverse gains in traction, it is expected that everyday objects may soon connect more closely with virtual elements. However, discovering this â€œhiddenâ€ virtual world will be a crucial first step to interacting with it in this new augmented world. In this paper, we address the problem of connecting phys-ical objects with their virtual counterparts, especially through connections built upon visual markers. We propose a unified recognition framework that guides approaches to the metaverse access points. We illustrate the use of the framework through experimental studies under different conditions, in which an interactive and visually attractive decoration pattern, an Artcode, is used as the approach to enable the connection. This paper will be of interest to, amongst others, researchers working in Interaction Design or Augmented Reality who are seeking techniques or guidelines for augmenting physical objects in an unobtrusive, complementary manner.",Artcode;augmented reality;interaction;meta-verse;visual marker,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2022,A Systematic Literature Review of Virtual and Augmented Reality Applications for Maintenance in Manufacturing,COMPSAC - International Computer Software and Applications Conference,B,"Virtual and augmented reality approaches, which support maintenance workers with up-to-date physical images and easy-to-understand data representations, are gaining importance. These approaches can increase the efficiency and quality of plant maintenance. In this review, based on the literature included in international peer-reviewed journals and conferences, we identify different areas and applications for virtual and augmented reality approaches that support maintenance in an in-dustrial environment. Furthermore, we motivate future research for technologies that are more manageable and better suited to the manufacturing environment.",Manufacturing;maintenance;virtual reality;augmented reality;data visualization,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2022,Color-Path: Hair Arrangement Reproduction Support System by Displaying Target Motion in AR,COMPSAC - International Computer Software and Applications Conference,B,"Curling irons can create curled hair by heating it. However, because the operation of the curling iron involves six degrees of freedom (6DoF) movements, such as translation and rotation, it is difficult to understand and reproduce by simply watching instructional videos. In this study, we proposed Color-Path, a smart mirror-shaped system that allows users to easily understand how to move a curling iron. The proposed system acquires the moving paths of the curling iron using a camera on a smart mirror and an accelerometer on the curling iron. The system displays the curling iron paths of the target hairstyle on a smart mirror in Augmented Reality (AR). From the evaluation experiments, we confirmed that the system contributed to the reproduction of the moving paths of the curling iron through quantitative evaluation. However, a subjective evaluation showed that our system could not reproduce the appearance of the target hairstyle. The results indicate that the time to heat the hair should be considered.",curling iron;hair arrangement;smart mirror;augmented reality;operational support,Abstract_Keywords,True,
IEEE,conferencePaper,2022,A Real-Time Background Replacement Method Based on Estimated Depth for AR Applications,COMPSAC - International Computer Software and Applications Conference,B,"Recent technological advances in Virtual Reality (VR) and Augmented Reality (AR) enable users to experience a high-quality virtual world. In VR applications, the user's physical movement is generally restricted because the situation around the real world cannot be seen. AR allows users to experience virtual worlds without restrictions on physical movement, but the extent to which they are replaced as virtual worlds is limited. In this research, assuming the use of smartphones and tablet devices, a partial virtual world system is implemented by removing only the background part from the real-time real-world image taken by the camera and replacing it with a virtual background.",Augmented reality;Virtual reality;Video processing,Abstract_Keywords,True,
IEEE,conferencePaper,2022,Lessons Learned from the Design and Evaluation of InterViewR: A Mixed-Reality Based Interview Training Simulation Platform for Individuals with Autism,COMPSAC - International Computer Software and Applications Conference,B,"As one of the most important and stressful steps towards gaining employment, job interviews are a social barrier that poses a unique challenge for individuals with Autism Spectrum Disorder (ASD). There are existing tools and in-person training available that show promise. However, these solutions are limited in the degree to which they can collect performance data, provide specific feedback, offer options to intelligently customize training, and simulate a variety of interviews in a realistic manner. InterViewR is a mixed reality job interview training simulator that addresses these shortcomings to reduce barriers in entering the workforce. The integration of Virtual Reality and wearable smart technology enables users to practice customizable interview simulations and receive both real-time and retrospective feedback. Findings are reported from a cognitive walkthrough (N=33) that highlights areas that need to be considered when designing such interview training platforms for individuals with autism.",human-centered computing;accessibility;virtual reality;stress;biofeedback;autism spectrum disorder;job interview training,Abstract_Keywords,True,
IEEE,conferencePaper,2022,Cultural Heritage Assets Optimization Workflow for Interactive System Development,COMPSAC - International Computer Software and Applications Conference,B,"An increasing number of reconstructed digital assets are being created worldwide to preserve cultural heritage. These assets can be used in interactive systems such as augmented reality (AR) and virtual reality (VR) to provide effective ways to access and learn about cultural heritage. One of the widely adopted reconstruction techniques is close-range photogram-metry. However, scanned models need to be processed and optimized before they can be used in interactive systems, which requires a series of retopology and baking work to reduce the size of models while maintaining visual fidelity. Nevertheless, manual retopology and baking are complex processes. An efficient optimization workflow is essential for the use of cultural heritage assets in interactive systems. This paper presents an optimization workflow for retopology and texture baking using free and open-source software. Evaluations show that the workflow demon-strates its strengths in its high efficiency, versatility, learnability, and low cost. This work contributes insights to researchers and practitioners in the field of cultural heritage.",digital heritage;interactive system;VR;AR;3D modelling;retopology;texture baking;photogrammetry,Abstract,True,
IEEE,conferencePaper,2021,"Panel Discussion: Deriving Past, Present, and Future Tech to More Intelligent and Resilient Digital Realities for a Collaborative World",COMPSAC - International Computer Software and Applications Conference,B,"No abstract or record of the panel discussion was made available for publication as part of the conference proceedings. There is the philosophical question of â€œdoes history repeat itselfâ€. One learns from the past to affect the present, which builds our future. Advances in technology are built upon previous innovations. New technologies are derived from existing technologies. The IEEE leverages past and current technologies to advance work on new and emerging technologies through serving as a catalyst for developing new innovations, products and services. IEEE Future Directions serves as an incubator for these new initiatives. One of its focus areas, Digital Reality serves to explore and enable the coming Intelligent and Resilient Digital Realities through collaboration among technologists, engineers, regulators, practitioners, and ethicists around the world. The Digital Transformation is fueled by advances in technology, such as Artificial Intelligence (AI), Machine Learning (ML), and applications using the copious amounts of continuously generated data. By leveraging these technologies and others developed such as Augmented Reality (AR), Virtual Reality (VR), and Digital Twins, the line between the physical world and the digital world will be increasingly less distinct. Applications are already quickly emerging across the broad fields of gaming, entertainment, medicine, automotive, education, manufacturing, enabling the sharing of services, and more. Emphasis is upon presenting practical applications and its implementations of interest to attendees. Subject matter expert speakers comment on current and past implementations. Of course, the speakers look ahead to the future.",,Abstract,True,
IEEE,conferencePaper,2021,An Immersive Virtual Reality Platform for Training CBRN Operators,COMPSAC - International Computer Software and Applications Conference,B,"In the domain of CBRN (Chemical, Biological, Radiological, Nuclear) hazards, first responders need a high-quality training to avoid fatal errors that can compromise the success of operations. Nevertheless, CBRN exercises are often expensive and require a complex management. Furthermore, for preserving traineesâ€™ safety and for logistic constraints, trials may reproduce just an approximation of the real hazard scenario. In order to cope with these issues, a prototype of an Immersive Virtual Reality (VR) training platform was developed by Politecnico di Torino and LINKS Foundation in cooperation with CBRN experts from the Italian Air Force, particularly from Terzo Stormo â€“ Aeronautica Militare di Villafranca di Verona. The platform aims at allowing CBRN operators to train, alone or in a team, testing their ability to carry out required operational procedures in a digital environment that exhibits the complexity of a real-life situation, but does not expose them to life-threatening dangers.",Virtual Reality;CBRN;Simulation;Training,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2021,Towards An Indoor Navigation System Using Monocular Visual SLAM,COMPSAC - International Computer Software and Applications Conference,B,"This paper presents a novel implementation of an augmented reality-aided indoor navigation system for mobile devices. The proposed system uses the device's camera to scan the environment, generate an abstracted 3D map of the indoor environment, and transfer it to a remote server. The 3D map of the indoor environment is achieved through a tracking and mapping ARKit module. Once the indoor map is stored in the server, it can be accessed simultaneously by multiple devices for localization and navigation. Leveraging Unity assets and the directions retrieved from the server, the application computes the shortest distance between the source and destination, and displays AR-based direction markers for navigation assistance.",Augmented reality;Indoor navigation systems;Simultaneous localization and mapping,Abstract_Keywords,True,
IEEE,conferencePaper,2021,Augmented Reality for Training and Maintenance of Reclosers: A Case Study of a Wearable Application,COMPSAC - International Computer Software and Applications Conference,B,"This paper presents a case study of Copelia, an augmented reality application targeted for smart glasses. Copelia was developed to optimize the workflow of COPEL (Companhia Paranaense de Energia), a major Brazilian electricity company. The application assists its users in the maintenance of reclosers with features such as object recognition, augmented reality (AR), and remote calls. These features allow generalist electricians to perform tasks that would otherwise be exclusively executed by skilled professionals. Copelia was evaluated in laboratory and field tests. Results show that users resisted the application due to its novel interaction paradigm. Most of the reported issues were related to the usability of smart glasses or the graphics interface. A list of good practices was derived from the discussion of these issues. The features themselves proved to be useful, especially the guided instructions. Despite an initial negative reaction, Copelia gained appraisal by its users over time, who highlighted its usefulness for training and maintenance.",augmented reality;maintenance;training,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2021,Resource Discovery for Edge Computing over Named Data Networking,COMPSAC - International Computer Software and Applications Conference,B,"Edge computing over Named Data Networking (NDN) is expected to be a promising approach for the deployment of applications such as connected cars and virtual/augmented reality. In conventional edge computing frameworks, computing request packets are not forwarded based on the resource availability statuses of edge execution nodes (e.g., CPU and memory utilization). This makes it possible for the requests to reach fully loaded nodes that cannot deal with these requests. To address this issue, this paper proposes a resource discovery scheme for edge computing over NDN. In this scheme, each execution node distributes its own resource availability status in its vicinity by scoped-flooding. This information guides the requests toward available nodes that meet the requirements of the requests. Our extensive simulations show that the proposed scheme helps request packets to discover available execution nodes effectively.",Edge computing;Named Data Networking;Resource discovery;Scoped-flooding,Abstract,True,
IEEE,conferencePaper,2021,A Multipath Routing Approach for Tile-based Virtual Reality Video Streaming Based on SDN,COMPSAC - International Computer Software and Applications Conference,B,"With the increasing demand of virtual reality (VR) video applications, it is necessary to adopt corresponding strategies to deal with the challenges they bring to the network. Multipath routing is proposed to address the VR video bandwidth problem by splitting a large flow into multiple subflows and routing them separately. In addition, Software Defined Networking (SDN) is used to manage these subflows so that they are assigned to the appropriate paths. This paper presents a MCTS-based VR video multipath transmission approach (MVRMPT), which allocates better paths to the VR video tiles that have greater impacts on the userâ€™s Quality of Experience (QoE). More specifically, the Monte Carlo tree search (MCTS) algorithm is modified to find multiple disjoint paths with the minimum delay between node pairs. Then the paths are sorted by the predicted QoE. Finally, the VR video is spatially divided into different zones, and these zones are assigned to different paths according to their impacts on the userâ€™s QoE. The proposed algorithm is implemented in the SDN controller, and the evaluation results show that our method achieves higher QoE and network throughput.",SDN;VR;multipath routing,Title_Abstract,True,
IEEE,conferencePaper,2021,Use of Augmented and 3D Visualization as a Tool to Support the Teaching of Spatial Geometry,COMPSAC - International Computer Software and Applications Conference,B,"Geometry is a branch of Mathematics that helps us to work with distances, volumes, and areas in the real world. However, learning content related to Spatial Geometry, whose visual appeal should be an invitation to study, presents an unusual obstacle for the student: how to understand the elements of a three-dimensional object if they are normally presented in a two-dimensional way (e.g., drawn on the whiteboard or book)? There is several software that can be used in the teaching of Space Geometry. However, in most times, the teacher has no contact with these tools, either due to unavailability of hardware to interact; or due to difficulty in acquiring the software, which in turn may be due to the cost or lack of knowledge of the means to make the acquisition. The focus of this paper is to present the experience report on the use of Augmented Reality using tutorials developed by NIEP group, aiming to generate personalized material for use in the classroom by high school students in Mathematics, in the teaching/learning of Space Geometry. The results found demonstrate improved learning, better understanding of the content and reduced learning time.",Augmented Reality;Education;Spatial Geometry,Abstract_Keywords,True,
IEEE,conferencePaper,2021,Creating a Virtual Reality OER Application to Teach Web Accessibility,COMPSAC - International Computer Software and Applications Conference,B,"Awareness of web accessibility issues is necessary for, amongst other things, good website design. Good website design can mean the difference between disabled users being able to access the website content, or not. This paper describes the impact of a student-led project to develop a VR application, as an Open Education Resource (OER), to increase usersâ€™ knowledge and awareness of accessibility. An evaluation for the intervention delivered by the VR application was conducted, according to which, the VR application generally increased knowledge and awareness of web accessibility, but also had a negative impact on some users. The project targetted a Human Computer Interaction (HCI) class taught at the first Sino-foreign higher education institution, University of Nottingham Ningbo China (UNNC). UNNC has already been involved in research into flipped classrooms, technology-enhanced teaching, and the development of several OERs. This paper introduces the background, motivation and objectives, design, and specification of the VR application. The impact of the application on users is evaluated, and some possible future work is discussed.",Virtual Reality (VR);Open Educational Resources (OERs);Human Computer Interaction (HCI);Flipped Classroom;Web Accessibility,Title_Keywords,True,
IEEE,conferencePaper,2021,A Model to Helping the Construction of Creative Service-Based Software,COMPSAC - International Computer Software and Applications Conference,B,"With the advent of the Service Oriented Architecture (SOA) in system design, various domain knowledges are included in a service-based application, such as the design of Artificial Intelligence (AI) or augmented reality (AR) systems. While merging one or multiple domains into computation systems, the computation systems can be widely applied in various domain usages with novelty, useful, and surprising properties, which are defined as systems of creative computing. In creative computing, several theoretical evaluation metrics and verification approaches have been proposed for system design in several domains. However, a solid practical design environment for creative service-based systems is rarely considered in current researches. In this paper, we propose a model for creative service software development based on semantic web, which is applied in two phases: (1) requirement specification and (2) service design. In order to bridge the knowledge gap between domain experts and software engineers, and provide a machine-readable format for creative computing, two sub-models, Requirement Specification and Service Structure Models, are constructed in both phases, sequentially. After the latter sub-model is validated, the creative service software is well-constructed based on the services definition and composition represented by the model.",Creative Computing;Semantic Web;Service-Oriented Architecture,Abstract,True,
IEEE,conferencePaper,2020,AI and ML-Driving and Exponentiating Sustainable and Quantifiable Digital Transformation,COMPSAC - International Computer Software and Applications Conference,B,"AI is a major transforming technology impacting every sector of life. AI is not a force to deprive humans and take over the control, rather a real enabler and lever for digital transformation. The former view aligns with Hollywood movies, and need to be undressed with the latter, which is realistic and becoming tangible over time as more organizations, and communities are leveraging AI's potential. Developing a practical understanding of AI, its capabilities, the challenges, and opportunities that it brings is fundamental to get the maximum out of its envisaged potential. The objective of this paper is to highlight how technology and industry have developed, discuss the role of AI in driving intelligent transformation concentrating on an applicable understanding of AI and related technologies. We introduce a new conceptual framework: AI's multi-dimensional role, to highlight its transformative power in multiple aspects. We also introduce an AI based Information and Model Governance Framework.","Artificial Intelligence, Machine Learning, Internet of things, Neuromorphic Computing, Digital Transformation, Industry 4.0, Augmented Reality.",Keywords,True,
IEEE,conferencePaper,2020,InterViewR: A Mixed-Reality Based Interview Training Simulation Platform for Individuals with Autism,COMPSAC - International Computer Software and Applications Conference,B,"Job interviews are uniquely challenging for individuals with Autism Spectrum Disorder. While digital interview training tools have shown promising results for improving vocational outcomes for individuals with Autism, existing solutions are largely limited in the degree to which they can simulate a realistic interview environment, collect performance data from the user, and provide actionable feedback for continued improvement. To address these shortcomings and understand how to create an effective training tool, we designed InterViewR, a simulation-based interview training system that combines virtual reality and wearable smart technology in an integrated platform. Our design emphasizes an immersive user experience for training effectiveness and utilizes physiological sensing to provide intelligent affective biofeedback. We report findings from a usability study (N=11) where participants evaluated InterViewR on feasibility, usability, and perceived usefulness.","Virtual Reality, Stress, Biofeedback, Autism Spectrum Disorder, Communication Skills Training",Abstract_Keywords,True,
IEEE,conferencePaper,2020,A Systematic Literature Review of Practical Virtual and Augmented Reality Solutions in Surgery,COMPSAC - International Computer Software and Applications Conference,B,"From a strong practical point of view, we offer an overview of virtual and augmented reality solutions in medicine. We thus analyzed practical and industrial work included in peer-reviewed articles and conference proceedings.",virtual reality solutions;augmented reality solutions;IT in healthcare;IT in practice;literature review,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2020,VRvisu++: A Tool for Virtual Reality-Based Visualization of MRI Images,COMPSAC - International Computer Software and Applications Conference,B,"With the emergence of sophisticated head-mounted displays (HMDs), virtual reality (VR) is gaining much interest in the field of medical science and diagnosis. There exist many software tools that support MRI imaging in 3D, however, limited attention has been paid to the VR domain. In this paper, we present a VR tool called VRvisu++ which attempts to bring the spatial advantage that VR has to offer to MRI imaging. This tool allows doctors and medical practitioners to directly interact with MRI images in a VR environment thereby supporting surgical training and clinical decision making.","Virtual Reality, Visualization, MRI, Tumors",Title_Abstract_Keywords,True,
IEEE,conferencePaper,2020,Modeling an AR Serious Game to Increase Attention of ADHD Patients,COMPSAC - International Computer Software and Applications Conference,B,"Attention, as a commodity in the human skills-set, increases through training over a quite relevant period of time. Deficit in attention in the short term may be based on multiple complications to entail stress and mined wondering. On the long term, however, it might be a symptom of chronic diseases that acquire attention such as ADD, ADHD, ASD, etc. In this paper, therefore, we introduce a generic theoretic model that helps patients in short of attention engage in a game-based environment to enhance the behavior of their current state of attention and leads to an improved focus. The model introduces six modular components capturing essential units that are intended to be modularly combined to define an augmented reality gamification therapy.",Game Design;Agent-Based Modeling;ADHD;Cognitive Behavioral Therapy,Abstract,True,
IEEE,conferencePaper,2020,The Development of Emerging Technological Applications for Not-for-Profit Organizations in Capstone Projects: A Case in Scout Association of Hong Kong,COMPSAC - International Computer Software and Applications Conference,B,"There is a growing interest in emerging technologies such as VR among practitioners from different sectors, including education and social welfare, given these technologies may enhance the experience of their service targets, and may be easily adopted by ordinary users. Additionally, the barriers of users' adoption are also lowering. Nevertheless, not-for-profit organizations may find significant barriers for developing their emerging technological applications, which are largely due to the resources and capabilities required. As a result, the adoption of these emerging technologies by not-for-profit organizations (NPO) is yet to be seen. Through a case-study in Scout Association of Hong Kong, we have learnt that NPOs might develop and adopt different emerging technological applications to fit their needs by collaborating with the universities, possibly with students' capstone project as a medium. These collaborations will be beneficial to not only the NPOs but also to the students and the university. Based on our findings, we also highlighted a few lessons for these collaborations.",Academic-community collaboration;Capstone project;Emerging Technology;Virtual reality;e-Learning,Keywords,True,
IEEE,conferencePaper,2020,A Virtual Reality OER Platform to Deliver Phobia-Motivated Experiences,COMPSAC - International Computer Software and Applications Conference,B,"This paper describes an on-going project to develop a Virtual Reality platform to deliver phobia-inspired experiences. These experiences could induce a reaction in the user that may help the user overcome, or alleviate, the phobia. The platform includes monitoring sensors that could be used to measure how much impact the experience is having. The project development has been taking place at a Sino-foreign Higher Education Institution in Mainland China, University of Nottingham Ningbo China (UNNC). UNNC has already been host to a number of OER (Open Educational Resource) development projects, and the current project is also anticipated to eventually be released to the OER community. This paper presents the background, development, and current state of the project. Challenges to project completion, and future work are also outlined.","Virtual Reality (VR), Sensors, Open Educational Resources (OERs), Sino-foreign Education, Wearable Technology, Mobile Devices, Metamorphic Testing (MT), Metamorphic Relation (MR)",Title_Abstract_Keywords,True,
IEEE,conferencePaper,2020,LESAR: Localization System for Environmental Sensors using Augmented Reality,COMPSAC - International Computer Software and Applications Conference,B,"With the rapid development of IoT (Internet of Things) technology, numerous sensors are being deployed to the smart society with the integration of IoT services. Since the collected sensor data reflect the current status of the surrounding environment, determining accurate positions for sensors is crucial from the stage of setting the sensors to realize meaningful sensor data analysis. In this paper, we propose LESAR, a localization system for environmental sensing using augmented reality. LESAR uses a smartphone camera with the AR (Augmented Reality) function to measure the distances between sensors, while the ID of each sensor is identified simultaneously by analyzing the collected Bluetooth signals. The vision-based approach used can enable three-dimensional localization through the simple use of a smartphone.",Sensor Network;Localization;Augmented Reality,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2018,Indoor Augmented Reality Using Deep Learning for Industry 4.0 Smart Factories,COMPSAC - International Computer Software and Applications Conference,B,"This paper proposes to design, develop and implement a fast and markerless mobile augmented reality system to achieve the registration for, the visualization of, and the interaction with machines in indoor smart factories with Industry 4.0 vision. A lightweight deep-learning image detection module based on MobileNets running on mobile devices is used to detect/recognize different machines and different portions of machines. Internet of Things (IoT) networking allows machines and sensors in machines to report data, such as machine settings and machine states, to the cloud-side server. Thus, augmented information associated with a machine portion can be derived from the server and superimposed with the portion image shown on the device display. Furthermore, interaction methods based on touch gestures and distance calculation are also implemented. A prototype system is developed and tested in a mechanical workshop for the purpose of validation and evaluation. The system is shown to achieve high detection accuracy, intuitive visualization, and unique interaction modes.","augmented reality, deep learning, indoor positioning, Industry 4.0, Internet of Things, smart factory",Title_Abstract_Keywords,True,
IEEE,conferencePaper,2017,Design of a Node Status Visualizing Software Utilizing the AR Technology for Multihop Wireless Networks,COMPSAC - International Computer Software and Applications Conference,B,"Along with the growth of the Internet of Things (IoT), the number of wireless devices and mobile nodes significantly increases and they are connected among them over wireless links. Many application software and network system software for these wireless devices/mobile nodes are being developing. However, we cannot confirm a wireless connection between the two adjacent devices, the network topology which consists of these devices, and the node status of a wireless device which is not equipped with the display for a software developer while the software is running. In this paper, we propose the design of a visualizing software to visualize the network topology and the node status by utilizing the Augmented Reality (AR) technology, and then present the demonstration of the prototype development.",Network visualization;Visualizing Software;M2M communication;Wireless networks,Abstract,True,
IEEE,conferencePaper,2016,A Panoramic Geology Field Trip System Using Image-Based Rendering,COMPSAC - International Computer Software and Applications Conference,B,"Geology field trip is an important teaching step for the majors of geology, natural resources prospecting engineering and geographical sciences to learn geology knowledge and grasp field working skills. There are some problems in the traditional geology field trip teaching. Inspired by the Google Maps Street View, the author proposes applying the Image-Based Rendering (IBR), a branch of Virtual Reality (VR), to the field of geology field trip teaching. In this paper, the author designs and implements a panoramic geology field trip system for College of Earth Sciences of Jilin University, in which students can preview the geology field trip location by roaming in the three-dimension (3D) virtual environment. The practice shows that not only can the system help teach 3D thinking, but also reduce the teaching cost, improve students' learning efficiency and enhance learning interest.",Image-Based Rendering;Geology Field Trip;Virtual Reality,Abstract_Keywords,True,
IEEE,conferencePaper,2015,"A Mobile Augmented Reality Service Model Based on Street Data, and Its Implementation",COMPSAC - International Computer Software and Applications Conference,B,"The popularity of smart devices and Location Based Services (LBSes) is increasing in part due to users demand for personalized information associated with their location. These services provide intuitive and realistic information by adopting Augmented Reality(AR) technology. This technology utilizes various sensors embedded in the mobile devices. However, these services have inherent problems due to the devices small screen size and the complexity of the real world environment, overlapping content on a small screen and placing icons without considering the user's possible movement. In order to solve these problems, this paper proposes a Mobile Augmented Reality Model with the application of Street Data. The model consists of two layers ""Real-world layer"" and ""Information layer"". In the model, a user creates a query by scanning the nearby street with a camera in real space and searches accessible content along the street through the use of the information space. Furthermore, the results are placed on both sides of the street to solve the issue of Overlapping. Also, the proposed model is implemented for region ""Aenigol"", and the efficiency and usefulness of the model are verified.",Mobile Augmented Reality;Location Based Services;Street Data;Numerical Map,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2014,Performance Evaluation of Reconfigurable TCP Communication on Wi-Fi Network Using GE Channel Model,COMPSAC - International Computer Software and Applications Conference,B,"Recently, many commercial cloud-computing services have rapidly evolved to process and back up large volumes of data. With wireless broadband networks also gaining popularity, users increasingly access cloud services using mobile devices and wearable devices for interactive communication, such as remote desktops and augmented reality (AR) applications. In many cases, interactive applications use the TCP protocol between a server and a mobile device. However, end-to-end TCP sessions are easily influenced by packet losses in wireless networks and long delays on international lines, which may degrade communication quality. To cope with these problems, we propose a dynamic TCP communication method that improves the response for interactive communication by dynamically changing congestion control algorithms in accordance with changes in network quality and application characteristics. In this paper, we implement the proposed method on the NS-3 network simulator and evaluate it by using the GE (Gilbert-Eliott) channel model, assuming that the method is applied to a Wi-Fi network. We find that our method improves response time for interactive communication with a remote desktop to the level where users will not feel uncomfortable.",TCP;Cloud;Mobile;Optimization;GE model,Abstract,True,
IEEE,conferencePaper,2014,Operation Support System Based on Augmented Reality Technology,COMPSAC - International Computer Software and Applications Conference,B,"In this paper, we propose a novel operation support system that utilizes the latest Augmented Reality (AR) technology. Our system consists of a tablet device equipped with a camera. The system automatically recognizes an appliance by reading a QR code captured by the camera and estimates the relative position and orientation between the appliance and the camera in real-time to augment the guidance information to the appliance. Experiments show that our system can stably track camera movement and augment guidance information.",Operation support;Augmented reality;vSLAM;Camera tracking,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2013,A Software Architecture for High-Level Development of Component-Based Distributed Virtual Reality Systems,COMPSAC - International Computer Software and Applications Conference,B,"Distributed virtual reality systems (DVR systems) have evolved significantly over the past twenty years and found wide spread in many applications for training, educational and entertainment purposes. Modern DVR systems require sophisticated data exchange mechanisms to provide consistent interaction of a large number of users over the Internet. While many of these mechanisms have been well studied, usually they represent isolated solutions requiring knowledge of low-level networking programming for implementation. In this regard, there is still a lack of universal, easily deployable and extensible software architecture that enables rapid creation of complete systems from scratch. In this paper we present architecture of a middleware allowing an application developer to easily implement and deploy custom DVR systems for specific tasks without direct low-level network programming.",software architecture design;middleware;high-level software development;virtual reality;distributed simulation,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2023,Toward a Secure Educational Metaverse: A Tale of Blockchain Design for Educational Environments,SEAA - Euromicro Conference on Software Engineering and Advanced Applications,B,"In the era of social distancing, distance learning represents a crucial educational challenge. Several 2D information technologies have been provided, yet these share multiple limitations and have negative social, educational, and psychological implications for learners. Metaverse promises to revolutionize education as we know it: this is a persistent, virtual, three-dimensional environment that is supposed to address most of the limitations of 2D information technologies. Nonetheless, there are still software engineering challenges to face to enable such a metaverse, especially when turning to software security and privacy. In this paper, we aim at performing the first steps toward an improved understanding of the security perspective of educational metaverse, by analyzing how blockchain can be employed within educational environments and how applications may be designed. Our ultimate goal is to provide insights into how blockchain can be further tailored in the context of educational metaverse. We conduct a systematic literature review, which targets 20 primary studies. The key findings of the study showcase the use of blockchain in 3 educational tasks, other than describing the blockchain design approaches, which protocol they commonly use and the associated limitations. We conclude by developing a conceptualization of a blockchain-based educational metaverse.",Bibliographies; Blockchain; Blockchains; Education; Educational Metaverse; Metaverse; Security; Software; Systematic Literature Review; Systematics; Turning,Title_Abstract_Keywords,True,
ACM DL,conferencePaper,2019,Exploring Virtual Reality as an Integrated Development Environment for Cyber-Physical Systems,SEAA - Euromicro Conference on Software Engineering and Advanced Applications,B,"Cyber Physical Systems (CPS) development approaches tend to start from the physical (hardware) perspective, and the software is the final element in the process. However, this approach is unfit for the more software-intensive world that is increasingly iterative, connected, and constantly online. Many constraints prevent the application of iterative, incremental, and agile development methodologies, which now are the norm for many other fields of software. Time-consuming system validation can only start when both hardware and software components are ready, which implies that the software delivery and quality is almost always the final bottleneck in the CPS development and integration. Also organizational issues raise concerns - CPS development teams are nowadays often geographically distributed, which can result in delays in the process, shortcomings, and even mistakes. In this paper, we propose using our envisioned open-source Virtual Reality-based Integrated software Development Environment (VRIDE) for developing the next generation, increasingly software-intensive CPSs in efficient ways.",Simulation; Digital Twin; Digital twin; Collaboration; Tools; Virtual environments; Solid modeling; Software; Virtualization; Embedded Systems; Agile Software Development; Production facilities; Cyber-Physical Systems (CPS); Integrated Development Environment (IDE); Reality (VR); Virtual Twin,Title_Abstract,True,
ACM DL,conferencePaper,2014,A Mobile Computing Framework Based on Adaptive Mobile Code Offloading,SEAA - Euromicro Conference on Software Engineering and Advanced Applications,B,"Smart phones are not capable of competing against their desktop counterparts or servers in terms of CPU speed, battery, memory and storage. However, a mobile device can transparently use cloud resources by employing an offloading mechanism. Offloading enables mobile devices to run computation intensive applications such as object recognition, Optical Character Recognition (OCR) and augmented reality. In this paper, an Inversion of Control (IoC) based offloading technique is proposed in order to overcome shortcomings and limitations of current approaches in the literature. A sample application has been implemented by using the proposed technique. The results show that the proposed offloading technique leads to energy savings of 66% to 81% and execution time savings by 76% to 81% with a small computational overhead.",Mobile Computing; Servers; Libraries; Smart phones; Optical character recognition software; Offloading; Cloud Computing; Mobile communication; Production facilities; Resource Intensive Algorithms,Abstract,True,
Scopus,conferencePaper,2020,"Proceedings - 2020 IEEE 31st International Symposium on Software Reliability Engineering, ISSRE 2020",ISSRE - International Symposium on Software Reliability Engineering,A,The proceedings contain 38 papers. The topics discussed include: investigating the configurations of an industrial path planner in terms of collision avoidance; an empirical study of thresholds for code measures; understanding merge conflicts and resolutions in git rebases; identifying and prioritizing chaos experiments by using established risk analysis techniques; an exploratory study of bugs in extended reality applications on the web; correlating UI contexts with sensitive API calls: dynamic semantic extraction and analysis; deep learning based valid bug reports determination and explanation; and QoS-aware metamorphic testing: an elevation case study.,,Abstract,True,
Scopus,conferencePaper,2020,An exploratory study of bugs in extended reality applications on the web,ISSRE - International Symposium on Software Reliability Engineering,A,"Extended Reality (XR) technologies are becoming increasingly popular in recent years. To help developers deploy XR applications on the Web, W3C released the WebXR Device API in 2019, which enable users to interact with browsers using XR devices. Given the convenience brought byWebXR, a growing number of WebXR projects have been deployed in practice. However, many WebXR applications are insufficiently tested before being released. They suffer from various bugs that can degrade user experience or cause undesirable consequences. Yet, the community has limited understanding towards the bugs in the WebXR ecosystem, which impedes the advance of techniques for assuring the reliability of WebXR applications. To bridge this gap, we conducted the first empirical study of WebXR bugs. We collected 368 real bugs from 33 WebXR projects hosted on GitHub. Via a seven-round manual analysis of these bugs, we built a taxonomy of WebXR bugs according to their symptoms and root causes. Furthermore, to understand the uniqueness of WebXR bugs, we compared them with bugs in conventional JavaScript programs and web applications. We believe that our findings can inspire future researches on relevant topics and we released our bug dataset to facilitate follow-up studies. © 2020 IEEE Computer Society. All rights reserved.",Bug taxonomy; Empirical study; WebXR,Title_Abstract,True,
Scopus,conferencePaper,2018,Safe-AR: Reducing Risk while Augmenting Reality,ISSRE - International Symposium on Software Reliability Engineering,A,"Augmented reality (AR) systems excel at offering users real-time, situation-aware information to support users' decision making. With AR, rich visualizations of relevant data can be displayed to users without blocking their view of the real world. For example, an AR-enabled automotive windshield can display a red outline around a pedestrian to alert a driver starting a turn into that cross street. Other critical uses of AR applications that are or will soon be deployed include surgery, emergency response, vehicle maintenance, and pilot training. Many of these applications can enhance operational safety. However, developing risk analysis methods to handle failure modes in the melded virtual and physical realities remains an open problem. This paper proposes a risk analysis method with which to study computer-generated AR visualizations of system and environment states. The analysis framework incorporates three levels at which AR interfaces with the user: perception, comprehension, and decision-making. This method enables broader risk analysis of the entire cyber-physical-human system that an AR application may indirectly control. Preliminary results show that this method yields improved coverage of user-involved failure modes over current approaches. While the focus here is on safety, the method also appears applicable to AR security risks. © 2018 IEEE.",,Abstract,True,
Scopus,conferencePaper,2014,Deadlock and temporal properties analysis in mixed reality applications,ISSRE - International Symposium on Software Reliability Engineering,A,"Mixed reality systems overlay real data with virtual information in order to assist users in their current task, they are used in many fields (surgery, maintenance, entertainment). Such systems generally combine several hardware components operating at different time scales, and software that has to cope with these timing constraints. MIRELA, for Mixed Reality Language, is a framework aimed at modelling, analysing and implementing systems composed of sensors, processing units, shared memories and rendering loops, communicating in a well-defined manner and submitted to timing constraints. The paper describes how harmful software behaviour, which may result in possible hardware deterioration or revert the system's primary goal from user assistance to user impediment, may be detected such as (global and local) deadlocks or starvation features. This also includes a study of temporal properties resulting in a finer understanding of the software timing behaviour, in order to fix it if needed. © 2014 IEEE.",deadlocks; Mixed reality; temporal properties; timed automata,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2017,An Empirical Study of Open Source Virtual Reality Software Projects,ESEM - International Symposium on Empirical Software Engineering and Measurement,A,"In this paper, we present an empirical study of 1,156 open source virtual reality (VR) projects from Unity List. Our study shows that the number of open source VR software projects are steadily growing, and some large projects attracting many developers are emerging. The most popular topic of VR software is still games. We also found that VR developers often face miss-commit of automatically generated files.",Virtual Reality; Virtual reality; Empirical Study; Software Engineering; Three-dimensional displays; Games; Google; Open source software; Engines; Software Repositories,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2024,Immersive Software Archaeology: Exploring Software Architecture and Design in Virtual Reality,"SANER - IEEE International Conference on Software Analysis, Evolution and Reengineering",A,"Comprehending large-scale software systems is a challenging and daunting task, particularly when only source code is available. While software visualization attempts to aid that process, existing tools primarily visualize a system's structure in terms of files, folders, packages, or namespaces, neglecting its logical decomposition into cohesive architectural components. We present the tool Immersive Software Archaeology (ISA) which (i) estimates a view of a system's architecture by utilizing concepts from software architecture recovery and (ii) visualizes the results in virtual reality (VR) so that users can explore a subject system interactively, making the process more engaging. In VR, a semantic zoom lets users gradually transition between architectural components of different granularity and class-level elements while relationship graphs let users navigate along connections across classes and architectural components. We present results from a controlled experiment with 54 partic-ipants to investigate the usefulness of ISA for assisting engineers with exploring an unfamiliar large-scale system compared to another state-of-the-art VR approach and an IDE. Video Demonstration-https://youtu.belFl_SsT1314k",Software Visualization;Software Architecture Visualization;Software Comprehension;Software Re-Engineering,Title_Abstract,True,
IEEE,conferencePaper,2022,A Preliminary Study on Accessibility of Augmented Reality Features in Mobile Apps,"SANER - IEEE International Conference on Software Analysis, Evolution and Reengineering",A,"The capabilities of Android applications have been increasing along with the advancements of the underlying mobile devices. This has allowed developers to include features that require more resources and provide more complex functionality inside mobile apps. An example of this is the usage of Augmented Reality (AR) in mobile applications. AR allows developers to experiment with new immersive experiences for their users in a wide variety of application areas. AR content is often visual and it represents a challenge for users with visual impairments, especially if these features are core to the underlying application. In this paper, we present a preliminary study aimed at investigating accessibility of AR functionalities of mobile applications that are not specifically designed for users with visual impairments (i.e., applications of which these users could be part of the intended audience, but they are not the specific audience). To accomplish this, we conducted a case study with 49 individuals without visual impairment and 5 individuals that are visually impaired, who used 5 applications with AR features. Our results demonstrate that the analyzed apps lack accessibility mechanisms within their AR functionalities.",Accessibility;Augmented Reality;Mobile apps,Title_Abstract_Keywords,True,
IEEE,conferencePaper,2021,"TinySpline: A Small, yet Powerful Library for Interpolating, Transforming, and Querying NURBS, B-Splines, and BÃ©zier Curves","SANER - IEEE International Conference on Software Analysis, Evolution and Reengineering",A,"NURBS, B-Splines, and BÃ©zier curves have a wide range of applications. One of them is software visualization, where these kinds of splines are often used to depict relations between objects in a visually appealing manner. For example, several visualization techniques make use of hierarchical edge bundles to reduce the visual clutter that can occur when drawing a large number of edges. Another example is the visualization of software in 3D, virtual reality, and augmented reality environments. In these environments edges can be drawn as splines in 3D space to overcome the natural limitations of the two-dimensional planeâ€”e.g., the collision of edges with other objects. While BÃ©zier curves are supported quite well by most UI frameworks and game engines, NURBS and B-Splines are not. Hence, spline-based visualizations are considerably more difficult to implement without in-depth knowledge in the area of splines.In this paper we present TinySpline, a general purpose library for NURBS, B-Splines, and BÃ©zier curves that is well suited for implementing advanced edge visualization techniquesâ€”e.g., but not limited to, hierarchical edge bundles. The core of the library is written in ANSI C with a C++ wrapper for an object-oriented programming model. Based on the C++ wrapper, auto-generated bindings for C#, D, Go, Java, Lua, Octave, PHP, Python, R, and Ruby are provided, which enables TinySpline to be integrated into a large number of applications.",Software Visualization;Splines;Hierarchical Edge Bundling,Abstract,True,
IEEE,conferencePaper,2020,How EvoStreets Are Observed in Three-Dimensional and Virtual Reality Environments,"SANER - IEEE International Conference on Software Analysis, Evolution and Reengineering",A,"When analyzing software systems, a large amount of data accumulates. In order to assist developers in the preparation, evaluation, and understanding of findings, different visualization techniques have been developed. Due to recent progress in immersive virtual reality, existing visualization tools were ported to this environment. However, three-dimensional and virtual reality environments have different advantages and disadvantages, and by transferring concepts, such as layout algorithms and user interaction mechanisms, more or less one-to-one, the characteristics of these environments are neglected. In order to develop techniques adapting to the circumstance of a particular environment, more research in this field is necessary. In previously conducted case studies, we compared EvoStreets deployed in three different environments: 2D, 2.5D, and virtual reality. We found evidence that movement patternsâ€”path length, average speed, and occupied volumeâ€”differ significantly between the 2.5D and virtual reality environments for some of the tasks that had to be solved by 34 participants in a controlled experiment. In this paper, we analyze the results of this experiment in more details, to study if not only movement is affected by these environments, but also the way how EvoStreets are observed. Although we could not find enough evidence that the number of viewpoints and their duration differ significantly, we found indications that in virtual reality viewpoints are located closer to the EvoStreets and that the distance between viewpoints is shorter. Based on our previous results and the findings of this paper, we present visualization and user interaction concepts specific to the kind of environment.",,Title_Abstract,True,
IEEE,conferencePaper,2020,Unleashing the Potentials of Immersive Augmented Reality for Software Engineering,"SANER - IEEE International Conference on Software Analysis, Evolution and Reengineering",A,"In immersive augmented reality (IAR), users can wear a head-mounted display to see computer-generated images superimposed to their view of the world. IAR was shown to be beneficial across several domains, e.g., automotive, medicine, gaming and engineering, with positive impacts on, e.g., collaboration and communication. We think that IAR bears a great potential for software engineering but, as of yet, this research area has been neglected. In this vision paper, we elicit potentials and obstacles for the use of IAR in software engineering. We identify possible areas that can be supported with IAR technology by relating commonly discussed IAR improvements to typical software engineering tasks. We further demonstrate how innovative use of IAR technology may fundamentally improve typical activities of a software engineer through a comprehensive series of usage scenarios outlining practical application. Finally, we reflect on current limitations of IAR technology based on our scenarios and sketch research activities necessary to make our vision a reality. We consider this paper to be relevant to academia and industry alike in guiding the steps to innovative research and applications for IAR in software engineering.",,Title_Abstract,True,
IEEE,conferencePaper,2023,A Mixed Reality Approach for Innovative Pair Programming Education with a Conversational AI Virtual Avatar,EASE - International Conference on Evaluation and Assessment in Software Engineering,A,"Pair Programming (PP) is an Agile software development methodology that involves two developers working together on a single computer. However, the physical presence of two developers has become a challenge in recent years due to the pandemic, necessitating remote collaboration methods such as Distributed Pair Programming (DPP). DPP has been found to have similar benefits to in-person PP, but the issue of team compatibility remains unresolved. These are more evident in the educational field of Agile methodologies. To address these challenges, we developed a novel approach by creating a Mixed Reality (MR) application that enables users to learn PP with the assistance of a conversational intelligent virtual avatar. The application uses the HoloLens MR device and a Conversational Agent (CA) extension integrated into Visual Studio Code to provide suggestions for improving the code written by the user. The virtual avatar animates these suggestions, making it appear to speak and interact with the user in real time. This system aims to overcome the limitations of common DPP methods, allowing a single developer to learn and apply the PP methodology even when a human partner is unavailable.",artificial intelligence; conversational agents; extended reality; pair programming,Title_Abstract_Keywords,True,
