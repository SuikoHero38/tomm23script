Database,Item Type,Publication Year,Title,Venue,Venue Rank,Abstract,Keywords,Found In Group1,Found In Group2,Duplicate
IEEE,conferencePaper,2024,Understanding Parentsâ€™ Perceptions and Practices Toward Childrenâ€™s Security and Privacy in Virtual Reality,SP - IEEE Symposium on Security and Privacy,A*,"Recent years have seen a sharp increase in the number of underage users in virtual reality (VR), where security and privacy (S&P) risks such as data surveillance and self-disclosure in social interaction have been increasingly prominent. Prior work shows children largely rely on parents to mitigate S&P risks in their technology use. Therefore, understanding parentsâ€™ S&P knowledge, perceptions, and practices is critical for identifying the gaps for parents, technology designers, and policymakers to enhance childrenâ€™s S&P. While such empirical knowledge is substantial in other consumer technologies, it remains largely unknown in the context of VR. To address the gap, we conducted in-depth semi-structured interviews with 20 parents of children under the age of 18 who use VR at home. Our findings highlight parents generally lack S&P awareness due to the perception that VR is still in its infancy. To protect their childrenâ€™s interactions with VR, parents currently primarily rely on active strategies such as verbal education about S&P. Passive strategies such as using parental controls in VR are not commonly used among our interviewees, mainly due to their perceived technical constraints. Parents also highlight that a multi-stakeholder ecosystem must be established towards more S&P support for children in VR. Based on the findings, we propose actionable S&P recommendations for critical stakeholders, including parents, educators, VR companies, and governments.",Virtual Reality;Security and Privacy;Qualitative studies;Human-centered computing;Technology Use of Parents and Children,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2024,Eavesdropping on Controller Acoustic Emanation for Keystroke Inference Attack in Virtual Reality,NDSS - Usenix Network and Distributed System Security Symposium,A*,"Understanding the vulnerability of virtual reality (VR) is crucial for protecting sensitive data and building user trust in VR ecosystems. Previous attacks have demonstrated the feasibility of inferring VR keystrokes inside head-mounted displays (HMDs) by recording side-channel signals generated during user-HMD interactions. However, these attacks are heavily constrained by the physical layout or victim pose in the attack scenario since the recording device must be strictly positioned and oriented in a particular way with respect to the victim. In this paper, we unveil a placement-flexible keystroke inference attack in VR by eavesdropping the clicking sounds of the moving hand controller during keystrokes. The malicious recording smartphone can be placed anywhere surrounding the victim, making the attack more flexible and practical to deploy in VR environments. As the first acoustic attack in VR, our system, Heimdall, overcomes unique challenges unaddressed by previous acoustic attacks on physical keyboards and touchscreens. These challenges include differentiating sounds in a 3D space, adaptive mapping between keystroke sound and key in varying recording placement, and handling occasional hand rotations. Experiments with 30 participants show that Heimdall achieves key inference accuracy of 96.51% and top-5 accuracy of 85.14%–91.22% for inferring passwords with 4–8 characters. Heimdall is also robust under various practical impacts such as smartphone-user placement, attack environments, hardware models, and victim conditions.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2024,That Doesn’t Go There: Attacks on Shared State in Multi-User Augmented Reality Applications,USS - Usenix Security Symposium,A*,"Augmented Reality (AR) can enable shared virtual experiences between multiple users. In order to do so, it is crucial for multi-user AR applications to establish a consensus on the “shared state” of the virtual world and its augmentations through which users interact. Current methods to create and access shared state collect sensor data from devices (e.g., camera images), process them, and integrate them into the shared state. However, this process introduces new vulnerabilities and opportunities for attacks. Maliciously writing false data to “poison” the shared state is a major concern for the security of the downstream victims that depend on it. Another type of vulnerability arises when reading the shared state: by providing false inputs, an attacker can view hologram augmentations at locations they are not allowed to access. In this work, we demonstrate a series of novel attacks on multiple AR frameworks with shared states, focusing on three publicly accessible frameworks. We show that these frameworks, while using different underlying implementations, scopes, and mechanisms to read from and write to the shared state, have shared vulnerability to a unified threat model. Our evaluations of these state-of-the-art AR frameworks demonstrate reliable attacks both on updating and accessing the shared state across different systems. To defend against such threats, we discuss a number of potential mitigation strategies that can help enhance the security of multi-user AR applications and implement an initial prototype.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2024,Penetration Vision through Virtual Reality Headsets: Identifying 360-degree Videos from Head Movements,USS - Usenix Security Symposium,A*,"In this paper, we present the first contactless side-channel attack for identifying 360◦ videos being viewed in a Virtual Reality (VR) Head Mounted Display (HMD). Although the video content is displayed inside the HMD without any external exposure, we observe that user head movements are driven by the video content, which creates a unique side channel that does not exist in traditional 2D videos. By recording the user whose vision is blocked by the HMD via a malicious camera, an attacker can analyze the correlation between the user’s head movements and the victim video to infer the video title. To exploit this new vulnerability, we present INTRUDE, a system for identifying 360◦ videos from recordings of user head movements. INTRUDE is empowered by an HMD-based head movement estimation scheme to extract a head movement trace from the recording and a video saliency-based trace-fingerprint matching framework to infer the video title. Evaluation results show that INTRUDE achieves over 96% of accuracy for video identification and is robust under different recording environments. Moreover, INTRUDE maintains its effectiveness in the open-world identification scenario.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2024,"Enabling Developers, Protecting Users: Investigating Harassment and Safety in VR",USS - Usenix Security Symposium,A*,"Virtual Reality (VR) has witnessed a rising issue of harassment, prompting the integration of safety controls like muting and blocking in VR applications. However, the lack of standardized safety measures across VR applications hinders their universal effectiveness, especially across contexts like socializing, gaming, and streaming. While prior research has studied safety controls in social VR applications, our user study (n = 27) takes a multi-perspective approach, examining both users’ perceptions of safety control usability and effectiveness as well as the challenges that developers face in designing and deploying VR safety controls. We identify challenges VR users face while employing safety controls, such as finding users in crowded virtual spaces to block them. VR users also find controls ineffective in addressing harassment; for instance, they fail to eliminate the harassers’ presence from the environment. Further, VR users find the current methods of submitting evidence for reports time-consuming and cumbersome. Improvements desired by users include live moderation and behavior tracking across VR apps; however, developers cite technological, financial, and legal obstacles to implementing such solutions, often due to a lack of awareness and high development costs. We emphasize the importance of establishing technical and legal guidelines to enhance user safety in virtual environments.",,Abstract,TRUE,
Scopus,conferencePaper,2024,Can Virtual Reality Protect Users from Keystroke Inference Attacks?,USS - Usenix Security Symposium,A*,"Virtual Reality (VR) has gained popularity by providing immersive and interactive experiences without geographical limitations. It also provides a sense of personal privacy through physical separation. In this paper, we show that despite assumptions of enhanced privacy, VR is unable to shield its users from side-channel attacks that steal private information. Ironically, this vulnerability arises from VR’s greatest strength, its immersive and interactive nature. We demonstrate this by designing and implementing a new set of keystroke inference attacks in shared virtual environments, where an attacker (VR user) can recover the content typed by another VR user by observing their avatar. While the avatar displays noisy telemetry of the user’s hand motion, an intelligent attacker can use that data to recognize typed keys and reconstruct typed content, without knowing the keyboard layout or gathering labeled data. We evaluate the proposed attacks using IRB-approved user studies across multiple VR scenarios. For 13 out of 15 tested users, our attacks accurately recognize 86%-98% of typed keys, and the recovered content retains up to 98% of the meaning of the original typed content. We also discuss potential defenses.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2024,When the User Is Inside the User Interface: An Empirical Study of UI Security Properties in Augmented Reality,USS - Usenix Security Symposium,A*,"Augmented reality (AR) experiences place users inside the user interface (UI), where they can see and interact with three-dimensional virtual content. This paper explores UI security for AR platforms, for which we identify three UI security-related properties: Same Space (how does the platform handle virtual content placed at the same coordinates?), Invisibility (how does the platform handle invisible virtual content?), and Synthetic Input (how does the platform handle simulated user input?). We demonstrate the security implications of different instantiations of these properties through five proof-of-concept attacks between distrusting AR application components (i.e., a main app and an included library) — including a clickjacking attack and an object erasure attack. We then empirically investigate these UI security properties on five current AR platforms: ARCore (Google), ARKit (Apple), Hololens (Microsoft), Oculus (Meta), and WebXR (browser). We find that all platforms enable at least three of our proofof-concept attacks to succeed. We discuss potential future defenses, including applying lessons from 2D UI security and identifying new directions for AR UI security.",,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs,CHI - Human Factors in Computing Systems,A*,"The progression to “Pervasive Augmented Reality” envisions easy access to multimodal information continuously. However, in many everyday scenarios, users are occupied physically, cognitively or socially. This may increase the friction to act upon the multimodal information that users encounter in the world. To reduce such friction, future interactive interfaces should intelligently provide quick access to digital actions based on users’ context. To explore the range of possible digital actions, we conducted a diary study that required participants to capture and share the media that they intended to perform actions on (e.g., images or audio), along with their desired actions and other contextual information. Using this data, we generated a holistic design space of digital follow-up actions that could be performed in response to different types of multimodal sensory inputs. We then designed OmniActions, a pipeline powered by large language models (LLMs) that processes multimodal sensory inputs and predicts follow-up actions on the target information grounded in the derived design space. Using the empirical data collected in the diary study, we performed quantitative evaluations on three variations of LLM techniques (intent classification, in-context learning and finetuning) and identified the most effective technique for our task. Additionally, as an instantiation of the pipeline, we developed an interactive prototype and reported preliminary user feedback about how people perceive and react to the action predictions and its errors.",dataset; diary study; digital follow-up actions; large language models; pervasive augmented reality; predictive interface,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring the Opportunity of Augmented Reality (AR) in Supporting Older Adults to Explore and Learn Smartphone Applications,CHI - Human Factors in Computing Systems,A*,"The global aging trend compels older adults to navigate the evolving digital landscape, presenting a substantial challenge in mastering smartphone applications. While Augmented Reality (AR) holds promise for enhancing learning and user experience, its role in aiding older adults’ smartphone app exploration remains insufficiently explored. Therefore, we conducted a two-phase study: (1) a workshop with 18 older adults to identify app exploration challenges and potential AR interventions, and (2) tech-probe participatory design sessions with 15 participants to co-create AR support tools. Our research highlights AR’s effectiveness in reducing physical and cognitive strain among older adults during app exploration, especially during multi-app usage and the trial-and-error learning process. We also examined their interactional experiences with AR, yielding design considerations on tailoring AR tools for smartphone app exploration. Ultimately, our study unveils the prospective landscape of AR in supporting the older demographic, both presently and in future scenarios.",augmented reality; independent learning; older adults; smartphone exploration,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Toward Making Virtual Reality (VR) More Inclusive for Older Adults: Investigating Aging Effect on Target Selection and Manipulation Tasks in VR,CHI - Human Factors in Computing Systems,A*,"Recent studies show the promise of VR in improving physical, cognitive, and emotional health of older adults. However, prior work on optimizing object selection and manipulation performance in VR was mostly conducted among younger adults. It remains unclear how older adults would perform such tasks compared to younger adults and the challenges they might face. To fill in this gap, we conducted two studies with both older and younger adults to understand their performances and user experiences of object selection and manipulation in VR respectively. Based on the results, we delineated interaction difficulties that older adults exhibited in VR and identified multiple factors, such as headset-related neck fatigue, extra head movements from out-of-view interactions, and slow spatial perceptions, that significantly decreased the motor performance of older adults. We further proposed design recommendations for improving the accessibility of direct interaction experiences in VR for older adults.",Empirical study that tells us about people; Lab Study; Older Adults; Virtual/Augmented Reality,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Communication, Collaboration, and Coordination in a Co-located Shared Augmented Reality Game: Perspectives From Deaf and Hard of Hearing People",CHI - Human Factors in Computing Systems,A*,"Co-located collaborative shared augmented reality (CS-AR) environments have gained considerable research attention, mainly focusing on design, implementation, accuracy, and usability. Yet, a gap persists in our understanding regarding the accessibility and inclusivity of such environments for diverse user groups, such as deaf and Hard of Hearing (DHH) people. To investigate this domain, we used Urban Legends, a multiplayer game in a co-located CS-AR setting. We conducted a user study followed by one-on-one interviews with 17 DHH participants. Our findings revealed the usage of multimodal communication (verbal and non-verbal) before and during the game, impacting the amount of collaboration among participants and how their coordination with AR components, their surroundings, and other participants improved throughout the rounds. We utilize our data to propose design enhancements, including onscreen visuals and speech-to-text transcription, centered on participant perspectives and our analysis.",Co-located AR; Collaborative AR; Deaf and Hard of Hearing; Shared AR,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,"FetchAid: Making Parcel Lockers More Accessible to Blind and Low Vision People With Deep-learning Enhanced Touchscreen Guidance, Error-Recovery Mechanism, and AR-based Search Support",CHI - Human Factors in Computing Systems,A*,"Parcel lockers have become an increasingly prevalent last-mile delivery method. Yet, a recent study revealed its accessibility challenges to blind and low-vision people (BLV). Informed by the study, we designed FetchAid, a standalone intelligent mobile app assisting BLV in using a parcel locker in real-time by integrating computer vision and augmented reality (AR) technologies. FetchAid first uses a deep network to detect the user’s fingertip and relevant buttons on the touch screen of the parcel locker to guide the user to reveal and scan the QR code to open the target compartment door and then guide the user to reach the door safely with AR-based context-aware audio feedback. Moreover, FetchAid provides an error-recovery mechanism and real-time feedback to keep the user on track. We show that FetchAid substantially improved task accomplishment and efficiency, and reduced frustration and overall effort in a study with 12 BLV participants, regardless of their vision conditions and previous experience.",Accessibility; Assistive technology; Augmented reality; Blind and low vision; Computer vision; KuaiDiGui; Mobile devices; Object detection; Package delivery; People with vision impairments,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Barriers to Photosensitive Accessibility in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) systems have grown in popularity as an immersive modality for daily activities such as gaming, socializing, and working. However, this technology is not always accessible for people with photosensitive epilepsy (PSE) who may experience seizures or other adverse symptoms when exposed to certain light stimuli (e.g., flashes or strobes). How can VR be made more inclusive and safer for people with PSE? In this paper, we report on a series of semi-structured interviews about current perceptions of accessibility in VR among people with PSE. We identify 12 barriers to accessibility that fall into four categories: physical VR equipment, VR interfaces and content, specific VR applications, and individual differences in sensitivity. Our findings allow researchers and practitioners to better understand the meaning of photosensitive accessibility in the context of VR, and provide a step towards enabling people with PSE to enjoy the benefits offered by immersive technology.",accessibility; photosensitive epilepsy; virtual reality,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Designing Upper-Body Gesture Interaction with and for People with Spinal Muscular Atrophy in VR,CHI - Human Factors in Computing Systems,A*,"Recent research proposed gaze-assisted gestures to enhance interaction within virtual reality (VR), providing opportunities for people with motor impairments to experience VR. Compared to people with other motor impairments, those with Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing them with more design space. However, it remains unknown what gaze-assisted upper-body gestures people with SMA would want and be able to perform. We conducted an elicitation study in which 12 VR-experienced people with SMA designed upper-body gestures for 26 VR commands, and collected 312 user-defined gestures. Participants predominantly favored creating gestures with their hands. The type of tasks and participants’ abilities influence their choice of body parts for gesture design. Participants tended to enhance their body involvement and preferred gestures that required minimal physical effort, and were aesthetically pleasing. Our research will contribute to creating better gesture-based input methods for people with motor impairments to interact with VR.",people with spinal muscular atrophy; upper-body gestures; user-defined gestures; virtual reality,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,People with Disabilities Redefining Identity through Robotic and Virtual Avatars: A Case Study in Avatar Robot Cafe,CHI - Human Factors in Computing Systems,A*,"Robotic avatars and telepresence technology enable people with disabilities to engage in physical work. Despite the recent popularity of the metaverse, few studies have explored the use of virtual avatars and environments by people with disabilities. In this study, seven disabled participants working in a cafe where remote customer service is provided via robotic avatars, were engaged in the development and use of personalized virtual avatars displayed on a large screen in-situ in combination with existing physical robots, creating a hybrid cyber-physical space. We conducted longitudinal semi-structured interviews to investigate the psychological changes experienced by the participants. The results revealed that mass-produced robotic avatars allowed participants to not disclose their disability if they did not want to, but also backgrounded their identities; by contrast, customized virtual avatars shaped without physical constraints, highlighted their personalities. The combined use of robotic and virtual avatars complemented each other and can support pilots in redefining their identity.",Avatar; people with disabilities; remote collaboration; remote customer survice,Abstract,TRUE,
ACM DL,conferencePaper,2024,Examining the Use of VR as a Study Aid for University Students with ADHD,CHI - Human Factors in Computing Systems,A*,"Attention-deficit/hyperactivity disorder (ADHD) is a neurodevelopmental condition characterized by patterns of inattention and impulsivity, which lead to difficulties maintaining concentration and motivation while completing academic tasks. University settings, characterized by a high student-to-staff ratio, make treatments relying on human monitoring challenging. One potential replacement is Virtual Reality (VR) technology, which has shown potential to enhance learning outcomes and promote flow experience. In this study, we investigate the usage of VR with 27 university students with ADHD in an effort to improve their performance in completing homework, including an exploration of automated feedback via a technology probe. Quantitative results show significant increases in concentration, motivation, and effort levels during these VR sessions and qualitative data offers insight into considerations like comfort and deployment. Together, the results suggest that VR can be a valuable tool in leveling the playing field for university students with ADHD.",,Abstract,TRUE,
ACM DL,conferencePaper,2024,From Letterboards to Holograms: Advancing Assistive Technology for Nonspeaking Autistic Individuals with the HoloBoard,CHI - Human Factors in Computing Systems,A*,"About one-third of autistic individuals are nonspeaking, i.e., they cannot use speech to convey their thoughts reliably. Many in this population communicate via spelling, a process in which they point to letters on a letterboard held upright in their field of view by a trained Communication and Regulation Partner (CRP). This paper focuses on transitioning such individuals to more independent, digital spelling that requires less support from the CRP, a goal most nonspeakers we consulted with desire. To enable this transition, we followed an approach that mimics an environment familiar to the nonspeaker and that harnesses the skills they already possess from physical letterboard training. Using this approach, we developed HoloBoard, a system that allows a nonspeaker, their CRP, and others, e.g., researchers, to share a common Augmented Reality (AR) environment containing a virtual letterboard. We configured the system to offer a brief (less than 10 minutes, on average) training module with graduated spelling tasks on the virtual letterboard. In a study involving 23 participants, 16 completed the entire module. These participants were able to spell words on the virtual letterboard without the CRP holding that board, an outcome we had not expected. When offered the opportunity to continue interacting with the virtual letterboard after the training module, 14 performed more complicated tasks than we had anticipated, spelling full sentences, or even offering feedback on the HoloBoard using solely the virtual board. Furthermore, five of these participants used the system solo, i.e., with the CRP and researchers absent from the virtual environment. These results suggest that training with the HoloBoard can lay the foundation for more independent communication, providing new social and educational opportunities for this marginalized population.",accessibility; assistive technology; Cross-reality; extended reality; nonspeaking autistic people,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Augmented Reality Cues Facilitate Task Resumption after Interruptions in Computer-Based and Physical Tasks,CHI - Human Factors in Computing Systems,A*,"Many work domains include numerous interruptions, which can contribute to errors. We investigated the potential of augmented reality (AR) cues to facilitate primary task resumption after interruptions of varying lengths. Experiment 1 (N&nbsp;=&nbsp;83) involved a computer-based primary task with a red AR arrow at the to-be-resumed task step which was placed via a gesture by the participants or automatically. Compared to no cue, both cues significantly reduced the resumption lag (i.e., the time between the end of the interruption and the resumption of the primary task) following long but not short interruptions. Experiment 2 (N&nbsp;=&nbsp;38) involved a tangible sorting task, utilizing only the automatic cue. The AR cue facilitated task resumption compared to not cue after both short and long interruptions. We demonstrated the potential of AR cues in mitigating the negative effects of interruptions and make suggestions for integrating AR technologies for task resumption.",Augmented Reality; Human Error; Interruption; Resumption Lag; Task Resumption,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Augmented Reality at Zoo Exhibits: A Design Framework for Enhancing the Zoo Experience,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) offers unique opportunities for contributing to zoos’ objectives of public engagement and education about animal and conservation issues. However, the diversity of animal exhibits pose challenges in designing AR applications that are not encountered in more controlled environments, such as museums. To support the design of AR applications that meaningfully engage the public with zoo objectives, we first conducted two scoping reviews to interrogate previous work on AR and broader technology use at zoos. We then conducted a workshop with zoo representatives to understand the challenges and opportunities in using AR to achieve zoo objectives. Additionally, we conducted a field trip to a public zoo to identify exhibit characteristics that impacts AR application design. We synthesise the findings from these studies into a framework that enables the design of diverse AR experiences. We illustrate the utility of the framework by presenting two concepts for feasible AR applications.",augmented reality; design framework; ethnography; field study,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Comfortable Mobility vs. Attractive Scenery: The Key to Augmenting Narrative Worlds in Outdoor Locative Augmented Reality Storytelling,CHI - Human Factors in Computing Systems,A*,"We investigate how path context, encompassing both comfort and attractiveness, shapes user experiences in outdoor locative storytelling using Augmented Reality (AR). Addressing a research gap that predominantly concentrates on indoor settings or narrative backdrops, our user-focused research delves into the interplay between perceived path context and locative AR storytelling on routes with diverse walkability levels. We examine the correlation and causation between narrative engagement, spatial presence, perceived workload, and perceived path context. Our findings show that on paths with reasonable path walkability, attractive elements positively influence the narrative experience. However, even in environments with assured narrative walkability, inappropriate safety elements can divert user attention to mobility, hindering the integration of real-world features into the narrative. These results carry significant implications for path creation in outdoor locative AR storytelling, underscoring the importance of ensuring comfort and maintaining a balance between comfort and attractiveness to enrich the outdoor AR storytelling experience.",Augmented Reality; Locative Storytelling; Mixed Reality; Walkability,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Investigating the Design of Augmented Narrative Spaces Through Virtual-Real Connections: A Systematic Literature Review,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) is regarded as an innovative storytelling medium that presents novel experiences by layering a virtual narrative space over a real 3D space. However, understanding of how the virtual narrative space and the real space are connected with one another in the design of augmented narrative spaces has been limited. For this, we conducted a systematic literature review of 64 articles featuring AR storytelling applications and systems in HCI, AR, and MR research. We investigated how virtual narrative spaces have been paired, functionalized, placed, and registered in relation to the real spaces they target. Based on these connections, we identified eight dominant types of augmented narrative spaces that are primarily categorized by whether they virtually narrativize reality or realize the virtual narrative. We discuss our findings to propose design recommendations on how virtual-real connections can be incorporated into a more structured approach to AR storytelling.",augmented narrative space; Augmented Reality; Mixed Reality; storytelling,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Jigsaw: Authoring Immersive Storytelling Experiences with Augmented Reality and Internet of Things,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) presents new opportunities for immersive storytelling. However, this immersiveness faces two main hurdles. First, AR’s immersive quality is often confined to visual elements, such as pixels on a screen. Second, crafting immersive narratives is complex and generally beyond the reach of amateurs due to the need for advanced technical skills. We introduce Jigsaw, a system that empowers beginners to both experience and craft immersive stories, blending virtual and physical elements. Jigsaw uniquely combines mobile AR with readily available Internet-of-things (IoT) devices. We conducted a qualitative study with 20 participants to assess Jigsaw’s effectiveness in both consuming and creating immersive narratives. The results were promising: participants not only successfully created their own immersive stories but also found the playback of three such stories deeply engaging. However, sensory overload emerged as a significant challenge in these experiences. We discuss design trade-offs and considerations for future endeavors in immersive storytelling involving AR and IoT.",augmented reality; authoring tool; internet-of-things; storytelling,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring the Impact of Interconnected External Interfaces in Autonomous Vehicles on Pedestrian Safety and Experience,CHI - Human Factors in Computing Systems,A*,"Policymakers advocate for the use of external Human-Machine Interfaces (eHMIs) to allow autonomous vehicles (AVs) to communicate their intentions or status. Nonetheless, scalability concerns in complex traffic scenarios arise, such as potentially increasing pedestrian cognitive load or conveying contradictory signals. Building upon precursory works, our study explores ‘interconnected eHMIs,’ where multiple AV interfaces are interconnected to provide pedestrians with clear and unified information. In a virtual reality study (N=32), we assessed the effectiveness of this concept in improving pedestrian safety and their crossing experience. We compared these results against two conditions: no eHMIs and unconnected eHMIs. Results indicated interconnected eHMIs enhanced safety feelings and encouraged cautious crossings. However, certain design elements, such as the use of the colour red, led to confusion and discomfort. Prior knowledge slightly influenced perceptions of interconnected eHMIs, underscoring the need for refined user education. We conclude with practical implications and future eHMI design research directions.",autonomous vehicles; eHMIs; external communication; scalability; vehicle-pedestrian interaction; vulnerable road users,Abstract,TRUE,
ACM DL,conferencePaper,2024,"Process, Roles, Tools, and Team: Understanding the Emerging Medium of Virtual Reality Theatre",CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) theatre artists are combining theatre production and game development practices to create live performances in VR. To date, little is known about VR theatre creators’ experiences of this process or how staging a play in VR might affect the audience’s experience. To capture the experience of developing a VR theatre production we interviewed the production team behind the VR play You Should Have Stayed Home. Members of this team felt the process was a learning experience and shared the lessons they plan to incorporate into their future work. We report on the team’s efforts to understand the VR theatre medium, how this team was constructed, and challenges that they encountered. In this paper we present the opportunities that the production team members identified for creating novel experiences for VR audiences, and their own needs as creators.",Design Process; Drama; Intermedial Theatre; Virtual Reality Theatre,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment,CHI - Human Factors in Computing Systems,A*,"Virtual co-embodiment enables two users to share a single avatar in Virtual Reality (VR). During such experiences, the illusion of shared motion control can break during joint-action activities, highlighting the need for position-aware feedback mechanisms. Drawing on the perceptual crossing paradigm, we explore how haptics can enable non-verbal coordination between co-embodied participants. In a within-subjects study (20 participant pairs), we examined the effects of vibrotactile haptic feedback (None, Present) and avatar control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks (Targeted, Free-choice) on participants’ Sense of Agency (SoA), co-presence, body ownership, and motion synchrony. We found (a) lower SoA in the free-choice with haptics than without, (b) higher SoA during the shared targeted task, (c) co-presence and body ownership were significantly higher in the free-choice task, (d) players’ hand motions synchronized more in the targeted task. We provide cautionary considerations when including haptic feedback mechanisms for avatar co-embodiment experiences.",avatar co-embodiment; body ownership; co-presence; haptics; perceptual crossing; sense of agency; Virtual reality,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,TimeTunnel: Integrating Spatial and Temporal Motion Editing for Character Animation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Editing character motion in Virtual Reality is challenging as it requires working with both spatial and temporal data using controls with multiple degrees-of-freedom. The spatial and temporal controls are separated, making it difficult to adjust poses over time and predict the effects across adjacent frames. To address this challenge, we propose TimeTunnel, an immersive motion editing interface that integrates spatial and temporal control for 3D character animation in VR. TimeTunnel provides an approachable editing experience via KeyPoses and Trajectories. KeyPoses are a set of representative poses automatically computed to concisely depict motion. Trajectories are 3D animation curves that pass through the joints of KeyPoses to represent in-betweens. TimeTunnel integrates spatial and temporal control by superimposing Trajectories and KeyPoses onto a 3D character. We conducted two studies to evaluate TimeTunnel. In our quantitative study, TimeTunnel reduced the amount of time required for editing motion, and saved effort in locating target poses. Our qualitative study with domain experts demonstrated how TimeTunnel is an approachable interface that can simplify motion editing, while still preserving a direct representation of motion.",3D interface; immersive animation authoring; keypose; motion editing; motion path,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,"""I Shot the Interviewer!"": The Effects of In-VR Interviews on Participant Feedback and Rapport",CHI - Human Factors in Computing Systems,A*,"The integration of questionnaires into virtual reality experiences has recently been proposed as a way to reduce the potential biases introduced through the negative effects of leaving VR, however there has been little attention paid to how qualitative interviews could similarly be integrated into the virtual world for the purposes of user evaluation. In this paper we explore how conducting interviews within the virtual environment may affect the outcome of the evaluation and the relationship between participant and interviewer, and how this may differ with and without visual representation of the interviewer through use of an avatar. We conclude that in-VR interviews are a valid and promising method of data collection for user evaluation with similar data quality to in-person interviews, but that the interviewer should have a visual presence in the environment to maintain their relationship with the participant and the perceived realism of the environment.",Interview; Qualitative Methods; Virtual/Augmented Reality,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Stretch your reach: Studying Self-Avatar and Controller Misalignment in Virtual Reality Interaction,CHI - Human Factors in Computing Systems,A*,"Immersive Virtual Reality typically requires a head-mounted display (HMD) to visualize the environment and hand-held controllers to interact with the virtual objects. Recently, many applications display full-body avatars to represent the user and animate the arms to follow the controllers. Embodiment is higher when the self-avatar movements align correctly with the user. However, having a full-body self-avatar following the user’s movements can be challenging due to the disparities between the virtual body and the user’s body. This can lead to misalignments in the hand position that can be noticeable when interacting with virtual objects. In this work, we propose five different interaction modes to allow the user to interact with virtual objects despite the self-avatar and controller misalignment and study their influence on embodiment, proprioception, preference, and task performance. We modify aspects such as whether the virtual controllers are rendered, whether controllers are rendered in their real physical location or attached to the user’s hand, and whether stretching the avatar arms to always reach the real controllers. We evaluate the interaction modes both quantitatively (performance metrics) and qualitatively (embodiment, proprioception, and user preference questionnaires). Our results show that the stretching arms solution, which provides body continuity and guarantees that the virtual hands or controllers are in the correct location, offers the best results in embodiment, user preference, proprioception, and performance. Also, rendering the controller does not have an effect on either embodiment or user preference.",3D interaction; avatars; embodiment; perception; virtual reality,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Virtual Body Swapping: A VR-Based Approach to Embodied Third-Person Self-Processing in Mind-Body Therapy,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers various opportunities for innovative therapeutic approaches, especially regarding self-related mind-body interventions. We introduce a VR body swap system enabling multiple users to swap their perspectives and appearances and evaluate its effects on virtual sense of embodiment (SoE) and perception- and cognition-based self-related processes. In a self-compassion-framed scenario, twenty participants embodied their personalized, photorealistic avatar, swapped bodies with an unfamiliar peer, and reported their SoE, interoceptive awareness (perception), and self-compassion (cognition). Participants’ experiences differed between bottom-up and top-down processes. Regarding SoE, their agency and self-location shifted to the swap avatar, while their top-down self-identification remained with their personalized avatar. Further, the experience positively affected interoceptive awareness but not self-compassion. Our outcomes offer novel insights into the SoE in a multiple-embodiment scenario and highlight the need to differentiate between the different processes in intervention design. They raise concerns and requirements for future research on avatar-based mind-body interventions.",body awareness; body swap; embodiment; perspective taking.; self-compassion; Virtual reality,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"""Pikachu would electrocute people who are misbehaving"": Expert, Guardian and Child Perspectives on Automated Embodied Moderators for Safeguarding Children in Social Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Automated embodied moderation has the potential to create safer spaces for children in social VR, providing a protective figure that takes action to mitigate harmful interactions. However, little is known about how such moderation should be employed in practice. Through interviews with 16 experts in online child safety and psychology, and workshops with 8 guardians and 13 children, we contribute a comprehensive overview of how Automated Embodied Moderators (AEMs) can safeguard children in social VR. We explore perceived concerns, benefits and preferences across the stakeholder groups and gather first-of-their-kind recommendations and reflections around AEM design. The results stress the need to adapt AEMs to children, whether victims or harassers, based on age and development, emphasising empowerment, psychological impact and humans/guardians-in-the-loop. Our work provokes new participatory design-led directions to consider in the development of AEMs for children in social VR taking child, guardian, and expert insights into account.",child online safety; children; design workshops; experts; grandparent; guardian; interviews; metaverse; parent; social virtual reality,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,LegacySphere: Facilitating Intergenerational Communication Through Perspective-Taking and Storytelling in Embodied VR,CHI - Human Factors in Computing Systems,A*,"Intergenerational communication can enhance well-being and family cohesion, but stereotypes and low empathy can be barriers to achieving effective communication. VR perspective-taking is a potential approach that is known to enhance understanding and empathy toward others by allowing a user to take another’s viewpoint. In this study, we introduce LegacySphere, a novel VR perspective-taking experience leveraging the combination of embodiment, role-play, and storytelling. To explore LegacySphere’s design and impact, we conducted an observational study involving five dyads with a one-generation gap. We found that LegacySphere promotes empathetic and reflexive intergenerational dialogue. Specifically, avatar embodiment encourages what we term “relationship cushioning,” fostering a trustful, open environment for genuine communications. The blending of real and embodied identities prompts insightful questions, merging both perspectives. The experience also nurtures a sense of unity and stimulates reflections on aging. Our work highlights the potential of immersive technologies for enhancing empathetic intergenerational relationships.",Empathy; Intergenerational communication; Perspective-taking; Proteus effect; Role-play; Storytelling; Virtual Reality,Keywords,TRUE,
ACM DL,conferencePaper,2024,An Artists' Perspectives on Natural Interactions for Virtual Reality 3D Sketching,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) applications like OpenBrush offer artists access to 3D sketching tools within the digital 3D virtual space. These 3D sketching tools allow users to “paint” using virtual digital strokes that emulate real-world mark-making. Yet, users paint these strokes through (unimodal) VR controllers. Given that sketching in VR is a relatively nascent field, this paper investigates ways to expand our understanding of sketching in virtual space, taking full advantage of what an immersive digital canvas offers. Through a study conducted with the participation of artists, we identify potential methods for natural multimodal and unimodal interaction techniques in 3D sketching. These methods demonstrate ways to incrementally improve existing interaction techniques and incorporate artistic feedback into the design.",3D Sketching; Gestures; Multimodal Interaction; Speech; Virtual Reality,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,EyeGuide &amp; EyeConGuide: Gaze-based Visual Guides to Improve 3D Sketching Systems,CHI - Human Factors in Computing Systems,A*,"Visual guides help to align strokes and raise accuracy in Virtual Reality (VR) sketching tools. Automatic guides that appear at relevant sketching areas are convenient to have for a seamless sketching with a guide. We explore guides that exploit eye-tracking to render them adaptive to the user’s visual attention. EyeGuide and EyeConGuide cause visual grid fragments to appear spatially close to the user’s intended sketches, based on the information of the user’s eye-gaze direction and the 3D position of the hand. Here we evaluated the techniques in two user studies across simple and complex sketching objectives in VR. The results show that gaze-based guides have a positive effect on sketching accuracy, perceived usability and preference over manual activation in the tested tasks. Our research contributes to integrating gaze-contingent techniques for assistive guides and presents important insights into multimodal design applications in VR.",3D Sketching; 3D User Interface; Eye-Gaze; VR,Abstract,TRUE,
ACM DL,conferencePaper,2024,Choosing the Right Reality: A Comparative Analysis of Tangibility in Immersive Trauma Simulations,CHI - Human Factors in Computing Systems,A*,"In the field of medical first responder training, the choice of training modality is crucial for skill retention and real-world application. This study introduces the Green Manikin, an advanced Mixed Reality (MR) tool, conceptually combining the immersiveness of Virtual Reality (VR) with the tangibility of real-world training, and compares it against traditional real-world simulations and VR training. Our findings indicate that MR and real-world settings excel in Self and Social Presence, and in intention to use, offering heightened psychological presence suitable for complex training scenarios. Effort expectancy was highest in real-world environments, suggesting their ease of use for basic skill acquisition. This nuanced understanding allows for better tailoring of training modalities to specific educational objectives. Our research validates the utility of MR and offers a framework for selecting the most effective training environment for different learning outcomes in medical first responder training.",chroma-key; comparative study; medical first responder; mixed reality; modality; training,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Comparison of Spatial Visualization Techniques for Radiation in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) provides a safe and low-cost option for hazardous safety training that allows for the visualization of aspects that may be invisible, such as radiation. Effectively visually communicating such threats in the environment around the user is not straightforward. This work describes visually encoding radiation using the spatial awareness mesh of an AR Head Mounted Display. We leverage the AR device’s GPUs to develop a real time solution that accumulates multiple dynamic sources and uses stencils to prevent an environment being over saturated with a visualization, as well as supporting the encoding of direction explicitly in the visualization. We perform a user study (25 participants) of different visualizations and obtain user feedback. Results show that there are complex interactions and while no visual representation was statistically superior or inferior, user opinions vary widely. We also discuss the evaluation approaches and provide recommendations.",Augmented Reality; CBRN Response Training; Spatial Awareness; Visualization,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring the Design Space of Optical See-through AR Head-Mounted Displays to Support First Responders in the Field,CHI - Human Factors in Computing Systems,A*,"First responders (FRs) navigate hazardous, unfamiliar environments in the field (e.g., mass-casualty incidents), making life-changing decisions in a split second. AR head-mounted displays (HMDs) have shown promise in supporting them due to its capability of recognizing and augmenting the challenging environments in a hands-free manner. However, the design space have not been thoroughly explored by involving various FRs who serve different roles (e.g., firefighters, law enforcement) but collaborate closely in the field. We interviewed 26 first responders in the field who experienced a state-of-the-art optical-see-through AR HMD, as well as its interaction techniques and four types of AR cues (i.e., overview cues, directional cues, highlighting cues, and labeling cues), soliciting their first-hand experiences, design ideas, and concerns. Our study revealed both generic and role-specific preferences and needs for AR hardware, interactions, and feedback, as well as identifying desired AR designs tailored to urgent, risky scenarios (e.g., affordance augmentation to facilitate fast and safe action). While acknowledging the value of AR HMDs, concerns were also raised around trust, privacy, and proper integration with other equipment. Finally, we derived comprehensive and actionable design guidelines to inform future AR systems for in-field FRs.",Augmented Reality; Design; First Responders; In-the-field Tasks,Keywords,TRUE,
ACM DL,conferencePaper,2024,VisTorch: Interacting with Situated Visualizations using Handheld Projectors,CHI - Human Factors in Computing Systems,A*,"Spatial data is best analyzed in situ, but existing mixed reality technologies can be bulky, expensive, or unsuitable for collaboration. We present VisTorch: a handheld device for projected situated analytics consisting of a pico-projector, a multi-spectrum camera, and a touch surface. VisTorch enables viewing charts situated in physical space by simply pointing the device at a surface to reveal visualizations in that location. We evaluated the approach using both a user study and an expert review. In the former, we asked 20 participants to first organize charts in space and then refer to these charts to answer questions. We observed three spatial and one temporal pattern in participant analyses. In the latter, four experts—a museum designer, a statistical software developer, a theater stage designer, and an environmental educator—utilized VisTorch to derive practical usage scenarios. Results from our study showcase the utility of situated visualizations for memory and recall.",augmented reality; immersive analytics; situated visualization.; Ubiquitous analytics,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D Spatio-Temporal Data in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Tangible interfaces in mixed reality (MR) environments allow for intuitive data interactions. Tangible cubes, with their rich interaction affordances, high maneuverability, and stable structure, are particularly well-suited for exploring multi-dimensional data types. However, the design potential of these cubes is underexplored. This study introduces a design space for tangible cubes in MR, focusing on interaction space, visualization space, sizes, and multiplicity. Using spatio-temporal data, we explored the interaction affordances of these cubes in a workshop (N=24). We identified unique interactions like rotating, tapping, and stacking, which are linked to augmented reality (AR) visualization commands. Integrating user-identified interactions, we created a design space for tangible-cube interactions and visualization. A prototype visualizing global health spending with small cubes was developed and evaluated, supporting both individual and combined cube manipulation. This research enhances our grasp of tangible interaction in MR, offering insights for future design and application in diverse data contexts.",mixed reality; spatio-temporal data; tangible interaction,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Embodied Tentacle: Mapping Design to Control of Non-Analogous Body Parts with the Human Body,CHI - Human Factors in Computing Systems,A*,"Manipulating a non-humanoid body using a mapping approach that translates human body activity into different structural movements enables users to perform tasks that are difficult with their innate bodies. However, a key challenge is how to design an effective mapping to control non-analogous body parts with the human body. To address this challenge, we designed an articulated virtual arm and investigated the effect of mapping methods on a user’s manipulation experience. Specifically, we developed an unbranched 12-joint virtual arm with an octopus-like appearance. Using this arm, we conducted a user study to compare the effects of several mapping methods with different arrangements on task performance and subjective evaluations of embodiment and user preference. As a result, we identified three important factors in mapping: “Visual and Configurational Similarity”, “Kinematics Suitability for the User”, and “Correspondence with Everyday Actions.” Based on these findings, we discuss a mapping design for non-humanoid body manipulation.",Virtual Reality; embodiment; human augmentation; gestural interaction; body schema; non-anthropomorphic avatars; non-humanoid avatars,Keywords,TRUE,
ACM DL,conferencePaper,2024,Grand challenges in WaterHCI,CHI - Human Factors in Computing Systems,A*,"Recent combinations of interactive technology, humans, and water have resulted in “WaterHCI”. WaterHCI design seeks to complement the many benefits of engagement with the aquatic domain, by offering, for example, augmented reality systems for snorkelers, virtual reality in floatation tanks, underwater musical instruments for artists, robotic systems for divers, and wearables for swimmers. We conducted a workshop in which WaterHCI experts articulated the field's grand challenges, aiming to contribute towards a systematic WaterHCI research agenda and ultimately advance the field.",grand challenges; fluid user interfaces; human-water interaction,Abstract,TRUE,
ACM DL,conferencePaper,2024,AdapTics: A Toolkit for Creative Design and Integration of Real-Time Adaptive Mid-Air Ultrasound Tactons,CHI - Human Factors in Computing Systems,A*,"Mid-air ultrasound haptic technology can enhance user interaction and immersion in extended reality (XR) applications through contactless touch feedback. Yet, existing design tools for mid-air haptics primarily support creating tactile sensations (i.e., tactons) which cannot change at runtime. These tactons lack expressiveness in interactive scenarios where a continuous closed-loop response to user movement or environmental states is desirable. This paper introduces AdapTics, a toolkit featuring a graphical interface for rapid prototyping of adaptive tactons—dynamic sensations that can adjust at runtime based on user interactions, environmental changes, or other inputs. A software library and a Unity package accompany the graphical interface to enable integration of adaptive tactons in existing applications. We present the design space offered by AdapTics for creating adaptive mid-air ultrasound tactons and show the design tool can improve Creativity Support Index ratings for Exploration and Expressiveness in a user study with 12 XR and haptic designers.",design tool; haptic design; mid-air ultrasound haptics; real-time adaptation; tacton,Abstract,TRUE,
ACM DL,conferencePaper,2024,To Reach the Unreachable: Exploring the Potential of VR Hand Redirection for Upper Limb Rehabilitation,CHI - Human Factors in Computing Systems,A*,"Rehabilitation therapies are widely employed to assist people with motor impairments in regaining control over their affected body parts. Nevertheless, factors such as fatigue and low self-efficacy can hinder patient compliance during extensive rehabilitation processes. Utilizing hand redirection in virtual reality (VR) enables patients to accomplish seemingly more challenging tasks, thereby bolstering their motivation and confidence. While previous research has investigated user experience and hand redirection among able-bodied people, its effects on motor-impaired people remain unexplored. In this paper, we present a VR rehabilitation application that harnesses hand redirection. Through a user study and semi-structured interviews, we examine the impact of hand redirection on the rehabilitation experiences of people with motor impairments and its potential to enhance their motivation for upper limb rehabilitation. Our findings suggest that patients are not sensitive to hand movement inconsistency, and the majority express interest in incorporating hand redirection into future long-term VR rehabilitation programs.",Motor impairments; Upper limb rehabilitation; Virtual hand redirection,Abstract,TRUE,
ACM DL,conferencePaper,2024,Can You Hazard a Guess?: Evaluating the Effect of Augmented Reality Cues on Driver Hazard Prediction,CHI - Human Factors in Computing Systems,A*,"Semi-autonomous vehicles allow drivers to engage with non-driving related tasks (NDRTs). However, these tasks interfere with the driver’s situational awareness, key when they need to safely retake control of the vehicle. This paper investigates if Augmented Reality (AR) could be used to present NDRTs to reduce their impact on situational awareness. Two experiments compared driver performance on a hazard prediction task whilst interacting with an NDRT, presented either as an AR Heads-Up Display or a traditional Heads-Down Display. The results demonstrate that an AR display including a novel dynamic attentional cue improves situational awareness, depending on the workload of the NDRT and design of the cue. The results provide novel insights for designers of in-car systems about how to design NDRTs to aid driver situational awareness in future vehicles.",Augmented Reality; Autonomous vehicles; In-car; Attention; Cueing; Situational Awareness; Takeover Request,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Portobello: Extending Driving Simulation from the Lab to the Road,CHI - Human Factors in Computing Systems,A*,"In automotive user interface design, testing often starts with lab-based driving simulators and migrates toward on-road studies to mitigate risks. Mixed reality (XR) helps translate virtual study designs to the real road to increase ecological validity. However, researchers rarely run the same study in both in-lab and on-road simulators due to the challenges of replicating studies in both physical and virtual worlds. To provide a common infrastructure to port in-lab study designs on-road, we built a platform-portable infrastructure, Portobello, to enable us to run twinned physical-virtual studies. As a proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32) both in-lab and on-road to investigate study design portability and platform-driven influences on study outcomes. To our knowledge, this is the first system that enables the twinning of studies originally designed for in-lab simulators to be carried out in an on-road platform.",Driving Simulations; Human-Autonomous Vehicle Interaction,Abstract,TRUE,
ACM DL,conferencePaper,2024,SYNC-VR: Synchronizing Your Senses to Conquer Motion Sickness for Enriching In-Vehicle Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Passengers can engage more in nondriving-related tasks owing to recent advancements in autonomous vehicles (AVs), making immersive tools such as virtual reality (VR) appealing; however, motion sickness (MS) remains a significant challenge. We present SYNC-VR, a system that aligns with visual, haptic, and auditory cues and provides proprioceptive feedback to illustrate its effect on MS and presence within the in-vehicle VR. We conducted an experiment with 24 participants using a real vehicle along a route with known MS-triggering events. Using subjective and physiological measures, we assessed participants’ presence and MS under four conditions by gradually varying the level of synchronized input sensations. Results reveal that SYNC-VR reduces MS and increases the sense of presence. Additionally, it emphasizes the impact of our interactive VR content and its role in achieving proprioceptive feedback with haptic feedback through electrical muscle stimulation, introducing an innovative approach to MS mitigation in in-vehicle VR.",presence; virtual reality; haptic feedback; autonomous vehicles; motion sickness; visual cues; kinesthetic feedback; sensory alignment,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,From Slow-Mo to Ludicrous Speed: Comfortably Manipulating the Perception of Linear In-Car VR Motion Through Vehicular Translational Gain and Attenuation,CHI - Human Factors in Computing Systems,A*,"To prevent motion sickness, Virtual Reality (VR) experiences for vehicle passengers typically present “matched motion”: real vehicle movements are replicated 1:1 by movements in VR. This significantly limits virtual applications. We provide foundations for in-car VR experiences that break this constraint by manipulating the passenger’s visual perception of linear velocity through amplifying and reducing the virtual speed. In two on-the-road studies, we examined the application of Vehicular Translational Gain (1.5-9.5x) and Attenuation (0.66-0.14x) to real car speeds ( 50km/h) across two VR tasks (reading and gaming), exploring journey perception, impact on motion sickness, travel experience and tasks. We found that vehicular gain/attenuation can be applied without significantly increasing motion sickness. Gain was more noticeable and affected perceived speed, distance, safety, relaxation and excitement, being well-suited to gaming, while attenuation was more suitable for productivity. Our work unlocks new ways that VR applications can enhance and alter the passenger experience through novel perceptual manipulations of vehicle velocity.",Virtual Reality; Motion Sickness; In-Car; Automated Vehicles; Perceptual Manipulation; Translational Gain; Vehicular Attenuation; Vehicular Gain; Velocity,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Investigating the Effects of External Communication and Platoon Behavior on Manual Drivers at Highway Access,CHI - Human Factors in Computing Systems,A*,"Automated vehicles are expected to improve traffic safety and efficiency. One approach to achieve this is via platooning, that is, (automated) vehicles can drive behind each other at very close proximity to reduce air resistance. However, this behavior could lead to difficulties in mixed traffic, for example, when manual drivers try to enter a highway. Therefore, we report the results of a within-subject Virtual Reality study (N=29) evaluating different platoon behaviors (single vs. multiple, i.e., four, gaps) and communication strategies (HUD, AR, attached displays). Results show that AR communication reduced mental workload, improved perceived safety, and a single big gap led to the safest merging behavior. Our work helps to incorporate novel behavior enabled by automation into general traffic better.",Automated vehicles; external communication; self-driving vehicles; Virtual Reality.,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Swarm Body: Embodied Swarm Robots,CHI - Human Factors in Computing Systems,A*,"The human brain’s plasticity allows for the integration of artificial body parts into the human body. Leveraging this, embodied systems realize intuitive interactions with the environment. We introduce a novel concept: embodied swarm robots. Swarm robots constitute a collective of robots working in harmony to achieve a common objective, in our case, serving as functional body parts. Embodied swarm robots can dynamically alter their shape, density, and the correspondences between body parts and individual robots. We contribute an investigation of the influence on embodiment of swarm robot-specific factors derived from these characteristics, focusing on a hand. Our paper is the first to examine these factors through virtual reality (VR) and real-world robot studies to provide essential design considerations and applications of embodied swarm robots. Through quantitative and qualitative analysis, we identified a system configuration to achieve the embodiment of swarm robots.",tangible interaction; embodiment; swarm robotics,Abstract,TRUE,
ACM DL,conferencePaper,2024,"Scientific and Fantastical: Creating Immersive, Culturally Relevant Learning Experiences with Augmented Reality and Large Language Models",CHI - Human Factors in Computing Systems,A*,"Motivating children to learn is a major challenge in education. One way to inspire motivation to learn is through immersion. We combine the immersive potential of augmented reality (AR), narrative, and large language models (LLMs) to bridge fantasy with reality in a mobile application, Moon Story, that teaches elementary schoolers astronomy and environmental science. Our system also builds upon learning theories such as culturally-relevant pedagogy. Using our application, a child embarks on a journey inspired by Chinese mythology, engages in real-world AR activities, and converses with a fictional character powered by an LLM. We conducted a controlled experiment (N = 50) with two conditions: one using an LLM and one that was hard-coded. Both conditions resulted in learning gains, high engagement levels, and increased science learning motivation. Participants in the LLM condition also wrote more relevant answers. Finally, participants of both Chinese and non-Chinese heritage found the culturally-based narrative compelling.",Artifact or System; Children/Parents; Education/Learning,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Promoting Eco-Friendly Behaviour through Virtual Reality - Implementation and Evaluation of Immersive Feedback Conditions of a Virtual CO2 Calculator,CHI - Human Factors in Computing Systems,A*,"Climate change is one of the most pressing global challenges in the 21st century. Urgent actions favoring the environment’s well-being are essential to mitigate its potentially irreversible consequences. However, the delayed and often distant nature of the effects of sustainable behavior makes it challenging for individuals to connect with the issue personally. Immersive media are an opportunity to introduce innovative feedback mechanisms to highlight the urgency of behavior effects. We introduce a VR carbon calculator that visualizes users’ annual carbon footprint as CO2-filled balloons over multiple periods. In a 2 × 2 design, participants calculated and visualized their carbon footprint numerically or as balloons over one or three years. We found no effect of our visualization but a significant impact of the visualized period on participants’ environmental self-efficacy. These findings emphasize the importance of target-oriented design in VR behavior interventions.",Virtual reality; intention-behavior gap; pro-environmental behavior.,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Born to Run, Programmed to Play: Mapping the Extended Reality Exergames Landscape",CHI - Human Factors in Computing Systems,A*,"Many people struggle to exercise regularly, raising the risk of serious health-related issues. Extended reality (XR) exergames address these hurdles by combining physical exercises with enjoyable, immersive gameplay. While a growing body of research explores XR exergames, no previous review has structured this rapidly expanding research landscape. We conducted a scoping review of the current state of XR exergame research to (i) provide a structured overview, (ii) highlight trends, and (iii) uncover knowledge gaps. After identifying 1318 papers in human-computer interaction and medical databases, we ultimately included 186 papers in our analysis. We provide a quantitative and qualitative summary of XR exergame research, showing current trends and potential future considerations. Finally, we provide a taxonomy of XR exergames to help future design and methodological investigation and reporting.",virtual reality; mixed reality; review; augmented reality; extended reality; exergames; taxonomy; exercise; games; active games; active video games; motion games; movement games; sports games,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring Opportunities for Augmenting Homes to Support Exercising,CHI - Human Factors in Computing Systems,A*,"Although exercising at home has benefits, it is not always engaging or motivating. Augmented Reality (AR) head-mounted displays (HMDs) offer the potential to make in-home exercising and exergaming more inclusive and immersive, but there is limited research investigating how such systems can be designed. We employed a participatory design approach involving semi-structured interviews to investigate how homes can be augmented to facilitate exercising experiences. We developed 10 recommendations for developing home-based exercising experiences using AR HMDs. Our results further contribute to the existing body of research on the use of AR for exercising, home applications, and everyday objects by presenting the first foundational study investigating the wide range of exercises that can be supported through AR HMDs in home environments and the different ways home elements may support these exercises, and laying the groundwork for future work developing home-based exergaming through AR HMDs to increase people’s physical activity levels.",Exergaming; Augmented reality; Participatory Design; Home environment; Exercising,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Tandem: Reproducible Digital Fabrication Workflows as Multimodal Programs,CHI - Human Factors in Computing Systems,A*,"Experimental digital fabrication workflows are increasingly common in human-computer interaction research, but are difficult to reproduce. We present Tandem, a software library that lets a fabricator implement an end-to-end fabrication workflow as a computational notebook program that others can run to physically reproduce the workflow. Tandem notebook programs read and write to CAD and CAM software, project augmented reality interfaces onto machines for manual interventions, and directly control fabrication machines. Fabricators can also denote potential mismatches between the physical and the digital as explicit assertions in code. Using two-sided CNC milling as an example, we demonstrate how to implement a complex workflow as a single program that can be re-run by others while supporting quality control and improving reproducibility.",open-source software; programming languages; computational notebooks; Digital fabrication,Abstract,TRUE,
ACM DL,conferencePaper,2024,pARam: Leveraging Parametric Design in Extended Reality to Support the Personalization of Artifacts for Personal Fabrication,CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) allows in-situ previewing of designs to be manufactured through Personal Fabrication (PF). These in-situ interactions exhibit advantages for PF, like incorporating the environment into the design process. However, design-for-fabrication in XR often happens through either highly complex 3D-modeling or is reduced to rudimentary adaptations of crowd-sourced models. We present pARam, a tool combining parametric designs (PDs) and XR, enabling in-situ configuration of artifacts for PF. In contrast to modeling- or search-focused approaches, pARam supports customization through embodied and practical inputs (e.g., gestures, recommendations) and evaluation (e.g., lighting estimation) without demanding complex 3D-modeling skills. We implemented pARam for HoloLens 2 and evaluated it (n = 20), comparing XR and desktop conditions. Users succeeded in choosing context-related parameters and took their environment into account for their configuration using pARam. We reflect on the prospects and challenges of PDs in XR to streamline complex design methods for PF while retaining suitable expressivity.",Mixed Reality; Personal Fabrication; 3D-modeling; Customizer Interfaces; Design Customization; In-Situ Design; In-Situ Modeling; pARam; Parametric Designs; Remixing,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Palette-PrintAR: augmented reality design and simulation for multicolor resin 3D printing,CHI - Human Factors in Computing Systems,A*,"While 3D printing affords designers unprecedented geometrical complexity, fewer interactive design tools for multimaterial platforms exist. Recent work in resin 3D printing specifically promises fast, multicolor printing by growing fluidic channels concurrent with the object itself, infusing different resins spatioselectively into the vat; however, no design tools have been developed enabling users to interact with such novel personal fabrication machines in situ. Here, we introduce an augmented reality-based design tool allowing users to engage with this multicolor fabrication method so as to ""paint"" growing 3D objects. We define the design process and mode of user interaction with our tool, Palette-PrintAR, which integrates situated 3D model manipulation with real-time computational fluid dynamics simulation and computer vision-based tracking and analysis. We detail our 3D printer hardware add-on implementation and AR software architecture, along with characterizing the design flexibilities and limitations of our AR-based multicolor fabrication method.",augmented reality; 3D printing; interactive fabrication,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Füpop: ""Real Food"" Flavor Delivery via Focused Ultrasound",CHI - Human Factors in Computing Systems,A*,"Food and flavors are integral to our existence in the world. Nonetheless, taste remains an under-explored sense in interaction design. We present Füpop, a technical platform for delivering in-mouth flavors that leverages advances in electronics and molecular gastronomy. Füpop comprises a fully edible pouch placed inside the mouth against a cheek that programmatically releases different flavors when wirelessly triggered by a focused ultrasound transducer from outside the cheek. Füpop does not interfere with activities such as chewing and drinking, and its electronics may be integrated into devices already used near the cheek, such as mobile phones, audio headphones, and head-mounted displays. Füpop’s flavors are from “real foods,” not ones imitated with synthetic reagents, providing authentic, nutritive flavors. We envision that with Füpop, flavors may be synced to music, a phone call, or events in virtual reality to enhance a user’s experience of their food and the world.",ultrasound; human-food interaction; edible; gastronomy; taste interactions,Abstract,TRUE,
ACM DL,conferencePaper,2024,FocusFlow: 3D Gaze-Depth Interaction in Virtual Reality Leveraging Active Visual Depth Manipulation,CHI - Human Factors in Computing Systems,A*,"Gaze interaction presents a promising avenue in Virtual Reality (VR) due to its intuitive and efficient user experience. Yet, the depth control inherent in our visual system remains underutilized in current methods. In this study, we introduce FocusFlow, a hands-free interaction method that capitalizes on human visual depth perception within the 3D scenes of Virtual Reality. We first develop a binocular visual depth detection algorithm to understand eye input characteristics. We then propose a layer-based user interface and introduce the concept of “Virtual Window” that offers an intuitive and robust gaze-depth VR interaction, despite the constraints of visual depth accuracy and precision spatially at further distances. Finally, to help novice users actively manipulate their visual depth, we propose two learning strategies that use different visual cues to help users master visual depth control. Our user studies on 24 participants demonstrate the usability of our proposed virtual window concept as a gaze-depth interaction method. In addition, our findings reveal that the user experience can be enhanced through an effective learning process with adaptive visual cues, helping users to develop muscle memory for this brand-new input mechanism. We conclude the paper by discussing potential future research topics of gaze-depth interaction.",Virtual Reality; Eye Tracking; Gaze Interaction; 3D User Interface; Visual Depth,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Gaze on the Go: Effect of Spatial Reference Frame on Visual Target Acquisition During Physical Locomotion in Extended Reality,CHI - Human Factors in Computing Systems,A*,"Spatial interaction relies on fast and accurate visual acquisition. In this work, we analyse how visual acquisition and tracking of targets presented in a head-mounted display is affected by the user moving linearly at walking and jogging paces. We study four reference frames in which targets can be presented: Head and World where targets are affixed relative to the head and environment, respectively; HeadDelay where targets are presented in the head coordinate system but follow head movement with a delay, and novel Path where targets remain at fixed distance in front of the user, in the direction of their movement. Results of our study in virtual reality demonstrate that the more stable the target is relative to the environment, the faster and more precise it can be fixated. The results have practical significance as head-mounted displays enable interaction during mobility, and in particular when eye tracking is considered as input.",extended reality; eye tracking; gaze interaction; physical locomotion; reference frames; spatial UIs; UI placement,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Snap, Pursuit and Gain: Virtual Reality Viewport Control by Gaze",CHI - Human Factors in Computing Systems,A*,"Head-mounted displays let users explore virtual environments through a viewport that is coupled with head movement. In this work, we investigate gaze as an alternative modality for viewport control, enabling exploration of virtual worlds with less head movement. We designed three techniques that leverage gaze based on different eye movements: Dwell Snap for viewport rotation in discrete steps, Gaze Gain for amplified viewport rotation based on gaze angle, and Gaze Pursuit for central viewport alignment of gaze targets. All three techniques enable 360-degree viewport control through naturally coordinated eye and head movement. We evaluated the techniques in comparison with controller snap and head amplification baselines, for both coarse and precise viewport control, and found them to be as fast and accurate. We observed a high variance in performance which may be attributable to the different degrees to which humans tend to support gaze shifts with head movement.",virtual reality; Eye tracking; user study; gaze interaction; gaze-based interaction; eye-head coordination; viewport control,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,EITPose: Wearable and Practical Electrical Impedance Tomography for Continuous Hand Pose Estimation,CHI - Human Factors in Computing Systems,A*,"Real-time hand pose estimation has a wide range of applications spanning gaming, robotics, and human-computer interaction. In this paper, we introduce EITPose, a wrist-worn, continuous 3D hand pose estimation approach that uses eight electrodes positioned around the forearm to model its interior impedance distribution during pose articulation. Unlike wrist-worn systems relying on cameras, EITPose has a slim profile (12 mm thick sensing strap) and is power-efficient (consuming only 0.3 W of power), making it an excellent candidate for integration into consumer electronic devices. In a user study involving 22 participants, EITPose achieves with a within-session mean per joint positional error of 11.06 mm. Its camera-free design prioritizes user privacy, yet it maintains cross-session and cross-user accuracy levels comparable to camera-based wrist-worn systems, thus making EITPose a promising technology for practical hand pose estimation.",Extended Reality; Input; Electrical Impedance Tomography; Natural User Interfaces; Hand Gesture; Hand Pose; Interaction Technique,Keywords,TRUE,
ACM DL,conferencePaper,2024,ArmDeformation: Inducing the Sensation of Arm Deformation in Virtual Reality Using Skin-Stretching,CHI - Human Factors in Computing Systems,A*,"With the development of virtual reality (VR) technology, research is being actively conducted on how incorporating multisensory feedback can create the illusion that virtual avatars are perceived as an extension of the body in VR. In line with this research direction, we introduce ArmDeformation, a wearable device employing skin-stretching to enhance virtual forearm ownership during arm deformation illusion. We conducted five user studies with 98 participants. Using a developed tabletop device, we confirmed the optimal number of actuators and the ideal skin-stretching design effectively increases the user’s body ownership. Additionally, we explored the maximum visual threshold for forearm bending and the minimum detectable bending direction angle when using skin-stretching in VR. Finally, our study demonstrates that using ArmDeformation in VR applications enhances user realism and enjoyment compared to relying on visual feedback alone.",Virtual Reality; Body Illusion; Body Ownership; Skin-stretching,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Voice assistants (VAs) like Siri and Alexa are transforming human-computer interaction; however, they lack awareness of users’ spatiotemporal context, resulting in limited performance and unnatural dialogue. We introduce GazePointAR, a fully-functional context-aware VA for wearable augmented reality that leverages eye gaze, pointing gestures, and conversation history to disambiguate speech queries. With GazePointAR, users can ask “what’s over there?” or “how do I solve this math problem?” simply by looking and/or pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1) comparing GazePointAR to two commercial systems, (2) examining GazePointAR’s pronoun disambiguation across three tasks; (3) and an open-ended phase where participants could suggest and try their own context-sensitive queries. Participants appreciated the naturalness and human-like nature of pronoun-driven queries, although sometimes pronoun use was counter-intuitive. We then iterated on GazePointAR and conducted a first-person diary study examining how GazePointAR performs in-the-wild. We conclude by enumerating limitations and design considerations for future context-aware VAs.",augmented reality; voice assistants; gaze tracking; multimodal input; LLM; pointing gesture recognition,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,QuadStretcher: A Forearm-Worn Skin Stretch Display for Bare-Hand Interaction in AR/VR,CHI - Human Factors in Computing Systems,A*,"The paradigm of bare-hand interaction has become increasingly prevalent in Augmented Reality (AR) and Virtual Reality (VR) environments, propelled by advancements in hand tracking technology. However, a significant challenge arises in delivering haptic feedback to users’ hands, due to the necessity for the hands to remain bare. In response to this challenge, recent research has proposed an indirect solution of providing haptic feedback to the forearm. In this work, we present QuadStretcher, a skin stretch display featuring four independently controlled stretching units surrounding the forearm. While achieving rich haptic expression, our device also eliminates the need for a grounding base on the forearm by using a pair of counteracting tactors, thereby reducing bulkiness. To assess the effectiveness of QuadStretcher in facilitating immersive bare-hand experiences, we conducted a comparative user evaluation (n = 20) with a baseline solution, Squeezer. The results confirmed that QuadStretcher outperformed Squeezer in terms of expressing force direction and heightening the sense of realism, particularly in 3-DoF VR interactions such as pulling a rubber band, hooking a fishing rod, and swinging a tennis racket. We further discuss the design insights gained from qualitative user interviews, presenting key takeaways for future forearm-haptic systems aimed at advancing AR/VR bare-hand experiences.",Haptics; Wearable; AR; VR; Bare-Hand Interaction; Skin Stretch,Abstract,TRUE,
ACM DL,conferencePaper,2024,"Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence for Operational Assessments in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Space agencies are in the process of drawing up carefully thought-out Concepts of Operations (ConOps) for future human missions on the Moon. These are typically assessed and validated through costly and logistically demanding analogue field studies. While interactive simulations in Virtual Reality (VR) offer a comparatively cost-effective alternative, they have faced criticism for lacking the fidelity of real-world deployments. This paper explores the applicability of passive haptic interfaces in bridging the gap between simulated and real-world ConOps assessments. Leveraging passive haptic props (equipment mockup and astronaut gloves), we virtually recreated the Apollo 12 mission procedure and assessed it with experienced astronauts and other space experts. Quantitative and qualitative findings indicate that haptics increased presence and embodiment, thus improving perceived simulation fidelity and validity of user reflections. We conclude by discussing the potential role of passive haptic modalities in facilitating early-stage ConOps assessments for human endeavours on the Moon and beyond.",presence; virtual reality; embodiment; passive haptic feedback; concepts of operations; scenario assessment; space exploration,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,ErgoPulse: Electrifying Your Lower Body With Biomechanical Simulation-based Electrical Muscle Stimulation Haptic System in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"This study presents ErgoPulse, a system that integrates biomechanical simulation with electrical muscle stimulation (EMS) to provide kinesthetic force feedback to the lower-body in virtual reality (VR). ErgoPulse features two main parts: a biomechanical simulation part that calculates the lower-body joint torques to replicate forces from VR environments, and an EMS part that translates torques into muscle stimulations. In the first experiment, we assessed users’ ability to discern haptic force intensity and direction, and observed variations in perceived resolution based on force direction. The second experiment evaluated ErgoPulse’s ability to increase haptic force accuracy and user presence in both continuous and impulse force VR game environments. The experimental results showed that ErgoPulse’s biomechanical simulation increased the accuracy of force delivery compared to traditional EMS, enhancing the overall user presence. Furthermore, the interviews proposed improvements to the haptic experience by integrating additional stimuli such as temperature, skin stretch, and impact.",Virtual Reality; Wearable Device; Simulation; Electrical Muscle Stimulation; Haptic; Biomechanics,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Augmenting Perceived Length of Handheld Controllers: Effects of Object Handle Properties,CHI - Human Factors in Computing Systems,A*,"In the realm of virtual reality (VR), shape-changing controllers have emerged as a means to enhance visuo-haptic congruence during user interactions. The major emphasis has been placed on manipulating the inertia tensor of a shape-changing controller to control the perceived shape. This paper delves deeper by exploring how the material properties of the controller’s handle, distinct from the inertial information, affect the perceived shape, focusing on the perceived length. We conducted three perceptual experiments to examine the effects of the handle’s softness, thermal conductivity, and texture, respectively. Results demonstrated that a softer handle increases the perceived length, whereas a handle with higher thermal conductivity reduces it. Texture, in the form of varying bumps, also alters the length perception. These results provide more comprehensive knowledge of the intricate relationship between perceived length and controller handle properties, expanding the design alternatives for shape-changing controllers for immersive VR experiences.",Virtual Reality; Dynamic Touch; Handheld Controller; Haptic Perceptual Cue; Shape Perception,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Experiencing Dynamic Weight Changes in Virtual Reality Through Pseudo-Haptics and Vibrotactile Feedback,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) objects react dynamically to users’ touch interactions in real-time. However, experiencing changes in weight through the haptic sense remains challenging with consumer VR controllers due to their limited vibrotactile feedback. While prior works successfully applied pseudo-haptics to perceive absolute weight by manipulating the control-display (C/D) ratio, we continuously adjusted the C/D ratio to mimic weight changes. Vibrotactile feedback additionally emphasises the modulation in the virtual object’s physicality. In a study (N=18), we compared our multimodal technique with pseudo-haptics alone and a baseline condition to assess participants’ experiences of weight changes. Our findings demonstrate that participants perceived varying degrees of weight change when the C/D ratio was adjusted, validating its effectiveness for simulating dynamic weight in VR. However, the additional vibrotactile feedback did not improve weight change perception. This work extends the understanding of designing haptic experiences for lightweight VR systems by leveraging perceptual mechanisms.",virtual reality; haptic illusions; pseudo-haptics; weight perception; vibration; multisensory integration; weight change,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring Mobile Devices as Haptic Interfaces for Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Dedicated handheld controllers facilitate haptic experiences of virtual objects in mixed reality (MR). However, as mobile MR becomes more prevalent, we observe the emergence of controller-free MR interactions. To retain immersive haptic experiences, we explore the use of mobile devices as a substitute for specialised MR controller. In an exploratory gesture elicitation study (n = 18), we examined users’ (1) intuitive hand gestures performed with prospective mobile devices and (2) preferences for real-time haptic feedback when exploring haptic object properties. Our results reveal three haptic exploration modes for the mobile device, as an object, hand substitute, or as an additional tool, and emphasise the benefits of incorporating the device’s unique physical features into the object interaction. This work expands the design possibilities using mobile devices for tangible object interaction, guiding the future design of mobile devices for haptic MR experiences.",mixed reality; mobile phones; haptic feedback; gesture elicitation; haptic exploration; haptic interfaces; mobile gestures,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,InflatableBots: Inflatable Shape-Changing Mobile Robots for Large-Scale Encountered-Type Haptics in VR,CHI - Human Factors in Computing Systems,A*,"We introduce InflatableBots, shape-changing inflatable robots for large-scale encountered-type haptics in VR. Unlike traditional inflatable shape displays, which are immobile and limited in interaction areas, our approach combines mobile robots with fan-based inflatable structures. This enables safe, scalable, and deployable haptic interactions on a large scale. We developed three coordinated inflatable mobile robots, each of which consists of an omni-directional mobile base and a reel-based inflatable structure. The robot can simultaneously change its height and position rapidly (horizontal: 58.5 cm/sec, vertical: 10.4 cm/sec, from 40 cm to 200 cm), which allows for quick and dynamic haptic rendering of multiple touch points to simulate various body-scale objects and surfaces in real-time across large spaces (3.5 m x 2.5 m). We evaluated our system with a user study (N = 12), which confirms the unique advantages in safety, deployability, and large-scale interactability to significantly improve realism in VR experiences.",Virtual Reality; Haptics; Inflatables; Mobile Robots; Shape-Changing Interfaces; Encountered-Type Haptics,Keywords,TRUE,
ACM DL,conferencePaper,2024,"VeeR: Exploring the Feasibility of Deliberately Designing VR Motion that Diverges from Mundane, Everyday Physical Motion to Create More Entertaining VR Experiences",CHI - Human Factors in Computing Systems,A*,"This paper explores the feasibility of deliberately designing VR motion that diverges from users’ physical movements to turn mundane, everyday transportation motion (e.g., metros, trains, and cars) into more entertaining VR motion experiences, in contrast to prior car-based VR approaches that synchronize VR motion to physical car movement exactly. To gain insight into users’ preferences for veering rate and veering direction for turning (left/right) and pitching (up/down) during the three phases of acceleration (accelerating, cruising, and decelerating), we conducted a formative, perceptual study (n=24) followed by a VR experience evaluation (n=18), all conducted on metro trains moving in a mundane, straight-line motion. Results showed that participants preferred relatively high veering rates, and preferred pitching upward during acceleration and downward during deceleration. Furthermore, while veering decreased comfort as expected, it significantly enhanced immersion (p&lt;.01) and entertainment (p&lt;.001) and the overall experience, with comfort being considered, was preferred by 89% of participants.",Virtual Reality; User Experience Design; Motion Sensation; Opportunistic Haptic,Keywords,TRUE,
ACM DL,conferencePaper,2024,Paired-EMS: Enhancing Electrical Muscle Stimulation (EMS)-based Force Feedback Experience by Stimulating Both Muscles in Antagonistic Pairs,CHI - Human Factors in Computing Systems,A*,"Electrical Muscle Stimulation (EMS) has emerged as a key wearable haptic feedback technology capable of simulating a wide range of force feedback, such as the impact force of boxing punches, the weight of virtual objects, and the reaction force from pushing on a wall. To simulate these external forces, EMS stimulates the muscles that oppose (i.e. antagonistic to) the actual muscles that users activate, causing involuntary muscle contraction and haptic sensations that differ from real-world experiences. In this work, we propose Paired-EMS which simultaneously stimulates both the muscles that users activate and that prior EMS stimulates (i.e. antagonistic muscle pairs) to enhance the external force feedback experience. We first conducted a small formative study (n=8) to help design the stimulation intensity of muscle pairs, then conducted a user experience study to evaluate Paired-EMS vs. prior EMS approaches for both isometric and isotonic user actions. Study results (n=32) showed that Paired-EMS significantly improved realism, harmony, and entertainment (p&lt;.05) with similar comfort (p&gt;.36), and was overall preferred by 78% of participants (p&lt;.01).",Virtual Reality; User Experience Design; Haptic,Keywords,TRUE,
ACM DL,conferencePaper,2024,ALCool: Utilizing Alcohol's Evaporative Cooling for Ubiquitous Cold Sensation Feedback,CHI - Human Factors in Computing Systems,A*,"Tactile technologies are important for novel user experiences. Among several tactile submodalities, cold sensation is essential for realistically portraying materials and environments. However, current cold presentations such as Peltier devices face challenges like low energy efficiency and the need for complicated equipment. To address these, we suggest leveraging alcohol’s endothermic property during evaporation. Our prototype, a wristwatch wearable with a fan, capitalizes on alcohol’s high volatility by absorbing ambient heat upon evaporation. The device further enhances the cooling effect by circulating air around the skin. This approach simplifies the setup required for cooling technologies and is more energy-efficient than Peltier-based systems. We also integrated perfume, which is a mixture of alcohol and scent substance, and presented a unique cooling and scent experience. The use of alcohol as a cooling method was not considered conventional, but social changes after COVID-19 made it easy to obtain a tiny amount of alcohol.",virtual reality; alcohol; chemical haptics,Keywords,TRUE,
ACM DL,conferencePaper,2024,AirPush: A Pneumatic Wearable Haptic Device Providing Multi-Dimensional Force Feedback on a Fingertip,CHI - Human Factors in Computing Systems,A*,"Finger wearable haptic devices enrich virtual reality experiences by offering haptic feedback corresponding to the virtual environment. However, despite the effectiveness of current finger wearable haptic devices in delivering haptic feedback, many are often constrained in their ability to provide force feedback across a diverse range of directions or to sustain it. Therefore, we present AirPush, a finger wearable haptic device capable of generating continuously adjustable force feedback in multiple directions using compressed air. To evaluate its usability, we conducted a technical evaluation and four user studies: (1) we obtained the user’s perceptual thresholds of angles under different directions on horizontal and vertical planes, (2) in perception studies, we found that users can identify five different magnitudes of force and eight different motion when using AirPush, and (3) using it in VR applications, we confirmed that users felt more realistic and immersed when using AirPush than the HTC VIVE Controller or AirPush with a fixed nozzle.",Virtual Reality; Compressed Air; Finger Wearable Haptic Device,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Shaping Compliance: Inducing Haptic Illusion of Compliance in Different Shapes with Electrotactile Grains,CHI - Human Factors in Computing Systems,A*,"Compliance, the degree of displacement under applied force, is pivotal in determining the material perception when touching an object. Vibrotactile actuators can be used for creating grain-based virtual compliance, but they have poor spatial resolution and a limiting rigid form factor. We propose a novel electrotactile compliance illusion that renders grains of electrical pulses on an electrode array in response to finger force changes. We demonstrate its ability to render compliance in distinct shapes through a thin, lightweight, and flexible finger-worn interface. Detailed technical parameters and the implementation of our device are provided. A controlled experiment confirms the technique can (1) create virtual compliance; (2) adjust the compliance magnitude with grain and electrode parameters; and (3) render compliance with specific shapes. In three example applications, we present how this illusion can enhance physical objects, elements in graphical user interfaces, and virtual reality experiences.",Haptics; virtual reality.; compliance; electrotactile; haptic illusion,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Stick&amp;Slip: Altering Fingerpad Friction via Liquid Coatings,CHI - Human Factors in Computing Systems,A*,"We present Stick&amp;Slip, a novel approach that alters friction between the fingerpad &amp; surfaces by depositing liquid droplets that coat the fingerpad. The liquid coating modifies the finger's coefficient of friction, allowing users to feel surfaces up to ±60% more slippery or sticky. We selected our fluids to rapidly evaporate so that the surface returns to its original friction. Unlike traditional friction-feedback, such as electroadhesion or vibration, our approach: (1) alters friction on a wide range of surfaces and geometries, making it possible to modulate nearly any non-absorbent surface; (2) scales to many objects without requiring instrumenting the target surfaces (e.g., with conductive electrode coatings or vibromotors); and (3) both in/decreases friction via a single device. We identified nine liquids and characterized their practicality by measuring evaporation rates, etc. To illustrate the applicability of our approach, we demonstrate how it enables friction in virtual/mixed-reality or, even, while using everyday objects/tools.",mixed reality; haptics; grasp; friction; adhesion,Keywords,TRUE,
ACM DL,conferencePaper,2024,Facilitating Virtual Reality Integration in Medical Education: A Case Study of Acceptability and Learning Impact in Childbirth Delivery Training,CHI - Human Factors in Computing Systems,A*,"Advancements in Virtual Reality (VR) technology have opened new frontiers in medical education, igniting interest among medical educators to incorporate it into mainstream curriculum, complementing traditional training modalities such as manikin training. Despite numerous VR simulators on the market, their uptake in medical education remains limited. This paper explores the acceptability and educational effectiveness of VR in the context of vaginal childbirth delivery training, with the simulator providing a walkthrough for the second and third stages of labour, contrasting it with established manikin-based methods. We conducted a large-scale empirical study with 117 medical students, revealing a significant 24.9% improvement in knowledge scores when using VR as compared to manikin. However, VR received significantly lower self-reported feasibility scores in Confidence, Usability, Enjoyment, Feedback and Presence, indicating low acceptance. The study provides critical insights into the relationship between technological innovation and educational impact, guiding future integration of VR into medical training curricula.",Virtual Reality; User Experience Design; Gynaecology; Medical Education; Obstetrics &amp,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,MR Microsurgical Suture Training System with Level-Appropriate Support,CHI - Human Factors in Computing Systems,A*,"The integration of advanced technologies in healthcare necessitates the development of systems accommodating the daily routines in medical practices. Neurosurgeons, in particular, require extensive practice in microsurgical suturing in the long term, even in the busy routine of a medical practice. This study collaboratively developed a Mixed Reality system with neurosurgeons to support self-training in microscopic suturing. Based on the neurosurgeons’ opinions, we implemented a level-appropriate microsurgical suture training system. For novices, the system offers shadow-matching training to support the practice of precise movements under the high-sensitivity environment of the microscope. For intermediates, it provides a real-time feedback system, which allows users to practice attention to details. Evaluation involved testing the novice system on students with no medical background and the intermediate system on neurosurgery residents. The effectiveness of the system was demonstrated through the experimental results and subsequent discussion.",MR; CV-based tracking; Neurosurgical training; Real-time feedback; Visual feedback,Abstract,TRUE,
ACM DL,conferencePaper,2024,LightSword: A Customized Virtual Reality Exergame for Long-Term Cognitive Inhibition Training in Older Adults,CHI - Human Factors in Computing Systems,A*,"The decline of cognitive inhibition significantly impacts older adults’ quality of life and well-being, making it a vital public health problem in today’s aging society. Previous research has demonstrated that Virtual reality (VR) exergames have great potential to enhance cognitive inhibition among older adults. However, existing commercial VR exergames were unsuitable for older adults’ long-term cognitive training due to the inappropriate cognitive activation paradigm, unnecessary complexity, and unbefitting difficulty levels. To bridge these gaps, we developed a customized VR cognitive training exergame (LightSword) based on Dual-task and Stroop paradigms for long-term cognitive inhibition training among healthy older adults. Subsequently, we conducted an eight-month longitudinal user study with 12 older adults aged 60 years and above to demonstrate the effectiveness of LightSword in improving cognitive inhibition. After the training, the cognitive inhibition abilities of older adults were significantly enhanced, with benefits persisting for 6 months. This result indicated that LightSword has both short-term and long-term effects in enhancing cognitive inhibition. Furthermore, qualitative feedback revealed that older adults exhibited a positive attitude toward long-term training with LightSword, which enhanced their motivation and compliance.",Virtual Reality; Health; Older Adults,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,“X-Ray Vision” as a Compensatory Augmentation for Slowing Cognitive Map Decay in Older Adults,CHI - Human Factors in Computing Systems,A*,"Safe and efficient navigation often relies on the development and retention of accurate cognitive maps that include inter-landmark relations. For many older adults, cognitive maps are difficult to form and remember over time, which introduces serious challenges for independence and mobility. To address this problem, we explore an innovative compensatory augmentation solution enabling enhanced inter-landmark learning via an “X-Ray Vision” simulation. Results with (n=45) user study participants suggest superior older adult cognitive map retention over time from a single learning session with the augmentation versus a control condition without the augmentation. Furthermore, results characterize differences in decay of cognitive maps between older adults and a control of younger adults. These findings suggest important implications for future augmented reality devices and the ways in which they can be used to promote memory and independence among older adults.",Augmented Reality; Older Adults; Cognitive Maps; X-Ray Vision,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,A Survey On Measuring Presence in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Presence is a defining element of virtual reality (VR), but it is also increasingly used when assessing mixed reality (MR) experiences. The increased interest in measuring presence in MR and recent works underpinning the specific nature of presence in MR raise the question of the current state and practice of assessing presence in MR. To address this question, we present an analysis of more than 320 studies that report on presence measurements in MR. Our analysis showed that questionnaires are the dominant measurement but also identify problematic trends that stem from the lack of a generally agreed-upon concept or measurement for presence in MR. More specifically, we show that using measurements that are not validated in MR or custom questionnaires limiting the comparability of results is commonplace and could contribute to a looming replication crisis in an increasingly relevant field.",Augmented Reality; Mixed Reality; Virtual Reality; Extended Reality; Sense of Presence; Spatial Presence,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,ARCADIA: A Gamified Mixed Reality System for Emotional Regulation and Self-Compassion,CHI - Human Factors in Computing Systems,A*,"Mental health and wellbeing have become one of the significant challenges in global society, for which emotional regulation strategies hold the potential to offer a transversal approach to addressing them. However, the persistently declining adherence of patients to therapeutic interventions, coupled with the limited applicability of current technological interventions across diverse individuals and diagnoses, underscores the need for innovative solutions. We present ARCADIA, a Mixed-Reality platform strategically co-designed with therapists to enhance emotional regulation and self-compassion. ARCADIA comprises several gamified therapeutic activities, with a strong emphasis on fostering patient motivation. Through a dual study involving therapists and mental health patients, we validate the fully functional prototype of ARCADIA. Encouraging results are observed in terms of system usability, user engagement, and therapeutic potential. These findings lead us to believe that the combination of Mixed Reality and gamified therapeutic activities could be a significant tool in the future of mental health.",mixed reality; mental health; gamification; emotional regulation,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Behind the Scenes: Adapting Cinematography and Editing Concepts to Navigation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Teleportation is a popular method of navigation in virtual reality (VR) because it does not induce symptoms of VR sickness, such as nausea and disorientation. However, teleportation may reduce spatial awareness, causing users to miss important aspects of their surroundings. We present ACTIVE, a novel approach to teleportation that uses techniques from cinematography to enhance the user experience of navigation in VR. ACTIVE adapts heuristics from continuity editing to dynamically reposition and reorient the camera after teleportation. This approach aims to improve the aesthetic quality of entities and environmental features while respecting users’ intended trajectory through the virtual environment. In a user study, we found that even though ACTIVE did not improve users’ recall of which entities were present in the environment, it increased engagement by significantly improving aesthetic appeal. Lastly, despite removing some agency from users, ACTIVE had no impact on presence or VR sickness compared to teleportation.",virtual reality; navigation; user engagement; multi-objective optimization; cinematography,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Implementation of Virtual Reality Motivated Physical Activity via Omnidirectional Treadmill in a Supported Living Facility for Older Adults: A Mixed-Methods Evaluation.: Virtual reality to motivate physical activity for older adults,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) can support healthy ageing, but few devices have been trialed with frail older adults to increase physical activity. We conducted a preliminary mixed-methods implementation evaluation of an omnidirectional VR treadmill and a static VR experience with seven older adults over a six-week period in a supported living facility. Frequency of use and pre-post physical functioning measures were collected, mainly to establish technology suitability based on person characteristics. Diary entries following technology use, resident focus group and staff interview revealed technology acceptance and perceived potential for increasing physical activity, health and wellbeing through accessing virtual environments, which motivated continued activity. Results demonstrated technology suitability for a range of older adults with various mobility and physical impairments. However, residents noted interest in a seated treadmill for physical activity without perceived risks of falls with standing treadmills. Staff raised considerations around care home implementations including usability, cost and space.",virtual reality; physical activity; innovation; technology; Social care,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,MobileGravity: Mobile Simulation of a High Range of Weight in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Simulating accurate weight forces in Virtual Reality (VR) is an unsolved challenge. Therefore, providing real weight sensations by transferring liquid mass has emerged as a promising approach. However, key objectives conceptually interfere with each other. In particular, previous designs that support a high range of weight or high flow rate lack mobility. In this work, we present MobileGravity, a system, that decouples the weight-changing object from the liquid supply and the pump. It enables weight changes of up to 1 kg at a rate of 235 g/s and allows the user to walk around freely. Through a study with 30 participants, we show that the system enables users to perceive the weight of different virtual objects and enhances realism, as well as enjoyment.",virtual reality; haptics; weight perception; weight simulation,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,The Impact of Avatar Completeness on Embodiment and the Detectability of Hand Redirection in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"To enhance interactions in VR, many techniques introduce offsets between the virtual and real-world position of users’ hands. Nevertheless, such hand redirection (HR) techniques are only effective as long as they go unnoticed by users—not disrupting the VR experience. While several studies consider how much unnoticeable redirection can be applied, these focus on mid-air floating hands that are disconnected from users’ bodies. Increasingly, VR avatars are embodied as being directly connected with the user’s body, which provide more visual cue anchoring, and may therefore reduce the unnoticeable redirection threshold. In this work, we studied more complete avatars and their effect on the sense of embodiment and the detectability of HR. We found that higher avatar completeness increases embodiment, and we provide evidence for the absence of practically relevant effects on the detectability of HR.",Virtual reality; illusions; avatar embodiment; detection thresholds; hand redirection,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,Stochastic Machine Witnesses at Work: Today's Critiques of Taylorism are Inadequate for Workplace Surveillance Epistemologies of the Future,CHI - Human Factors in Computing Systems,A*,"I argue that epistemologies of workplace surveillance are shifting in fundamental ways, and so critiques must shift accordingly. I begin the paper by relating Scientific Management to Human-Centred Computing’s ways of knowing through a study of ‘metaverse’ virtual reality workplaces. From this, I develop two observations. The first is that today’s workplace measurement science does not resemble the science that Taylor developed for Scientific Management. Contemporary workplace science is more passive, more intermediated and less controlled. The second observation is that new forms of workplace measurement challenge the norms of empirical science. Instead of having credentialed human witnesses observe phenomena and agree facts about them, we instead make outsourced, uncredentialed stochastic machine witnesses responsible for producing facts about work. With these observations in mind, I assert that critiques of workplace surveillance still framed by Taylorism will not be fit for interrogating workplace surveillance practices of the future.",Metaverse; Taylorism; Ubiquitous Computing; Neo-Taylorism; Scientific Management; Work Measurement; Workplace Surveillance,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Cohabitant: The Design, Implementation, and Evaluation of a Virtual Reality Application for Interfaith Learning and Empathy Building",CHI - Human Factors in Computing Systems,A*,"Lack of interfaith communication often gives rise to prejudice and group-based conflict in multi-faith societies. Nurturing this communication via interfaith learning may reduce this conflict by fostering interfaith empathy. HCI has a dearth of knowledge on interfaith coexistence and empathy building. To address this gap, we present the design, implementation, and usability of Cohabitant: a virtual reality (VR) application that promotes interfaith learning and empathy. Cohabitant’s design is theoretically underpinned by Allport’s intergroup contact theory and informed by insights from a participatory workshop we ran with members of three religious groups: Christians, Hindus, and Muslims. Our evaluation study, combining quantitative and qualitative data from 30 participants, suggests that Cohabitant may enhance general interpersonal empathy, but falls short for ethnocultural empathy. We discuss the possible design and policy implications of using this kind of VR technology for interfaith learning and empathy building.",Virtual Reality; Learning; Empathy; Interfaith,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Navigating the Virtual Gaze: Social Anxiety's Role in VR Proxemics,CHI - Human Factors in Computing Systems,A*,"For individuals with Social Anxiety (SA), interacting with others can be a challenging experience, a concern that extends into the virtual world. While technology has made significant strides in creating more realistic virtual human agents (VHA), the interplay of gaze and interpersonal distance when interacting with VHAs is often neglected. This paper investigates the effect of dynamic and static Gaze animations in VHAs on interpersonal distance and their relation to SA. A Bayesian analysis shows that static centered and dynamic centering gaze led participants to stand closer to VHAs than static averted and dynamic averting gaze, respectively. In the static gaze conditions, this pattern was found to be reversed in SA: participants with higher SA kept larger distances for static-centered gaze than for averted gaze VHAs. These findings update theory, elucidate how nuanced interactions with VHAs must be designed, and offer renewed guidelines for pleasant VHA interaction design.",Virtual Reality; Proxemics; Virtual Human Agents,Keywords,TRUE,
ACM DL,conferencePaper,2024,LLMR: Real-time Prompting of Interactive Worlds using Large Language Models,CHI - Human Factors in Computing Systems,A*,"We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR’s cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.",mixed reality; artificial intelligence; large language model; spatial reasoning,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Meaning Follows Purpose: Unravelling the Architectural Design Conventions in the Contemporary Metaverse,CHI - Human Factors in Computing Systems,A*,"Thousands of people regularly meet, work and play in the architectural spaces that the metaverse offers today. Yet despite the creative potential to disrupt how the built environment is represented, there exists a prevalent belief that the architectural design of the metaverse is rather conventional and reliant on simulating physical reality. We investigated this claim by conducting a design critique study of the most apparent architectural design conventions within the current most popular metaverse platforms, as determined by a scoping review and Google Trends analysis. Based on the opinions of 21 architectural experts on the design of interiors, buildings, and plazas within these platforms, we elicited three overarching design conventions that capture the representation, engagement, and purpose of metaverse architecture. By discussing the impact of these conventions on architectural quality, we inform the future design of metaverse spaces to more purposefully, and perhaps less frequently, use realism to convey meaning.",Virtual Reality; User Study; Metaverse; Virtual Worlds; Scoping Review; Architectural Design; Architectural Drawing; Cyberspace; Design Science; Design Studies,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Unlocking Understanding: An Investigation of Multimodal Communication in Virtual Reality Collaboration,CHI - Human Factors in Computing Systems,A*,"Communication in collaboration, especially synchronous, remote communication, is crucial to the success of task-specific goals. Insufficient or excessive forms of communication may lead to detrimental effects on task performance while increasing mental fatigue. However, identifying which combinations of communication modalities provide the most efficient transfer of information in collaborative settings will greatly improve collaboration. To investigate this, we developed a remote, synchronous, asymmetric VR collaborative assembly task application, where users play the role of either mentor or mentee, and were exposed to different combinations of three communication modalities: voice, gestures, and gaze. Through task-based experiments with 25 pairs of participants (50 individuals), we evaluated quantitative and qualitative data and found that gaze did not differ significantly from multiple combinations of communication modalities. Our qualitative results indicate that mentees experienced more difficulty and frustration in completing tasks than mentors, with both types of users preferring all three modalities to be present.",virtual reality; communication; collaboration,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,Using the Visual Language of Comics to Alter Sensations in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) excels at altering what we see but non-visual sensations are difficult to augment. To augment non-visual sensations in AR, we draw on the visual language of comic books. Synthesizing comic studies, we create a design space describing how to use comic elements (e.g., onomatopoeia) to depict non-visual sensations (e.g., hearing). To demonstrate this design space, we built eight demos, such as speed lines to make a user think they are faster and smell lines to make a scent seem stronger. We evaluate these elements in a qualitative user study (N=20) where participants performed everyday tasks with comic elements added as augmentations. All participants stated feeling a change in perception for at least one sensation, with perceived changes detected by between four participants (touch) and 15 participants (hearing). The elements also had positive effects on emotion and user experience, even when participants did not feel changes in perception.",augmented reality; sensory augmentation; comics,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Assessing User Apprehensions About Mixed Reality Artifacts and Applications: The Mixed Reality Concerns (MRC) Questionnaire,CHI - Human Factors in Computing Systems,A*,"Current research in Mixed Reality (MR) presents a wide range of novel use cases for blending virtual elements with the real world. This yet-to-be-ubiquitous technology challenges how users currently work and interact with digital content. While offering many potential advantages, MR technologies introduce new security, safety, and privacy challenges. Thus, it is relevant to understand users’ apprehensions towards MR technologies, ranging from security concerns to social acceptance. To address this challenge, we present the Mixed Reality Concerns (MRC) Questionnaire, designed to assess users’ concerns towards MR artifacts and applications systematically. The development followed a structured process considering previous work, expert interviews, iterative refinements, and confirmatory tests to analytically validate the questionnaire. The MRC Questionnaire offers a new method of assessing users’ critical opinions to compare and assess novel MR artifacts and applications regarding security, privacy, social implications, and trust.",Mixed Reality; Privacy; Security; Trust; Safety; Concerns; Social Acceptance; User Apprehensions,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Development and Validation of the Collision Anxiety Questionnaire for VR Applications,CHI - Human Factors in Computing Systems,A*,"The high degree of sensory immersion is a distinctive feature of head-mounted virtual reality (VR) systems. While the visual detachment from the real world enables unique immersive experiences, users risk collisions due to their inability to perceive physical obstacles in their environment. Even the mere anticipation of a collision can adversely affect the overall experience and erode user confidence in the VR system. However, there are currently no valid tools for assessing collision anxiety. We present the iterative development and validation of the Collision Anxiety Questionnaire (CAQ), involving an exploratory and a confirmatory factor analysis with a total of 159 participants. The results provide evidence for both discriminant and convergent validity and a good model fit for the final CAQ with three subscales: general collision anxiety, orientation, and interpersonal collision anxiety. By utilizing the CAQ, researchers can examine potential confounding effects of collision anxiety and evaluate methods for its mitigation.",virtual reality; discomfort; user experience; fear; assessment; collision anxiety,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Evaluating Navigation and Comparison Performance of Computational Notebooks on Desktop and in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The computational notebook serves as a versatile tool for data analysis. However, its conventional user interface falls short of keeping pace with the ever-growing data-related tasks, signaling the need for novel approaches. With the rapid development of interaction techniques and computing environments, there is a growing interest in integrating emerging technologies in data-driven workflows. Virtual reality, in particular, has demonstrated its potential in interactive data visualizations. In this work, we aimed to experiment with adapting computational notebooks into VR and verify the potential benefits VR can bring. We focus on the navigation and comparison aspects as they are primitive components in analysts’ workflow. To further improve comparison, we have designed and implemented a Branching&amp;Merging functionality. We tested computational notebooks on the desktop and in VR, both with and without the added Branching&amp;Merging capability. We found VR significantly facilitated navigation compared to desktop, and the ability to create branches enhanced comparison.",immersive analytics; data science; interaction; 3D UI &amp; computational notebook system,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Virtual Unreality: Augmentation-Oriented Ideation Through Design Cards,CHI - Human Factors in Computing Systems,A*,"While realism is a common design goal for virtual reality (VR), VR also offers opportunities that are impossible in the real world (e.g., telekinesis). So far, there is no design support to exploit the potential of such “impossible” augmentations, especially for serious applications. We developed a card set and a workshop format, which features 15 opportunities to facilitate the ideation of augmentation-oriented VR. We piloted the method in five workshops with people in the early stages of developing a VR application (N=35). Participants found the cards easy to use and to inspire viable new concepts that differed from earlier ideas. Analysis of the concepts with interaction criticism identified two strategies: (1) augmentations that are only loosely related to the purpose of the application, simply to increase “fun”, and (2) augmentations that are closely related to the core purpose and thereby subtly facilitate its fulfillment. The latter has the greater potential.",virtual reality; design tools; augmented human capabilities; superpowers,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,MineXR: Mining Personalized Extended Reality Interfaces,CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) interfaces offer engaging user experiences, but their effective design requires a nuanced understanding of user behavior and preferences. This knowledge is challenging to obtain without the widespread adoption of XR devices. We introduce &nbsp;MineXR, a design mining workflow and data analysis platform for collecting and analyzing personalized XR user interaction and experience data. &nbsp;MineXR enables elicitation of personalized interfaces from participants of a data collection: for any particular context, participants create interface elements using application screenshots from their own smartphone, place them in the environment, and simultaneously preview the resulting XR layout on a headset. Using &nbsp;MineXR, we contribute a dataset of personalized XR interfaces collected from 31 participants, consisting of 695 XR widgets created from 178 unique applications. We provide insights for XR widget functionalities, categories, clusters, UI element types, and placement. Our open-source tools and data support researchers and designers in developing future XR interfaces.",Extended Reality; Datasets; Personalized UI,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"UI Mobility Control in XR: Switching UI Positionings between Static, Dynamic, and Self Entities",CHI - Human Factors in Computing Systems,A*,"Extended reality (XR) has the potential for seamless user interface (UI) transitions across people, objects, and environments. However, the design space, applications, and common practices of 3D UI transitions remain underexplored. To address this gap, we conducted a need-finding study with 11 participants, identifying and distilling a taxonomy based on three types of UI placements — affixed to static, dynamic, or self entities. We further surveyed 113 commercial applications to understand the common practices of 3D UI mobility control, where only 6.2% of these applications allowed users to transition UI between entities. In response, we built interaction prototypes to facilitate UI transitions between entities. We report on results from a qualitative user study (N=14) on 3D UI mobility control using our FingerSwitches technique, which suggests that perceived usefulness is affected by types of entities and environments. We aspire to tackle a vital need in UI mobility within XR.",Virtual Reality; Extended Reality; Hand Gestures; Embodied Interactions; Mode Switching; UI Mobility; User Interface Behaviors,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,On the Role of Materials Experience for Novel Interactions with Digital Representations of Historical Pop-up and Movable Books,CHI - Human Factors in Computing Systems,A*,"Direct interaction with cultural heritage (CH) artefacts is frequently unavailable to visitors, offering an opportunity for HCI designers to explore integrating material aspects into digitally-mediated encounters with CH artefacts. We argue that a thorough understanding of the material experiences of CH artefacts can open a novel design space, enabling engaging and meaningful interactions with digital representations. Capitalising on this potential, we present a user study where we systematically explore the material experiences of historic pop-up and movable books. Our analysis identifies five key material qualities to inspire augmentation: fold-ability, slide-ability, tear-ability, age-ability, and print-ability. Highlighting how these material qualities can inspire novel interactions with their digital representations, we present two extended-reality (XR) prototypes of a CH book. With our work, we present HCI designers with a novel approach on designing CH experiences, firmly rooted in materiality, challenging the prevalent paradigms of ‘technology-driven’ or ‘as-realistic-as-possible’ sensory experiences often found in CH-HCI.",Mixed Reality; Virtual Reality; Materiality; Cultural Heritage; Books; Materials Experience,Keywords,TRUE,
ACM DL,conferencePaper,2024,Effects of Device Environment and Information Layout on Spatial Memory and Performance in VR Selection Tasks,CHI - Human Factors in Computing Systems,A*,"Virtual Reality systems are increasingly proposed as a platform for everyday interactive software. Many applications are dependent on actions such as navigation and selection, but it is not clear how well immersive environments support these basic activities. Previous studies have suggested advantages for spatial learning in VR, so we carried out a study that investigated two aspects of immersion on spatial memory and selection: the degree to which the user is immersed in the data, and whether the system uses immersive input and output. The study showed that more-immersive conditions had substantially worse selection performance, and did not improve spatial learning. However, most participants believed that the immersive conditions were better for learning object locations, and most people preferred the immersive layout and the HMD. Our study suggests that designers should be cautious about assuming that everyday software applications will benefit from being deployed in an immersive VR environment.",virtual reality; spatial memory; immersiveness; selection performance,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Rowing Beyond: Investigating Steering Methods for Rowing-based Locomotion in Virtual Environments,CHI - Human Factors in Computing Systems,A*,"Rowing has great potential in Virtual Reality (VR) exergames as it requires physical effort and uses physical motion to map the locomotion in a virtual space. However, rowing in VR is currently restricted to locomotion along one axis, leaving 2D and 3D locomotion out of the scope. To facilitate rowing-based locomotion, we implemented three steering techniques based on head, hands, and feet movements for 2D and 3D VR environments. To investigate these methods, we conducted a controlled experiment (N = 24) to assess the user performance, experience and VR sickness. We found that head steering leads to fast and precise steering in 2D and 3D, and hand steering is the most realistic. Feet steering had the largest performance difference between 2D and 3D but comparable precision to hands in 2D. Lastly, head steering is the least mentally demanding, and all methods had comparable VR sickness.",virtual reality; locomotion; exergame; steering; rowing,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Sicknificant Steps: A Systematic Review and Meta-analysis of VR Sickness in Walking-based Locomotion for Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Walking-based locomotion techniques in virtual reality (VR) can use redirection to enable walking in a virtual environment larger than the physical one. This results in a mismatch between the perceived virtual and physical movement, which is known to cause VR sickness. However, it is unclear if different types of walking techniques (e.g., resetting, reorientation, or self-overlapping spaces) affect VR sickness differently. To address this, we conducted a systematic review and meta-analysis of 96 papers published in 2016–2022 that measure VR sickness in walking-based locomotion. We find different VR sickness effects between types of redirection and between normal walking and redirection. However, we also identified several problems with the use and reporting of VR sickness measures. We discuss the challenges in understanding VR sickness differences between walking techniques and present guidelines for measuring VR sickness in locomotion studies.",locomotion; walking; virtual; vr; reality; sickness; ssq; vrise,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Spatial Gaze Markers: Supporting Effective Task Switching in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Task switching can occur frequently in daily routines with physical activity. In this paper, we introduce Spatial Gaze Markers, an augmented reality tool to support users in immediately returning to the last point of interest after an attention shift. The tool is task-agnostic, using only eye-tracking information to infer distinct points of visual attention and to mark the corresponding area in the physical environment. We present a user study that evaluates the effectiveness of Spatial Gaze Markers in simulated physical repair and inspection tasks against a no-marker baseline. The results give insights into how Spatial Gaze Markers affect user performance, task load, and experience of users with varying levels of task type and distractions. Our work is relevant to assist physical workers with simple AR techniques and render task switching faster with less effort.",augmented reality; gaze interaction; eye-tracking; task switching; attention switching,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,The RayHand Navigation: A Virtual Navigation Method with Relative Position between Hand and Gaze-Ray,CHI - Human Factors in Computing Systems,A*,"In this paper, we introduce a novel Virtual Reality (VR) navigation method using gaze ray and hand, named RayHand navigation. It supports controlling navigation speed and direction by quickly indicating the initial direction using gaze and then using dexterous hand movement for controlling the speed and direction based on the relative position between the gaze ray and user's hand. We conducted a user study comparing our approach to the head-hand and torso-leaning-based navigation methods, and also evaluated their learning effect. The results showed that the RayHand and head-hand navigations were less physically demanding than the torso-leaning navigation, and the RayHand supported rich navigation experience with high hedonic quality and solved the issue of the user unintentionally stepping out from the designated interaction area. In addition, our approach showed a significant improvement over time with a learning effect.",virtual reality; navigation; gaze-ray,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Assessing the Influence of Visual Cues in Virtual Reality on the Spatial Perception of Physical Thermal Stimuli,CHI - Human Factors in Computing Systems,A*,"Advancements in haptics for Virtual Reality (VR) increased the quality of immersive content. Particularly, recent efforts to provide realistic temperature sensations have gained traction, but most often require very specialized or large complex devices to create precise thermal actuations. However, being largely detached from the real world, such a precise correspondence between the physical location of thermal stimuli and the shown visuals in VR might not be necessary for an authentic experience. In this work, we contribute the findings of a controlled experiment with 20 participants, investigating the spatial localization accuracy of thermal stimuli while having matching and non-matching visual cues of a virtual heat source in VR. Although participants were highly confident in their localization decisions, their ability to accurately pinpoint thermal stimuli was notably deficient.",virtual reality; user study; haptic feedback; temperature; thermal stimuli,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring Visualizations for Precisely Guiding Bare Hand Gestures in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Bare hand interaction in augmented or virtual reality (AR/VR) systems, while intuitive, often results in errors and frustration. However, existing methods, such as a static icon or a dynamic tutorial, can only inform simple and coarse hand gestures and lack corrective feedback. This paper explores various visualizations for enhancing precise hand interaction in VR. Through a comprehensive two-part formative study with 11 participants, we identified four types of essential information for visual guidance and designed different visualizations that manifest these information types. We further distilled four visual designs and conducted a controlled lab study with 15 participants to assess their effectiveness for various single- and double-handed gestures. Our results demonstrate that visual guidance significantly improved users’ gesture performance, reducing time and workload while increasing confidence. Moreover, we found that the visualization did not disrupt most users’ immersive VR experience or their perceptions of hand tracking and gesture recognition reliability.",Virtual reality; visual guidance; error visualization; hand gesture recognition.,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Improving Electromyographic Muscle Response Times through Visual and Tactile Prior Stimulation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Electromyography (EMG) enables hands-free interactions by detecting muscle activity at different human body locations. Previous studies have demonstrated that input performance based on isometric contractions is muscle-dependent and can benefit from synchronous biofeedback. However, it remains unknown whether stimulation before interaction can help to localize and tense a muscle faster. In a response-based VR experiment (N=21), we investigated whether prior stimulation using visual or tactile cues at four different target muscles (biceps, triceps, upper leg, calf) can help reduce the time to perform isometric muscle contractions. The results show that prior stimulation decreases EMG reaction times with visual, vibrotactile, and electrotactile cues. Our experiment also revealed important findings regarding learning and fatigue at the different body locations. We provide qualitative insights into the participants’ perceptions and discuss potential reasons for the improved interaction. We contribute with implications and use cases for prior stimulated muscle activation.",Virtual Reality; Electromyography; Physiological Sensing; Electrical Muscle Stimulation; Assistive Systems,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,PhoneInVR: An Evaluation of Spatial Anchoring and Interaction Techniques for Smartphone Usage in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"When users wear a virtual reality (VR) headset, they lose access to their smartphone and accompanying apps. Past work has proposed smartphones as enhanced VR controllers, but little work has explored using existing smartphone apps and performing traditional smartphone interactions while in VR. In this paper, we consider three potential spatial anchorings for rendering smartphones in VR: On top of a tracked physical smartphone which the user holds (Phone-locked), on top of the user’s empty hand, as if holding a virtual smartphone (Hand-locked), or in a static position in front of the user (World-locked). We conducted a comparative study of target acquisition, swiping, and scrolling tasks across these anchorings using direct Touch or above-the-surface Pinch. Our findings indicate that physically holding a smartphone with Touch improves accuracy and speed for all tasks, and Pinch performed better with virtual smartphones. These findings provide a valuable foundation to enable smartphones in VR.",Virtual Reality; Smartphones; Touch Input,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,ClassMeta: Designing Interactive Virtual Classmate to Promote VR Classroom Participation,CHI - Human Factors in Computing Systems,A*,"Peer influence plays a crucial role in promoting classroom participation, where behaviors from active students can contribute to a collective classroom learning experience. However, the presence of these active students depends on several conditions and is not consistently available across all circumstances. Recently, Large Language Models (LLMs) such as GPT have demonstrated the ability to simulate diverse human behaviors convincingly due to their capacity to generate contextually coherent responses based on their role settings. Inspired by this advancement in technology, we designed ClassMeta, a GPT-4 powered agent to help promote classroom participation by playing the role of an active student. These agents, which are embodied as 3D avatars in virtual reality, interact with actual instructors and students with both spoken language and body gestures. We conducted a comparative study to investigate the potential of ClassMeta for improving the overall learning experience of the class.",collaborative learning; pedagogical agent; large language Model; VR classroom,Abstract,TRUE,
ACM DL,conferencePaper,2024,Simulator-based Mixed Reality eVTOL Pilot Training: The Instructor Operator Station,CHI - Human Factors in Computing Systems,A*,"Advanced Air Mobility aircraft designs following the Simplified Vehicle Operations (SVO) concept require novel environments for practical and intuitive pilot training. Mixed Reality (MR) technologies can support immersive and interactive learning methods for operating several SVO aircraft, including electric Vertical Take-Off and Landing (eVTOL) systems. Despite this potential, regulatory guidelines for simulator-based eVTOL pilot training, especially concerning the Instructor Operator Station (IOS) design, are nascent and require substantive development. This paper investigates the feasibility of an MR eVTOL research simulator as a training tool for instructors. A user study forms the basis for a bottom-up categorization of the instructor’s performance shaping factors, which are pivotal for the design of an MR IOS. This paper contributes to the discourse on MR integration in pilot training by identifying key enhancements necessary for an IOS design.",virtual reality; mixed reality; aviation; cockpit interaction; flight simulator; flight training; human performance shaping factors,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Virtual Reality, Real Pedagogy: A Contextual Inquiry of Instructor Practices with VR Video",CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers promise in education given its immersive and socially engaging nature, but it can pose challenges for educators when creating VR-specific content. VR videos can function as a new educational tool for VR content creation due to their creation affordability and user-friendliness. However, little empirical research exists on how educators utilize VR videos and associated pedagogy in real classes. Our research employed a contextual inquiry, through in-person interviews and online surveys with 11 instructors to gain actionable insights from envisioned teaching scenarios for VR videos that are informed by actual instructional practices. Our study aims to understand the factors that motivate instructors’ adoption of VR videos, identify challenges educators face when incorporating VR videos into instructional units, and examine pedagogical adjustments when integrating VR videos into teaching. Through empirical evidence, we provide design implications for the development of VR-based learning experiences across diverse educational contexts. Our study also serves as a practical case of how VR can be adopted and integrated into education.",Virtual Reality; higher education; educational VR; 360-degree videos; VR videos,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,“Oh My God! It’s Recreating Our Room!” Understanding Children’s Experiences with A Room-Scale Augmented Reality Authoring Toolkit,CHI - Human Factors in Computing Systems,A*,"Human-Computer Interaction (HCI) and education researchers have applied Augmented Reality (AR) to support spatial thinking in K-12 education. However, fewer studies support spatial thinking through spatial exploration. Room-scale AR, a recent technology development, brings new opportunities not yet researched. We developed NetLogo AR, an AR authoring toolkit, that allows children to play with, design, and create room-scale AR experiences that combine AR with computational models. To acquire a deeper and more nuanced understanding of children’s interactions with this new technology, we conducted eight-week participatory design sessions with seven children aged 11-13. We analyzed 48 hours of video data, interview transcripts, and design artifacts. Children were enthusiastic and engaged in spatial thinking activities. We affirmed room-scale AR’s role in spatial exploration by comparing it with other supported modalities. Building on existing studies, we propose a new AR design framework around spatial movement and exploration that could help inform design decisions.",Augmented Reality; Participatory Design; AR and Children; AR Authoring Toolkit; NetLogo AR; Spatial AR,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Designing Instructions using Self-Determination Theory to Improve Motivation and Engagement for Learning Craft,CHI - Human Factors in Computing Systems,A*,"Recent HCI research has shown significant interest in investigating digital working instructions for guiding novices to perform manual tasks. While performance enhancement has been a primary focus, it is increasingly recognized that technology’s impact extends beyond objective metrics. Trainee motivation and engagement plays a pivotal role in enhancing learning outcomes and effectiveness. This paper investigates the utilization of principles from Self Determination Theory–clear attainable goals, meaningful rationale, and perspective taking–in designing multimedia instructions to enhance novice users’ indicators of psychological well-being. We present findings from an experiment involving real-world woodworking, where novice users, in a between-subjects study, followed interactive, in-situ projection-based guidance. Results demonstrate that adhering to SDT postulates can positively influence perceived competence, intrinsic motivation and task execution quality. These findings offer valuable insights for designing digital instructions to guide and train novices, emphasizing the importance of psychological well-being alongside task performance.",augmented reality; intrinsic motivation; instructions; perceived self–competence; self–determination theory; well–being,Keywords,TRUE,
ACM DL,conferencePaper,2024,Doorways Do Not Always Cause Forgetting: Studying the Effect of Locomotion Technique and Doorway Visualization in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The “doorway effect” predicts that crossing an environmental boundary affects memory negatively. In virtual reality (VR), we can design the crossing and the appearance of such boundaries in non-realistic ways. However, it is unclear whether locomotion techniques like teleportation, which avoid crossing the boundary altogether, still induce the effect. Furthermore, it is unclear how different appearances of a doorway act as a boundary and thus induce the effect. To address these questions, we conducted two lab studies. First, we conceptually replicated prior doorway effect studies in VR using natural walking and teleportation. Second, we investigated the effect of five doorway visualizations, ranging from doors to portals. The results show no difference in object recognition performance due to the presence of a doorway, locomotion technique, or doorway visualization. We discuss the implications of these findings on the role of boundaries in event-based memory and the design of boundary interactions in VR.",recognition; forgetting; memory; walking; virtual; vr; environment; event; teleportation; object; reality; boundary; doorway; effect,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Investigating Virtual Reality Locomotion Techniques with Blind People,CHI - Human Factors in Computing Systems,A*,"Many Virtual Reality (VR) locomotion techniques have been proposed, but those explored for and with blind people are often custom-made or require specialized equipment. Consequently, it is unclear how popular techniques can support blind people’s VR locomotion, blocking access to most VR experiences. We implemented three popular techniques — Arm Swinging, Linear Movement (joystick-based steering), and Point &amp; Teleport — with minor adaptations for accessibility. We conducted a study with 14 blind participants consisting of navigation tasks with these techniques and a semi-structured interview. We found no differences in overall performance (e.g., completion time), but contrasting preferences. Findings highlight the challenges and advantages of each technique and participants’ strategies. We discuss, among others, how augmenting the techniques enabled blind people to navigate in VR, the greater control of movement of Arm Swinging, the simplicity and familiarity of Linear Movement, and the potential for efficiency and for scanning the environment of Point &amp; Teleport.",Navigation; Visual Impairment; Virtual Reality.; Arm Swinging; Linear Movement; Point and Teleport,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Stacked Retargeting: Combining Redirected Walking and Hand Redirection to Expand Haptic Retargeting's Coverage,CHI - Human Factors in Computing Systems,A*,"We present Stacked Retargeting—combining haptic retargeting and redirected walking—to maximise the use of passive proxy objects for VR haptics. Haptic retargeting work to date has considered stationary reaching and grasping interactions, and this inherently limits a proxy object’s scope. We consider exactly where this reaching and grasping occurs from, to increase the potential of each proxy. We present (a) a staged approach to implementing Stacked Retargeting, (b) five redirected walking approaches that enable users to arrive anywhere at the site of interaction, and (c) a usability magnitude estimation evaluation of these techniques. We demonstrate how Stacked Retargeting can meaningfully increase the practical use of proxy objects for VR haptics without degrading the user experience.",Virtual Reality; Haptics; Redirection,Keywords,TRUE,
ACM DL,conferencePaper,2024,The Effect of Spatial Audio on Curvature Gains in VR Redirected Walking,CHI - Human Factors in Computing Systems,A*,"Redirected walking (RDW) is a technique that allows users to navigate larger physical spaces in virtual reality (VR) environments by manipulating the users’ view of the virtual world. In this study, we investigate the effect of adding spatial audio elements to curvature gains in RDW aiming to increase the perceptual threshold for the manipulation and allowing for higher levels of unnoticed redirection. We conducted a user study (n = 18), evaluating perceptual thresholds across conditions with and without spatial audio elements across different curvature gains. We found that spatial audio can significantly increase thresholds with a large effect size. This finding indicates the value of spatial audio for RDW. It could facilitate higher levels of redirection, while maintaining a convincing experience, leading to more freedom to navigate virtual environments in even smaller physical spaces.",virtual reality; spatial audio; redirected walking; curvature gains,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,'I Call Upon a Friend': Virtual Reality-Based Supports for Cognitive Reappraisal Identified through Co-designing with Adolescents,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers great promise to expand delivery models for therapeutic interventions to help adolescents develop adaptive emotion regulation skills. Cognitive reappraisal (CR) is an emotion regulation skill that involves changing your thinking to improve your emotional state. However, adolescents face developmental and implementation barriers to do CR successfully. To better understand adolescents’ (15-18 years) lived experience of CR challenges and how they envision VR could support their skills learning and transfer to everyday life, we ran three co-design workshops (N=69). Our research weaves together the workshop findings with prior literature to identify directions for future VR-based CR interventions. From our study results, we generated design strategies leveraging best practices of existing research: embedded and embodied scaffolds, providing different points of view, and externalizing the inner self. To illustrate these strategies in practice, we show how each would work in a challenging emotional scenario identified by adolescents.",participatory design; co-design; mental health; adolescents; emotion regulation; cognitive reappraisal,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Stairway to Heaven: A Gamified VR Journey for Breath Awareness,CHI - Human Factors in Computing Systems,A*,"Gamification and virtual reality (VR) are increasingly being explored for their potential to enhance mindful practices and well-being. We further explore the potential of gamification and VR for breath awareness and mindfulness, and contribute Stairway to Heaven, a VR artifact that combines gamification with respiratory sensor biofeedback to cultivate mindful awareness of breathing. In our mixed-method study with 21 participants, we evaluated the usability and effectiveness of our artifact in promoting breathing frequencies between 4 and 10 breaths per minute (BPM). We integrate breath-driven teleportation as a virtual locomotion technique (VLT) using respiratory biofeedback to gamify progression through a virtual wilderness. Additionally, we supplement our design with a mindfulness audio guide. The results of our user study showcase the potential of combining actionable gamification and VR, guided mindfulness, and breath-driven VLT to foster slow breathing self-regulation successfully.",Virtual Reality; Gamification; Mindfulness; Breathing Training; Respiration Sensor,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Designing for Human Operations on the Moon: Challenges and Opportunities of Navigational HUD Interfaces,CHI - Human Factors in Computing Systems,A*,"Future crewed missions to the Moon will face significant environmental and operational challenges, posing risks to the safety and performance of astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs) have proven effective in providing intuitive navigational support on Earth, the design of novel human-spaceflight solutions typically relies on costly and time-consuming analogue deployments, leaving the potential use of lunar HUDs largely under-explored. This paper explores an alternative approach by simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR) representation of the lunar environment. In evaluating these concepts with astronauts and other aerospace experts (n=25), our mixed methods study demonstrates the efficacy of simulated analogues in facilitating rapid design assessments of early-stage HUD solutions. We illustrate this by elaborating key design challenges and guidelines for future lunar HUDs. In reflecting on the limitations of our approach, we propose directions for future design exploration of human-machine interfaces for the Moon.",virtual reality; augmented reality; human factors; head-up display; astronaut; human space flight; human-system exploration; lunar exploration,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,WAVE: Anticipatory Movement Visualization for VR Dancing,CHI - Human Factors in Computing Systems,A*,"Dance games are one of the most popular game genres in Virtual Reality (VR), and active dance communities have emerged on social VR platforms such as VR Chat. However, effective instruction of dancing in VR or through other computerized means remains an unsolved human-computer interaction problem. Existing approaches either only instruct movements partially, abstracting away nuances, or require learning and memorizing symbolic notation. In contrast, we investigate how realistic, full-body movements designed by a professional choreographer can be instructed on the fly, without prior learning or memorization. Towards this end, we describe the design and evaluation of WAVE, a novel anticipatory movement visualization technique where the user joins a group of dancers performing the choreography with different time offsets, similar to spectators making waves in sports events. In our user study (N=36), the participants more accurately followed a choreography using WAVE, compared to following a single model dancer.",VR; dance game; dance instruction,Abstract,TRUE,
ACM DL,conferencePaper,2024,Watch This! Observational Learning in VR Promotes Better Far Transfer than Active Learning for a Fine Psychomotor Task,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) holds great potential for psychomotor training, with existing applications using almost exclusively a ‘learning-by-doing’ active learning approach, despite the possible benefits of incorporating observational learning. We compared active learning (n=26) with different variations of observational learning in VR for a manual assembly task. For observational learning, we considered three levels of visual similarity between the demonstrator avatar and the user, dissimilar (n=25), minimally similar (n=26), or a self-avatar (n=25), as similarity has been shown to improve learning. Our results suggest observational learning can be effective in VR when combined with ‘hands-on’ practice and can lead to better far skill transfer to real-world contexts that differ from the training context. Furthermore, we found self-similarity in observational learning can be counterproductive when focusing on a manual task, and skills decay quickly without further training. We discuss these findings and derive design recommendations for future VR training.",Virtual Reality; Psychomotor; Skills Training; Active; Avatar Similarity; Observational,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Design Space of Visual Feedforward And Corrective Feedback in XR-Based Motion Guidance Systems,CHI - Human Factors in Computing Systems,A*,"Extended reality (XR) technologies are highly suited in assisting individuals in learning motor skills and movements—referred to as motion guidance. In motion guidance, the “feedforward’’ provides instructional cues of the motions that are to be performed, whereas the “feedback’’ provides cues which help correct mistakes and minimize errors. Designing synergistic feedforward and feedback is vital to providing an effective learning experience, but this interplay between the two has not yet been adequately explored. Based on a survey of the literature, we propose design space for both motion feedforward and corrective feedback in XR, and describe the interaction effects between them. We identify common design approaches of XR-based motion guidance found in our literature corpus, and discuss them through the lens of our design dimensions. We then discuss additional contextual factors and considerations that influence this design, together with future research opportunities for motion guidance in XR.",Extended Reality; Visualization; Design Space; Motion Guidance,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Metrics of Motor Learning for Analyzing Movement Mapping in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) techniques can modify how physical body movements are mapped to the virtual body. However, it is unclear how users learn such mappings and, therefore, how the learning process may impede interaction. To understand and quantify the learning of the techniques, we design new metrics explicitly for VR interactions based on the motor learning literature. We evaluate the metrics in three object selection and manipulation tasks, employing linear-translational and nonlinear-rotational gains and finger-to-arm mapping. The study shows that the metrics demonstrate known characteristics of motor learning similar to task completion time, typically with faster initial learning followed by more gradual improvements over time. More importantly, the metrics capture learning behaviors that task completion time does not. We discuss how the metrics can provide new insights into how users adapt to movement mappings and how they can help analyze and improve such techniques.",Interaction techniques; beyond-real interactions; motor adaptation; visual-motor mismatches,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,WieldingCanvas: Interactive Sketch Canvases for Freehand Drawing in VR,CHI - Human Factors in Computing Systems,A*,"Sketching in Virtual Reality (VR) is challenging mainly due to the absence of physical surface support and virtual depth perception cues, which induce high cognitive and sensorimotor load. This paper presents WieldingCanvas, an interactive VR sketching platform that integrates canvas manipulations to draw lines and curves in 3D. Informed by real-life examples of two-handed creative activities, WieldingCanvas interprets users’ spatial gestures to move, swing, rotate, transform, or fold a virtual canvas, whereby users simply draw primitive strokes on the canvas, which are turned into finer and more sophisticated shapes via the manipulation of the canvas. We evaluated the capability and user experience of WieldingCanvas with two studies where participants were asked to sketch target shapes. A set of freehand sketches of high aesthetic qualities were created, and the results demonstrated that WieldingCanvas can assist users with creating 3D sketches.",Virtual Reality; freehand drawing; interactive canvas; two-handed interaction,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Beyond the Blink: Investigating Combined Saccadic &amp; Blink-Suppressed Hand Redirection in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"In pursuit of hand redirection techniques that are ever more tailored to human perception, we propose the first algorithm for hand redirection in virtual reality that makes use of saccades, i.e., fast ballistic eye movements that are accompanied by the perceptual phenomenon of change blindness. Our technique combines the previously proposed approaches of gradual hand warping and blink-suppressed hand redirection with the novel approach of saccadic redirection in one unified yet simple algorithm. We compare three variants of the proposed Saccadic &amp; Blink-Suppressed Hand Redirection (SBHR) technique with the conventional approach to redirection in a psychophysical study (N = 25). Our results highlight the great potential of our proposed technique for comfortable redirection by showing that SBHR allows for significantly greater magnitudes of unnoticeable redirection while being perceived as significantly less intrusive and less noticeable than commonly employed techniques that only use gradual hand warping.",virtual reality; change blindness; eye blinks; saccades; detection thresholds; hand redirection,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Big or Small, It’s All in Your Head: Visuo-Haptic Illusion of Size-Change Using Finger-Repositioning",CHI - Human Factors in Computing Systems,A*,"Haptic perception of physical sizes increases the realism and immersion in Virtual Reality (VR). Prior work rendered sizes by exerting pressure on the user’s fingertips or employing tangible, shape-changing devices. These interfaces are constrained by the physical shapes they can assume, making it challenging to simulate objects growing larger or smaller than the perceived size of the interface. Motivated by literature on pseudo-haptics describing the strong influence of visuals over haptic perception, this work investigates modulating the perception of size beyond this range. We developed a fixed-sized VR controller leveraging finger-repositioning to create a visuo-haptic illusion of dynamic size-change of handheld virtual objects. Through two user studies, we found that with an accompanying size-changing visual context, users can perceive virtual object sizes up to &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX 44.2%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name=""chi24-364-inline1"" type=""svg""/&gt;&lt;/Formula&gt; smaller to &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX 160.4%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 2&lt;/AltText&gt;&lt;File name=""chi24-364-inline2"" type=""svg""/&gt;&lt;/Formula&gt; larger than the perceived size of the device. Without the accompanying visuals, a constant size (&lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX 141.4%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 3&lt;/AltText&gt;&lt;File name=""chi24-364-inline3"" type=""svg""/&gt;&lt;/Formula&gt; of device size) was perceived.",pseudo-haptics; cross-modal integration; perceptual illusion; visuo-haptic perception,Abstract,TRUE,
ACM DL,conferencePaper,2024,Flicker Augmentations: Rapid Brightness Modulation for Real-World Visual Guidance using Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Providing attention guidance, such as assisting in search tasks, is a prominent use for Augmented Reality. Typically, this is achieved by graphically overlaying geometrical shapes such as arrows. However, providing visual guidance can cause side effects such as attention tunnelling or scene occlusions, and introduce additional visual clutter. Alternatively, visual guidance can adjust saliency but this comes with different challenges such as hardware requirements and environment dependent parameters. In this work we advocate for using flicker as an alternative for real-world guidance using Augmented Reality. We provide evidence for the effectiveness of flicker from two user studies. The first compared flicker against alternative approaches in a highly controlled setting, demonstrating efficacy (N = 28). The second investigated flicker in a practical task, demonstrating feasibility with higher ecological validity (N = 20). Finally, our discussion highlights the opportunities and challenges when using flicker to provide real-world visual guidance using Augmented Reality.",augmented reality; eye tracking; gaze; visual guidance; flicker,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,STMG: A Machine Learning Microgesture Recognition System for Supporting Thumb-Based VR/AR Input,CHI - Human Factors in Computing Systems,A*,"AR/VR devices have started to adopt hand tracking, in lieu of controllers, to support user interaction. However, today’s hand input rely primarily on one gesture: pinch. Moreover, current mappings of hand motion to use cases like VR locomotion and content scrolling involve more complex and larger arm motions than joystick or trackpad usage. STMG increases the gesture space by recognizing additional small thumb-based microgestures from skeletal tracking running on a headset. We take a machine learning approach and achieve a 95.1% recognition accuracy across seven thumb gestures performed on the index finger surface: four directional thumb swipes (left, right, forward, backward), thumb tap, and fingertip pinch start and pinch end. We detail the components to our machine learning pipeline and highlight our design decisions and lessons learned in producing a well generalized model. We then demonstrate how these microgestures simplify and reduce arm motions for hand-based locomotion and scrolling interactions.",virtual reality; machine learning; augmented reality; neural networks; microgestures,Keywords,TRUE,
ACM DL,conferencePaper,2024,TriPad: Touch Input in AR on Ordinary Surfaces with Hand Tracking Only,CHI - Human Factors in Computing Systems,A*,"TriPad enables opportunistic touch interaction in Augmented Reality using hand tracking only. Users declare the surface they want to appropriate with a simple hand tap gesture. They can then use this surface at will for direct and indirect touch input. TriPad only involves analyzing hand movements and postures, without the need for additional instrumentation, scene understanding or machine learning. TriPad thus works on a variety of flat surfaces, including glass. It also ensures low computational overhead on devices that typically have a limited power budget. We describe the approach, and report on two user studies. The first study demonstrates the robustness of TriPad’s hand movement interpreter on different surface materials. The second study compares TriPad against direct mid-air AR input techniques on both discrete and continuous tasks and with different surface orientations. TriPad achieves a better speed-accuracy trade-off overall, improves comfort and minimizes fatigue.",augmented reality; touch input; passive surfaces,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Sweating the Details: Emotion Recognition and the Influence of Physical Exertion in Virtual Reality Exergaming,CHI - Human Factors in Computing Systems,A*,"There is great potential for adapting Virtual Reality (VR) exergames based on a user’s affective state. However, physical activity and VR interfere with physiological sensors, making affect recognition challenging. We conducted a study (n=72) in which users experienced four emotion inducing VR exergaming environments (happiness, sadness, stress and calmness) at three different levels of exertion (low, medium, high). We collected physiological measures through pupillometry, electrodermal activity, heart rate, and facial tracking, as well as subjective affect ratings. Our validated virtual environments, data, and analyses are openly available. We found that the level of exertion influences the way affect can be recognised, as well as affect itself. Furthermore, our results highlight the importance of data cleaning to account for environmental and interpersonal factors interfering with physiological measures. The results shed light on the relationships between physiological measures and affective states and inform design choices about sensors and data cleaning approaches for affective VR.",virtual reality; emotion recognition; physiological sensing; affect recognition; psychophysiological correlates; exergaming; high-intensity exercise,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,What You Experience is What We Collect: User Experience Based Fine-Grained Permissions for Everyday Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Everyday Augmented Reality (AR) headsets pose significant privacy risks, potentially allowing prolonged sensitive data collection of both users and bystanders (e.g. members of the public). While users control data access through permissions, current AR systems inherit smartphone permission prompts, which may be less appropriate for all-day AR. This constrains informed choices and risks over-privileged access to sensors. We propose (N=20) a novel AR permission control system that allows better-informed privacy decisions and evaluate it using five mock application contexts. Our system’s novelty lies in enabling users to experience the varying impacts of permission levels on not only a) privacy, but also b) application functionality. This empowers users to better understand what data an application depends on and how its functionalities are impacted by limiting said data. Participants found that our method allows for making better informed privacy decisions, and deemed it more transparent and trustworthy than state-of-the-art AR and smartphone permission systems taken from Android and iOS. Our results offer insights into new and necessary AR permission systems, improving user understanding and control over data access.",Augmented Reality; Privacy; AR sensing,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Kinetic Signatures: A Systematic Investigation of Movement-Based User Identification in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Behavioral Biometrics in Virtual Reality (VR) enable implicit user identification by leveraging the motion data of users’ heads and hands from their interactions in VR. This spatiotemporal data forms a Kinetic Signature, which is a user-dependent behavioral biometric trait. Although kinetic signatures have been widely used in recent research, the factors contributing to their degree of identifiability remain mostly unexplored. Drawing from existing literature, this work systematically examines the influence of static and dynamic components in human motion. We conducted a user study (N&nbsp;=&nbsp;24) with two sessions to reidentify users across different VR sports and exercises after one week. We found that the identifiability of a kinetic signature depends on its inherent static and dynamic factors, with the best combination allowing for 90.91% identification accuracy after one week had passed. Therefore, this work lays a foundation for designing and refining movement-based identification protocols in immersive environments.",virtual reality; usable security; identification; task-driven biometrics; kinetic signatures,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Privacy in Immersive Extended Reality: Exploring User Perceptions, Concerns, and Coping Strategies",CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) technology is changing online interactions, but its granular data collection sensors may be more invasive to user privacy than web, mobile, and the Internet of Things technologies. Despite an increased interest in studying developers’ concerns about XR device privacy, user perceptions have rarely been addressed. We surveyed 464 XR users to assess their awareness, concerns, and coping strategies around XR data in 18 scenarios. Our findings demonstrate that many factors, such as data types and sensitivity, affect users’ perceptions of privacy in XR. However, users’ limited awareness of XR sensors’ granular data collection capabilities, such as involuntary body signals of emotional responses, restricted the range of privacy-protective strategies they used. Our results highlight a need to enhance users’ awareness of data privacy threats in XR, design privacy-choice interfaces tailored to XR environments, and develop transparent XR data practices.",Augmented Reality; Mixed Reality; Virtual Reality; Extended Reality; User privacy; Privacy Perception; Privacy-Seeking Strategies,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Socially Late, Virtually Present: The Effects of Transforming Asynchronous Social Interactions in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Social Virtual Reality (VR) typically entails users interacting in real time. However, asynchronous Social VR presents the possibility of combining the convenience of asynchronous communication with the high presence of VR. Because the tools to easily record and replay VR social interactions are fairly new, scholars have not yet examined how users perceive asynchronous VR social interactions, and how nonverbal transformations of recorded interactions influence user behavior. In this work, we study nonverbal transformations of group interactions around proxemics and gaze and present results from an exploratory user study (N=128) investigating their effects. We found that the combination of spatial accommodation and added gaze increases social presence, perceived attention, and mutual gaze. Results also showed an inverse relationship between interpersonal distance and perceived levels of dominance and threat of the recorded group. Finally, we outline implications for educators and virtual meeting organizers to incorporate these transformations into real-world scenarios.",Virtual Reality; Social Interaction; Proxemics; Eye Gaze; Transformed Social Interaction,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,The Effects of False but Stable Heart Rate Feedback on Cybersickness and User Experience in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) offers a compelling and immersive experience; however, cybersickness (or VR sickness) stands as a significant obstacle to its widespread adoption. When a user experiences cybersickness, one’s physical condition deteriorates with various symptoms, often accompanied by an increased and destabilized heart rate and even altered perception of one’s state. In this paper, we propose to provide “False but Stable Heart rate (FSH)” feedback through auditory and vibrotactile stimulation to reversely induce a stably perceived heart rate and, thereby, alleviate cybersickness while navigating a sickness-inducing VR content. The validation of the human experiment confirmed the intended effect in a statistically significant way. Furthermore, it was found that the lesser compatible FSH feedback had a more substantial sickness reduction effect but distracted the user with the reduced immersive experience. The compatible FSH feedback still showed moderate sickness reduction with the maintained sense of presence and immersion.",Cybersickness; Cognitive Distraction; False Heart Rate,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Was it Real or Virtual? Confirming the Occurrence and Explaining Causes of Memory Source Confusion between Reality and Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Source confusion occurs when individuals attribute a memory to the wrong source (e.g., confusing a picture with an experienced event). Virtual Reality (VR) represents a new source of memories particularly prone to being confused with reality. While previous research identified causes of source confusion between reality and other sources (e.g., imagination, pictures), there is currently no understanding of what characteristics specific to VR (e.g., immersion, presence) could influence source confusion. Through a laboratory study (n=29), we 1) confirm the existence of VR source confusion with current technology, and 2) present a quantitative and qualitative exploration of factors influencing VR source confusion. Building on the Source Monitoring Framework, we identify VR characteristics and assumptions about VR capabilities (e.g., poor rendering) that are used to distinguish virtual from real memories. From these insights, we reflect on how the increasing realism of VR could leave users vulnerable to memory errors and perceptual manipulations.",Virtual Reality; Memory; Source Confusion; Source Misattribution,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,“I’d rather drink in VRChat”: Understanding Drinking in Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Drinking in social VR has become popular, yet little is known about how users perceive and experience alcohol consumption while immersed in virtual spaces with others, as well as its potential harm and negative effects on their offline and online lives. To better understand this emerging phenomenon from the perspective of both drinkers and non-drinkers, we analyzed public discussions from the r/VRchat online community on users’ perceptions, and experiences with alcohol consumption in social VR. Heavy drinking is prevalent. We find that VR drinkers feel less intoxicated, which makes them drink more without being aware of it. Anti-cybersickness designs may affect users’ perception of vertigo, even if the vertigo is not caused by VR. We discuss how affordances that support meaningful activities (i.e., sense of presence, embodiment, and social interactions) exacerbate alcohol abuse. We propose implications for the design of safer social VR experiences for both drinkers and non-drinkers.",Virtual Reality; social VR; social interaction; alcohol intoxication; drinking,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,Blended Whiteboard: Physicality and Reconfigurability in Remote Mixed Reality Collaboration,CHI - Human Factors in Computing Systems,A*,"The whiteboard is essential for collaborative work. To preserve its physicality in remote collaboration, Mixed Reality (MR) can blend real whiteboards across distributed spaces. Going beyond reality, MR can further enable interactions like panning and zooming in a virtually reconfigurable infinite whiteboard. However, this reconfigurability conflicts with the sense of physicality. To address this tension, we introduce Blended Whiteboard, a remote collaborative MR system enabling reconfigurable surface blending across distributed physical whiteboards. Blended Whiteboard supports a unique collaboration style, where users can sketch on their local whiteboards but also reconfigure the blended space to facilitate transitions between loosely and tightly coupled work. We describe design principles inspired by proxemics; supporting users in changing between facing each other and being side-by-side, and switching between navigating the whiteboard synchronously and independently. Our work shows exciting benefits and challenges of combining physicality and reconfigurability in the design of distributed MR whiteboards.",mixed reality; avatars; proxemics; remote collaboration; f-formations; 3C collaboration model,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,From Real to Virtual: Exploring Replica-Enhanced Environment Transitions along the Reality-Virtuality Continuum,CHI - Human Factors in Computing Systems,A*,"Recent Head-Mounted Displays enable users to perceive the real environment using a video-based see-through mode and the fully virtual environment within a single display. Leveraging these advancements, we present a generic concept to seamlessly transition between the real and virtual environment, with the goal of supporting users in engaging with and disengaging from any real environment into Virtual Reality. This transition process uses a digital replica of the real environment and incorporates various stages of Milgram’s Reality-Virtuality Continuum, along with visual transitions that facilitate gradual navigation between them. We implemented the overall transition concept and four object-based transition techniques. The overall transition concept and four techniques were evaluated in a qualitative user study, focusing on user experience, the use of the replica and visual coherence. The results of the user study show, that most participants stated that the replica facilitates the cognitive processing of the transition and supports spatial orientation.",Augmented Reality; Virtual Reality; User Study; Cross-Reality; Transitions; Augmented Virtuality; Replica; Visual Coherence,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,SwitchSpace: Understanding Context-Aware Peeking Between VR and Desktop Interfaces,CHI - Human Factors in Computing Systems,A*,"Cross-reality tasks, like creating or consuming virtual reality (VR) content, often involve inconvenient or distracting switches between desktop and VR. An initial formative study explores cross-reality switching habits, finding most switches are momentary “peeks” between interfaces, with specific habits determined by current context. The results inform a design space for context-aware “peeking” techniques that allow users to view or interact with desktop from VR, and vice versa, without fully switching. We implemented a set of peeking techniques and evaluated them in two levels of a cross-reality task: one requiring only viewing, and another requiring input and viewing. Peeking techniques made task completion faster, with increased input accuracy and reduced perceived workload.",Virtual Reality; interaction techniques; transitional interfaces; controlled experiments,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Volumetric Hybrid Workspaces: Interactions with Objects in Remote and Co-located Telepresence,CHI - Human Factors in Computing Systems,A*,"Volumetric telepresence aims to create a shared space, allowing people in local and remote settings to collaborate seamlessly. Prior telepresence examples typically have asymmetrical designs, with volumetric capture in one location and objects in one format. In this paper, we present a volumetric telepresence mixed reality system that supports real-time, symmetrical, multi-user, partially distributed interactions, using objects in multiple formats, across multiple locations. We align two volumetric environments around a common spatial feature to create a shared workspace for remote and co-located people using objects in three formats: physical, virtual, and volumetric. We conducted a study with 18 participants over 6 sessions, evaluating how telepresence workspaces support spatial coordination and hybrid communication for co-located and remote users undertaking collaborative tasks. Our findings demonstrate the successful integration of remote spaces, effective use of proxemics and deixis to support negotiation, and strategies to manage interactivity in hybrid workspaces.",mixed reality; augmented reality; collaboration; telepresence; volumetric capture; workspace awareness; partially distributed teams,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Connecting Home: Human-Centric Setup Automation in the Augmented Smart Home,CHI - Human Factors in Computing Systems,A*,"Controlling smart homes via vendor-specific apps on smartphones is cumbersome. Augmented Reality (AR) offers a promising alternative by enabling direct interactions with Internet of Things (IoT) devices. However, using AR for smart home control requires knowledge of each device’s 3D position. In this paper, we introduce and evaluate three concepts for identifying IoT device positions with varying degrees of automation. Our mixed-methods laboratory study with 28 participants revealed that, despite being recognized as the most efficient option, the majority of participants opted against a fast, fully automated detection, favoring a balance between efficiency and perceived autonomy and control. We link this decision to psychological needs grounded in self-determination theory and discuss the strengths and weaknesses of each alternative, motivating a user-adaptive solution. Additionally, we observed a “wow-effect” in response to AR interaction for smart homes, suggesting potential benefits of a human-centric approach to the smart home of the future.",Augmented Reality; Smart Home; Self-Determination Theory; Laboratory Experiment,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Engaging recently incarcerated and gang affiliated Black and Latino/a young adults in designing social collocated applications for mixed reality smart glasses through community-based participatory design workshops.,CHI - Human Factors in Computing Systems,A*,"Involving Black and Latina/o communities early and often in emerging technology design can make innovation more democratic, address bias, and reduce harm against these marginalized groups. To the best of our knowledge, no work has examined how recently incarcerated and gang affiliated young adults conceptualize mixed reality (MR) use for social collocated scenarios based on their everyday interactions and meaning-making. To explore this topic, we used a design-based implementation research (DBIR) and community-based participatory design (CBPD) approach to elicit social-technical insights grounded in the personal and critical perspectives of these youth. We find participants frequently grounded design ideas as embodied design elements to surface intangible and invisible qualities such as emotions and reflections on lived experiences, namely criticizing institutional structures that have maintained exclusionary practices against them. We discuss how DBIR and CBPD can uncover larger societal issues impacting marginalized communities through emerging technology design, and we contribute design recommendations for social collocated interactions in MR.",,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Interactive Shape Sonification for Tumor Localization in Breast Cancer Surgery,CHI - Human Factors in Computing Systems,A*,"About 20 percent of patients undergoing breast-conserving surgery require reoperation due to cancerous tissue remaining inside the breast. Breast cancer localization systems utilize auditory feedback to convey the distance between a localization probe and a small marker (seed) implanted into the breast tumor prior to surgery. However, no information on the location of the tumor margin is provided. To reduce the reoperation rate by improving the usability and accuracy of the surgical task, we developed an auditory display using shape sonification to assist with tumor margin localization. Accuracy and usability of the interactive shape sonification were determined on models of the female breast in three user studies with both breast surgeons and non-clinical participants. The comparative studies showed a significant increase in usability (p&lt;0.05) and localization accuracy (p&lt;0.001) of the shape sonification over the auditory feedback currently used in surgery.",Augmented reality; Sonification; Auditory display; Breast cancer localization; Breast cancer surgery; Computer assisted interventions; Lumpectomy; Shape sonification; Surgical sonification,Keywords,TRUE,
ACM DL,conferencePaper,2024,A Change of Scenery: Transformative Insights from Retrospective VR Embodied Perspective-Taking of Conflict With a Close Other,CHI - Human Factors in Computing Systems,A*,"Close relationships are irreplaceable social resources, yet prone to high-risk conflict. Building on findings from the fields of HCI, virtual reality, and behavioral therapy, we evaluate the unexplored potential of retrospective VR-embodied perspective-taking to fundamentally influence conflict resolution in close others. We develop a biographically-accurate Retrospective Embodied Perspective-Taking system (REPT) and conduct a mixed-methods evaluation of its influence on close others’ reflection and communication, compared to video-based reflection methods currently used in therapy (treatment as usual, or TAU). Our key findings provide evidence that REPT was able to significantly improve communication skills and positive sentiment of both partners during conflict, over TAU. The qualitative data also indicated that REPT surpassed basic perspective-taking by exclusively stimulating users to embody and reflect on both their own and their partner’s experiences at the same level. In light of these findings, we provide implications and an agenda for social embodiment in HCI design: conceptualizing the use of ‘embodied social cognition,’ and envisioning socially-embodied experiences as an interactive context.",virtual reality; communication; reflection; behavior change; perspective-taking; embodiment; empathy; mixed-methods; social cognition; close relationships; conflict,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST&nbsp;lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality.",augmented reality; large language model; text simplification,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploration of Foot-based Text Entry Techniques for Virtual Reality Environments,CHI - Human Factors in Computing Systems,A*,"Foot-based input can serve as a supplementary or alternative approach to text entry in virtual reality (VR). This work explores the feasibility and design of foot-based techniques that are hands-free. We first conducted a preliminary study to assess foot-based text entry in standing and seated positions with tap and swipe input approaches. The findings showed that foot-based text input was feasible, with the possibility for performance and usability improvements. We then developed three foot-based techniques, including two tap-based techniques (FeetSymTap and FeetAsymTap) and one swipe-based technique (FeetGestureTap), and evaluated their performance via another user study. The results show that the two tap-based techniques supported entry rates of 11.12 WPM and 10.80 WPM, while the swipe-based technique led to 9.16 WPM. Our findings provide a solid foundation for the future design and implementation of foot-based text entry in VR and have the potential to be extended to MR and AR.",virtual reality; text entry; hands-free interaction; foot-based interaction,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Fast-Forward Reality: Authoring Error-Free Context-Aware Policies with Real-Time Unit Tests in Extended Reality,CHI - Human Factors in Computing Systems,A*,"Advances in ubiquitous computing have enabled end-user authoring of context-aware policies (CAPs) that control smart devices based on specific contexts of the user and environment. However, authoring CAPs accurately and avoiding run-time errors is challenging for end-users as it is difficult to foresee CAP behaviors under complex real-world conditions. We propose Fast-Forward Reality, an Extended Reality (XR) based authoring workflow that enables end-users to iteratively author and refine CAPs by validating their behaviors via simulated unit test cases. We develop a computational approach to automatically generate test cases based on the authored CAP and the user’s context history. Our system delivers each test case with immersive visualizations in XR, facilitating users to verify the CAP behavior and identify necessary refinements. We evaluated Fast-Forward Reality in a user study (N=12). Our authoring and validation process improved the accuracy of CAPs and the users provided positive feedback on the system usability.",Extended Reality; Validation; Context-Aware Policy; Unit Test,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality,CHI - Human Factors in Computing Systems,A*,"With the proliferation of VR and a metaverse on the horizon, many multi-user activities are migrating to the VR world, calling for effective collaboration support. As one key feature, traditional collaborative systems provide users with undo mechanics to reverse errors and other unwanted changes. While undo has been extensively researched in this domain and is now considered industry standard, it is strikingly absent for VR systems in research and industry. This work addresses this research gap by exploring different undo techniques for basic object manipulation in different collaboration modes in VR. We conducted a study involving 32 participants organized in teams of two. Here, we studied users’ performance and preferences in a tower stacking task, varying the available undo techniques and their mode of collaboration. The results suggest that users desire and use undo in VR and that the choice of the undo technique impacts users’ performance and social connection.",Virtual Reality; CSCW; Undo; Connectedness; Multi-User; SocialVR,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,On the Benefits of Image-Schematic Metaphors when Designing Mixed Reality Systems,CHI - Human Factors in Computing Systems,A*,"A Mixed Reality (MR) system encompasses various aspects, such as visualization and spatial registration of user interface elements, user interactions and interaction feedback. Image-schematic metaphors (ISMs) are universal knowledge structures shared by a wide range of users. They hold a theoretical promise of facilitating greater ease of learning and use for interactive systems without costly adaptations. This paper investigates whether image-schematic metaphors (ISMs) can improve user learning, by comparing an existing MR instruction authoring system with or without ISM enhancements. In a user study with 32 participants, we found that the ISM-enhanced system significantly improved task performance, learnability and mental efficiency compared to the baseline. Participants also rated the ISM-enhanced system significantly higher in terms of perspicuity, efficiency, and novelty. These results empirically demonstrate multiple benefits of ISMs when integrated into the design of this MR system and encourage further studies to explore the wider applicability of ISMs in user interface design.",Mixed Reality; Image Schema; Instruction Authoring,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Predicting the Noticeability of Dynamic Virtual Elements in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"While Virtual Reality (VR) systems can present virtual elements such as notifications anywhere, designing them so they are not missed by or distracting to users is highly challenging for content creators. To address this challenge, we introduce a novel approach to predict the noticeability of virtual elements. It computes the visual saliency distribution of what users see, and analyzes the temporal changes of the distribution with respect to the dynamic virtual elements that are animated. The computed features serve as input for a long short-term memory (LSTM) model that predicts whether a virtual element will be noticed. Our approach is based on data collected from 24 users in different VR environments performing tasks such as watching a video or typing. We evaluate our approach (n = 12), and show that it can predict the timing of when users notice a change to a virtual element within 2.56 sec compared to a ground truth, and demonstrate the versatility of our approach with a set of applications. We believe that our predictive approach opens the path for computational design tools that assist VR content creators in creating interfaces that automatically adapt virtual elements based on noticeability.",Mixed Reality; Virtual Reality; Computational Interaction,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Towards Building Condition-Based Cross-Modality Intention-Aware Human-AI Cooperation under VR Environment,CHI - Human Factors in Computing Systems,A*,"To address critical challenges in effectively identifying user intent and forming relevant information presentations and recommendations in VR environments, we propose an innovative condition-based multi-modal human-AI cooperation framework. It highlights the intent tuples (intent, condition, intent prompt, action prompt) and 2-Large-Language-Models (2-LLMs) architecture. This design, utilizes “condition” as the core to describe tasks, dynamically match user interactions with intentions, and empower generations of various tailored multi-modal AI responses. The architecture of 2-LLMs separates the roles of intent detection and action generation, decreasing the prompt length and helping with generating appropriate responses. We implemented a VR-based intelligent furniture purchasing system based on the proposed framework and conducted a three-phase comparative user study. The results conclusively demonstrate the system’s superiority in time efficiency and accuracy, intention conveyance improvements, effective product acquisitions, and user satisfaction and cooperation preference. Our framework provides a promising approach towards personalized and efficient user experiences in VR.",Virtual Reality; Action Generation; Human-AI Cooperation; Intention Detection,Keywords,TRUE,
ACM DL,conferencePaper,2024,Screenless Interactive Tabletop Gaming with Capacitive Surface Sensing,CHI - Human Factors in Computing Systems,A*,"Many interactive systems that support tabletop games either augment the experience with additional elements or transform game components into digital counterparts, e.g., using mixed reality. However, as many users prefer tangible game elements, digital augmentations can disrupt the immersion they seek to enhance, often due to the complexity of the hardware used. Responding to this challenge, we designed a screenless interactive tabletop system with capacitive sensing. The system is suitable for novice players and provides automatic score-keeping. Our method eliminates the need for external sensors and retains all original game pieces intact. We evaluated our system in a study with a forest planting game (n = 20). Gameplay with our system exhibited shorter turn duration, and participants adopted more effective strategies than in traditional gameplay. These results underscore the potential of screenless interactive tabletops to amplify the gaming experience without causing distractions.",Machine Learning; 3D Printing; Board Games; Tangibles; Touchscreen; Capacitive Sensing,Abstract,TRUE,
ACM DL,conferencePaper,2024,Human I/O: Towards a Unified Approach to Detecting Situational Impairments,CHI - Human Factors in Computing Systems,A*,"Situationally Induced Impairments and Disabilities (SIIDs) can significantly hinder user experience in contexts such as poor lighting, noise, and multi-tasking. While prior research has introduced algorithms and systems to address these impairments, they predominantly cater to specific tasks or environments and fail to accommodate the diverse and dynamic nature of SIIDs. We introduce Human I/O, a unified approach to detecting a wide range of SIIDs by gauging the availability of human input/output channels. Leveraging egocentric vision, multimodal sensing and reasoning with large language models, Human I/O achieves a 0.22 mean absolute error and a 82% accuracy in availability prediction across 60 in-the-wild egocentric video recordings in 32 different scenarios. Furthermore, while the core focus of our work is on the detection of SIIDs rather than the creation of adaptive user interfaces, we showcase the efficacy of our prototype via a user study with 10 participants. Findings suggest that Human I/O significantly reduces effort and improves user experience in the presence of SIIDs, paving the way for more adaptive and accessible interactive systems in the future.",augmented reality; context awareness; large language models; situational impairments; multimodal sensing,Keywords,TRUE,
ACM DL,conferencePaper,2024,A Design Space for Vision Augmentations and Augmented Human Perception using Digital Eyewear,CHI - Human Factors in Computing Systems,A*,"Head-mounted displays were originally introduced to directly present computer-generated information to the human eye. More recently, the potential to use this kind of technology to support human vision and augment human perception has become actively pursued with applications such as compensating for visual impairments or aiding unimpaired vision. Unfortunately, a systematic analysis of the field is missing. Within this work, we close that gap by presenting a design space for vision augmentations that allows research to systematically explore the field of digital eyewear for vision aid and how it can augment the human visual system. We test our design space against currently available solutions and conceptually develop new solutions. The design space and findings can guide future development and can lead to a consistent categorisation of the many existing approaches.",mixed reality; augmented reality; accessibility; design space; human augmentation; visual impairments; sensory augmentation; augmented human; vision augmentation,Keywords,TRUE,
ACM DL,conferencePaper,2024,RASSAR: Room Accessibility and Safety Scanning in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"The safety and accessibility of our homes is critical to quality of life and evolves as we age, become ill, host guests, or experience life events such as having children. Researchers and health professionals have created assessment instruments such as checklists that enable homeowners and trained experts to identify and mitigate safety and access issues. With advances in computer vision, augmented reality (AR), and mobile sensors, new approaches are now possible. We introduce RASSAR, a mobile AR application for semi-automatically identifying, localizing, and visualizing indoor accessibility and safety issues such as an inaccessible table height or unsafe loose rugs using LiDAR and real-time computer vision. We present findings from three studies: a formative study with 18 participants across five stakeholder groups to inform the design of RASSAR, a technical performance evaluation across ten homes demonstrating state-of-the-art performance, and a user study with six stakeholders. We close with a discussion of future AI-based indoor accessibility assessment tools, RASSAR’s extensibility, and key application scenarios.",Augmented Reality; Accessibility; Computer Vision; Object Detection; Indoor Accessibility Auditing,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring an Extended Reality Floatation Tank Experience to Reduce the Fear of Being in Water,CHI - Human Factors in Computing Systems,A*,"People with a fear of being in water rarely engage in water activities and hence miss out on the associated health benefits. Prior research suggested virtual exposure to treat fears. However, when it comes to a fear of being in water, virtual water might not capture water’s immersive qualities, while real water can pose safety risks. We propose extended reality to combine both advantages: We conducted a study (N=12) where participants with a fear of being in water interacted with playful water-inspired virtual reality worlds while floating inside a floatation tank. Our findings, supported quantitatively by heart rate variability and qualitatively by interviews, suggest that playful extended reality could mitigate fear responses in an entertaining way. We also present insights for the design of future systems that aim to help people with a fear of being in water and other phobias by using the best of the virtual and physical worlds.",virtual reality; extended reality; exposure therapy; fear of being in water; floatation tank; flotation pod; phobia; water,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,An Iterative Participatory Design Approach to Develop Collaborative Augmented Reality Activities for Older Adults in Long-Term Care Facilities,CHI - Human Factors in Computing Systems,A*,"Over four million older adults living in long-term care (LTC) communities experience loneliness, adversely impacting their health. Increased contact with friends and family is an evidence-based intervention to reduce loneliness, but in-person visits are not always possible. Augmented Reality (AR)-based telepresence activities can offer viable alternatives with increased immersion and presence compared to video calls. However, its feasibility as an interaction technology for older adults is not known. In this paper, we detail the design of two dyadic collaborative AR activities that accommodate diminished physical and cognitive abilities of older adults. The findings include a general design framework based on an iterative participatory design focusing on preferred activities, modes of interaction, and overall AR experience of eight older adults, two family members, and five LTC staff. Results demonstrate the potential of collaborative AR as an effective means of interaction for older adults with their family, if designed to cater to their needs.",accessibility; older adults; collaborative augmented reality; iterative participatory design; long term care settings,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,"Lessons From Working in the Metaverse: Challenges, Choices, and Implications from a Case Study",CHI - Human Factors in Computing Systems,A*,"Although the metaverse workspace has the potential to solve some of the drawbacks of remote work while maintaining its benefits, there are few real-world cases of adopting the metaverse as a legitimate workspace and fewer subsequent studies on how to design and operate the metaverse workspace. Thus, questions exist about the organizational or sociotechnical challenges that may emerge and how decisions are made when adopting and operating the metaverse workspace in a real-world setting. To answer such questions, we scrutinized the startup company Zigbang, which has completely replaced their physical office with Soma— a metaverse platform they developed where thousands of people work and other cooperative companies have moved in as tenants. By conducting field observations and semi-structured interviews with various workers and Zigbang's stakeholders, we identify essential design challenges and decisions when adopting a metaverse workspace and highlight the key takeaways learned from the company's trials and errors.",Metaverse; Virtual Environment; Future of Work; Remote Work; Case Study; Metaverse Workspace,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Practice-informed Patterns for Organising Large Groups in Distributed Mixed Reality Collaboration,CHI - Human Factors in Computing Systems,A*,"Collaborating across dissimilar, distributed spaces presents numerous challenges for computer-aided spatial communication. Mixed reality (MR) can blend selected surfaces, allowing collaborators to work in blended f-formations (facing formations), even when their workstations are physically misaligned. Since collaboration often involves more than just participant pairs, this research examines how we might scale MR experiences for large-group collaboration. To do so, this study recruited collaboration designers (CDs) to evaluate and reimagine MR for large-scale collaboration. These CDs were engaged in a four-part user study that involved a technology probe, a semi-structured interview, a speculative low-fidelity prototyping activity and a validation session. The outcomes of this paper contribute (1) a set of collaboration design principles to inspire future computer-supported collaborative work, (2) eight collaboration patterns for blended f-formations and collaboration at scale and (3) theoretical implications for f-formations and space-place relationships. As a result, this work creates a blueprint for scaling collaboration across distributed spaces.",mixed reality; collaboration; scale; f-formations; space and place,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,An Empirical Study on Oculus Virtual Reality Applications: Security and Privacy Perspectives,ICSE - International Conference on Software Engineering,A*,"Although Virtual Reality (VR) has accelerated its prevalent adoption in emerging metaverse applications, it is not a fundamentally new technology. On one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS (e.g., Android). As a result, VR apps also inherit privacy and security deficiencies from conventional mobile apps. On the other hand, in contrast to conventional mobile apps, VR apps can achieve immersive experience via diverse VR devices, such as head-mounted displays, body sensors, and controllers though achieving this requires the extensive collection of privacy-sensitive human biometrics (e.g., hand-tracking and face-tracking data). Moreover, VR apps have been typically implemented by 3D gaming engines (e.g., Unity), which also contain intrinsic security vulnerabilities. Inappropriate use of these technologies may incur privacy leaks and security vulnerabilities although these issues have not received significant attention compared to the proliferation of diverse VR apps. In this paper, we develop a security and privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP detector has integrated program static analysis tools and privacy-policy analysis methods. Using the VR-SP detector, we conduct a comprehensive empirical study on 500 popular VR apps. We obtain the original apps from the popular Oculus and SideQuest app stores and extract APK files via the Meta Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy analysis. We find that a number of security vulnerabilities and privacy leaks widely exist in VR apps. Moreover, our results also reveal conflicting representations in the privacy policies of these apps and inconsistencies of the actual data collection with the privacy-policy statements of the apps. Based on these findings, we make suggestions for the future development of VR apps.",metaverse; security and privacy; static analysis; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Machine Learning in Metaverse Security: Current Solutions and Future Challenges,CSUR - Computing Surveys,A*,"The Metaverse, positioned as the next frontier of the Internet, has the ambition to forge a virtual shared realm characterized by immersion, hyper-spatiotemporal dynamics, and self-sustainability. Recent technological strides in AI, Extended Reality, 6G, and blockchain propel the Metaverse closer to realization, gradually transforming it from science fiction into an imminent reality. Nevertheless, the extensive deployment of the Metaverse faces substantial obstacles, primarily stemming from its potential to infringe on privacy and be susceptible to security breaches, whether inherent in its underlying technologies or arising from the evolving digital landscape. Metaverse security provisioning is poised to confront various foundational challenges owing to its distinctive attributes, encompassing immersive realism, hyper-spatiotemporally, sustainability, and heterogeneity. This article undertakes a comprehensive study of the security and privacy challenges facing the Metaverse, leveraging machine learning models for this purpose. In particular, our focus centers on an innovative distributed Metaverse architecture characterized by interactions across 3D worlds. Subsequently, we conduct a thorough review of the existing cutting-edge measures designed for Metaverse systems while also delving into the discourse surrounding security and privacy threats. As we contemplate the future of Metaverse systems, we outline directions for open research pursuits in this evolving landscape.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Blockchain; Digital Twin; Extended Reality; Generative AI; Machine Learning; Metaverse Security,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Extended Reality (XR) Toward Building Immersive Solutions: The Key to Unlocking Industry 4.0,CSUR - Computing Surveys,A*,"When developing XR applications for Industry 4.0, it is important to consider the integration of visual displays, hardware components, and multimodal interaction techniques that are compatible with the entire system. The potential use of multimodal interactions in industrial applications has been recognized as a significant factor in enhancing humans' ability to perform tasks and make informed decisions. To offer a comprehensive analysis of the current advancements in industrial XR, this review presents a structured tutorial that provides answers to the following research questions: (R.Q.1) What are the similarities and differences between XR technologies, including augmented reality (AR), mixed reality (MR), Augmented Virtuality (AV), and virtual reality (VR) under Industry 4.0 consideration? (R.Q.2) What types of visual displays and hardware devices are needed to present XR for Industry 4.0? (R.Q.3) How did the multimodal interaction in XR perceive and relate to Industry 4.0? (R.Q.4) How have modern adaptations of XR technologies dealt with the theme of Industry 4.0? (R.Q.5) How can XR technologies in Industry 4.0 develop their services and usages to be more solution-inclusive? This review showcases various instances that demonstrate XR's potential to transform how humans interact with the physical world in Industry 4.0. These advancements can increase productivity, reduce costs, and enhance safety.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",4IR; and augmented virtuality (AV); augmented reality (AR); Extended reality (XR); Industry 4.0; mixed reality (MR); virtual reality (VR),Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,A Review of Olfactory Display Designs for Virtual Reality Environments,CSUR - Computing Surveys,A*,"The field of Virtual Reality continues to evolve to provide an ever-greater sense of immersion to the user. However, VR experiences are still primarily constrained through the human senses of vision and audition, with some interest in haptic (mainly vibrotactile) applications. Only recently have olfactory displays—technologies that generate and deliver scent stimuli—been examined to provide the sense of smell to the human olfactory organ in virtual environments. This article presents a classification and review of olfactory-enhanced virtual reality systems, particularly those that deployed a Head-mounted Display or Cave Automatic Virtual Environment system. In addition, the article provides a discussion of the various technological and design challenges for developing an olfactory display suitable for enhancing virtual reality experiences. Finally, the article proposes future perspectives on the field and includes a table summarizing the characteristics and features of the reviewed systems. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",immersion; multisensory; odor; olfaction; presence; smell; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,The First Principles: Setting the Context for a Safe and Secure Metaverse,CSUR - Computing Surveys,A*,"The metaverse delivered through converged and amalgamated technologies holds promise. No wonder technology heavyweights, large corporates, research organizations and businesses cutting across industry verticals are racing to put in place a metaverse-first strategy. The bets on consumers rapidly migrating from traditional social networks and collaborative applications to more immersive digital experiences have been placed. However, the transition is not expected to be seamless. Privacy, safety and security concerns abound in the early versions of the metaverse. Increased regulatory oversight and diverse national laws threaten to derail the hype around the metaverse. It is increasingly clear that the final iteration of the metaverse will need to assuage the concerns of individual users while addressing complex legal and regulatory requirements. Thus, a multi-perspective approach needs to be adopted to help set the agenda for the evolution of the metaverse. This research paper examines the different aspects and challenges which the future metaverse will need to address. A set of “first principles” are formulated, which if implemented will lead to the development of an equitable, inclusive, safe and secure metaverse. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",first principles for the metaverse; Metaverse; metaverse security,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Secure and Trustworthy Artificial Intelligence-extended Reality (AI-XR) for Metaverses,CSUR - Computing Surveys,A*,"Metaverse is expected to emerge as a new paradigm for the next-generation Internet, providing fully immersive and personalized experiences to socialize, work, and play in self-sustaining and hyper-spatio-temporal virtual world(s). The advancements in different technologies such as augmented reality, virtual reality, extended reality (XR), artificial intelligence (AI), and 5G/6G communication will be the key enablers behind the realization of AI-XR metaverse applications. While AI itself has many potential applications in the aforementioned technologies (e.g., avatar generation, network optimization), ensuring the security of AI in critical applications like AI-XR metaverse applications is profoundly crucial to avoid undesirable actions that could undermine users' privacy and safety, consequently putting their lives in danger. To this end, we attempt to analyze the security, privacy, and trustworthiness aspects associated with the use of various AI techniques in AI-XR metaverse applications. Specifically, we discuss numerous such challenges and present a taxonomy of potential solutions that could be leveraged to develop secure, private, robust, and trustworthy AI-XR applications. To highlight the real implications of AI-associated adversarial threats, we designed a metaverse-specific case study and analyzed it through the adversarial lens. Finally, we elaborate upon various open issues that require further research interest from the community.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",AR; Metaverse; MR; robust ML; secure ML; trustworthy ML; VR; XR,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,“Are you feeling sick?” – A systematic literature review of cybersickness in virtual reality,CSUR - Computing Surveys,A*,"Cybersickness (CS), also known as visually induced motion sickness (VIMS), is a condition that can affect individuals when they interact with virtual reality (VR) technology. This condition is characterized by symptoms such as nausea, dizziness, headaches, eye fatigue, and so on, and can be caused by a variety of factors. Finding a feasible solution to reduce the impact of CS is extremely important as it will greatly enhance the overall user experience and make VR more appealing to a wider range of people. We have carefully compiled a list of 223 highly pertinent studies to review the current state of research on the most essential aspects of CS. We have provided a novel taxonomy that encapsulates various aspects of CS measurement techniques found in the literature. We have proposed a set of CS mitigation guidelines for both developers and users. We have also discussed various CS-inducing factors and provided a taxonomy that tries to capture the same. Overall, our work provides a comprehensive overview of the current state of research in CS with a particular emphasis on different measurement techniques and CS mitigation strategies, identifies research gaps in the literature, and provides recommendations for future research in the field. © 2024 Association for Computing Machinery. All rights reserved.",cause; cybersickness; guidelies; measurement; mitigation; systematic review; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,From Digital Media to Empathic Spaces: A Systematic Review of Empathy Research in Extended Reality Environments,CSUR - Computing Surveys,A*,"Recent advances in extended reality (XR) technologies have enabled new and increasingly realistic empathy tools and experiences. In XR, all interactions take place in different spatial contexts, all with different features, affordances, and constraints. We present a systematic literature survey of recent work on empathy in XR. As a result, we contribute a research roadmap with three future opportunities and six open questions in XR-enabled empathy research across both physical and virtual spaces. © 2023 Copyright held by the owner/author(s).",empathy; Extended reality (XR); human-computer interaction (HCI); metaverse; spatiality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Evaluation of XR Applications: A Tertiary Review,CSUR - Computing Surveys,A*,"Extended reality (XR) applications—encompassing virtual reality, augmented reality, and mixed reality—are finding their way into multiple domains. Each area has different motivations for employing and different criteria for evaluating XR. Multiple surveys describe XR and its evaluation in particular fields. However, these surveys do not always agree on the definition of XR. This lack of consensus makes it hard to compare and use learnings from XR research across areas. Through a tertiary systematic literature review, we analyzed 81 surveys from several fields to provide a comprehensive summary of the state of XR research regarding the evaluation of XR applications. We seek to understand (i) how is XR defined? (ii) why is XR employed? (iii) how is XR evaluated? (iv) what are the main criticisms and future research paths outlined by the surveys? and (v) how good are the surveys? We present our findings describing XR research in 10 categories. Given our findings, we propose that future research should build upon a solid XR taxonomy and depart from effectiveness into efficiency research—to understand not only if but also how XR achieves the desired outcomes. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",augmented reality; Evaluation; systematic review; tertiary review; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Integration of Sensing, Communication, and Computing for Metaverse: A Survey",CSUR - Computing Surveys,A*,"The metaverse is an Artificial Intelligence (AI)-generated virtual world, in which people can game, work, learn, and socialize. The realization of metaverse not only requires a large amount of computing resources to realize the rendering of the virtual world, but also requires communication resources to realize real-time transmission of massive data to ensure a good user experience. The metaverse is currently moving from fiction to reality with the development of advanced technologies represented by AI, blockchain, extended reality, and Digital Twins (DT). However, due to the shortage of communication as well as computing resources, how to realize secure and efficient data interaction between the virtual and the real is an important issue for the metaverse. In this article, we first discuss the characteristics and architecture of the metaverse and introduce its enabling technologies. To cope with the conflict between limited resources and user demands, the article next introduces an Integrated Sensing, Communication, and Computing (SCC) technology and describes its basic principles and related characteristics of SCC. After that, solutions based on SCC in the metaverse scenarios are summarized and relevant lessons are summarized. Finally, we discuss some research challenges and open issues.  © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",communication and computing integration; digital twins; edge computing; Metaverse; sensing,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Deceived by Immersion: A Systematic Analysis of Deceptive Design in Extended Reality,CSUR - Computing Surveys,A*,"The well-established deceptive design literature has focused on conventional user interfaces. With the rise of extended reality (XR), understanding deceptive design's unique manifestations in this immersive domain is crucial. However, existing research lacks a full, cross-disciplinary analysis that analyzes how XR technologies enable new forms of deceptive design. Our study reviews the literature on deceptive design in XR environments. We use thematic synthesis to identify key themes. We found that XR's immersive capabilities and extensive data collection enable subtle and powerful manipulation strategies. We identified eight themes outlining these strategies and discussed existing countermeasures. Our findings show the unique risks of deceptive design in XR, highlighting implications for researchers, designers, and policymakers. We propose future research directions that explore unintentional deceptive design, data-driven manipulation solutions, user education, and the link between ethical design and policy regulations.  Copyright © 2024 held by the owner/author(s).",augmented reality; dark pattern; Deceptive design; extended reality; mixed reality; user manipulation; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Soft Delivery: Survey on a New Paradigm for Wireless and Mobile Multimedia Streaming,CSUR - Computing Surveys,A*,"The increasing demand for video streaming services is the key driver of modern wireless and mobile communications. Although many studies have designed digital-based delivery schemes to send video content over wireless and mobile networks, significant quality degradation, known as cliff and leveling effects, often occurs owing to fluctuating channel characteristics. In this article, we present a comprehensive summary of soft delivery, which is a new paradigm for wireless and mobile video streaming and discuss the future directions of soft delivery. Existing studies found that introducing multi-dimensional cosine transform, human vision system, and graph signal processing can make soft delivery schemes more effective in untethered immersive experiences, including virtual reality and volumetric media, than digital-based delivery schemes. In addition, this study finds that soft delivery has the potential to be a new standard to deliver deep neural network models and tactile information over wireless and mobile networks. © 2023 Copyright held by the owner/author(s).",extended reality; hybrid digital-analog delivery; Soft delivery,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,A Scoping Survey on Cross-reality Systems,CSUR - Computing Surveys,A*,"Immersive technologies such as Virtual Reality (VR) and Augmented Reality (AR) empower users to experience digital realities. Known as distinct technology classes, the lines between them are becoming increasingly blurry with recent technological advancements. New systems enable users to interact across technology classes or transition between them - referred to as cross-reality systems. Nevertheless, these systems are not well understood. Hence, in this article, we conducted a scoping literature review to classify and analyze cross-reality systems proposed in previous work. First, we define these systems by distinguishing three different types. Thereafter, we compile a literature corpus of 306 relevant publications, analyze the proposed systems, and present a comprehensive classification, including research topics, involved environments, and transition types. Based on the gathered literature, we extract nine guiding principles that can inform the development of cross-reality systems. We conclude with research challenges and opportunities. © 2023 held by the owner/author(s).",augmented reality; augmented virtuality; bystander inclusion; collaboration; Cross-reality systems; reality-virtuality continuum; transitional interfaces; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Economic Systems in the Metaverse: Basics, State of the Art, and Challenges",CSUR - Computing Surveys,A*,"Economic systems play pivotal roles in the metaverse. However, we have not yet found an overview that systematically introduces economic systems for the metaverse. Therefore, we review the state-of-the-art solutions, architectures, and systems related to economic systems. When investigating those state-of-the-art studies, we keep two questions in mind: (1) What is the framework of economic systems in the context of the metaverse? and (2) What activities would economic systems engage in the metaverse? This article aims to disclose insights into the economic systems that work for both the current and the future metaverse. To have a clear overview of the economic system framework, we mainly discuss the connections among three fundamental elements in the metaverse, i.e., digital creation, digital assets, and the digital trading market. After that, we elaborate on each topic of the proposed economic system framework. Those topics include incentive mechanisms, monetary systems, digital wallets, decentralized finance activities, and cross-platform interoperability for the metaverse. For each topic, we mainly discuss three questions: (a) the rationale of this topic, (b) why the metaverse needs this topic, and (c) how this topic will evolve in the metaverse. Through this overview, we wish readers can better understand what economic systems the metaverse needs and the insights behind the economic activities in the metaverse.  © 2023 held by the owner/author(s). Publication rights licensed to ACM.",blockchain; cross-metaverse interoperability; cryptocurrency; decentralized finance; economic system; incentive mechanism; Metaverse; non-fungible tokens,Title_Abstract_Keywords,TRUE,
ACM DL,journalPaper,2024,RE Methods for Virtual Reality Software Product Development: A Mapping Study,TOSEM - Transactions on Software Engineering and Methodology,A*,"Software practitioners use various methods in Requirements Engineering (RE) to elicit, analyze, and specify the requirements of enterprise products. The methods impact the final product characteristics and influence product delivery. Ad-hoc usage of the methods by software practitioners can lead to inconsistency and ambiguity in the product. With the notable rise in enterprise products, games, and so forth across various domains, Virtual Reality (VR) has become an essential technology for the future. The methods adopted for RE for developing VR products requires a detailed study. This article presents a mapping study on RE methods prescribed and used for developing VR applications including requirements elicitation, requirements analysis, and requirements specification. Our study provides insights into the use of such methods in the VR community and suggests using specific RE methods in various fields of interest. We also discuss future directions in RE for VR products. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",industrial practices; requirements elicitation; Software requirements; virtual reality,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,TouchInsight: Uncertainty-aware Rapid Touch and Text Input for Mixed Reality from Egocentric Vision,UIST - Symposium on User Interface Software and Technology,A*,"While passive surfaces offer numerous benefits for interaction in mixed reality, reliably detecting touch input solely from head-mounted cameras has been a long-standing challenge. Camera specifics, hand self-occlusion, and rapid movements of both head and fingers introduce considerable uncertainty about the exact location of touch events. Existing methods have thus not been capable of achieving the performance needed for robust interaction. In this paper, we present a real-time pipeline that detects touch input from all ten fingers on any physical surface, purely based on egocentric hand tracking. Our method TouchInsight comprises a neural network to predict the moment of a touch event, the finger making contact, and the touch location. TouchInsight represents locations through a bivariate Gaussian distribution to account for uncertainties due to sensing inaccuracies, which we resolve through contextual priors to accurately infer intended user input. We first evaluated our method offline and found that it locates input events with a mean error of 6.3&nbsp;mm, and accurately detects touch events (F1 = 0.99) and identifies the finger used (F1 = 0.96). In an online evaluation, we then demonstrate the effectiveness of our approach for a core application of dexterous touch input: two-handed text entry. In our study, participants typed 37.0 words per minute with an uncorrected error rate of 2.9% on average.",Bayesian inference; egocentric hand tracking; language models; mixed reality; text entry; Touch detection; uncertainty estimation,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Augmented Object Intelligence with XR-Objects,UIST - Symposium on User Interface Software and Technology,A*,"Seamless integration of physical objects as interactive digital entities remains a challenge for spatial computing. This paper explores Augmented Object Intelligence&nbsp; (AOI) in the context of XR, an interaction paradigm that aims to blur the lines between digital and physical by equipping real-world objects with the ability to interact as if they were digital, where every object has the potential to serve as a portal to digital functionalities. Our approach utilizes real-time object segmentation and classification, combined with the power of Multimodal Large Language Models (MLLMs), to facilitate these interactions without the need for object pre-registration. We implement the AOI concept in the form of XR-Objects, an open-source prototype system that provides a platform for users to engage with their physical environment in contextually relevant ways using object-based context menus. This system enables analog objects to not only convey information but also to initiate digital actions, such as querying for details or executing tasks. Our contributions are threefold: (1) we define the AOI concept and detail its advantages over traditional AI assistants, (2) detail the XR-Objects&nbsp; system’s open-source design and implementation, and (3) show its versatility through various use cases and a user study.",augmented objects; augmented reality; context menus; extended reality; mixed reality; spatial computing; user interfaces,Keywords,TRUE,
ACM DL,conferencePaper,2024,Predicting the Limits: Tailoring Unnoticeable Hand Redirection Offsets in Virtual Reality to Individuals' Perceptual Boundaries,UIST - Symposium on User Interface Software and Technology,A*,"Many illusion and interaction techniques in Virtual Reality (VR) rely on Hand Redirection (HR), which has proved to be effective as long as the introduced offsets between the position of the real and virtual hand do not noticeably disturb the user experience. Yet calibrating HR offsets is a tedious and time-consuming process involving psychophysical experimentation, and the resulting thresholds are known to be affected by many variables—limiting HR’s practical utility. As a result, there is a clear need for alternative methods that allow tailoring HR to the perceptual boundaries of individual users. We conducted an experiment with 18 participants combining movement, eye gaze and EEG data to detect HR offsets Below, At, and Above individuals’ detection thresholds. Our results suggest that we can distinguish HR At and Above from no HR. Our exploration provides a promising new direction with potentially strong implications for the broad field of VR illusions.",detection thresholds; EEG; eye gaze; hand movement; hand redirection; Virtual reality; VR illusions,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Augmented Breathing via Thermal Feedback in the Nose,UIST - Symposium on User Interface Software and Technology,A*,"We propose, engineer, and study a novel method to augment the feeling of breathing—enabling interactive applications to let users feel like they are inhaling more/less air (perceived nasal airflow). We achieve this effect by cooling or heating the nose in sync with the user’s inhalation. Our illusion builds on the physiology of breathing: we perceive our breath predominantly through the cooling of our nasal cavities during inhalation. This is why breathing in a “fresh” cold environment feels easier than in a “stuffy” hot environment, even when the inhaled volume is the same. Our psychophysical study confirmed that our in-nose temperature stimulation significantly influenced breathing perception in both directions: making it feel harder &amp; easier to breathe. Further, we found that &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX sim 90 ,%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 1&lt;/AltText&gt;&lt;File name=""uist24-115-inline1"" type=""svg""/&gt;&lt;/Formula&gt; of the trials were described as a change in perceived airflow/breathing, while only &lt;Formula format=""inline""&gt;&lt;TexMath&gt;&lt;?TeX sim 8 ,%?&gt;&lt;/TexMath&gt;&lt;AltText&gt;Math 2&lt;/AltText&gt;&lt;File name=""uist24-115-inline2"" type=""svg""/&gt;&lt;/Formula&gt; as temperature. Following, we engineered a compact device worn across the septum that uses Peltier elements. We illustrate the potential of this augmented breathing in interactive contexts, such as for virtual reality (e.g., rendering ease of breathing crisp air or difficulty breathing with a deteriorated gas mask) and everyday interactions (e.g., in combination with a relaxation application or to alleviate the perceived breathing resistance when wearing a mask).",Breathing; Perception; Respiration; Thermal; Trigeminal,Abstract,TRUE,
ACM DL,conferencePaper,2024,Thermal In Motion: Designing Thermal Flow Illusions with Tactile and Thermal Interaction,UIST - Symposium on User Interface Software and Technology,A*,"This study presents a novel method for creating moving thermal sensations by integrating the thermal referral illusion with tactile motion. Conducted through three experiments on human forearms, the first experiment examined the impact of temperature and thermal actuator placement on perceived thermal motion, finding the clearest perception with a centrally positioned actuator under both hot and cold conditions. The second experiment identified the speed thresholds of perceived thermal motion, revealing a wider detectable range in hot conditions (1.8 cm/s to 9.5cm/s) compared to cold conditions (2.4cm/s to 5.0cm/s). Finally, we integrated our approach into virtual reality (VR) to assess its feasibility through two interaction scenarios. Our results shed light on the comprehension of thermal perception and its integration with tactile cues, promising significant advancements in incorporating thermal motion into diverse thermal interfaces for immersive VR experiences.",Haptics; Thermal Feedback; Thermal Masking; Thermal Motion; Thermal Referral; Vibration-induced Thermal Illusions; VR,Abstract,TRUE,
ACM DL,conferencePaper,2024,VisCourt: In-Situ Guidance for Interactive Tactic Training in Mixed Reality,UIST - Symposium on User Interface Software and Technology,A*,"In team sports like basketball, understanding and executing tactics—coordinated plans of movements among players—are crucial yet complex, requiring extensive practice. These tactics require players to develop a keen sense of spatial and situational awareness. Traditional coaching methods, which mainly rely on basketball tactic boards and video instruction, often fail to bridge the gap between theoretical learning and the real-world application of tactics, due to shifts in view perspectives and a lack of direct experience with tactical scenarios. To address this challenge, we introduce VisCourt, a Mixed Reality (MR) tactic training system, in collaboration with a professional basketball team. To set up the MR training environment, we employed semi-automatic methods to simulate realistic 3D tactical scenarios and iteratively designed visual in-situ guidance. This approach enables full-body engagement in interactive training sessions on an actual basketball court and provides immediate feedback, significantly enhancing the learning experience. A user study with athletes and enthusiasts shows the effectiveness and satisfaction with VisCourt in basketball training and offers insights for the design of future SportsXR training systems.",Immersive Training; In-Situ Visualization; Mixed Reality; SportsXR,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,EVE: Enabling Anyone to Train Robots using Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"The increasing affordability of robot hardware is accelerating the integration of robots into everyday activities. However, training a robot to automate a task requires expensive trajectory data where a trained human annotator moves a physical robot to train it. Consequently, only those with access to robots produce demonstrations to train robots. In this work, we remove this restriction with EVE, an iOS app that enables everyday users to train robots using intuitive augmented reality visualizations, without needing a physical robot. With EVE, users can collect demonstrations by specifying waypoints with their hands, visually inspecting the environment for obstacles, modifying existing waypoints, and verifying collected trajectories. In a user study (N = 14, D = 30) consisting of three common tabletop tasks, EVE outperformed three state-of-the-art interfaces in success rate and was comparable to kinesthetic teaching—physically moving a physical robot—in completion time, usability, motion intent communication, enjoyment, and preference (meanp = 0.30). EVE allows users to train robots for personalized tasks, such as sorting desk supplies, organizing ingredients, or setting up board games. We conclude by enumerating limitations and design considerations for future AR-based demonstration collection systems for robotics.",augmented reality; demonstration collection; robotics,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,avaTTAR: Table Tennis Stroke Training with Embodied and Detached Visualization in Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"Table tennis stroke training is a critical aspect of player development. We designed a new augmented reality (AR) system, avaTTAR, for table tennis stroke training. The system provides both “on-body” (first-person view) and “detached” (third-person view) visual cues, enabling users to visualize target strokes and correct their attempts effectively with this dual perspectives setup. By employing a combination of pose estimation algorithms and IMU sensors, avaTTAR&nbsp; captures and reconstructs the 3D body pose and paddle orientation of users during practice, allowing real-time comparison with expert strokes. Through a user study, we affirm avaTTAR&nbsp;’s capacity to amplify player experience and training results.",Augmented Reality; Motor Learning; Table Tennis,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,SpaceBlender: Creating Context-Rich Collaborative Spaces Through Generative 3D Scene Blending,UIST - Symposium on User Interface Software and Technology,A*,"There is increased interest in using generative AI to create 3D spaces for Virtual Reality (VR) applications. However, today’s models produce artificial environments, falling short of supporting collaborative tasks that benefit from incorporating the user’s physical context. To generate environments that support VR telepresence, we introduce SpaceBlender, a novel pipeline that utilizes generative AI techniques to blend users’ physical surroundings into unified virtual spaces. This pipeline transforms user-provided 2D images into context-rich 3D environments through an iterative process consisting of depth estimation, mesh alignment, and diffusion-based space completion guided by geometric priors and adaptive text prompts. In a preliminary within-subjects study, where 20 participants performed a collaborative VR affinity diagramming task in pairs, we compared SpaceBlender with a generic virtual environment and a state-of-the-art scene generation framework, evaluating its ability to create virtual spaces suitable for collaboration. Participants appreciated the enhanced familiarity and context provided by SpaceBlender but also noted complexities in the generative environments that could detract from task focus. Drawing on participant feedback, we propose directions for improving the pipeline and discuss the value and design of blended spaces for different scenarios.",generative AI; VR telepresence,Abstract,TRUE,
ACM DL,conferencePaper,2024,SituationAdapt: Contextual UI Optimization in Mixed Reality with Situation Awareness via LLM Reasoning,UIST - Symposium on User Interface Software and Technology,A*,"Mixed Reality is increasingly used in mobile settings beyond controlled home and office spaces. This mobility introduces the need for user interface layouts that adapt to varying contexts. However, existing adaptive systems are designed only for static environments. In this paper, we introduce SituationAdapt, a system that adjusts Mixed Reality UIs to real-world surroundings by considering environmental and social cues in shared settings. Our system consists of perception, reasoning, and optimization modules for UI adaptation. Our perception module identifies objects and individuals around the user, while our reasoning module leverages a Vision-and-Language Model to assess the placement of interactive UI elements. This ensures that adapted layouts do not obstruct relevant environmental cues or interfere with social norms. Our optimization module then generates Mixed Reality interfaces that account for these considerations as well as temporal constraints. For evaluation, we first validate our reasoning module’s capability of assessing UI contexts in comparison to human expert users. In an online user study, we then establish SituationAdapt’s capability of producing context-aware layouts for Mixed Reality, where it outperformed previous adaptive layout methods. We conclude with a series of applications and scenarios to demonstrate SituationAdapt’s versatility.",Adaptive User Interfaces; Large Language Models.; Mixed Reality,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Desk2Desk: Optimization-based Mixed Reality Workspace Integration for Remote Side-by-side Collaboration,UIST - Symposium on User Interface Software and Technology,A*,"Mixed Reality enables hybrid workspaces where physical and virtual monitors are adaptively created and moved to suit the current environment and needs. However, in shared settings, individual users’ workspaces are rarely aligned and can vary significantly in the number of monitors, available physical space, and workspace layout, creating inconsistencies between workspaces which may cause confusion and reduce collaboration. We present Desk2Desk, an optimization-based approach for remote collaboration in which the hybrid workspaces of two collaborators are fully integrated to enable immersive side-by-side collaboration. The optimization adjusts each user’s workspace in layout and number of shared monitors and creates a mapping between workspaces to handle inconsistencies between workspaces due to physical constraints (e.g. physical monitors). We show in a user study how our system adaptively merges dissimilar physical workspaces to enable immersive side-by-side collaboration, and demonstrate how an optimization-based approach can effectively address dissimilar physical layouts.",gesture retargeting; mixed reality; remote collaboration; shared workspace,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Exploring the Effects of Sensory Conflicts on Cognitive Fatigue in VR Remappings,UIST - Symposium on User Interface Software and Technology,A*,"Virtual reality (VR) is found to present significant cognitive challenges due to its immersive nature and frequent sensory conflicts. This study systematically investigates the impact of sensory conflicts induced by VR remapping techniques on cognitive fatigue, and unveils their correlation. We utilized three remapping methods (haptic repositioning, head-turning redirection, and giant resizing) to create different types of sensory conflicts, and measured perceptual thresholds to induce various intensities of the conflicts. Through experiments involving cognitive tasks along with subjective and physiological measures, we found that all three remapping methods influenced the onset and severity of cognitive fatigue, with visual-vestibular conflict having the greatest impact. Interestingly, visual-experiential/memory conflict showed a mitigating effect on cognitive fatigue, emphasizing the role of novel sensory experiences. This study contributes to a deeper understanding of cognitive fatigue under sensory conflicts and provides insights for designing VR experiences that align better with human perceptual and cognitive capabilities.",Cognitive Fatigue; Cognitive Load; Sensory Conflict; VR Remapping,Abstract,TRUE,
ACM DL,conferencePaper,2024,VirtualNexus: Enhancing 360-Degree Video AR/VR Collaboration with Environment Cutouts and Virtual Replicas,UIST - Symposium on User Interface Software and Technology,A*,"Asymmetric AR/VR collaboration systems bring a remote VR user to a local AR user’s physical environment, allowing them to communicate and work within a shared virtual/physical space. Such systems often display the remote environment through 3D reconstructions or 360° videos. While 360° cameras stream an environment in higher quality, they lack spatial information, making them less interactable. We present VirtualNexus, an AR/VR collaboration system that enhances 360° video AR/VR collaboration with environment cutouts and virtual replicas. VR users can define cutouts of the remote environment to interact with as a world-in-miniature, and their interactions are synchronized to the local AR perspective. Furthermore, AR users can rapidly scan and share 3D virtual replicas of physical objects using neural rendering. We demonstrated our system’s utility through 3 example applications and evaluated our system in a dyadic usability test. VirtualNexus extends the interaction space of 360° telepresence systems, offering improved physical presence, versatility, and clarity in interactions.",Computer Mediated Communication; Virtual/Augmented Reality,Keywords,TRUE,
ACM DL,conferencePaper,2024,Personal Time-Lapse,UIST - Symposium on User Interface Software and Technology,A*,"Our bodies are constantly in motion—from the bending of arms and legs to the less conscious movement of breathing, our precise shape and location change constantly. This can make subtler developments (e.g., the growth of hair, or the healing of a wound) difficult to observe. Our work focuses on helping users record and visualize this type of subtle, longer-term change. We present a mobile tool that combines custom 3D tracking with interactive visual feedback and computational imaging to capture personal time-lapse, which approximates longer-term video of the subject (typically, part of the capturing user’s body) under a fixed viewpoint, body pose, and lighting condition. These personal time-lapses offer a powerful and detailed way to track visual changes of the subject over time. We begin with a formative study that examines what makes personal time-lapse so difficult to capture. Building on our findings, we motivate the design of our capture tool, evaluate this design with users, and demonstrate its effectiveness in a variety of challenging examples.",Camera-based UIs; Graphics / 3D; Virtual/Augmented Reality,Keywords,TRUE,
ACM DL,conferencePaper,2024,TouchpadAnyWear: Textile-Integrated Tactile Sensors for Multimodal High Spatial-Resolution Touch Inputs with Motion Artifacts Tolerance,UIST - Symposium on User Interface Software and Technology,A*,"This paper presents TouchpadAnyWear, a novel family of textile-integrated force sensors capable of multi-modal touch input, encompassing micro-gesture detection, two-dimensional (2D) continuous input, and force-sensitive strokes. This thin (&lt;1.5&nbsp;mm) and conformal device features high spatial resolution sensing and motion artifact tolerance through its unique capacitive sensor architecture. The sensor consists of a knitted textile compressive core, sandwiched by stretchable silver electrodes, and conductive textile shielding layers on both sides. With a high-density sensor pixel array (25/cm2), TouchpadAnyWear can detect touch input locations and sizes with millimeter-scale spatial resolution and a wide range of force inputs (0.05&nbsp;N to 20&nbsp;N). The incorporation of miniature polymer domes, referred to as “poly-islands”, onto the knitted textile locally stiffens the sensing areas, thereby reducing motion artifacts during deformation. These poly-islands also provide passive tactile feedback to users, allowing for eyes-free localization of the active sensing pixels. Design choices and sensor performance are evaluated using in-depth mechanical characterization. Demonstrations include an 8-by-8 grid sensor as a miniature high-resolution touchpad and a T-shaped sensor for thumb-to-finger micro-gesture input. User evaluations validate the effectiveness and usability of TouchpadAnyWear in daily interaction contexts, such as tapping, forceful pressing, swiping, 2D cursor control, and 2D stroke-based gestures. This paper further discusses potential applications and explorations for TouchpadAnyWear in wearable smart devices, gaming, and augmented reality devices.",Capacitive; Fabrication; Gesture Recognition; High Resolution; Motion Artifacts; Multimodal; Printing; Soft Wearable; Tactile Display; Textile; Touch Sensor,Abstract,TRUE,
ACM DL,conferencePaper,2024,Gait Gestures: Examining Stride and Foot Strike Variation as an Input Method While Walking,UIST - Symposium on User Interface Software and Technology,A*,"Walking is a cyclic pattern of alternating footstep strikes, with each pair of steps forming a stride, and a series of strides forming a gait. We conduct a systematic examination of different kinds of intentional variations from a normal gait that could be used as input actions without interrupting overall walking progress. A design space of 22 candidate Gait Gestures is generated by adapting previous standing foot input actions and identifying new actions possible in a walking context. A formative study (n=25) examines movement easiness, social acceptability, and walking compatibility with foot movement logging to calculate temporal and spatial characteristics. Using a categorization of these results, 7 gestures are selected for a wizard-of-oz prototype demonstrating an AR interface controlled by Gait Gestures for ordering food and audio playback while walking. As a technical proof-of-concept, a gait gesture recognizer is developed and tested using the formative study data.",foot-based gesture; interaction technique; mixed reality; walking,Keywords,TRUE,
ACM DL,conferencePaper,2024,EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras,UIST - Symposium on User Interface Software and Technology,A*,"In augmented and virtual reality (AR/VR) experiences, a user’s arms and hands can provide a convenient and tactile surface for touch input. Prior work has shown on-body input to have significant speed, accuracy, and ergonomic benefits over in-air interfaces, which are common today. In this work, we demonstrate high accuracy, bare hands (i.e., no special instrumentation of the user) skin input using just an RGB camera, like those already integrated into all modern XR headsets. Our results show this approach can be accurate, and robust across diverse lighting conditions, skin tones, and body motion (e.g., input while walking). Finally, our pipeline also provides rich input metadata including touch force, finger identification, angle of attack, and rotation. We believe these are the requisite technical ingredients to more fully unlock on-skin interfaces that have been well motivated in the HCI literature but have lacked robust and practical methods.",AR/VR; Computer Vision; On-Body Computing; Touch Surfaces and Touch Interaction,Abstract,TRUE,
ACM DL,conferencePaper,2024,SIM2VR: Towards Automated Biomechanical Testing in VR,UIST - Symposium on User Interface Software and Technology,A*,"Automated biomechanical testing has great potential for the development of VR applications, as initial insights into user behaviour can be gained in silico early in the design process. In particular, it allows prediction of user movements and ergonomic variables, such as fatigue, prior to conducting user studies. However, there is a fundamental disconnect between simulators hosting state-of-the-art biomechanical user models and simulators used to develop and run VR applications. Existing user simulators often struggle to capture the intricacies of real-world VR applications, reducing ecological validity of user predictions. In this paper, we introduce sim2vr, a system that aligns user simulation with a given VR application by establishing a continuous closed loop between the two processes. This, for the first time, enables training simulated users directly in the same VR application that real users interact with. We demonstrate that sim2vr can predict differences in user performance, ergonomics and strategies in a fast-paced, dynamic arcade game. In order to expand the scope of automated biomechanical testing beyond simple visuomotor tasks, advances in cognitive models and reward function design will be needed.",automated testing; biomechanical simulation; deep reinforcement learning; interaction design; virtual reality; VR application; VR development; VR simulation alignment,Keywords,TRUE,
ACM DL,conferencePaper,2024,"Hands-on, Hands-off: Gaze-Assisted Bimanual 3D Interaction",UIST - Symposium on User Interface Software and Technology,A*,"Extended Reality (XR) systems with hand-tracking support direct manipulation of objects with both hands. A common interaction in this context is for the non-dominant hand (NDH) to orient an object for input by the dominant hand (DH). We explore bimanual interaction with gaze through three new modes of interaction where the input of the NDH, DH, or both hands is indirect based on Gaze+Pinch. These modes enable a new dynamic interplay between our hands, allowing flexible alternation between and pairing of complementary operations. Through applications, we demonstrate several use cases in the context of 3D modelling, where users exploit occlusion-free, low-effort, and fluid two-handed manipulation. To gain a deeper understanding of each mode, we present a user study on an asymmetric rotate-translate task. Most participants preferred indirect input with both hands for lower physical effort, without a penalty on user performance. Otherwise, they preferred modes where the NDH oriented the object directly, supporting preshaping of the hand, which is more challenging with indirect gestures. The insights gained are of relevance for the design of XR interfaces that aim to leverage eye and hand input in tandem.",3D manipulation; bimanual interaction; eye-tracking; gaze input; virtual reality,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,GradualReality: Enhancing Physical Object Interaction in Virtual Reality via Interaction State-Aware Blending,UIST - Symposium on User Interface Software and Technology,A*,"We present GradualReality, a novel interface enabling a Cross Reality experience that includes gradual interaction with physical objects in a virtual environment and supports both presence and usability. Daily Cross Reality interaction is challenging as the user’s physical object interaction state is continuously changing over time, causing their attention to frequently shift between the virtual and physical worlds. As such, presence in the virtual environment and seamless usability for interacting with physical objects should be maintained at a high level. To address this issue, we present an Interaction State-Aware Blending approach that (i) balances immersion and interaction capability and (ii) provides a fine-grained, gradual transition between virtual and physical worlds. The key idea includes categorizing the flow of physical object interaction into multiple states and designing novel blending methods that offer optimal presence and sufficient physical awareness at each state. We performed extensive user studies and interviews with a working prototype and demonstrated that GradualReality provides better Cross Reality experiences compared to baselines.",Adaptive System; Context-Awareness; Cross Reality; Mixed Reality,Title_Keywords,TRUE,
ACM DL,conferencePaper,2024,StegoType: Surface Typing from Egocentric Cameras,UIST - Symposium on User Interface Software and Technology,A*,"Text input is a critical component of any general purpose computing system, yet efficient and natural text input remains a challenge in AR and VR. Headset based hand-tracking has recently become pervasive among consumer VR devices and affords the opportunity to enable touch typing on virtual keyboards. We present an approach for decoding touch typing on uninstrumented flat surfaces using only egocentric camera-based hand-tracking as input. While egocentric hand-tracking accuracy is limited by issues like self occlusion and image fidelity, we show that a sufficiently diverse training set of hand motions paired with typed text can enable a deep learning model to extract signal from this noisy input. Furthermore, by carefully designing a closed-loop data collection process, we can train an end-to-end text decoder that accounts for natural sloppy typing on virtual keyboards. We evaluate our work with a user study (n=18) showing a mean online throughput of 42.4 WPM with an uncorrected error rate (UER) of 7% with our method compared to a physical keyboard baseline of 74.5 WPM at 0.8% UER, showing progress towards unlocking productivity and high throughput use cases in AR/VR.",augmented reality; hand-tracking; mixed reality; text input; virtual reality,Keywords,TRUE,
ACM DL,conferencePaper,2024,Eye-Hand Movement of Objects in Near Space Extended Reality,UIST - Symposium on User Interface Software and Technology,A*,"Hand-tracking in Extended Reality (XR) enables moving objects in near space with direct hand gestures, to pick, drag and drop objects in 3D. In this work, we investigate the use of eye-tracking to reduce the effort involved in this interaction. As the eyes naturally look ahead to the target for a drag operation, the principal idea is to map the translation of the object in the image plane to gaze, such that the hand only needs to control the depth component of the operation. We have implemented four techniques that explore two factors: the use of gaze only to move objects in X-Y vs. extra refinement by hand, and the use of hand input in the Z axis to directly move objects vs. indirectly via a transfer function. We compared all four techniques in a user study (N=24) against baselines of direct and indirect hand input. We detail user performance, effort and experience trade-offs and show that all eye-hand techniques significantly reduce physical effort over direct gestures, pointing toward effortless drag-and-drop for XR environments.",drag&amp; drop; eye-tracking; gaze interaction; near space; object manipulation,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Computational Trichromacy Reconstruction: Empowering the Color-Vision Deficient to Recognize Colors Using Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"We propose an assistive technology that helps individuals with Color Vision Deficiencies (CVD) to recognize/name colors. A dichromat’s color perception is a reduced two-dimensional (2D) subset of a normal trichromat’s three dimensional color (3D) perception, leading to confusion when visual stimuli that appear identical to the dichromat are referred to by different color names. Using our proposed system, CVD individuals can interactively induce distinct perceptual changes to originally confusing colors via a computational color space transformation. By combining their original 2D precepts for colors with the discriminative changes, a three dimensional color space is reconstructed, where the dichromat can learn to resolve color name confusions and accurately recognize colors. Our system is implemented as an Augmented Reality (AR) interface on smartphones, where users interactively control the rotation through swipe gestures and observe the induced color shifts in the camera view or in a displayed image. Through psychophysical experiments and a longitudinal user study, we demonstrate that such rotational color shifts have discriminative power (initially confusing colors become distinct under rotation) and exhibit structured perceptual shifts dichromats can learn with modest training. The AR App is also evaluated in two real-world scenarios (building with lego blocks and interpreting artistic works); users all report positive experience in using the App to recognize object colors that they otherwise could not.",Assistive Technology; Augmented Reality; Color Naming; Color Recognition; Color Vision Deficiency; Dichromat,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,VRCopilot: Authoring 3D Layouts with Generative AI Models in VR,UIST - Symposium on User Interface Software and Technology,A*,"Immersive authoring provides an intuitive medium for users to create 3D scenes via direct manipulation in Virtual Reality (VR). Recent advances in generative AI have enabled the automatic creation of realistic 3D layouts. However, it is unclear how capabilities of generative AI can be used in immersive authoring to support fluid interactions, user agency, and creativity. We introduce VRCopilot, a mixed-initiative system that integrates pre-trained generative AI models into immersive authoring to facilitate human-AI co-creation in VR. VRCopilot presents multimodal interactions to support rapid prototyping and iterations with AI, and intermediate representations such as wireframes to augment user controllability over the created content. Through a series of user studies, we evaluated the potential and challenges in manual, scaffolded, and automatic creation in immersive authoring. We found that scaffolded creation using wireframes enhanced the user agency compared to automatic creation. We also found that manual creation via multimodal specification offers the highest sense of creativity and agency.",Generative AI; Human-AI Co-creation; Virtual Reality,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,AniCraft: Crafting Everyday Objects as Physical Proxies for Prototyping 3D Character Animation in Mixed Reality,UIST - Symposium on User Interface Software and Technology,A*,"We introduce AniCraft, a mixed reality system for prototyping 3D character animation using physical proxies crafted from everyday objects. Unlike existing methods that require specialized equipment to support the use of physical proxies, AniCraft only requires affordable markers, webcams, and daily accessible objects and materials. AniCraft allows creators to prototype character animations through three key stages: selection of virtual characters, fabrication of physical proxies, and manipulation of these proxies to animate the characters. This authoring workflow is underpinned by diverse physical proxies, manipulation types, and mapping strategies, which ease the process of posing virtual characters and mapping user interactions with physical proxies to animated movements of virtual characters. We provide a range of cases and potential applications to demonstrate how diverse physical proxies can inspire user creativity. User experiments show that our system can outperform traditional animation methods for rapid prototyping. Furthermore, we provide insights into the benefits and usage patterns of different materials, which lead to design implications for future research.",3D Animation; Mixed Reality; Physical Proxy; Rigged Character,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Fiery Hands: Designing Thermal Glove through Thermal and Tactile Integration for Virtual Object Manipulation,UIST - Symposium on User Interface Software and Technology,A*,"We present a novel approach to render thermal and tactile feedback to the palm and fingertips through thermal and tactile integration. Our approach minimizes the obstruction of the palm and inner side of the fingers and enables virtual object manipulation while providing localized and global thermal feedback. By leveraging thermal actuators positioned strategically on the outer palm and back of the fingers in interplay with tactile actuators, our approach exploits thermal referral and tactile masking phenomena. Through a series of user studies, we validate the perception of localized thermal sensations across the palm and fingers, showcasing the ability to generate diverse thermal patterns. Furthermore, we demonstrate the efficacy of our approach in VR applications, replicating diverse thermal interactions with virtual objects. This work represents significant progress in thermal interactions within VR, offering enhanced sensory immersion at an optimal energy cost.",Thermal and Haptic Interfaces; Thermal Gloves; Thermal Illusions; Thermal Referral; Virtual Reality,Keywords,TRUE,
ACM DL,conferencePaper,2024,Flip-Pelt: Motor-Driven Peltier Elements for Rapid Thermal Stimulation and Congruent Pressure Feedback in Virtual Reality,UIST - Symposium on User Interface Software and Technology,A*,"This study introduces ""Flip-Pelt,"" a motor-driven peltier device designed to provide rapid thermal stimulation and congruent pressure feedback in virtual reality (VR) environments. Our system incorporates eight motor-driven peltier elements, allowing for the flipping of preheated or cooled elements to the opposite side. In evaluating the Flip-Pelt device, we assess user ability to distinguish between heat/cold sources by their patterns and stiffness, and its impact on enhancing haptic experiences in VR content that involves contact with various thermal sources. Our findings demonstrate that rapid thermal stimulation and congruent pressure feedback provided by Flip-Pelt enhance the recognition accuracy of thermal patterns and the stiffness of virtual objects. These features also improve haptic experiences in VR scenarios through their temporal congruency between tactile and thermal stimuli. Additionally, we discuss the scalability of the Flip-Pelt system to other body parts by proposing design prototypes.",Multimodal Haptics; Thermal Feedback; Virtual Reality,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Hydroptical Thermal Feedback: Spatial Thermal Feedback Using Visible Lights and Water,UIST - Symposium on User Interface Software and Technology,A*,"We control the temperature of materials in everyday interactions, recognizing temperature’s important influence on our bodies, minds, and experiences. However, thermal feedback is an under-explored modality in human-computer interaction partly due to its limited temporal (slow) and spatial (small-area and non-moving) capabilities. We introduce hydroptical thermal feedback, a spatial thermal feedback method that works by applying visible lights on body parts in water. Through physical measurements and psychophysical experiments, our results show: (1) Humans perceive thermal sensations when visible lights are cast on the skin under water, and perceived warmth is greater for lights with shorter wavelengths, (2) temporal capabilities, (3) apparent motion (spatial) of warmth and coolness sensations, and (4) hydroptical thermal feedback can support the perceptual illusion that the water itself is warmer. We propose applications, including virtual reality (VR), shared water experiences, and therapies. Overall, this paper contributes hydroptical thermal feedback as a novel method, empirical results demonstrating its unique capabilities, proposed applications, and design recommendations for using hydroptical thermal feedback. Our method introduces controlled, spatial thermal perceptions to water experiences.",Thermal display; Thermal feedback; Visible light; WaterHCI,Abstract,TRUE,
ACM DL,conferencePaper,2024,SonoHaptics: An Audio-Haptic Cursor for Gaze-Based Object Selection in XR,UIST - Symposium on User Interface Software and Technology,A*,"We introduce SonoHaptics, an audio-haptic cursor for gaze-based 3D object selection. SonoHaptics addresses challenges around providing accurate visual feedback during gaze-based selection in Extended Reality&nbsp;(XR), e.&nbsp;g., lack of world-locked displays in no- or limited-display smart glasses and visual inconsistencies. To enable users to distinguish objects without visual feedback, SonoHaptics employs the concept of cross-modal correspondence in human perception to map visual features of objects (color, size, position, material) to audio-haptic properties (pitch, amplitude, direction, timbre). We contribute data-driven models for determining cross-modal mappings of visual features to audio and haptic features, and a computational approach to automatically generate audio-haptic feedback for objects in the user’s environment. SonoHaptics provides global feedback that is unique to each object in the scene, and local feedback to amplify differences between nearby objects. Our comparative evaluation shows that SonoHaptics enables accurate object identification and selection in a cluttered scene without visual feedback.",Computational Interaction; Extended Reality; Gaze-based Selection; Haptics; Multimodal Feedback; Sonification,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,Auptimize: Optimal Placement of Spatial Audio Cues for Extended Reality,UIST - Symposium on User Interface Software and Technology,A*,"Spatial audio in Extended Reality (XR) provides users with better awareness of where virtual elements are placed, and efficiently guides them to events such as notifications, system alerts from different windows, or approaching avatars. Humans, however, are inaccurate in localizing sound cues, especially with multiple sources due to limitations in human auditory perception such as angular discrimination error and front-back confusion. This decreases the efficiency of XR interfaces because users misidentify from which XR element a sound is coming from. To address this, we propose Auptimize, a novel computational approach for placing XR sound sources, which mitigates such localization errors by utilizing the ventriloquist effect. Auptimize disentangles the sound source locations from the visual elements and relocates the sound sources to optimal positions for unambiguous identification of sound cues, avoiding errors due to inter-source proximity and front-back confusion. Our evaluation shows that Auptimize decreases spatial audio-based source identification errors compared to playing sound cues at the paired visual-sound locations. We demonstrate the applicability of Auptimize for diverse spatial audio-based interactive XR scenarios.",audio perception; computational interaction; Extended Reality,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,SonifyAR: Context-Aware Sound Generation in Augmented Reality,UIST - Symposium on User Interface Software and Technology,A*,"Sound plays a crucial role in enhancing user experience and immersiveness in Augmented Reality (AR). However, current platforms lack support for AR sound authoring due to limited interaction types, challenges in collecting and specifying context information, and difficulty in acquiring matching sound assets. We present SonifyAR, an LLM-based AR sound authoring system that generates context-aware sound effects for AR experiences. SonifyAR expands the current design space of AR sound and implements a Programming by Demonstration (PbD) pipeline to automatically collect contextual information of AR events, including virtual-content-semantics and real-world context. This context information is then processed by a large language model to acquire sound effects with Recommendation, Retrieval, Generation, and Transfer methods. To evaluate the usability and performance of our system, we conducted a user study with eight participants and created five example applications, including an AR-based science experiment, and an assistive application for low-vision AR users.",Augmented Reality; Authoring Tool; Mixed Reality; Sound,Title_Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,LoopBot: Representing Continuous Haptics of Grounded Objects in Room-scale VR,UIST - Symposium on User Interface Software and Technology,A*,"In room-scale virtual reality, providing continuous haptic feedback from touching grounded objects, such as walls and handrails, has been challenging due to the user’s walking range and the required force. In this study, we propose LoopBot, a novel technique to provide continuous haptic feedback from grounded objects using only a single user-following robot. Specifically, LoopBot is equipped with a loop-shaped haptic prop attached to an omnidirectional robot that scrolls to cancel out the robot’s displacement, giving the user the haptic sensation that the prop is actually fixed in place, or “grounded.” We first introduce the interaction design space of LoopBot and, as one of its promising interaction scenarios, implement a prototype for the experience of walking while grasping handrails. A performance evaluation shows that scrolling the prop cancels 77.5% of the robot’s running speed on average. A preliminary user test (N = 10) also shows that the subjective realism of the experience and the sense of the virtual handrails being grounded were significantly higher than when the prop was not scrolled. Based on these findings, we discuss possible further development of LoopBot.",Encountered-type haptic device; Room-scale virtual reality; Wheeled robot,Abstract_Keywords,TRUE,
ACM DL,conferencePaper,2024,JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets,UIST - Symposium on User Interface Software and Technology,A*,"We propose JetUnit, a water-based VR haptic system designed to produce force feedback with a wide spectrum of intensities and frequencies through water jets. The key challenge in designing this system lies in optimizing parameters to enable the haptic device to generate force feedback that closely replicates the most intense force produced by direct water jets while ensuring the user remains dry. In this paper, we present the key design parameters of the JetUnit wearable device determined through a set of quantitative experiments and a perception study. We further conducted a user study to assess the impact of integrating our haptic solutions into virtual reality experiences. The results revealed that, by adhering to the design principles of JetUnit, the water-based haptic system is capable of delivering diverse force feedback sensations, significantly enhancing the immersive experience in virtual reality.",force feedback; haptics; VR; water jets,Title_Abstract,TRUE,
ACM DL,conferencePaper,2024,Selfrionette: A Fingertip Force-Input Controller for Continuous Full-Body Avatar Manipulation and Diverse Haptic Interactions,UIST - Symposium on User Interface Software and Technology,A*,"We propose Selfrionette, a controller that uses fingertip force input to drive avatar movements in virtual reality (VR). This system enables users to interact with virtual objects and walk in VR using only fingertip force, overcoming physical and spatial constraints. Additionally, by fixing users’ fingers, it provides users with counterforces equivalent to the applied force, allowing for diverse and wide dynamic range haptic feedback by adjusting the relationship between force input and virtual movement. To evaluate the effectiveness of the proposed method, this paper focuses on hand interaction as a first step. In User Study 1, we measured usability and embodiment during reaching tasks under Selfrionette, body tracking, and finger tracking conditions. In User Study 2, we investigated whether users could perceive haptic properties such as weight, friction, and compliance under the same conditions as User Study 1. Selfrionette was found to be comparable to body tracking in realism of haptic interaction, enabling embodied avatar experiences even in limited spatial conditions.",avatar manipulation; embodiment; force-based input; haptics,Abstract,TRUE,
ACM DL,conferencePaper,2024,"SpinShot: Optimizing Both Physical and Perceived Force Feedback of Flywheel-Based, Directional Impact Handheld Devices",UIST - Symposium on User Interface Software and Technology,A*,"Real-world impact, such as hitting a tennis ball and a baseball, generates instantaneous, directional impact forces. However, current ungrounded force feedback technologies, such as air jets and propellers, can only generate directional impulses that are 10x-10,000x weaker. We present SpinShot, a flywheel-based device with a solenoid-actuated stopper capable of generating directional impulse of 22Nm in 1ms, which is more than 10x stronger than prior ungrounded directional technologies. Furthermore, we present a novel force design that reverses the flywheel immediately after the initial impact, to significantly increase the perceived magnitude. We conducted a series of two formative, perceptual studies (n=16, 18), followed by a summative user experience study (n=16) that compared SpinShot vs. moving mass (solenoid) and vs. air jets in a VR baseball hitting game. Results showed that SpinShot significantly improved realism, immersion, magnitude (p &lt; .01) compared to both baselines, but significantly reduced comfort vs. air jets primarily due to the 2.9x device weight. Overall, SpinShot was preferred by 63-75% of the participants.",Flywheel; Handheld Device; Haptic; Impact Force; Perceptual Design; Ungrounded Force Feedback; Virtual Reality,Keywords,TRUE,
ACM DL,conferencePaper,2024,CookAR: Affordance Augmentations in Wearable AR to Support Kitchen Tool Interactions for People with Low Vision,UIST - Symposium on User Interface Software and Technology,A*,"Cooking is a central activity of daily living, supporting independence as well as mental and physical health. However, prior work has highlighted key barriers for people with low vision (LV) to cook, particularly around safely interacting with tools, such as sharp knives or hot pans. Drawing on recent advancements in computer vision (CV), we present CookAR, a head-mounted AR system with real-time object affordance augmentations to support safe and efficient interactions with kitchen tools. To design and implement CookAR, we collected and annotated the first egocentric dataset of kitchen tool affordances, fine-tuned an affordance segmentation model, and developed an AR system with a stereo camera to generate visual augmentations. To validate CookAR, we conducted a technical evaluation of our fine-tuned model as well as a qualitative lab study with 10 LV participants for suitable augmentation design. Our technical evaluation demonstrates that our model outperforms the baseline on our tool affordance dataset, while our user study indicates a preference for affordance augmentations over the traditional whole object augmentations.",accessibility; affordance segmentation; augmented reality; visual augmentation,Keywords,TRUE,
IEEE,conferencePaper,2024,Context-Relevant Locations as an Alternative to the Place Illusion in Augmented Reality,VR - International Symposium Virtual Reality,A*,"Presence is a powerful aspect of Virtual Reality (VR). However, there has been no consensus on how to achieve presence in Augmented Reality (AR) or whether it exists at all. The Place Illusion, a key component in presence as defined in VR, cannot be obtained in AR as there is no way to make the user feel as though they are transported somewhere else when they are limited to what they can physically see in front of them. However, recently it has been argued that coherence or congruence are important parts of the Place and Plausibility Illusions. The implication for AR is that the AR content might invoke a higher Plausibility Illusion if it is consistent with the physical place the content is situated in. In this study, we define the concept of a Context-Relevant Location (CRL), a physical place that is congruent with the experience. We present a study with a between-subjects design that allowed users to interact with AR objects in a CRL and in a generic environment. The results indicate that presence was higher in the CRL setting than the generic environment, contribute to the debate about providing a concrete description of presence-like phenomena in AR, and posit that CRLs play a similar role to the Place Illusion in an AR setting.",Augmented reality; context-relevant location; plausibility; presence,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,"Lifter for VR Headset: Enhancing Immersion, Presence, Flow, and Alleviating Mental and Physical Fatigue during Prolonged Use",VR - International Symposium Virtual Reality,A*,"The virtual reality (VR) headset is still relatively heavy, causing a significant physical and mental burden and negatively affecting the VR user experience, particularly during extended periods of use. In this paper, we present a prototype design of the “Lifter,” which utilizes a counterbalanced wire-pulley mechanism to partially relieve the weight of the VR headset (between 50% and 85%). The human subject study has confirmed that the Lifter relieved not only physical fatigue but also significantly improved mental burden, sense of immersion, presence, and flow (perception of time passing) during prolonged usage (30 minutes or more).",Head-Mounted Display; Headset Weight; Weight Reduction,Abstract,TRUE,
IEEE,conferencePaper,2024,"MeetingBenji: Tackling Cynophobia with Virtual Reality, Gamification, and Biofeedback",VR - International Symposium Virtual Reality,A*,"Phobias, particularly animal phobias like cynophobia (fear of dogs), disrupt the lives of those affected by, for instance, limiting outdoor activities. While virtual reality exposure therapy (VRET) has emerged as a potential treatment for this phobia, these efforts have been limited by high dropout rates and a lack of ability to handle stressful situations in people who suffer from cynophobia. Inspired by these challenges, we present MeetingBenji, a VRET system for cynophobia that uses (i) gamification to enhance motivation and engagement, and (ii) biofeedback to facilitate self-control and reduce physiological responses. In a study (N=10) that compared the effects of displaying dogs in 3D scenes and 360º videos using the Behavioral Approach Test (BAT) – in which participants are increasingly exposed to the source of phobia – participants reported a high level of immersion to the exposure sequence. Further, they reported feeling more anxiety with 3D content than 360º video (60%), lower heart rates in the presence of biofeedback (between 1.71% and 7.46%), and improved self-control across the three exposure levels. They appreciated our gamified elements – completing all exposure levels. This study suggests that VRET with gamification and biofeedback is an effective approach to stimulate the habituation of people with cynophobia.",biofeedback; cynophobia; gamification; VR exposure therapy,Title_Abstract,TRUE,
IEEE,conferencePaper,2024,iStrayPaws: Immersing in a Stray Animal's World through First-Person VR to Bridge Human-Animal Empathy,VR - International Symposium Virtual Reality,A*,"While Virtual Reality Perspective-Taking (VRPT) demonstrates its efficiency in inducing empathy, its application primarily focuses on vulnerable humans, not animals. Existing animal-related works mainly targets farm animals and wildlife. In this work, we focus on stray animals and introduce iStrayPaws, a VRPT system that simulates stray animals’ challenging lives. The system offers users an immersive first-person journey into the world of stray animals encountering different difficulties like inclement weather, hunger, and illnesses. Enriched with audio-visual and kinesthetic design, the system seeks to deepen users’ understanding of stray animals’ life and foster profound emotional connections. To evaluate the system, a user study was conducted, which showed that VRPT recipients exhibited significant improvement in both state and trait empathy compared to traditional method. Our research not only delivers a novel, accessible, and interactive animal empathy experience but also provides innovative solutions for addressing stray animal issues and advancing broader animal welfare work.",Embodied Experience; Empathy; Hand Mocap; Stray Animals; Virtual Reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Exploring Presence in Interactions with LLM-Driven NPCs: A Comparative Study of Speech Recognition and Dialogue Options,VR - International Symposium Virtual Reality,A*,"Combining modern technologies like large-language models (LLMs), speech-to-text, and text-to-speech can enhance immersion in virtual reality (VR) environments. However, challenges exist in effectively implementing LLMs and educating users. This paper explores implementing LLM-powered virtual social actors and facilitating user communication. We developed a murder mystery game where users interact with LLM-based non-playable characters (NPCs) through interrogation, clue-gathering, and exploration. Two versions were tested: one using speech recognition and another with traditional dialog boxes. While both provided similar social presence, users felt more immersed with speech recognition but found it overwhelming, while the dialog version was more challenging. Slow NPC response times were a source of frustration, highlighting the need for faster generation or better masking for a seamless experience.",Immersive systems; Large Language Models (LLM); NPC; Presence; Social Actors; Speech Recognition; VR,Abstract,TRUE,
IEEE,conferencePaper,2024,Effects of Different Tracker-driven Direction Sources on Continuous Artificial Locomotion in VR,VR - International Symposium Virtual Reality,A*,"Continuous artificial locomotion in VR typically involves users selecting their direction using controller input, with the forward direction determined by the Head, Hands, or less commonly, the Hip. The effects of these different sources on user experience are under-explored, and Feet have not been used as a direction source. To address these gaps, we compared these direction sources, including a novel Feet-based technique. A user study with 22 participants assessed these methods in terms of performance, preference, motion sickness, and sense of presence. Our findings indicate high levels of presence and minimal motion sickness across all methods. Performance differences were noted in one task, where the Head outperformed the Hand. The Hand method was the least preferred, feeling less natural and realistic. The Feet method was found to be more natural than the Head and more realistic than the Hip. This study enhances understanding of direction sources in VR locomotion and introduces Feet-based direction as a viable alternative.",Continuous Locomotion; User Studies; Virtual Reality,Keywords,TRUE,
IEEE,conferencePaper,2024,Influence of Rotation Gains on Unintended Positional Drift during Virtual Steering Navigation in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Unintended Positional Drift (UPD) is a phenomenon that occurs during navigation in Virtual Reality (VR). It is characterized by the user’s unconscious or unintentional physical movements in the workspace while using a locomotion technique (LT) that does not require physical displacement (e.g., steering, teleportation). Recent work showed that some factors, such as the LT used and the type of trajectory, can influence UPD. However, little is known about the influence of rotation gains (commonly used in redirection-based LTs) on UPD during navigation in VR. In this paper, we conducted two user studies to assess the influence of rotation gains on UPD. In the first study, participants had to perform consecutive turns in a corridor virtual environment. In the second study, participants had to explore a large office floor and collect spheres freely. We compared the conditions between rotation gains and without gains, and we also varied the turning angle to perform the turns while considering factors such as sensitivity to cybersickness and the learning effect. We found that rotation gains and lower turning angles decreased UPD during the first study, but the presence of rotation gains increased UPD in the second study. This work contributes to the understanding of UPD, which tends to be an overlooked topic and discusses the design implications of these results for improving navigation in VR.",Locomotion Techniques; Rotation Gains; Unintended Positional Drift; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Semi-Automated Guided Teleportation through Immersive Virtual Environments,VR - International Symposium Virtual Reality,A*,"Immersive knowledge spaces like museums or cultural sites are often explored by traversing pre-defined paths that are curated to unfold a specific educational narrative. To support this type of guided exploration in VR, we present a semi-automated, hands-free path traversal technique based on teleportation that features a slow-paced interaction workflow targeted at fostering knowledge acquisition and maintaining spatial awareness. In an empirical user study with 34 participants, we evaluated two variations of our technique, differing in the presence or absence of intermediate teleportation points between the main points of interest along the route. While visiting additional intermediate points was objectively less efficient, our results indicate significant benefits of this approach regarding the user’s spatial awareness and perception of interface dependability. However, the user’s perception of flow, presence, attractiveness, perspicuity, and stimulation did not differ significantly. The overall positive reception of our approach encourages further research into semi-automated locomotion based on teleportation and provides initial insights into the design space of successful techniques in this domain.",3D Navigation; 3D User Interfaces; Guided Navigation; Guided Tour; Head-Mounted Display; Teleportation; Virtual Reality,Keywords,TRUE,
IEEE,conferencePaper,2024,The Effects of Electrical Stimulation of Ankle Tendons on Redirected Walking with the Gradient Gain,VR - International Symposium Virtual Reality,A*,"As a redirected walking technique, a method has been proposed to enable users to walk in an undulating virtual space even in a flat physical environment by setting the slope of the floor in the virtual environment to be different from that in the physical environment without causing discomfort. However, the slope range in which discrepancies between visual and proprioceptive sensations are not perceived is limited, restricting the slopes that can be presented. In this study, we proposed redirected walking using electrical stimulation of the Achilles and tibialis anterior muscle tendons, extending the applicable slope range of redirected walking without compromising the natural gait sensation. Electrical stimulation of the ankle tendons affects the proprioceptive sensation and gives the illusion of tilting in the standing posture, expanding the applicable slope range. Two experiments showed that the proposed method improved the experience of uphill and downhill walking in terms of the range of the virtual slope where a high naturalness of gait and a high congruency of visual and proprioceptive sensations are maintained. Notably, electrical stimulation of the Achilles tendons significantly improved the naturalness of the walking experience during virtual downhill walking, which has been considered more challenging in previous studies.",Electrical stimulation of ankle tendons; Locomotion technique; Redirected walking; Transcutaneous electrical stimulation; Virtual reality,Keywords,TRUE,
IEEE,conferencePaper,2024,Neural Motion Tracking: Formative Evaluation of Zero Latency Rendering,VR - International Symposium Virtual Reality,A*,"Low motion-to-photon latencies between physical movement and rendering updates are crucial for an immersive virtual reality (VR) experience and to avoidusers’ discomfort and sickness. Current methods aim to minimize the delay between the motion measurement and rendering at the cost of increasing technical complexity and possibly decreasing accuracy. By relying on capturing physical motion, these strategies will, by nature, not result in zero latency rendering or will be based on prediction and resulting uncertainty. This paper presents and evaluates a novel alternative and proof of principle for VR motion tracking that could enable motion-to-photon latencies of zero and below zero in time. We termed our concept Neural Motion Tracking, which we define as the sensing and assessment of motion through human neural activation of the somatic nervous system. In contrast to measuring physical activity, the key principle is that we aim to utilize the physiological timeframe between a user’s intention and the execution of motion. We aim to foresee upcoming motion ahead of the physical movement, by sampling preceding electromyographic signals before the muscle activation. The electromechanical delay (EMD) between potential change in the muscle activation and actual physical movement opens a gap in which measurement can be taken and evaluated before the physical motion. In a first proof of principle, we evaluated the concept with two activities, arm bending and head rotation, measured with a binary activation measure. Our results indicate that it is possible to predict movement and update a rendering up to 2&nbsp;ms before its physical execution, which is assessed by optical tracking after approximately 4&nbsp;ms. However, to make the best use of this advantage, electromyography (EMG) sensor data should be as high quality as possible (i.e., low noise and from muscle-near electrodes). Our results empirically quantify this characteristic for the first time when compared to state-of-the-art optical tracking systems for VR. We discuss our results and potential pathways to motivate further work toward marker- and latency-less motion tracking.",augmented reality; electromyography; latency; mixed reality; tracking; Virtual reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Investigation of Redirection Algorithms in Small Tracking Spaces,VR - International Symposium Virtual Reality,A*,"In virtual reality, redirected walking lets users walk in larger virtual spaces than the physical tracking space set aside for their movements. This benefits immersion and spatial navigation compared to virtual locomotion techniques such as teleportation or joystick control. Different algorithms have tried to optimise redirected walking. These algorithms have been tested in simulation in large spaces and with small user studies. However, few studies have looked at the user experience of these algorithms in small tracking spaces. We conducted a user study to compare the performance of different redirected walking algorithms in a small tracking space of 3.5m x 3.5m. Three algorithms were chosen based on their approaches to redirection – Reset Only, Steer to Centre and Alignment Based Redirection Control. 36 people participated in the study. It was found users preferred Reset Only in the tracking space. Reset Only redirects users less and is easier to implement than Steer to Centre or Alignment Based Redirection Control. Additionally, Reset Only had similar performance to Steer to Centre and better task performance than Alignment Based Redirection Control despite resetting users more often. Based on these findings, we provide guidelines for developers working in small tracking spaces.",locomotion; redirected walking; user experience; user study; virtual reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Interactive Multi-GPU Light Field Path Tracing Using Multi-Source Spatial Reprojection,VR - International Symposium Virtual Reality,A*,"Path tracing combined with multiview displays enables progress towards achieving ultrarealistic virtual reality. However, multiview displays based on light field technology impose a heavy workload for real-time graphics due to the large number of views to be rendered. In order to achieve low latency performance, computational effort can be reduced by path tracing only some views (source views), and synthesizing the remaining views (target views) through spatial reprojection, which reuses path traced pixels from source views to target views. Deciding the number of source views with respect to the computational resources is not trivial, since spatial reprojection introduces dependencies in the otherwise trivially parallel rendering pipeline and path tracing multiple source views increases the computation time. In this paper, we demonstrate how to reach near-perfect linear multi-GPU scalability through a coarse-grained distribution of the light field path tracing workload. Our multi-source method path traces a single source view per GPU, which helps decreasing the number of dependencies. Reducing dependencies reduces the overhead of image transfers and G-Buffers rasterization used for spatial reprojection. In a node of 4 × RTX A6000 GPUs, given 4 source views, we reach a light field rendering frequency of 3–19 Hz, which corresponds to interactive rate. On four test scenes, we outperform state-of-the-art multi-GPU light field path tracing pipelines, achieving a speedup of 1.65 × up to 4.63 × for 1D light fields of dimension 100 × 1, each view having a resolution of 768 × 432, and 1.51 × up to 3.39 × for 2D stereo near-eye light fields of size 12 × 6 (left eye: 6 × 6 views and right eye: 6 × 6 views), 1024 × 1024 per view.",Dependencies; Multiview; Parallel Rendering; View Synthesis,Abstract,TRUE,
IEEE,conferencePaper,2024,Exploring Visual Conditions in Virtual Reality for the Teleoperation of Robots,VR - International Symposium Virtual Reality,A*,"In the teleoperation of robots, the absence of proprioception means that visual information plays a crucial role. Previous research has investigated methods to offer optimal vantage points to operators during teleoperation, with virtual reality (VR) being proposed as a mechanism to give the operator intuitive control over the viewpoint for improved visibility and interaction. However, the most effective perspective for robot operation and the optimal portrayal of the robot within the virtual environment remain unclear. This paper examines the impact of various visual conditions on users’ efficiency and preference in controlling a simulated robot via VR. We present a user study that compares two operating perspectives and three robot appearances. The findings indicate mixed user preferences and highlight distinct advantages associated with each perspective and appearance combination. We conclude with recommendations on selecting the most beneficial perspective and appearance based on specific application requirements.",3D User Interfaces; Teleoperation; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Choose Your Reference Frame Right: An Immersive Authoring Technique for Creating Reactive Behavior,VR - International Symposium Virtual Reality,A*,"Immersive authoring enables content creation for virtual environments without a break of immersion. To enable immersive authoring of reactive behavior for a broad audience, we present modulation mapping, a simplified visual programming technique. To evaluate the applicability of our technique, we investigate the role of reference frames in which the programming elements are positioned, as this can affect the user experience. Thus, we developed two interface layouts: ""surround-referenced"" and ""object-referenced"". The former positions the programming elements relative to the physical tracking space, and the latter relative to the virtual scene objects. We compared the layouts in an empirical user study (n = 34) and found the surround-referenced layout faster, lower in task load, less cluttered, easier to learn and use, and preferred by users. Qualitative feedback, however, revealed the object-referenced layout as more intuitive, engaging, and valuable for visual debugging. Based on the results, we propose initial design implications for immersive authoring of reactive behavior by visual programming. Overall, modulation mapping was found to be an effective means for creating reactive behavior by the participants.",Empirical Evaluation; Immersive Authoring; Spatial Reference Frames; Virtual Reality; Visual Programming,Keywords,TRUE,
IEEE,conferencePaper,2024,Motion Passwords,VR - International Symposium Virtual Reality,A*,"This paper introduces “Motion Passwords”, a novel biometric authentication approach where virtual reality users verify their identity by physically writing a chosen word in the air with their hand controller. This method allows combining three layers of verification: knowledge-based password input, handwriting style analysis, and motion profile recognition. As a first step towards realizing this potential, we focus on verifying users based on their motion profiles. We conducted a data collection study with 48 participants, who performed over 3800 Motion Password signatures across two sessions. We assessed the effectiveness of feature-distance and similarity-learning methods for motion-based verification using the Motion Passwords as well as specific and uniform ball-throwing signatures used in previous works. In our results, the similarity-learning model was able to verify users with the same accuracy for both signature types. This demonstrates that Motion Passwords, even when applying only the motion-based verification layer, achieve reliability comparable to previous methods. This highlights the potential for Motion Passwords to become even more reliable with the addition of knowledge-based and handwriting style verification layers. Furthermore, we present a proof-of-concept Unity application demonstrating the registration and verification process with our pretrained similarity-learning model. We publish our code, the Motion Password dataset, the pretrained model, and our Unity prototype on https://github.com/cschell/MoPs",Authentication; Biometrics; Extended Reality; Verification,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Out-Of-Virtual-Body Experiences: Virtual Disembodiment Effects on Time Perception in VR,VR - International Symposium Virtual Reality,A*,"This paper presents a novel experiment investigating the relationship between virtual disembodiment and time perception in Virtual Reality (VR). Recent work demonstrated that the absence of a virtual body in a VR application changes the perception of time. However, the effects of simulating an out-of-body experience (OBE) in VR on time perception are still unclear. We designed an experiment with two types of virtual disembodiment techniques based on viewpoint gradual transition: a virtual body’s behind view and facing view transitions. We investigated their effects on forty-four participants in an interactive scenario where a lamp was repeatedly activated and time intervals were estimated. Our results show that, while both techniques elicited a significant virtual disembodiment perception, time duration estimations in the minute range were only shorter in the facing view compared to the eye view condition. We believe that reducing agency in the facing view is a key factor in the time perception alteration. This provides first steps towards a novel approach to manipulating time perception in VR, with potential applications for mental health treatments such as schizophrenia or depression and for improving our understanding of the relation between body, virtual body, and time.",Avatar; Disembodiment; Embodiment; Plausibility; Presence; Time Perception; Virtual Body; Virtual Reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Some Times Fly: The Effects of Engagement and Environmental Dynamics on Time Perception in Virtual Reality,VR - International Symposium Virtual Reality,A*,"An hour spent with friends seems shorter than an hour waiting for a medical appointment. Many physiological and psychological factors, such as body temperature and emotions, have been shown to correlate with our subjective perception of time. Experiencing virtual reality (VR) has been observed to make users significantly underestimate the duration. This paper explores the effect of virtual environment characteristics on time perception, focusing on two key parameters: user engagement and environmental dynamics. We found that increased presence and interaction with the environment significantly decreased the users’ estimation of the VR experience duration. Furthermore, while a dynamic environment lacks significance in shifting perception toward one specific direction, that is, underestimation or overestimation of the durations, it significantly distorts perceived temporal length. Exploiting these two factors’ influence smartly constitutes a powerful tool in designing intelligent and adaptive virtual environments that can reduce stress, alleviate boredom, and improve well-being by adjusting the pace at which we experience the passage of time.",Environmental Dynamics; Time Perception; User Engagement; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Enhancing VR Sketching with a Dynamic Shape Display,VR - International Symposium Virtual Reality,A*,"Sketching on virtual objects in Virtual Reality (VR) can be challenging due to the lack of a physical surface that constrains the movement and provides haptic feedback for contact and movement. While using a flat physical drawing surface has been proposed, it creates a significant discrepancy between the physical and virtual surfaces when sketching on non-planar virtual objects. We propose using a dynamic shape display that physically mimics the shape of a virtual surface, allowing users to sketch on a virtual surface as if they are sketching on a physical object’s surface. We demonstrate this using VRScroll, a shape-changing device that features seven independently controlled flaps to imitate the shape of a virtual surface automatically. Our user study showed that participants exhibited higher precision when tracing simple shapes with the dynamic shape display and produced clearer sketches. We also provided several design implications for dynamic shape displays aimed at enabling precise sketching in VR.",dynamic shape display; on-surface interactions; virtual reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Simulating Object Weight in Virtual Reality: The Role of Absolute Mass and Weight Distributions,VR - International Symposium Virtual Reality,A*,"Weight interfaces enable users of Virtual Reality (VR) to perceive the weight of virtual objects, significantly enhancing realism and enjoyment. While research on these systems primarily focused on their implementation, little attention has been given to determining the weight to be rendered by them: As the perceived weight of objects is influenced not only by their absolute mass, but also by their weight distribution and prior expectations, it is currently unknown which simulated mass provides the most realistic representation of a given object. We conducted a study, in which 30 participants chose the best fitting weight for a virtual object in 54 experimental trials. Across these trials, we systematically varied the virtual objects’ visual mass (three levels), their weight distribution (six levels), and the position of the physical mass on the grip (three levels). Our Bayesian analysis suggests that the visual weight distribution of objects does not affect which absolute physical mass best represents them, whereas the position of the provided physical mass does. Additionally, participants overweighted virtual objects with lower visual mass while underweighting objects with higher visual mass. We discuss how these findings can be leveraged by designers of weight interfaces and VR experiences to optimize realism.",multisensory integration; virtual reality; weight interfaces; weight perception; weight simulation,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Enriching Industrial Training Experience in Virtual Reality with Pseudo-Haptics and Vibrotactile Stimulation,VR - International Symposium Virtual Reality,A*,"Virtual Reality (VR) technology facilitates effective, flexible, and safe industrial training for novice technicians when on-site training is not feasible. However, previous research has shown that training in VR may be less successful than traditional learning approaches in real-world settings, and haptic interaction may be the key to improving virtual training. In this study, we integrated pseudo-haptic feedback from motion delay with vibrotactile stimulation to enhance the sense of presence, enjoyment, and the perception of physical properties in VR, which may be crucial for achieving faithful simulations. The impact of combined haptic support was assessed in a complex industrial training procedure completing a variety of tasks such as component assembly and cleaning. The results indicate that vibrotactile cues are beneficial for presence and enjoyment, whereas pseudo-haptic illusions effectively enable kinesthetic sensations. Furthermore, multimodal haptic feedback that mixed the two yielded the most advantageous outcomes. Our findings highlight the potential of the pseudo-haptic and vibrotactile fusion in industrial training scenarios, presenting practical implications of the state-of-the-art haptic technologies for virtual learning.",Haptics; Industrial training; Multimodal interaction; User study; Virtual reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Investigating the Impact of Odors and Visual Congruence on Motion Sickness in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Motion sickness is a prevalent side effect of exposure to virtual reality (VR). Previous work found that pleasant odors can be effective in alleviating symptoms of motion sickness such as nausea. However, it is unknown whether pleasant odors that do not match the anticipated scent of the virtual environment are also effective as they could, in turn, amplify symptoms such as disorientation. Therefore, we conducted a study with 24 participants experiencing a pleasant odor (rose) and an unpleasant odor (garlic) while being immersed in a virtual environment involving either virtual roses or garlic. We found that participants had lower motion sickness when experiencing the rose odor, however, only in the rose environment. Accordingly, we also showed that the sense of disorientation was lower for the rose odor, however, only while being immersed in the rose environment. Results indicate that whether pleasant odors are effective in alleviating motion sickness symptoms depends on the visual appearance of the virtual environment. We discuss possible explanations for such effects to occur. Our work contributes to the goal of mitigating visually induced motion sickness in VR.",motion sickness; odor; olfaction; virtual reality; visually induced motion sickness,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Generative Terrain Authoring with Mid-air Hand Sketching in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Terrain generation and authoring in Virtual Reality (VR) offers unique benefits, including 360-degree views, improved spatial perception, immersive and intuitive design experience and natural input modalities. Yet even in VR it can be challenging to integrate natural input modalities, preserve artistic controls and lower the effort of landscape prototyping. To tackle these challenges, we present our VR-based terrain generation and authoring system, which utilizes hand tracking and a generative model to allow users to quickly prototype natural landscapes, such as mountains, mesas, canyons and volcanoes. Via positional hand tracking and hand gesture detection, users can use their hands to draw mid-air strokes to indicate desired shapes for the landscapes. A Conditional Generative Adversarial Network trained by using real-world terrains and their height maps then helps to generate a realistic landscape which combines features of training data and the mid-air strokes. In addition, users can use their hands to further manipulate their mid-air strokes to edit the landscapes. In this paper, we explore this design space and present various scenarios of terrain generation. Additionally, we evaluate our system across a diverse user base that varies in VR experience and professional background. The study results indicate that our system is feasible, user-friendly and capable of fast prototyping.",Generative Terrain Authoring; Hand Gesture Control; Hand Sketching; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,How Different Is the Perception of Vibrotactile Texture Roughness in Augmented versus Virtual Reality?,VR - International Symposium Virtual Reality,A*,"Wearable haptic devices can modify the haptic perception of an object touched directly by the finger in a portable and unobtrusive way. In this paper, we investigate whether such wearable haptic augmentations are perceived differently in Augmented Reality (AR) vs. Virtual Reality (VR) and when touching with a virtual hand instead of one’s own hand. We first designed a system for real-time rendering of vibrotactile virtual textures without constraints on hand movements, integrated with an immersive visual AR/VR headset. We then conducted a psychophysical study with 20 participants to evaluate the haptic perception of virtual roughness textures on a real surface touched directly with the finger (1) without visual augmentation, (2) with a realistic virtual hand rendered in AR, and (3) with the same virtual hand in VR. On average, participants overestimated the roughness of haptic textures when touching with their real hand alone and underestimated it when touching with a virtual hand in AR, with VR in between. Exploration behaviour was also slower in VR than with real hand alone, although subjective evaluation of the texture was not affected. We discuss how the perceived visual delay of the virtual hand may produce this effect.",Augmented Reality; Haptic Perception; Psychophysical Study; Roughness Textures; Virtual Hands; Virtual Reality; Wearable Haptics,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,TeenWorlds: Supporting Emotional Expression for Teenagers with their Parents and Peers through a Collaborative VR Experience,VR - International Symposium Virtual Reality,A*,"Adolescence is a period of growth and exploration, marked by influential relationships with peers and parents. These relationships are essential for teenagers’ well-being, highlighting the need to support their interpersonal interactions. Emotional expression is key in resolving conflicts that can frequently arise. This paper investigates the potential of TeenWorlds, a Virtual Reality (VR) application, to facilitate emotional expression and shared understanding among teenagers and their peers and parents. In our study, teenagers, accompanied by either a peer or a parent (total n=42), used TeenWorlds to visually represent their emotions during a shared conflict, discuss them, and collaborate on a joint VR drawing. Our findings indicate that TeenWorlds can foster communication, reflection, and strengthen interpersonal relationships. However, notable differences were observed in interactions with peers versus parents. We contribute insights into designing VR systems that support reflective experiences and meaningful family interactions, ultimately enhancing the well-being of adolescents, parents, and families.",adolescent; CCI; collaboration; emotional expression; family; parents; reflection; teenager; Virtual Reality; youth,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,HistoLab VR: A User Elicitation Study Exploring the Potential of Virtual Reality Game-based Learning for Hazard Awareness,VR - International Symposium Virtual Reality,A*,"Occupational medicine is a vital field for workplace safety and health but often encounters challenges in engaging students and effectively communicating subtle yet critical workplace hazards. To tackle these issues, we developed HistoLab VR, a Virtual Reality (VR) game that immerses participants in a histology lab environment based on real-world practice. Our comprehensive user study with 17 students and experts assessed the game’s impact on hazard awareness, interest in occupational medicine, and user experience through quantitative and qualitative measures. Our findings show that HistoLab VR not just immersed participants in a relatable histology lab worker experience but that it effectively raised awareness about subtle hazards and conveyed the inherent stress of the job. We discuss our results and highlight the potential of VR as a valuable educational tool for occupational medicine training.",anxiety; education; ergonomics; hazard awareness; histology laboratory; occupational medicine; serious games.; workplace,Title_Abstract,TRUE,
IEEE,conferencePaper,2024,Game-Based Motivation: Enhancing Learning with Achievements in a Customizable Virtual Reality Environment,VR - International Symposium Virtual Reality,A*,"Digital learning experiences that promote interactive learning and engagement are becoming increasingly relevant. Educational games can be used to create an engaging learning atmosphere that allows knowledge acquisition through hands-on activities. Combining it with virtual reality (VR) allows users to interact with virtual environments, leading to a highly immersive learning experience. In this study, we explore how game achievements impact motivation and learning in a customizable VR learning environment. Using an A/B test involving 50 students, we utilized an interactive wave simulation to assess motivation, engagement, and the overall learning experience. Data collection involved standardized questionnaires, along with tracking interaction time and interactions within the virtual environment. The findings revealed that users who earned game achievements to unlock customization features felt significantly more accomplished when they mastered challenges and obtained all achievements. However, it was observed that adding achievements could also create pressure on students, leading to feelings of embarrassment when facing task failures. While achievements have the potential to enhance engagement and motivation, their excessive use may lead to distractions, anxiety, and reduced overall engagement. It shows that is crucial to find a good balance in employing game achievements within educational environments to ensure they contribute positively to the learning experience without causing undue stress or deterring learners.",customizable learning; immersive learning; interactive simulations; STEM education; virtual reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Hands or Controllers? How Input Devices and Audio Impact Collaborative Virtual Reality,VR - International Symposium Virtual Reality,A*,"Advancing virtual reality technologies are enabling real-time virtual-face to virtual-face communication. Hand tracking systems that are integrated into Head-Mounted Displays (HMD) enable users to directly interact with their environments and with each other using their hands as opposed to using controllers. Due to the novelties of these technologies our understanding of how they impact our interactions is limited. In this paper, we investigate the consequences of using different interaction control systems, hand tracking or controllers, when interacting with others in a virtual environment. We design and implement NASA’s Survival on the Moon teamwork evaluation exercise in virtual reality (VR) and test for effects with and without allowing verbal communication. We evaluate social presence, perceived comprehension, team cohesion, group synergy, task workload, as well as task performance and duration. Our findings reveal that audio communication significantly enhances social presence, perceived comprehension, and team cohesion, but it also increases effort workload and negatively impacts group synergy. The choice of interaction control systems has limited impact on various aspects of virtual collaboration in this scenario, although participants using hand tracking reported lower effort workload, while participants using controllers reported lower mental workload in the absence of audio.",avatars; collaboration; Communication; gestures,Title_Abstract,TRUE,
IEEE,conferencePaper,2024,Exploring User Placement for VR Remote Collaboration in a Constrained Passenger Space,VR - International Symposium Virtual Reality,A*,"Extended Reality (XR) offers the potential to transform the passenger experience by allowing users to inhabit varied virtual spaces for entertainment, work or social interaction, whilst escaping the constrained transit environment. XR allows remote collaborators to feel like they are together and enables them to perform complex 3D tasks. However, the social and physical constraints of the passenger space pose unique challenges to productive and socially acceptable collaboration. Using a collaborative VR puzzle task, we examined the effects of five different f-formations of collaborator placement and orientation in an interactive workspace on social presence, task workload, and implications for social acceptability. Our quantitative and qualitative results showed that face-to-face formations were preferred for tasks with a high need for verbal communication but may lead to social collisions, such as inadvertently staring at a neighbouring passenger, or physical intrusions, such as gesturing in another passenger’s personal space. More restrictive f-formations, however, were preferred for passenger use as they caused fewer intrusions on other passengers’ visual and physical space.",Collaboration; Constrained Spaces; Mixed Reality; Passenger Spaces; Social Acceptability; Virtual Reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Stand Alone or Stay Together: An In-situ Experiment of Mixed-Reality Applications in Embryonic Anatomy Education,VR - International Symposium Virtual Reality,A*,"Where traditional media and methods reach their limits in anatomy education, mixed-reality (MR) environments can provide effective learning support because of their high interactivity and spatial visualization capabilities. However, the underlying design and pedagogical requirements are as diverse as the technologies themselves. This paper examines the effectiveness of individual- and collaborative learning environments for anatomy education, using embryonic heart development as an example. Both applications deliver the same content using identical visualizations and hardware but differ in interactivity and pedagogical approach. The environments were evaluated in a user study with medical students (n = 90) during their examination phase, assessing usability, user experience, social interaction/co-presence, cognitive load, and personal preference. Additionally, we conducted a knowledge test before and after an MR learning session to determine educational effects compared to a conventional anatomy seminar. Results indicate that the individual learning environment was generally preferred. However, no significant difference in learning effectiveness could be shown between the conventional approach and the MR applications. This suggests that both can effectively complement traditional seminars despite their different natures. Our study contributes to understanding how different MR settings could be tailored for anatomical education.",Collaborative Learning; Immersive Learning Environments; Individual Adaptive Learning; Medical Education; Mixed Reality,Keywords,TRUE,
IEEE,conferencePaper,2024,Contextual Matching Between Learning and Testing Within VR Does Not Always Enhance Memory Retrieval,VR - International Symposium Virtual Reality,A*,"Episodic memory is influenced by environmental contexts, such as location and auditory stimuli. The most well-known effect is the reinstatement effect, which refers to the phenomenon where contextual matching between learning and testing enhances memory retrieval. Previous studies have investigated whether the reinstatement effect can be observed within immersive virtual environments. However, only a limited number of studies have reported a significant reinstatement effect using virtual reality, while most have failed to detect it. In this study, we re-examined the reinstatement effect using 360-degree video-based virtual environments. Specifically, we carefully selected virtual environments to elicit different emotional responses, which has been suggested as a key factor in inducing a robust reinstatement effect in the physical world. Surprisingly, we found a significant reversed reinstatement effect with a large effect size. This counter-intuitive result suggests that contextual congruence does not necessarily enhance memory and may even interfere with it. This outcome may be explained by the retrieval-induced forgetting phenomenon, but further exploration is needed. This finding is particularly important for virtual reality-based, educational applications and highlights the need for a deeper understanding of the complex interactions between memory and contextual cues within virtual environments.",360-degree video; environmental context-dependent memory; reinstatement; retrieval-induced forgetting; virtual reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Toward Facilitating Search in VR With the Assistance of Vision Large Language Models,VR - International Symposium Virtual Reality,A*,"While search is a common need in Virtual Reality (VR) applications, current approaches are cumbersome, often requiring users to type on a mid-air keyboard using controllers in VR or remove VR equipment to search on a computer. We first conducted a literature review and a formative study, identifying six common search needs: knowing about one object, knowing about the object’s partial details, knowing objects with environmental context, knowing about interactions with objects, and finding objects within field of view (FOV) and out of FOV in the VR scene. Informed by these needs, we designed technology probes that leveraged recent advances in Vision Large Language Models and conducted a probe-based study with users to elicit feedback. Based on the findings, we derived design principles for VR designers and developers to consider when designing a user-friendly search interface in VR. While prior work about VR search tended to address specific aspects of search, our work contributes design considerations aimed at enhancing the ease of search in VR and potential future directions.",participatory design; Virtual reality; vision large language model; VR search,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Evaluating Gaze Interactions within AR for Nonspeaking Autistic Users,VR - International Symposium Virtual Reality,A*,"Nonspeaking autistic individuals often face significant inclusion barriers in various aspects of life, mainly due to a lack of effective communication means. Specialized computer software, particularly delivered via Augmented Reality (AR), offers a promising and accessible way to improve their ability to engage with the world. While research has explored near-hand interactions within AR for this population, gaze-based interactions remain unexamined. Given the fine motor skill requirements and potential for fatigue associated with near-hand interactions, there is a pressing need to investigate the potential of gaze interactions as a more accessible option. This paper presents a study investigating the feasibility of eye gaze interactions within an AR environment for nonspeaking autistic individuals. We utilized the HoloLens 2 to create an eye gaze-based interactive system, enabling users to select targets either by fixating their gaze for a fixed period or by gazing at a target and triggering selection with a physical button (referred to as a ‘clicker’). We developed a system called HoloGaze that allows a caregiver to join an AR session to train an autistic individual in gaze-based interactions as appropriate. Using HoloGaze, we conducted a study involving 14 nonspeaking autistic participants. The study had several phases, including tolerance testing, calibration, gaze training, and interacting with a complex interface: a virtual letterboard. All but one participant were able to wear the device and complete the system’s default eye calibration; 10 participants completed all training phases that required them to select targets using gaze only or gaze-click. Interestingly, the 7 users who chose to continue to the testing phase with gaze-click were much more successful than those who chose to continue with gaze alone. We also report on challenges and improvements needed for future gaze-based interactive AR systems for this population. Our findings pave the way for new opportunities for specialized AR solutions tailored to the needs of this under-served and under-researched population.",assistive technology; augmented reality; eye tracking; nonspeaking autistic people,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Exploring Immersive Debriefing in Virtual Reality Training: A Comparative Study,VR - International Symposium Virtual Reality,A*,"Simulation and debriefing are two essential and inseparable phases of virtual reality training. With the widespread adoption of these training tools, it is crucial to define the best pedagogical approaches for trainers and learners to maximize their effectiveness. However, despite their educational benefits, virtual reality-specific debriefing methods remain underexplored in research. This article proposes an architecture and interface for an all-in-one immersive debriefing module that is adaptable to different types of training, including a complete system for recording, replaying, and redoing actions. A study with 36 participants compared this immersive debriefing system with traditional discussion-based and video-supported debriefing. Participants were divided into three groups to evaluate the effectiveness of each method. The results showed no significant differences between these debriefing methods across several criteria, such as satisfaction, motivation, or information retention. Immersive debriefing is as usable and retentive as traditional or video debriefing in this context. The next step will be to evaluate the redo system in other training courses involving more dynamic scenarios.",Debriefing; Immersive Learning; Simulation; Trainer; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,"A Critical Review of Virtual and Extended Reality Immersive Police Training: Application Areas, Benefits &amp; Vulnerabilities",VR - International Symposium Virtual Reality,A*,"Virtual and Extended Reality (VR/XR) headsets have promised to enhance police training through the delivery of immersive simulations able to be conducted anywhere, anytime. However, little consideration has been given to reviewing the evidenced benefits and potential issues posed by XR police training. In this paper, we summarise the evidenced usage and benefits of XR police training through a formative targeted literature review (n=41 publications). We then reflect on the prospective technical, security, social and legal issues posed by XR police training, identifying four areas where issues or vulnerabilities exist: training content, trainees and trainers, systems and devices, and state and institutional stakeholders. We highlight significant concerns around e.g. the validity of training; the psychological impact and risks of trauma; the safety and privacy risks posed to trainees and trainers; and the risks to policing institutions. We aim to encourage end-user communities (e.g. police forces) to more openly reflect on the risks of immersive training, so we can ultimately move towards transparent, validated, trusted training that is evidenced to improve policing outcomes.",Extended Reality; Police Training; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,The Impact of Task-Responsibility on User Experience and Behaviour under Asymmetric Knowledge Conditions,VR - International Symposium Virtual Reality,A*,"Virtual Reality presents a promising tool for knowledge transfer, allowing users to learn in different environments and with the help of three-dimensional visualizations. At the same time, having to learn new ways of interacting with their environment can present a significant hurdle for novice users. When users enter a virtual space to receive knowledge from a more experienced person, the question arises as to whether they benefit from learning VR-specific interaction techniques instead of letting the expert take over some or all interactions. Based on related work about expert-novice interaction in virtual spaces, this paper presents a user study comparing three different distributions of interaction responsibilities between participants and an expert user. The Role-Based interaction mode gives the expert the full interaction responsibility. The Shared interaction mode gives both users the same interaction capabilities, allowing them to share the responsibility of interacting with the virtual space. Finally, the Parallel interaction mode gives participants full interaction responsibility, while the expert can provide guidance through oral communication and visual demonstration. Our results indicate that assuming interaction responsibility led to higher task loads but also increased the participant’s engagement and feeling of presence. For most participants, sharing interaction responsibilities with the expert represented the best trade-off between engagement and challenge. While we did not measure a significant increase in learning success, participant comments indicated that they also paid more attention to details when assuming more interaction responsibility.",3D User Interfaces; Collaboration; Head-Mounted Display; Instruction; Knowledge-Transfer; Virtual Reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Evaluating the effects of Situated and Embedded Visualisation in Augmented Reality Guidance for Isolated Medical Assistance,VR - International Symposium Virtual Reality,A*,"One huge advantage of Augmented Reality (AR) is its numerous possibilities of displaying information in the physical world, especially when applying Situated Analytics (SitA). AR devices and their respective interaction techniques allow for supplementary guidance to assist an operator carrying out complex procedures such as medical diagnosis and surgery, for instance. Their usage promotes user autonomy by presenting relevant information when the operator may not necessarily possess expert knowledge of every procedure and may also not have access to external help such as in a remote or isolated situation (e.g., International Space Station, middle of an ocean, desert). In this paper, we propose a comparison of two different forms of AR visualisation: An embedded visualisation and a situated projected visualisation, with the aim to assist operators with the most appropriate visualisation format when carrying out procedures (medical in our case). To evaluate these forms of visualisation, we carried out an experiment involving 23 participants possessing latent/novice medical knowledge. These participant profiles were representative of operators who are medically trained yet do not apply their knowledge every day (e.g., an astronaut in orbit or a sailor out at sea). We discuss our findings which include the advantages of embedded visualised information in terms of precision compared to situated projected information with the accompanying limitations in addition to future improvements to our proposition. We conclude with the prospects of our work, notably the continuation and possibility of evaluating our proposition in a less controlled and real context in collaboration with our national space agency.",AR Guidance; Augmented Reality; Immersive Analytics; Isolated Situation; Medical Assistance; Procedure Execution; Situated Analytics; Situated Visualisation,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,An Evaluation of Targeting Methods in Spatial Computing Interfaces with Visual Distractions,VR - International Symposium Virtual Reality,A*,"In modern spatial computing devices, users are confronted with diverse methods for object selection, including eye gaze (cf. Apple Vision Pro), hand gestures (cf. Microsoft HoloLens 2), touch gestures (cf. Google Glass Enterprise Edition 2), and external controllers (cf. Magic Leap 2). Although there are a plethora of empirical studies on which selection techniques perform best, a common limiting factor stems from the partly artificial setups. These typically exclude practical influences such as visual distraction. In this paper, we present a user study comparing two hand-based and two gaze-based state-of-the-art selection methods, using the HoloLens 2. We extended a traditional Fitts’ law-inspired study design by incorporating a visual task that simulates changes in the user interface after a successful selection. Without a visual task, gaze-based techniques were on average faster than hand-based techniques. This performance gain was eliminated (for head gaze) or even reversed (for eye gaze) when the visual task was active. These findings underscore the value of continued practice-oriented research of targeting methods in virtual environments.",Augmented Reality; Interaction Techniques; Selection,Keywords,TRUE,
IEEE,conferencePaper,2024,Evaluation of AR Pattern Guidance Methods for a Surface Cleaning Task,VR - International Symposium Virtual Reality,A*,"Cleanroom cleaning is a surface coverage task where the pattern should be followed correctly, and the entire surface should be covered. We investigate the efficacy of augmented reality (AR) by implementing various pattern guidance designs to enhance a cleanroom cleaning task. We developed an AR guidance system for cleaning procedures and evaluated four distinct pattern guidance methods: (1) breadcrumbs, (2) examples, (3) middle lines, and (4) outlines. We vary the instructions on the entire surface or as a single step. To measure performance, accuracy, and user satisfaction associated with each guidance method, we conducted a large-scale (n=864) between-subjects study. Our findings indicate that single step instructions proved to be more intuitive and efficient than full instructions, especially for the breadcrumbs. We also discussed the implications of our results for the development of AR applications for surface coverage and pattern optimization.",Augmented Reality; Motion control.; Pattern guidance,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Editing Immersive Recordings: An Elicitation Study,VR - International Symposium Virtual Reality,A*,"Immersive recordings capture virtual reality interactions and are used in various contexts such as education and entertainment. However, there has been only limited research on requirements and techniques for editing such recordings. We interviewed expert editors of video recordings to understand their workflows, familiarised them with immersive recordings, and asked them about what editing challenges and capabilities they can envision for immersive recordings. The experts identified several functionalities they considered relevant for editing, including viewer placement, control over the viewer’s size, support for live and asynchronous collaboration, and different transition types.",Immersive Editing; Immersive Recordings; Virtual Reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Towards an Avatar Customization System for Semi-realistic Ethnically-diverse Virtual Reality Avatars,VR - International Symposium Virtual Reality,A*,"Due to the Proteus effect, in which people modify their behaviour based on their avatar, participant avatar representation is an important factor in virtual reality (VR) studies. We develop an open source prototype avatar customization system that enables quick customization of semi-realistic, ethnically-diverse avatars. The prototype provides options for customizing body and face shape, hairstyle, glasses, religious clothing, and skin, eye, and hair colour. The prototype generates avatar assets that are fully rigged and textured for incorporation into VR study code, and it serves as a step towards designing more inclusive VR research studies.",avatar customization; Proteus effect; virtual avatars,Title_Abstract,TRUE,
IEEE,conferencePaper,2024,"Comparing Tracking Accuracy in Standalone MR-HMDs: Apple Vision Pro, Hololens 2, Meta Quest 3, and Pico 4 Pro",VR - International Symposium Virtual Reality,A*,"Modern Mixed Reality Head-Mounted Displays (MR-HMDs) can track user movements across large spaces without external markers. This study evaluates the tracking accuracy and the loop closure capabilities of four commercially available MR-HMDs across four distinct scenarios. We found consistent tracking performance in well-lit and expansive environments for all devices. Tracking accuracy remained stable even in outdoor nighttime conditions. Furthermore, most HMDs demonstrated effective error correction during loop closure, with errors in non-loop scenarios consistently exceeding those in loop scenarios.",Mixed Reality; Tracking Accuracy; Visual Inertial Odometry,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Exploring Alternative Text Input Modalities in Virtual Reality: A Comparative Study,VR - International Symposium Virtual Reality,A*,"Text input in Virtual Reality (VR) is crucial for communication, search, and productivity. We compared four keyboard designs for VR text entry, leveraging the flexibility and the tracking options of a 3D environment. We used the Dvorak layout to control for experience differences. The designs were: (a) a floating keyboard with touch input, (b) a keyboard attached on the back of the hand with touch input, (c) a floating keyboard with eye tracking and pinch input, and (d) a keyboard laid out over a rolling shape with touch input. Designs (b), (c), and (d) can move in 3D space, while design (a) is static. Design (d) had similar efficiency to design (a) but with better usability and lower Physical Demand. Design (b) led to higher Physical Demand, Effort, and Frustration. Design (c) had lower Physical Demand but higher Mental Demand, Effort, and error rates. Typing speeds averaged 6.51 WPM (1.24% error rate) for (a), 5.56 WPM (3.82% error rate) for (b), 5.33 WPM (1.43% error rate) for (c), and 6.70 WPM (1.64% error rate) for (d).",Interface design; Text input; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,A Study on the Effectiveness of Augmented Reality Signal-Integrated Camera Monitor Systems for Safe Lane Changing,VR - International Symposium Virtual Reality,A*,"This study investigates the effectiveness of augmented reality (AR) signals in camera monitor systems (CMS) for enhancing safety during lane changes. Seventy participants used seven side mirror conditions, including traditional side mirrors and six CMS conditions with and without AR signals. Results showed that CMS with AR signals significantly reduced the number of collisions and reaction time compared to CMS without AR signals.",Augmented Reality; Camera Monitor Systems; Driving Safety; Driving Simulation; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Exploring Influencers' and Users' Experiences in Douyin's Virtual Reality Live-Streaming,VR - International Symposium Virtual Reality,A*,"VR live-streaming has become an emerging part on Douyin. This study aims to explore the technical modes, content strategies, user experiences in Douyin‘s VR live-streaming. Through interviews and focus groups, we found that VR technology is recognized by influencers and has become an essential part of their creative practice. For some influencers, VR technology is a key factor in enhancing audience engagement and immersive experiences, although technical literacy barriers may arise when setting up VR scenes. We also provide dimensions for improving and developing user adoption and experience of VR technology in social media environments.",Douyin; Influencers; Live-Streaming; Users; Virtual Reality,Title_Keywords,TRUE,
IEEE,conferencePaper,2024,Digital Eyes: Social Implications of XR EyeSight,VR - International Symposium Virtual Reality,A*,"The EyeSight feature, introduced with the new Apple Vision Pro XR headset, promises to revolutionize user interaction by simulating real human eye expressions on a digital display. This feature could enhance XR devices’ social acceptability and social presence when communicating with others outside the XR experience. In this pilot study, we explore the implications of the EyeSight feature by examining social acceptability, social presence, emotional responses, and technology acceptance. Eight participants engaged in conversational tasks in three conditions to contrast experiencing the Apple Vision Pro with EyeSight, the Meta Quest 3 as a reference XR headset, and a face-to-face setting. Our preliminary findings indicate that while the EyeSight feature improves perceptions of social presence and acceptability compared to the reference headsets, it does not match the social connectivity of direct human interactions.",Extended Reality; Social Acceptability; Social Presence,Keywords,TRUE,
IEEE,conferencePaper,2024,SOLDAR: Supporting Low-Volume PCB Prototyping Using Collaborative Robots and Augmented Reality,VR - International Symposium Virtual Reality,A*,"Printed circuit boards (PCBs) are fundamental to modern electronics and are present in almost every electronic device. However, despite their ubiquity, current PCB assembly methods can be time-consuming and lack flexibility for one-off designs. This poster investigates how low-volume PCB prototyping can be enhanced by integrating collaborative robots (cobots) and Augmented Reality (AR). Specifically, we introduce SOLDAR, a system that facilitates the soldering of electronic through-hole components on PCBs. By using a cobot for optimal PCB positioning and AR glasses for step-by-step guidance, SOLDAR aims to streamline the assembly process. The expected outcomes are increased efficiency, reduced assembly time, and greater flexibility for low-volume PCB prototyping designs. To validate these hypotheses, user experiments are necessary.",Augmented Reality; Collaborative Robots; Prototyping,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Earscape: A VR Auditory Educational Escape Room,VR - International Symposium Virtual Reality,A*,"According to the World Health Organisation’s World Report on Hearing, there is a strong need to provide better education on hearing loss from a young age. This project aims to educate the Danish young population (13 to 17-year-olds) about the hearing sense through an educational multiplayer virtual reality-based escape room with the benefits of educational escape rooms. In collaboration with relevant audiologist stakeholders, this project follows an iterative process of design, implementation, and evaluation of the application. The developed solution will undergo several user studies in the following months.",Educational Escape Room; Hearing Loss; Multiplayer; Virtual Reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Haptic and Auditory Feedback on Immersive Media in Virtual Reality,VR - International Symposium Virtual Reality,A*,"In Virtual Reality (VR), visual and auditory sensations are effectively leveraged to create immersive experiences. However, touch is significantly underutilized in immersive media. We enhance the VR image viewing experience by integrating haptic and auditory feedback into 3D environments constructed from immersive media. We address the challenges of utilizing depth maps from various image formats to create intractable environments. The VR experience is enhanced using vibrohaptic feedback and audio cues triggered by controller collisions with haptic materials.",Auditory Feedback; Haptic Feedback; Immersive Media; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,From Ground to Sky: Flying-motion Generation via Motion Dataset Adaptation,VR - International Symposium Virtual Reality,A*,"We conducted a study utilizing a lightweight generative network to create flying motions. The existing datasets used for training did not include any data on flying motions. Therefore, we selected certain classes from the existing motion datasets and transformed these motions to resemble flying actions. By training the existing generative network with the modified dataset, we were able to generate motions that closely resemble flying. The results of this study demonstrate the potential for generating flying motions. The generation of flying motions for human avatars is expected to be a critical technology not only in 3D animation or game industry but also in virtual environments, enabling users to experience various activities through their avatars.",Avatar motion; dataset; Virtual Reality,Keywords,TRUE,
IEEE,conferencePaper,2024,Walking of uphill slopes in immersive virtual environments,VR - International Symposium Virtual Reality,A*,"We explore three visual manipulation techniques aiming to create a realistic feeling of walking an uphill slope while in reality being on flat ground. The techniques are based on real physical visual perception and consist of modification of height and display of virtual shoes, modification of speed, and modification of view pitch. Quantitative and qualitative evaluation indicated that modification of speed, and pitch contributed to user discomfort, as well as a general increase in discomfort correlating with the slope’s increasing inclination. However, height manipulation was well received and can be used in future projects for more realistic landscape.",human factors; Virtual reality; walking,Keywords,TRUE,
IEEE,conferencePaper,2024,Enhanced Wayfinding Insights Through VR and Eye-Tracking Analysis,VR - International Symposium Virtual Reality,A*,"This paper presents a novel method for evaluating wayfinding within a public building to provide meaningful insights for stakeholders. Our approach features unique methods for both data collection and evaluation, with a holistic digital capture of the entire virtual environment experienced by participants, maintained in an interactive format for in-depth analysis. We also captured and output data in point cloud formats, raw data text files, and task-specific metrics, which support interactive replays of participants’ experiences. We developed algorithms to extract meaningful insights from the raw data based on assumptions about wayfinding characteristics. The contribution is a flexible framework that can be easily adapted for future projects with adjustable variables to suit specific applications.",gaze tracking; point cloud; signage; unreal engine; virtual reality; wayfinding,Keywords,TRUE,
IEEE,conferencePaper,2024,Pipelining Processors for Decomposing Character Animation,VR - International Symposium Virtual Reality,A*,"This paper presents an openly available implementation of a modular pipeline architecture for character animation. It effectively decomposes frequently necessary processing steps into dedicated character processors, such as copying data from various motion sources, applying inverse kinematics, or scaling the character. Processors can easily be parameterized, extended (e.g., with AI), and freely arranged or even duplicated in any order necessary, greatly reducing side effects and fostering fine-tuning, maintenance, and reusability of the complex interplay of real-time animation steps.",Agents; Avatars; Embodiment; Extended Reality.; Humanoid Characters; Open-Source; Virtual Humans; Virtual Reality,Keywords,TRUE,
IEEE,conferencePaper,2024,Study of inpainting based on generative AI for noise-canceling HMDs,VR - International Symposium Virtual Reality,A*,"Entering a small space such as an elevator or a crowded train with a stranger can cause discomfort and suffocation. This is because the stranger is invading the individual’s personal space. However, it is difficult to maintain an appropriate interpersonal distance from others at all times in various situations. Therefore, a noise-canceling HMD [2][3] that uses AR to change the size of the person in the field of vision has been proposed as a means of reducing noise such as discomfort caused by inappropriate interpersonal distance. In this paper, we propose an improvement method using generative AI for background completion in noise-canceling HMDs.",Augmented Reality; Noise-canceling HMD,Keywords,TRUE,
IEEE,conferencePaper,2024,A Comparison between Vibrotactile Error-correction Feedback on Upper and Lower Body in the VR Snowboard Balancing Task,VR - International Symposium Virtual Reality,A*,"This study investigated the effect of vibrotactile stimulus location on the balancing task in virtual reality (VR). Using a virtual snowboarding system with wearable haptic devices, we conducted a between-subject user study comparing the effectiveness of two different body locations–upper body (UB; torso vibrations) and lower body (LB; ankle vibrations). The real-time vibrotactile balance-correction feedback was generated by the Center of Pressure (CoP) calculated from the sensor array on insoles. The initial results showed that UB feedback is better than LB to improve users’ balance ability.",balancing; center-of-pressure; Vibrotactile wearables; virtual reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,The MASTER XR Platform for Robotics Training in Manufacturing,VR - International Symposium Virtual Reality,A*,"The MASTER project introduces an open Extended Reality (XR) platform designed to enhance human-robot collaboration and train workers in robotics within manufacturing settings. It includes modules for creating safe workspaces, intuitive robot programming, and user-friendly human-robot interactions (HRI), including eye-tracking technologies. The development of the platform is supported by two open calls targeting technical SMEs and educational institutes to enhance and test its functionalities. By employing the learning-by-doing methodology and integrating effective teaching principles, the MASTER platform aims to provide a comprehensive learning environment, preparing students and professionals for the complexities of flexible and collaborative manufacturing settings.",Extended Reality (XR); Eye Tracking; Human-Robot Collaboration; Industry 4.0; Manufacturing; Robotics; Worker Training,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,VR4UrbanDev: An Immersive Virtual Reality Experience for Energy Data Visualization,VR - International Symposium Virtual Reality,A*,"In this demonstration paper, we present our interactive virtual reality (VR) experience, which has been designed to facilitate interaction with energy-related information. This experience consists of two main modes: the world in miniature for large-scale and first-person for real-world scale visualizations. Additionally, we presented our approach to potential target groups in interviews. The results of these interviews can help developers for future implementation considering the requirements of each group.",Building Information Modeling; Data Visualization; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,UXR-kit: An Ideation Kit and Method for Collaborative and User-Centered Design about Extended Reality systems.,VR - International Symposium Virtual Reality,A*,"Emerging kits and methods about Extended Reality (XR) systems are mainly centered on the prototyping phase. The ideation phase, which comes before prototyping, is currently still under-explored. In this work, we propose UXR-kit: a toolkit and a method for the co-design of ideas for XR systems. UXR-kit is based on an approach inspired by design studios and generative techniques and highlights the specificities of XR systems. Results from an experimental study suggest that UXR-kit allows the emergence of ideas for XR designs through both World-In-Miniature representations and first-person representations at scale 1:1.",Design toolkit; Extended Reality; Ideation; Mixed Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Cultural Windows: Towards Immersive Journeys into Global Living Spaces,VR - International Symposium Virtual Reality,A*,"“Cultural Windows” is a research initiative aimed at enhancing cross-cultural understanding through immersive extended reality (XR) experiences. The project deploys AR and VR platforms to allow users to explore diverse living spaces, bridging the gap between preconceived notions and the actual appearance of these spaces. By using 3D scanning to create accurate models of culturally significant objects and integrating them into immersive systems, the project provides insights into the use of immersive technologies in cultural education, promoting engagement with global living designs.",Cross-Cultural Visualization; Cultural Awareness in Design; Extended Reality (XR),Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,A Volumetric Video Application to Enhance Museum Experiences,VR - International Symposium Virtual Reality,A*,"Volumetric video (VV) is an emerging 3D format that allows the integration of real people into XR (extended reality) applications. Recent cost-effective AI-based methods have enabled VV capture using single handheld cameras or mobile phones. This study addresses the quality, integration, and acceptance of AI-based VV content creation in an augmented reality (AR) application designed to enhance museum experiences. The main result reveals that, although the current VV quality is lower than professional standards, users still find significant added value and enjoy its immersive experience.",,Abstract,TRUE,
IEEE,conferencePaper,2024,Effectiveness of Adaptive Difficulty Settings on Self-efficacy in VR Exercise,VR - International Symposium Virtual Reality,A*,"The difficulty is a fundamental factor of the user’s motivation and engagement in some tasks. Dynamic difficulty adjustment (DDA) systems provide users with an optimal level of challenge. Previously, some studies developed a DDA system that can set the task’s difficulty to any level. However, these studies lack the investigation of the influence of the difficulty levels on the psychological aspect. For this purpose, we consider a difficulty setting that consists of stepwise difficulty levels (e.g., hard, normal, and easy) set to adapt to each user’s skill and evaluate it using self-efficacy. In the experiment, we employ a Kendama task in a VR space where the difficulty level can be easily adjusted. The result shows that the difficulty levels in our method can be set according to the user’s skill. Moreover, we experimentally clarify a strong correlation between successful experiences in imagination and the enhancement of self-efficacy in the difficulty setting, which means that adapting difficulty levels to the user’s skill has the potential to enhance self-efficacy effectively.",Difficulty adjustment; Self-efficacy; User experience; Virtual reality,Keywords,TRUE,
IEEE,conferencePaper,2024,Investigation of Simulator Sickness in Walking with Multiple Locomotion Technologies in Virtual Reality,VR - International Symposium Virtual Reality,A*,"With the increasing development of Virtual Reality, locomotion has become an essential component of interaction in VR. Currently, various locomotion technologies have been developed to provide users with a natural walking experience in virtual environments. However, the multiple walking techniques impact users’ walking experience in different ways. Simulator sickness is a common issue in VR experiences. Since different walking methods may influence simulator sickness differently, we conducted a user study to evaluate simulator sickness in walking with three relevant walking methods: real walking, arm-swing, and omnidirectional treadmill, and the results indicated that these three walking methods caused different levels of simulator sickness, and people perceived stronger sickness when they walked on the omnidirectional treadmill.",Locomotion Technologies; Natural Walking Techniques; Simulator Sickness; Virtual Reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Exploring an XR Indoor Navigation System for Remote Collaboration,VR - International Symposium Virtual Reality,A*,"While collaboration in shared extended reality spaces has been extensively explored, larger environments like entire floors or buildings have garnered less attention. To address this gap, spatial navigation and collaboration across realities must be made possible so that users can find each other and foster shared spatial understanding independent from reality. Current developments target either navigation or collaboration but lack the combination. In this poster, we present an extended reality remote collaboration system using an augmented reality (AR) based indoor navigation for on-site and a Building Information Model (BIM) of the physical environment Virtual Reality (VR) system for remote users. We conducted a user study with ten participants (five pairs) to gather initial insights into the system’s usability and preferences for collaborative tools. The results offer initial insights into creating shared spatial understanding across realities. Our work contributes to a collaborative XR navigation system for extensive shared spaces.",AR; indoor navigation; remote; VR; XR collaboration,Abstract,TRUE,
IEEE,conferencePaper,2024,Wheel-Based Attachable Footwear for VR: Challenges and Opportunities in Seated Walking-in-Place Locomotion,VR - International Symposium Virtual Reality,A*,"This poster explores the potential of Cybershoes, a foot-based consumer input device, used with a swivel chair to enable seated walking-in-place (WIP) locomotion in virtual reality (VR). Through a qualitative study with 12 participants, we investigated the effects of Cybershoes on user comfort, presence, motion sickness, and overall experience during various sightseeing tasks. Our findings reveal both opportunities and challenges for Cybershoes as a seated-WIP solution. Participants perceived Cybershoes as more natural for navigation compared to handheld controllers, with most reporting reduced motion sickness. However, challenges included perceived slower movement speed, ergonomic issues, and limited action detection. Our work also highlights Cybershoes’ potential beyond gaming, including applications in exercise, professional training, remote work, and accessibility.",input.; locomotion; seated walking; shoes; virtual travel; VR; wheel,Abstract,TRUE,
IEEE,conferencePaper,2024,White Lies in Virtual Reality: Impact on Enjoyment and Fatigue,VR - International Symposium Virtual Reality,A*,"This study examined the impact of a ""white lie"" designed to boost motivation during virtual reality exercise on enjoyment and mental fatigue. Participants engaged in a ball-throwing or ball-targeting task and were randomly assigned to groups with or without the white lie. Results indicated that both groups experienced similar levels of enjoyment and fatigue, suggesting the white lie had minimal effect on these factors. All participants, regardless of group, reported high levels of enjoyment, with 17 out of 18 indicating they had fun, no significant differences in mental fatigue were found between groups while participants generally favored the white lie. However, the positive experience across all participants highlights the potential of Virtual Reality for promoting exercise engagement.",Enjoyment; Fatigue; Virtual Reality; White Lies,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Dynamic Difficulty Adjustment in Virtual Reality Exergaming to Regulate Exertion Levels via Heart Rate Monitoring,VR - International Symposium Virtual Reality,A*,"By regulating exertion levels, Dynamic difficulty adjustment (DDA) has the potential to enhance user experience and optimize exercise in Virtual Reality (VR) exergames. This pilot study assesses the effectiveness of adjusting the difficulty of gameplay challenges based on heart rate (HR) data to control the intensity of physical activity in VR exergaming. Observational results from 13 participants indicate that the HR-based DDA more effectively maintained target heart rate zones compared to randomized adjustments. Improved perceived exertion, and increased enjoyment underlines the potential of this approach for VR-based exercise and rehabilitation programs.",dynamic difficulty adjustment; heart rate; virtual reality exergames,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Rendering diffraction Phenomena on rough surfaces in Virtual Reality,VR - International Symposium Virtual Reality,A*,"Wave-optical phenomena, such as diffraction, significantly impact the visual appearance of surfaces. Despite their importance, wave-optical reflection models are rare and computationally expensive. Recently, we presented a real-time model that accounts for diffraction-induced color shifts and speckle. Given that diffraction phenomena are highly dependent on illumination and viewing directions, as well as stereoscopic vision, we developed a VR demo to evaluate the new model. This demo shows the substantial impact of diffraction on the appearance of rough surfaces, particularly in stereoscopic viewing.",Diffraction; Modeling; Predictive Rendering; Virtual Reality,Title_Keywords,TRUE,
IEEE,conferencePaper,2024,Supporting Wildfire Evacuation Preparedness through a Virtual Reality Simulation,VR - International Symposium Virtual Reality,A*,This demo presents a virtual reality simulation of a wildfire evacuation. Players are tasked with going through a home environment and collecting items they believe they would need and want to take if they were under an evacuation notice. The experience is playable on the Meta Quest 2 headset.,Evacuation; Training; Virtual Reality; Wildfires,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Row your boat in VR and solve thinking exercises on the way: The Brain-Row Challenge,VR - International Symposium Virtual Reality,A*,"In this demo, we showcase Brain-Row Challenge. Brain-Row Challenge is a research prototype for dual-task training in Virtual Reality (VR). Dual-task training combines a mental and a physical task. This training is relevant in neurodegenerative diseases, especially in Parkinson’s disease. The user is rowing with a Concept 2 ergometer over a Nordic lake, must follow a marked route and answers multiple-choice questions by rowing through gates. Steering is done with an inertial measurement unit that is attached to the handlebar. The VR experience can also be compared to a less immersive representation of the rowing course on a TV screen.",Dual Tasking; Ergometers; Excer Game; Medical Application,Abstract,TRUE,
IEEE,conferencePaper,2024,ChronoShore: Diegetic Temporal Exploration in a Simulated Virtual Coast Environment,VR - International Symposium Virtual Reality,A*,"This paper introduces ChronoShore, an immersive virtual reality (VR) experience designed to explore diegetic time manipulation mechanics within a semi-realistic coastal environment. Traditional 2D video scrubbing methods fall short in immersive settings, particularly for understanding time-bound processes such as simulations of geology or biology. ChronoShore addresses this by allowing users to interact with celestial bodies to dynamically control and experience the passage of time, currently showcasing different weather events and atmospheric phenomena.",simulation; Time manipulation; virtual reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Hands-On Plant Root System Reconstruction in Virtual Reality,VR - International Symposium Virtual Reality,A*,"VRoot is an immersive extended reality reconstruction tool for root system architectures from 3D volumetric scans of soil columns. We have conducted a laboratory user study to assess the performance of new users with our software in comparison to established software. We utilize a plant model to derive a synthetic root architecture, providing a baseline for reconstruction. This demo showcases the processes and techniques contributing to exact and efficient manual root architecture reconstruction in Virtual Reality. The extraction task typically is the sparse graph-structure extraction from a 3D magnetic-resonance imaging (MRI) data set. We visualize the RSA directly within the MRI and offer selection-set-based methods of adapting and augmenting the root architecture. This application is in productive use at our partner institute, where it is used to analyze complex root images.",3D imaging; root reconstruction; virtual reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Off-The-Shelf: Exploring 3D Arrangements of See-Through Masks to Switch between Virtual Environments,VR - International Symposium Virtual Reality,A*,"This demo explores prioritization techniques to arrange see-through masks in virtual reality (VR). The oval masks show live previews of different virtual environments (VEs) and allow for seamless teleportation into a corresponding VE by putting the mask on the face. Each environment includes a mini-game (e.g., basketball and archery) in which the user has to perform a small task. The arrangement of the masks changes depending on a calculated rating, which considers the time since the game was last played and the game score. We envision this system to help users to multitask in VR. For example, to control multiple characters in VR games, to experience multi-strand (nonlinear) narratives, and to supervise semi-autonomous agents in different VEs.",Mask; Multiverse; Transitions; Virtual Reality,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,EcoDive: Enhancing Presence and Ambient Environmental Awareness in a Virtual Reality Experience for Underwater Marine Debris Collection,VR - International Symposium Virtual Reality,A*,"This paper presents a VR-based serious game. The game aims to raise awareness about ocean pollution by immersing players in a virtual underwater world where they collect trash to prevent coral bleaching and save marine life. Despite their efforts, players inevitably face game over, highlighting the futility of merely collecting trash and underscoring the need to prevent waste from entering oceans. The game uses various diegetic feedback mechanisms and enhanced user presence features to deepen emotional engagement and promote pro-environmental behavior.",Coral Bleaching; Diegetic Feedback; Environmental Awareness; Environmental Education; Marine Conservation; Ocean Pollution; Pro-Environmental Behavior; Serious Game; Virtual Reality (VR),Title_Keywords,TRUE,
IEEE,conferencePaper,2024,GazeLock: Gaze- and Lock Pattern-Based Authentication,VR - International Symposium Virtual Reality,A*,"Password entry is common authentication approach in Extended Reality (XR) applications for its simplicity and familiarity, but it faces challenges in public and dynamic environments due to its cumbersome nature and susceptibility to observation attacks. Manual password input can be disruptive and prone to theft through shoulder surfing or surveillance. While alternative knowledge-based approaches exist, they often require complex physical gestures and are impractical for frequent public use. We present GazeLock, an eye-tracking and lock pattern-based authentication method. This method aims to provide an easy-to-learn and efficient alternative by leveraging familiar lock patterns operated through gaze. It ensures resilience to external observation, as physical interaction is unnecessary and eyes are obscured by the headset. Its hands-free, discreet nature makes it suitable for secure public use. We demonstrate this method by simulating the unlocking of a smart lock via an XR headset, showcasing its potential applications and benefits in real-world scenarios.",Authentication; Extended Reality (XR); Eye Tracking; Gaze-based Interaction,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Us Xtended - Tracking and Sensing through Embedded and Embodied Design in Virtual Reality,VR - International Symposium Virtual Reality,A*,"This short paper presents an embodied and embedded design method via biometric data tracking on the example of the virtual reality prototype Us Xtended. Users are taken through different immersive worlds and their task is to manipulate the environments via a certain type of physiological interaction (i.e. heart rate, gaze, voice, cognitive load). By employing biofeedback, the system tailors the immersive environment via audiovisual and haptic stimuli to user’s psycho-physiological responses and reflects them on its scale which is part of the virtual environment. By recording their voice, users can self-assess their own affects. In the finale, users stand in a pastiche-like world filled with different artifacts of psycho-physiological evaluations they co-created with the biofeedback system throughout their journey.",affect; biometrics; embedded and embodied design; psycho-physiology; self-quantification; virtual reality,Title_Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Make America Great Again and Again: How to Adapt Interactive Installation Art for Virtual Reality,VR - International Symposium Virtual Reality,A*,"Make America Great Again and Again features a large, fluttering American flag accompanied by the Star-Spangled Banner, with sixty small screens displaying one-minute video clips in sequence. This one-hour loop continues until participants upload their own videos, transforming the flag into a collage of visitor selfies. By providing a public sphere for local visitors, this interactive art project encourages them to share their opinions on this controversial issue. To capture global perspectives on the topic, the project was adapted into a virtual reality environment using the metaverse platform Styly. This paper outlines the process of converting the installation into virtual reality artwork.",Installation art; Interactive art; VR conversion,Title_Abstract,TRUE,
IEEE,conferencePaper,2024,Real-Time Scent Prediction and Release for Video Games,VR - International Symposium Virtual Reality,A*,"This demo explores the use of computer vision technologies for the integration of scent in video games and interactive applications. We present an extendable system that is domain-independent and allows for customization and debugging based on the targeted game. Using Minecraft as a case study, we optimized the system configuration and evaluated its performance. Our aim is to advance the exploration of scent integration in gaming and inspire future designs for olfactory experiences.",Computer Vision; Scent Integration; Virtual Reality,Keywords,TRUE,
IEEE,conferencePaper,2024,ICELab Demo: an industrial digital-twin and simulator in VR,VR - International Symposium Virtual Reality,A*,"In this demo we present an application featuring the integration of Virtual Reality (VR) technologies with the demonstration laboratory (ICELab) built around Industry 4.0/5.0 concepts. In particular, we showcase a digital twin of the real laboratory that allows the user to explore its environment in VR and interact with the different machinery to obtain several data and information.",Computer Graphics; Cyber-Physical Factory; Digital Twin,Abstract,TRUE,
IEEE,conferencePaper,2024,"Travel Speed, Spatial Awareness, And Implications for Egocentric Target-Selection-Based Teleportation - A Replication Design",VR - International Symposium Virtual Reality,A*,"Virtual travel in Virtual Reality experiences is common, offering users the ability to explore expansive virtual spaces. Various interfaces exist for virtual travel, with speed playing a crucial role in user experience and spatial awareness. Teleportation-based interfaces provide instantaneous transitions, whereas continuous and semi-continuous methods vary in speed and control. Prior research by Bowman et al. highlighted the impact of travel speed on spatial awareness demonstrating that instantaneous travel can lead to user disorientation. However, additional cues, such as visual target selection, can aid in reorientation. This study replicates and extends Bowman’s experiment, investigating the influence of travel speed and visual target cues on spatial orientation.",,Abstract,TRUE,
IEEE,conferencePaper,2024,Walking &gt; Walking-in-Place &gt; Flying/Steering &gt; Teleportation? Designing Locomotion Research for Replication and Extension,VR - International Symposium Virtual Reality,A*,"In this abstract, we discuss the demand for replication and extension efforts related to two seminal studies focused on virtual reality (VR) locomotion interfaces, initially centered around a VR implementation of the Visual Cliff, often referred to as Virtual Pit. The original experiments by Slater et al. (1995) and Usoh et al. (1999) compared different locomotion methods, including Real Walking, Walking-in-Place, and Flying/Steering, with a focus on presence and ease of use. We discuss the importance of these studies for the field, motivate replication efforts focused on these studies, discuss potential confounding factors, and present considerations for a concerted effort to reproduce the findings with state-of-the-art VR systems and measures, extensions to locomotion methods like Teleportation, and means to support future replications and extensions.",locomotion; presence; replication; steering; teleportation; user study; Virtual reality; walking,Abstract_Keywords,TRUE,
IEEE,conferencePaper,2024,Fade-to-Black Duration in Egocentric Target-Selection-Based Teleport - A Replication Design,VR - International Symposium Virtual Reality,A*,"Fade-to-black animations are a commonly used technique to visualize transitions during teleportation. However, their duration varies across different implementations and has not been extensively researched. This abstract details a study design to understand how the level of environmental detail affects the preferred duration of fade-to-black animations. We propose a within-subject study, comparing participants’ preferred duration across three virtual environments with varying levels of detail. We discuss improvements to the task design of an existing study. Other than the level of environmental detail, we motivate research into the effects of different tasks (i.e. hurried or calm) on the preferred duration.",locomotion; replication; teleportation; transitions; user study; Virtual reality,Keywords,TRUE,
IEEE,conferencePaper,2024,Generative Multi-Modal Artificial Intelligence for Dynamic Real-Time Context-Aware Content Creation in Augmented Reality,VR - International Symposium Virtual Reality,A*,"We introduce a framework that uses generative Artificial Intelligence (AI) for dynamic and context-aware content creation in Augmented Reality (AR). By integrating Vision Language Models (VLMs), our system detects and understands the physical space around the user, recommending contextually relevant objects. These objects are transformed into 3D models using a text-to-3D generative AI techniques, allowing for real-time content inclusion within the AR space. This approach enhances user experience by enabling intuitive customization through spoken commands, while reducing costs and improving accessibility to advanced AR interactions. The framework’s vision and language capabilities support the generation of comprehensive and context-specific 3D objects.",3D object generation; Augmented reality; generative AI; vision language models,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2024,A Generative Framework for Low-Cost Result Validation of Machine Learning-as-a-Service Inference,AsiaCCS - Asia Conference on Computer and Communications Security,A,"The growing popularity of Machine Learning (ML) has led to its deployment in various sensitive domains, which has resulted in significant research focused on ML security and privacy. However, in some applications, such as Augmented/Virtual Reality, integrity verification of the outsourced ML tasks is more critical-a facet that has not received much attention. Existing solutions, such as multi-party computation and proof-based systems, impose significant computation overhead, which makes them unfit for real-time applications. We propose Fides, a novel framework for real-time integrity validation of ML-as-a-Service (MLaaS) inference. Fides features a novel and efficient distillation technique-Greedy Distillation Transfer Learning-that dynamically distills and fine-tunes a space and compute-efficient verification model for verifying the corresponding service model while running inside a trusted execution environment. Fides features a client-side attack detection model that uses statistical analysis and divergence measurements to identify, with a high likelihood, if the service model is under attack. Fides also offers a re-classification functionality that predicts the original class whenever an attack is identified. We devised a generative adversarial network framework for training the attack detection and re-classification models. The evaluation shows that Fides achieves an accuracy of up to 98% for attack detection and 94% for re-classification.",edge computing; machine learning as a service; result verification; trusted execution environment; verifiable computing,Abstract,TRUE,
Scopus,conferencePaper,2024,SoK: Data Privacy in Virtual Reality,PETS - International Symposium on Privacy Enhancing Technologies,A,"The adoption of virtual reality (VR) technologies has rapidly gained momentum in recent years as companies around the world begin to position the so-called “metaverse” as the next major medium for accessing and interacting with the internet. While consumers have become accustomed to a degree of data harvesting on the web, the real-time nature of data sharing in the metaverse indicates that privacy concerns are likely to be even more prevalent in the new “Web 3.0.” Research into VR privacy has demonstrated that a plethora of sensitive personal information is observable by various would-be adversaries from just a few minutes of telemetry data. On the other hand, we have yet to see VR parallels for many privacy-preserving tools aimed at mitigating threats on conventional platforms. This paper aims to systematize knowledge on the landscape of VR privacy threats and countermeasures by proposing a comprehensive taxonomy of data attributes, protections, and adversaries based on the study of 74 collected publications. We complement our qualitative discussion with a statistical analysis of the risk associated with various data sources inherent to VR in consideration of the known attacks and defenses. By focusing on highlighting the clear outstanding opportunities, we hope to motivate and guide further research into this increasingly important field.",,Title_Abstract,TRUE,
Scopus,journalPaper,2024,<italic>xr-droid</italic>: A Benchmark Dataset for AR/VR and Security Applications,TODSC -Transactions on Dependable and Secure Computing,A,"The development of metaverses and virtual worlds on various platforms, including mobile devices, has led to the growth of applications in virtual reality (VR) and augmented reality (AR) in recent years. This application growth is paralleled by a growth of interest in analyzing and understanding AR/VR applications from security and performance standpoints. Despite this growing interest, benchmark datasets are lacking to facilitate this research pursuit. In this paper, we collect a dataset that consists of 408 diverse AR/VR applications from the Google Play Store and acquire various data modalities associated with those applications standardized in the form of seven features: control flow graphs, strings, functions, permissions, API calls, hexdump, and metadata. We highlight various research endeavors (applications) that can benefit from our dataset for each data modality. IEEE",Android; AR; Benchmark testing; Dataset; Hardware; Internet; Operating systems; Security; Security Applications; Solid modeling; Three-dimensional displays; VR,Abstract,TRUE,
Scopus,journalPaper,2024,Time to Think the Security of WiFi-Based Behavior Recognition Systems,TODSC -Transactions on Dependable and Secure Computing,A,"Behavior recognition plays an essential role in numerous behavior-driven applications (e.g., virtual reality and smart home) and even in the security-critical applications (e.g., security surveillance and elder healthcare). Recently, WiFi-based behavior recognition (WBR) technique stands out among many behavior recognition techniques due to its advantages of being non-intrusive, device-free, and ubiquitous. However, existing WBR research mainly focuses on improving the recognition precision, while rarely studying the security aspects. In this article, we reveal that WBR systems are vulnerable to manipulating physical signals. For instance, our observation shows that WiFi signals can be changed by jamming signals. By exploiting the vulnerability, we propose two approaches to generate physically online adversarial samples to perform untargeted attack and targeted attack, respectively. The effectiveness of these attacks are extensively evaluated over four real-world WBR systems. The experiment results show that our attack approaches can achieve 80% and 60% success rates for untargeted attack and targeted attack in physical world, respectively. We also show that our attack approaches can be generalized to other WiFi-based sensing applications, such as user authentication.  © 2004-2012 IEEE.",Adversarial sample; behavior recognition; genetic algorithm; WiFi,Abstract,TRUE,
Scopus,journalPaper,2024,Dangers Behind Charging VR Devices: Hidden Side Channel Attacks via Charging Cables,TOIFS - Transactions on Information Forensics and Security,A,"Virtual reality (VR), offering 3D visuals and stereophonic sounds, significantly enhances users' immersive experiences and has become a milestone in the era of the metaverse. However, due to the limited battery capacity of VR devices, it is common for users to rely on charging cables, which serve the dual purpose of power supply and audio output, to recharge their VR devices while in use. In this study, we propose an inconspicuous and stealthy side channel attack, coined as LineTalker, which can unveil visual-related and audio-related activities from VR devices during the charging process. The insight behind LineTalker is rooted in the observation that visual-related activities (e.g., 3D image rendering) are power-intensive and result in fluctuations in the current strength of the cable's power supply line, which can be leveraged as side channel information. Similarly, audio-related activities (e.g., playing music) leave traces on the cable's audio output line. Rather than providing a user with a compromised charging cable (i.e., embedding a current sensor) to measure the current strength, to make the attack less conspicuous, LineTalker employs the Hall effect to indirectly access side channel information. This is achieved by capturing magnetic signals using a Hall sensor placed near the target cable in a contactless manner. Experimental results demonstrate that LineTalker achieves an overall accuracy of 94.60% and 64.38% in inferring user activities in VR devices with intrusive and non-intrusive attack manners, respectively.  © 2005-2012 IEEE.",charging cable; non-intrusive attack; privacy inference; side channel attack; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,An Anti-Disguise Authentication System Using the First Impression of Avatar in Metaverse,TOIFS - Transactions on Information Forensics and Security,A,"Metaverse is a vast virtual world parallel to the physical world, where the user acts as an avatar to enjoy various services that break through the temporal and spatial limitations of the physical world. Metaverse allows users to create arbitrary digital appearances as their own avatars by which an adversary may disguise his/her avatar to fraud others. In this paper, we propose an anti-disguise authentication method that draws on the idea of the first impression from the physical world to recognize an old friend. Specifically, the first meeting scenario in the metaverse is stored and recalled to help the authentication between avatars. To prevent the adversary from replacing and forging the first impression, we construct a chameleon-based signcryption mechanism and design a ciphertext authentication protocol to ensure the public verifiability of encrypted identities. The security analysis shows that the proposed signcryption mechanism meets not only the security requirement but also the public verifiability. Besides, the ciphertext authentication protocol has the capability of defending against the replacing and forging attacks on the first impression. Extensive experiments show that the proposed avatar authentication system is able to achieve anti-disguise authentication at a low storage consumption on the blockchain.  © 2005-2012 IEEE.",anti-disguise; authentication; avatar; Metaverse,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Inclusion of individuals with autism spectrum disorder in Software Engineering,IST - Information and Software Technology,A,"Context: Software Engineering is dedicated to the systematic and efficient development of software, which necessitates the active participation of all team members and a recognition of their unique skills and abilities, including those with autism spectrum disorders (ASD). The inclusion of individuals with ASD presents new perspectives, yet there is a lack of systematic evidence regarding the primary obstacles and potential benefits associated with their inclusion. Objective: This paper aims to identify, characterize, and describe barriers, facilitators, and methodological proposals described by the community to include individuals with ASD in the discipline of Software Engineering. Methods: We conducted a comprehensive systematic multivocal mapping study to evaluate the existing evidence on the inclusion of individuals with ASD in Software Engineering. Results: We obtained 34 primary studies from which we identified the main facilitators of motivation to learn new skills, attention to detail, and the ability to report and visualize patterns. In contrast, the main barriers detected were communication, a lack of neurodivergent computational thinking, and sensory integration. Additionally, we identified and classified four categories of proposals that allowed the inclusion of individuals with ASD: (i) using virtual reality, (ii) creating more inclusive workspaces, (iii) encouraging neurodivergent computational thinking, and (iv) improving social skills. Conclusions: This study identifies the principal elements that ought to be taken into consideration when allocating tasks and roles to individuals with ASD in software development. © 2024 Elsevier B.V.",Autism spectrum disorders; Software engineering; Systematic multivocal mapping study,Abstract,TRUE,
Scopus,journalPaper,2024,SENEM: A software engineering-enabled educational metaverse,IST - Information and Software Technology,A,"Context: The term metaverse refers to a persistent, virtual, three-dimensional environment where individuals may communicate, engage, and collaborate. One of the most multifaceted and challenging use cases of the metaverse is education, where educators and learners may require multiple technical, social, psychological, and interaction instruments to accomplish their learning objectives. While the characteristics of the metaverse might nicely fit the problem's needs, our research points out a noticeable lack of knowledge into (1) the specific requirements that an educational metaverse should actually fulfill to let educators and learners successfully interact towards their objectives and (2) how to design an appropriate educational metaverse for both educators and learners. Objective: In this paper, we aim to bridge this knowledge gap by proposing SENEM, a novel software engineering-enabled educational metaverse. We first elicit a set of functional requirements that an educational metaverse should fulfill. Method: In this respect, we conduct a literature survey to extract the currently available knowledge on the matter discussed by the research community, and afterward, we assess and complement such knowledge through semi-structured interviews with educators and learners. Upon completing the requirements elicitation stage, we then build our prototype implementation of SENEM, a metaverse that makes available to educators and learners the features identified in the previous stage. Finally, we evaluate the tool in terms of learnability, efficiency, and satisfaction through a Rapid Iterative Testing and Evaluation research approach, leading us to the iterative refinement of our prototype. Results: Through our survey strategy, we extracted nine requirements that guided the tool development that the study participants positively evaluated. Conclusion: Our study reveals that the target audience appreciates the elicited design strategy. Our work has the potential to form a solid contribution that other researchers can use as a basis for further improvements. © 2024 The Author(s)",Human-centered studies; Metaverse engineering; Software engineering in practice; Virtual learning environments,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Collaborative software design and modeling in virtual reality,IST - Information and Software Technology,A,"Context: Software engineering is becoming more and more distributed. Developers and other stakeholders are often located in different locations, departments, and countries and operating within different time zones. Most online software design and modeling tools are not adequate for distributed collaboration since they do not support awareness and lack features for effective communication. Objective: The aim of our research is to support distributed software design activities in Virtual Reality (VR). Method: Using design science research methodology, we design and evaluate a tool for collaborative design in VR. We evaluate the collaboration efficiency and recall of design information when using the VR software design environment compared to a non-VR software design environment. Moreover, we collect the perceptions and preferences of users to explore the opportunities and challenges that were incurred by using the VR software design environment. Results: We find that there is no significant difference in the efficiency and recall of design information when using the VR compared to the non-VR environment. Furthermore, we find that developers are more satisfied with collaboration in VR. Conclusion: The results of our research and similar studies show that working in VR is not yet faster or more efficient than working on standard desktops. It is very important to improve the interface in VR (gestures with haptics, keyboard and voice input), as confirmed by the difference in results between the first and second evaluation. © 2023 Elsevier B.V.",Collaboration; Immersion; Software development; Software modeling; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Emerging technologies in higher education assessment and feedback practices: A systematic literature review,JSS - Journal of Systems and Software,A,"The use of Emerging Technologies, such as Artificial Intelligence (AI), Learning Analytics (LA) and Extended Reality (XR) applications, in higher education has proliferated in recent times, as these technologies are considered to have a significant impact on the future of postsecondary teaching and learning. We wanted to find out the emerging technologies used in computing education, its evaluation and effectiveness, and limitations and gaps for future research. We carried out a Systematic Literature Review study on the use of Emerging Technologies in higher education computing education to identify the state of the art in the use of these three groups of technologies for assessment and feedback practices. After systematic search and filtering from a search pool of 3038 studies published between 2016 and 2021, we selected 38 articles for detailed meta-analysis. Our findings reveal that 71% of the reviewed studies are journal articles, 50% studies focus on learning analytics, and the majority of the studies employ quantitative approaches. The results from this systematic review suggest that XR technologies have received least attention to date in computing education (amongst the emerging technologies considered for the review) and there is a lack of frameworks for design, evaluation and use of emerging technologies in higher education. The findings of this review will be beneficial for researchers and educators to obtain an in-depth understanding of the main areas of application of emerging technologies in higher education computing education, an inventory of emerging technology tools used for assessment and feedback, effectiveness indicators, and evaluation approaches that have been used. For evidence-based guidance on future assessment and feedback practices using emerging technologies, we also present a brief research agenda, drawing attention to the need to trial more XR, focus on formative assessment and feedback practices, better understand impact of human-centric issues and take more thoughtful consideration of ethics in the use of emerging technologies in computing education. © 2024 The Author(s)",Artificial intelligence; Assessment; Emerging technologies; Extended reality; Feedback; Learning analytics; Systematic literature review,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,The influence of the city metaphor and its derivates in software visualization,JSS - Journal of Systems and Software,A,"Context: The city metaphor is widely used in software visualization to represent complex systems as buildings and structures, providing an intuitive way for developers to understand software components. Various software visualization tools have utilized this approach. Objective: Identify the influence of the city metaphor on software visualization research, determine its state-of-the-art status, and identify derived tools and their main characteristics. Method: Conduct a systematic mapping study of 406 publications that reference the first paper on the use of the city metaphor in software visualization and/or the main paper of the CodeCity tool. Analyze the 168 publications from which valuable information could be extracted, and build a complete categoric analysis. Results: The field has grown considerably, with an increasing number of publications since 2001, and a changing research community with evolving interconnections between groups. Researchers have developed more tools that support the city metaphor, but less than 50% of the tools were referenced in their papers. Moreover, 85% of the tools did not use extended reality environments, indicating an opportunity for further exploration. Conclusion: The study demonstrates the active and continually growing presence of the city metaphor in research and its impact on software visualization and its derivatives. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2024",City metaphor; Extended reality; Software comprehension; Software visualization; State of the art; Systematic mapping study; Visualizations,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Stripe Sensitive Convolution for Omnidirectional Image Dehazing,TVCG - Transactions on Visualization and Computer Graphics,A,"The haze in a scenario may affect the 360 photo/video quality and the immersive 360° virtual reality (VR) experience. The recent single image dehazing methods, to date, have been only focused on plane images. In this work, we propose a novel neural network pipeline for single omnidirectional image dehazing. To create the pipeline, we build the first hazy omnidirectional image dataset, which contains both synthetic and real-world samples. Then, we propose a new stripe sensitive convolution (SSConv) to handle the distortion problems due to the equirectangular projections. The SSConv calibrates distortion in two steps: 1) extracting features using different rectangular filters and, 2) learning to select the optimal features by a weighting of the feature stripes (a series of rows in the feature maps). Subsequently, using SSConv, we design an end-to-end network that jointly learns haze removal and depth estimation from a single omnidirectional image. The estimated depth map is leveraged as the intermediate representation and provides global context and geometric information to the dehazing module. Extensive experiments on challenging synthetic and real-world omnidirectional image datasets demonstrate the effectiveness of SSConv, and our network attains superior dehazing performance. The experiments on practical applications also demonstrate that our method can significantly improve the 3-D object detection and 3-D layout performances for hazy omnidirectional images. © 1995-2012 IEEE.",Omnidirectional image dehazing; omnidirectional image depth estimation; stripe sensitive convolution; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,VR-HandNet: A Visually and Physically Plausible Hand Manipulation System in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—This study aims to allow users to perform dexterous hand manipulation of objects in virtual environments with handheld VR controllers. To this end, the VR controller is mapped to the virtual hand and the hand motions are dynamically synthesized when the virtual hand approaches an object. At each frame, given the information about the virtual hand, VR controller input, and hand-object spatial relations, the deep neural network determines the desired joint orientations of the virtual hand model in the next frame. The desired orientations are then converted into a set of torques acting on hand joints and applied to a physics simulation to determine the hand pose at the next frame. The deep neural network, named VR-HandNet, is trained with a reinforcement learning-based approach. Therefore, it can produce physically plausible hand motion since the trial-and-error training process can learn how the interaction between hand and object is performed under the environment that is simulated by a physics engine. Furthermore, we adopted an imitation learning paradigm to increase visual plausibility by mimicking the reference motion datasets. Through the ablation studies, we validated the proposed method is effectively constructed and successfully serves our design goal. A live demo is demonstrated in the supplementary video. © 2024 IEEE Computer Society. All rights reserved.",Hand manipulation; physics-based animation; reinforcement learning; virtual reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,ClockRay: A Wrist-Rotation Based Technique for Occluded-Target Selection in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Target selection is one of essential operation made available by interaction techniques in virtual reality (VR) environments. However, effectively positioning or selecting occluded objects is under-investigated in VR, especially in the context of high-density or a high-dimensional data visualization with VR. In this paper, we propose ClockRay, an occluded-object selection technique that can maximize the intrinsic human wrist rotation skills through the integration of emerging ray selection techniques in VR environments. We describe the design space of the ClockRay technique and then evaluate its performance in a series of user studies. Drawing on the experimental results, we discuss the benefits of ClockRay compared to two popular ray selection techniques – RayCursor and RayCasting. Our findings can inform the design of VR-based interactive visualization systems for high-density data. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",3D data visualization; disambiguation; object selection; RayCasting; Virtual reality; wrist rotation,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,An Examination of the Relationship between Visualization Media and Consumer Product Evaluation,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual product presentations that rely on static images and text are often insufficient to communicate all the information that is necessary to accurately evaluate a product. Technologies such as Virtual Reality (VR) or Augmented Reality (AR) have enabled more sophisticated representation methods, but certain product characteristics are difficult to assess and may result in perceptual differences when a product is evaluated in different visual media. In this article, we report two case studies in which a group of participants evaluated three designs of two product typologies (i.e., a desktop telephone and a coffee maker) as presented in three different visual media (i.e., photorealistic renderings, AR, and VR for the first case study; and photographs, a non-immersive virtual environment, and AR for the second case study) using eight semantic scales. An inferential statistical method using Aligned Rank Transform (ART) proceedings was applied to determine perceptual differences between groups. Our results show that in both cases product attributes in Jordan's physio-pleasure category are the most affected by the presentation media. The socio-pleasure category was also affected for the case of the coffee makers. The level of immersion afforded by the medium significantly affects product evaluation. © 1995-2012 IEEE.",and virtual realities; Artificial; augmented; consumer products; perception and psychophysics; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Redirected Walking on Omnidirectional Treadmill,TVCG - Transactions on Visualization and Computer Graphics,A,"—Redirected walking (RDW) and omnidirectional treadmill (ODT) are two effective solutions to the natural locomotion interface in virtual reality. ODT fully compresses the physical space and can be used as the integration carrier of all kinds of devices. However, the user experience varies in different directions of ODT, and the premise of interaction between users and integrated devices is a good match between virtual and real objects. RDW technology uses visual cues to guide the user’s location in physical space. Based on this principle, combining RDW technology with ODT to guide the user’s walking direction through visual cues can effectively improve user experience on ODT and make full use of various devices integrated on ODT. This paper explores the novel prospects of combining RDW technology with ODT and formally puts forward the concept of O-RDW (ODT-based RDW). Two baseline algorithms, i.e., OS2MD (ODT-based steer to multi-direction), and OS2MT (ODT-based steer to multi-target), are proposed to combine the merits of both RDW and ODT. With the help of the simulation environment, this paper quantitatively analyzes the applicable scenarios of the two algorithms and the influence of several main factors on the performance. Based on the conclusions of the simulation experiments, the two O-RDW algorithms are successfully applied in the practical application case of multi-target haptic feedback. Combined with the user study, the practicability and effectiveness of O-RDW technology in practical use are further verified. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Device integration haptic feedback locomotion interfaces; omnidirectional treadmill; redirected walking,Abstract,TRUE,
Scopus,journalPaper,2024,Real-Time Multi-Map Saliency-Driven Gaze Behavior for Non-Conversational Characters,TVCG - Transactions on Visualization and Computer Graphics,A,"—Gaze behavior of virtual characters in video games and virtual reality experiences is a key factor of realism and immersion. Indeed, gaze plays many roles when interacting with the environment; not only does it indicate what characters are looking at, but it also plays an important role in verbal and non-verbal behaviors and in making virtual characters alive. Automated computing of gaze behaviors is however a challenging problem, and to date none of the existing methods are capable of producing close-to-real results in an interactive context. We therefore propose a novel method that leverages recent advances in several distinct areas related to visual saliency, attention mechanisms, saccadic behavior modelling, and head-gaze animation techniques. Our approach articulates these advances to converge on a multi-map saliency-driven model which offers real-time realistic gaze behaviors for non-conversational characters, together with additional user-control over customizable features to compose a wide variety of results. We first evaluate the benefits of our approach through an objective evaluation that confronts our gaze simulation with ground truth data using an eye-tracking dataset specifically acquired for this purpose. We then rely on subjective evaluation to measure the level of realism of gaze animations generated by our method, in comparison with gaze animations captured from real actors. Our results show that our method generates gaze behaviors that cannot be distinguished from captured gaze animations. Overall, we believe that these results will open the way for more natural and intuitive design of realistic and coherent gaze animations for real-time applications. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Animation dataset eye-tracking data gaze behavior; neural networks; simulation,Abstract,TRUE,
Scopus,journalPaper,2024,A Comparative Evaluation of Optical See-Through Augmented Reality in Surgical Guidance,TVCG - Transactions on Visualization and Computer Graphics,A,"During traditional surgeries, planning and instrument guidance is displayed on an external screen. Recent developments of augmented reality (AR) techniques can overcome obstacles including hand-eye discoordination and heavy mental load. Among these AR technologies, optical see-through (OST) schemes with stereoscopic displays can provide depth perception and retain the physical scene for safety considerations. However, limitations still exist in certain AR systems and the influence of these factors on surgical performance is yet to explore. To this end, experiments of multi-scale surgical tasks were carried out to compare head-mounted display (HMD) AR and autostereoscopic image overlay (AIO) AR, concerning objective performance and subjective evaluation. To solely analyze effects brought by display techniques, the tracking system in each included display system was identical and similar tracking accuracy was proved by a preliminary experiment. Focus and context rendering was utilized to enhance in-situ visualization for surgical guidance. Latency values of all display systems were assessed and a delay experiment proved the latency differences had no significant impact on user performance. Results of multi-scale surgical tasks showed that HMD outperformed in detailed operations probably due to stable resolution along the depth axis, while AIO had better performance in larger-scale operations for better depth perception. This article helps point out the critical limitations of current OST AR techniques and potentially promotes the progress of AR applications in surgical guidance. © 2023 IEEE.",Augmented reality; autostereoscopic image overlay; head-mounted display; optical see-through; surgical guidance,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Sensory Attenuation With a Virtual Robotic Arm Controlled Using Facial Movements,TVCG - Transactions on Visualization and Computer Graphics,A,"—When humans generate stimuli voluntarily, they perceive the stimuli more weakly than those produced by others, which is called sensory attenuation (SA). SA has been investigated in various body parts, but it is unclear whether an extended body induces SA. This study investigated the SA of audio stimuli generated by an extended body. SA was assessed using a sound comparison task in a virtual environment. We prepared robotic arms as extended bodies, and the robotic arms were controlled by facial movements. To evaluate the SA of robotic arms, we conducted two experiments. Experiment 1 investigated the SA of the robotic arms under four conditions. The results showed that robotic arms manipulated by voluntary actions attenuated audio stimuli. Experiment 2 investigated the SA of the robotic arm and innate body under five conditions. The results indicated that the innate body and robotic arm induced SA, while there were differences in the sense of agency between the innate body and robotic arm. Analysis of the results indicated three findings regarding the SA of the extended body. First, controlling the robotic arm with voluntary actions in a virtual environment attenuates the audio stimuli. Second, there were differences in the sense of agency related to SA between extended and innate bodies. Third, the SA of the robotic arm was correlated with the sense of body ownership. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",human augmentation; robotic arm; Sensory attenuation; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,An Evaluation of View Rotation Techniques for Seated Navigation in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Head tracking is commonly used in VR applications to allow users to naturally view 3D content using physical head movement, but many applications also support turning with hand-held controllers. Controller and joystick controls are convenient for practical settings where full 360-degree physical rotation is not possible, such as when the user is sitting at a desk. Though controller-based rotation provides the benefit of convenience, previous research has demonstrated that virtual or joystick-controlled view rotation to have drawbacks of sickness and disorientation compared to physical turning. To combat such issues, researchers have considered various techniques such as speed adjustments or reduced field of view, but data is limited on how different variations for joystick rotation influences sickness and orientation perception. Our studies include different variations of techniques such as joystick rotation, resetting, and field-of-view reduction. We investigate trade-offs among different techniques in terms of sickness and the ability to maintain spatial orientation. In two controlled experiments, participants traveled through a sequence of rooms and were tested on spatial orientation, and we also collected subjective measures of sickness and preference. Our findings indicate a preference by users towards directly-manipulated joystick-based rotations compared to user-initiated resetting and minimal effects of technique on spatial awareness. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Human-centered computing; human-computer interaction; virtual reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,MobileSky: Real-Time Sky Replacement for Mobile AR,TVCG - Transactions on Visualization and Computer Graphics,A,"We present MobileSky, the first automatic method for real-time high-quality sky replacement for mobile AR applications. The primary challenge of this task is how to extract sky regions in camera feed both quickly and accurately. While the problem of sky replacement is not new, previous methods mainly concern extraction quality rather than efficiency, limiting their application to our task. We aim to provide higher quality, both spatially and temporally consistent sky mask maps for all camera frames in real time. To this end, we develop a novel framework that combines a new deep semantic network called FSNet with novel post-processing refinement steps. By leveraging IMU data, we also propose new sky-aware constraints such as temporal consistency, position consistency, and color consistency to help refine the weakly classified part of the segmentation output. Experiments show that our method achieves an average of around 30 FPS on off-the-shelf smartphones and outperforms the state-of-the-art sky replacement methods in terms of execution speed and quality. In the meantime, our mask maps appear to be visually more stable across frames. Our fast sky replacement method enables several applications, such as AR advertising, art making, generating fantasy celestial objects, visually learning about weather phenomena, and advanced video-based visual effects. To facilitate future research, we also create a new video dataset containing annotated sky regions with IMU data. © 2023 IEEE.",Mobile augmented reality; semantic segmentation; sky replacement,Keywords,TRUE,
Scopus,journalPaper,2024,Real-Time High-Quality Computer-Generated Hologram Using Complex-Valued Convolutional Neural Network,TVCG - Transactions on Visualization and Computer Graphics,A,"—Holographic displays are ideal display technologies for virtual and augmented reality because all visual cues are provided. However, real-time high-quality holographic displays are difficult to achieve because the generation of high-quality computer-generated hologram (CGH) is inefficient in existing algorithms. Here, complex-valued convolutional neural network (CCNN) is proposed for phase-only CGH generation. The CCNN-CGH architecture is effective with a simple network structure based on the character design of complex amplitude. A holographic display prototype is set up for optical reconstruction. Experiments verify that state-of-the-art performance is achieved in terms of quality and generation speed in existing end-to-end neural holography methods using the ideal wave propagation model. The generation speed is three times faster than HoloNet and one-sixth faster than Holo-encoder, and the Peak Signal to Noise Ratio (PSNR) is increased by 3 dB and 9 dB, respectively. Real-time high-quality CGHs are generated in 1920 × 1072 and 3840 × 2160 resolutions for dynamic holographic displays. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",augmented reality; Holography; neural models; virtual,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Studying the Influence of Multisensory Stimuli on a Firefighting Training Virtual Environment,TVCG - Transactions on Visualization and Computer Graphics,A,"—How we perceive and experience the world around us is inherently multisensory. Most of the Virtual Reality (VR) literature is based on the senses of sight and hearing. However, there is a lot of potential for integrating additional stimuli into Virtual Environments (VEs), especially in a training context. Identifying the relevant stimuli for obtaining a virtual experience that is perceptually equivalent to a real experience will lead users to behave the same across environments, which adds substantial value for several training areas, such as firefighters. In this article, we present an experiment aiming to assess the impact of different sensory stimuli on stress, fatigue, cybersickness, Presence and knowledge transfer of users during a firefighter training VE. The results suggested that the stimulus that significantly impacted the user’s response was wearing a firefighter’s uniform and combining all sensory stimuli under study: heat, weight, uniform, and mask. The results also showed that the VE did not induce cybersickness and that it was successful in the task of transferring knowledge. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Biofeedback; computer graphics; professional training; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Path Tracing in 2D, 3D, and Physicalized Networks",TVCG - Transactions on Visualization and Computer Graphics,A,"It is common to advise against using 3D to visualize abstract data such as networks, however Ware and Mitchell's 2008 study showed that path tracing in a network is less error prone in 3D than in 2D. It is unclear, however, if 3D retains its advantage when the 2D presentation of a network is improved using edge-routing, and when simple interaction techniques for exploring the network are available. We address this with two studies of path tracing under new conditions. The first study was preregistered, involved 34 users, and compared 2D and 3D layouts that the user could rotate and move in virtual reality with a handheld controller. Error rates were lower in 3D than in 2D, despite the use of edge-routing in 2D and the use of mouse-driven interactive highlighting of edges. The second study involved 12 users and investigated data physicalization, comparing 3D layouts in virtual reality versus physical 3D printouts of networks augmented with a Microsoft HoloLens headset. No difference was found in error rate, but users performed a variety of actions with their fingers in the physical condition which can inform new interaction techniques. © 1995-2012 IEEE.",3D printing; augmented reality; data physicalization; graph visualization; path finding; path following; tangible,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,StyleVR: Stylizing Character Animations With Normalizing Flows,TVCG - Transactions on Visualization and Computer Graphics,A,"—The significance of artistry in creating animated virtual characters is widely acknowledged, and motion style is a crucial element in this process. There has been a long-standing interest in stylizing character animations with style transfer methods. However, this kind of models can only deal with short-term motions and yield deterministic outputs. To address this issue, we propose a generative model based on normalizing flows for stylizing long and aperiodic animations in the VR scene. Our approach breaks down this task into two sub-problems: motion style transfer and stylized motion generation, both formulated as the instances of conditional normalizing flows with multi-class latent space. Specifically, we encode high-frequency style features into the latent space for varied results and control the generation process with style-content labels for disentangled edits of style and content. We have developed a prototype, StyleVR, in Unity, which allows casual users to apply our method in VR. Through qualitative and quantitative comparisons, we demonstrate that our system outperforms other methods in terms of style transfer as well as stochastic stylized motion generation. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Character animation motion eneration st le transfer; normalizing flow; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,Interactive Virtual Ankle Movement Controlled by Wrist sEMG Improves Motor Imagery: An Exploratory Study,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual reality (VR) techniques can significantly enhance motor imagery training by creating a strong illusion of action for central sensory stimulation. In this article, we establish a precedent by using surface electromyography (sEMG) of contralateral wrist movement to trigger virtual ankle movement through an improved data-driven approach with a continuous sEMG signal for fast and accurate intention recognition. Our developed VR interactive system can provide feedback training for stroke patients in the early stages, even if there is no active ankle movement. Our objectives are to evaluate: 1) the effects of VR immersion mode on body illusion, kinesthetic illusion, and motor imagery performance in stroke patients; 2) the effects of motivation and attention when utilizing wrist sEMG as a trigger signal for virtual ankle motion; 3) the acute effects on motor function in stroke patients. Through a series of well-designed experiments, we have found that, compared to the 2D condition, VR significantly increases the degree of kinesthetic illusion and body ownership of the patients, and improves their motor imagery performance and motor memory. When compared to conditions without feedback, using contralateral wrist sEMG signals as trigger signals for virtual ankle movement enhances patients' sustained attention and motivation during repetitive tasks. Furthermore, the combination of VR and feedback has an acute impact on motor function. Our exploratory study suggests that the sEMG-based immersive virtual interactive feedback provides an effective option for active rehabilitation training for severe hemiplegia patients in the early stages, with great potential for clinical application.  © 1995-2012 IEEE.",motor imagery; sEMG-based virtual feedback; VR-based stroke rehabilitation training,Abstract,TRUE,
Scopus,journalPaper,2024,Visual Illusion Created by a Striped Pattern Through Augmented Reality for the Prevention of Tumbling on Stairs,TVCG - Transactions on Visualization and Computer Graphics,A,"A fall on stairs can be a dangerous accident. An important indicator of falling risk is the foot clearance, which is the height of the foot when ascending stairs or the distance of the foot from the step when descending. We developed an augmented reality system with a holographic lens using a visual illusion to improve the foot clearance on stairs. The system draws a vertical striped pattern on the stair riser as the participant ascends the stairs to create the illusion that the steps are higher than the actual steps, and draws a horizontal striped pattern on the stair tread as the participant descends the stairs to create the illusion of narrower stairs. We experimentally evaluated the accuracy of the system and fitted a model to determine the appropriate stripe thickness. Finally, participants ascended and descended stairs before, during, and after using the augmented reality system. The foot clearance significantly improved, not only while the participants used the system but also after they used the system compared with before.  © 1995-2012 IEEE.",Human-computer interaction; user interfaces; virtual and augmented reality; virtual device interfaces,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,VR Blowing: A Physically Plausible Interaction Method for Blowing Air in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"This article introduces an interaction method allowing virtual reality (VR) users to interact with virtual objects by blowing air. The proposed method allows users to interact with virtual objects in a physically plausible way by recognizing the intensity of the wind generated by the user's actual wind blowing activity in the physical world. This is expected to provide immersed VR experience since it enables users to interact with virtual objects in the same way they do in the real world. Three experiments were carried out to develop and improve this method. In the first experiment, we collected the user's blowing data and used it to model a formula to estimate the speed of the wind from the sound waves obtained through a microphone. In the second experiment, we investigated how much gain can be applied to the formula obtained in the first experiment. The aim is to reduce the lung capacity required to generate wind without compromising physical plausibility. In the third experiment, the advantages and disadvantages of the proposed method compared to the controller-based method were investigated in two scenarios of blowing a ball and a pinwheel. According to the experimental results and participant interview, participants felt a stronger sense of presence and found the VR experience more fun with the proposed blowing interaction method. © 1995-2012 IEEE.",Blowing control; human-computer interaction; immersion; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Specular Path Generation and Near-Reflective Diffraction in Interactive Acoustical Simulations,TVCG - Transactions on Visualization and Computer Graphics,A,"Most systems for simulating sound propagation in a virtual environment for interactive applications use ray- or path-based models of sound. With these models, the 'early' (low-order) specular reflection paths play a key role in defining the 'sound' of the environment. However, the wave nature of sound, and the fact that smooth objects are approximated by triangle meshes, pose challenges for creating realistic approximations of the reflection results. Existing methods which produce accurate results are too slow to be used in most interactive applications with dynamic scenes. This paper presents a method for reflections modeling called spatially sampled near-reflective diffraction (SSNRD), based on an existing approximate diffraction model, Volumetric Diffraction and Transmission (VDaT). The SSNRD model addresses the challenges mentioned above, produces results accurate to within 1-2 dB on average compared to edge diffraction, and is fast enough to generate thousands of paths in a few milliseconds in large scenes. This method encompasses scene geometry processing, path trajectory generation, spatial sampling for diffraction modeling, and a small deep neural network (DNN) to produce the final response of each path. All steps of the method are GPU-accelerated, and NVIDIA RTX real-time ray tracing hardware is used for spatial computing tasks beyond just traditional ray tracing. © 1995-2012 IEEE.",Acoustics; graph and tree search strategies; neural nets; parallel algorithms; raytracing; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,PalmEx: Adding Palmar Force-Feedback for 3D Manipulation With Haptic Exoskeleton Gloves,TVCG - Transactions on Visualization and Computer Graphics,A,"—Haptic exoskeleton gloves are a widespread solution for providing force-feedback in Virtual Reality (VR), especially for 3D object manipulations. However, they are still lacking an important feature regarding in-hand haptic sensations: the palmar contact. In this paper, we present PalmEx, a novel approach which incorporates palmar force-feedback into exoskeleton gloves to improve the overall grasping sensations and manual haptic interactions in VR. PalmEx’s concept is demonstrated through a self-contained hardware system augmenting a hand exoskeleton with an encountered palmar contact interface – physically encountering the users’ palm. We build upon current taxonomies to elicit PalmEx’s capabilities for both the exploration and manipulation of virtual objects. We first conduct a technical evaluation optimising the delay between the virtual interactions and their physical counterparts. We then empirically evaluate PalmEx’s proposed design space in a user study (n=12) to assess the potential of a palmar contact for augmenting an exoskeleton. Results show that PalmEx offers the best rendering capabilities to perform believable grasps in VR. PalmEx highlights the importance of the palmar stimulation, and provides a low-cost solution to augment existing high-end consumer hand exoskeletons. © 2023 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Artefact; encountered-type of haptic device; ETHD; exoskeleton; haptics; on-demand; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,STTAR: Surgical Tool Tracking Using Off-the-Shelf Augmented Reality Head-Mounted Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"The use of Augmented Reality (AR) for navigation purposes has shown beneficial in assisting physicians during the performance of surgical procedures. These applications commonly require knowing the pose of surgical tools and patients to provide visual information that surgeons can use during the performance of the task. Existing medical-grade tracking systems use infrared cameras placed inside the Operating Room (OR) to identify retro-reflective markers attached to objects of interest and compute their pose. Some commercially available AR Head-Mounted Displays (HMDs) use similar cameras for self-localization, hand tracking, and estimating the objects' depth. This work presents a framework that uses the built-in cameras of AR HMDs to enable accurate tracking of retro-reflective markers without the need to integrate any additional electronics into the HMD. The proposed framework can simultaneously track multiple tools without having previous knowledge of their geometry and only requires establishing a local network between the headset and a workstation. Our results show that the tracking and detection of the markers can be achieved with an accuracy of 0.09±0.06mm on lateral translation, 0.42±0.32mm on longitudinal translation and 0.80±0.39° for rotations around the vertical axis. Furthermore, to showcase the relevance of the proposed framework, we evaluate the system's performance in the context of surgical procedures. This use case was designed to replicate the scenarios of k-wire insertions in orthopedic procedures. For evaluation, seven surgeons were provided with visual navigation and asked to perform 24 injections using the proposed framework. A second study with ten participants served to investigate the capabilities of the framework in the context of more general scenarios. Results from these studies provided comparable accuracy to those reported in the literature for AR-based navigation procedures. © 1995-2012 IEEE.",Augmented reality; computer-assisted medical procedures; navigation; tracking,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"The Effects of Auditory, Visual, and Cognitive Distractions on Cybersickness in Virtual Reality",TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness (CS) is one of the challenges that has hindered the widespread adoption of Virtual Reality (VR). Consequently, researchers continue to explore novel means to mitigate the undesirable effects associated with this affliction, one that may require a combination of remedies as opposed to a solitary stratagem. Inspired by research probing into the use of distractions as a means to control pain, we investigated the efficacy of this countermeasure against CS, studying how the introduction of temporally time-gated distractions affects this malady during a virtual experience featuring active exploration. Downstream of this, we studied how other aspects of the VR experience are affected by this intervention. We discuss the results of a between-subjects study manipulating the presence, sensory modality, and nature of periodic and short-lived (5-12 seconds) distractor stimuli across four experimental conditions: 1) no-distractors (ND); 2) auditory distractors (AD); 3) visual distractors (VD); 4) cognitive distractors (CD). Two of these conditions (VD and AD) formed a yoked control design wherein every matched pair of 'seers' and 'hearers' was periodically exposed to distractors that were identical in terms of content, temporality, duration, and sequence. In the CD condition, each participant had to periodically perform a 2-back working memory task, the duration and temporality of which was matched to distractors presented in each matched pair of the yoked conditions. These three conditions were compared to a baseline control group featuring no distractions. Results indicated that the reported sickness levels were lower in all three distraction groups in comparison to the control group. The intervention also increased the amount of time users were able to endure the VR simulation and avoided causing detriments to spatial memory and virtual travel efficiency. Overall, it appears that it may be possible to make users less consciously aware and bothered by the symptoms of CS, thereby reducing its perceived severity.  © 1995-2012 IEEE.",Active exploration; cybersickness; distractions; electrodermal activity; information recall; pain reduction; spatial memory; virtual motion; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Hold Tight: Identifying Behavioral Patterns During Prolonged Work in VR Through Video Analysis,TVCG - Transactions on Visualization and Computer Graphics,A,"VR devices have recently been actively promoted as tools for knowledge workers and prior work has demonstrated that VR can support some knowledge worker tasks. However, only a few studies have explored the effects of prolonged use of VR such as a study observing 16 participants working in VR and a physical environment for one work-week each and reporting mainly on subjective feedback. As a nuanced understanding of participants' behavior in VR and how it evolves over time is still missing, we report on the results from an analysis of 559 hours of video material obtained in this prior study. Among other findings, we report that (1) the frequency of actions related to adjusting the headset reduced by 46% and the frequency of actions related to supporting the headset reduced by 42% over the five days; (2) the HMD was removed 31% less frequently over the five days but for 41% longer periods; (3) wearing an HMD is disruptive to normal patterns of eating and drinking, but not to social interactions, such as talking. The combined findings in this work demonstrate the value of long-term studies of deployed VR systems and can be used to inform the design of better, more ergonomic VR systems as tools for knowledge workers.  © 1995-2012 IEEE.",future of work; long-term; office work; productivity work; prolonged use; video-analysis; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,Breaking the Isolation: Exploring the Impact of Passthrough in Shared Spaces on Player Performance and Experience in VR Exergames,TVCG - Transactions on Visualization and Computer Graphics,A,"VR exergames offer an engaging solution to combat sedentary behavior and promote physical activity. However, challenges emerge when playing these games in shared spaces, particularly due to the presence of bystanders. VR's passthrough functionality enables players to maintain awareness of their surrounding environment while immersed in VR gaming, rendering it a promising solution to improve users' awareness of the environment. This study investigates the passthrough's impact on player performance and experiences in shared spaces, involving an experiment with 24 participants that examines Space (Office vs. Corridor) and Passthrough Function (With vs. Without). Results reveal that Passthrough enhances game performance and environmental awareness while reducing immersion. Players prefer an open area to an enclosed room, whether with or without Passthrough, finding it more socially acceptable. Additionally, Passthrough appears to encourage participation among players with higher self-consciousness, potentially alleviating their concerns about being observed by bystanders. Our findings provide valuable insights for designing VR experiences in shared spaces, underscoring the potential of VR's passthrough to enhance user experiences and promote VR adoption in these environments.  © 1995-2012 IEEE.",Exergames; Immersion; Passthrough; Shared Spaces; Social Acceptability; Virtual Reality,Keywords,TRUE,
Scopus,journalPaper,2024,MOUNT: Learning 6DoF Motion Prediction Based on Uncertainty Estimation for Delayed AR Rendering,TVCG - Transactions on Visualization and Computer Graphics,A,"The delay of rendering on AR devices requires prediction of head motion using sensor data acquired tens of even one hundred milliseconds ago to avoid misalignment between the virtual content and the physical world, where the misalignment will lead to a sense of time latency and dizziness for users. To solve the problem, we propose a method for the 6DoF motion prediction to compensate for the time latency. Compared with traditional hand-crafted methods, our method is based on deep learning, which has better motion prediction ability to deal with complex human motion. In particular, we propose a MOtion UNcerTainty encode decode network (MOUNT) that estimates the uncertainty of input data and predicts the uncertainty of output motion to improve the prediction accuracy and smoothness. Experiments on the EuRoC and our collected dataset demonstrate that our method significantly outperforms the traditional method and greatly improves AR visual effects. © 1995-2012 IEEE.",learning environments; learning technologies; Virtual and augmented reality,Keywords,TRUE,
Scopus,journalPaper,2024,The Differential Effects of Multisensory Attentional Cues on Task Performance in VR Depending on the Level of Cognitive Load and Cognitive Capacity,TVCG - Transactions on Visualization and Computer Graphics,A,"As the utilization of VR is expanding across diverse fields, research on devising attentional cues that could optimize users' task performance in VR has become crucial. Since the cognitive load imposed by the context and the individual's cognitive capacity are representative factors that are known to determine task performance, we aimed to examine how the effects of multisensory attentional cues on task performance are modulated by the two factors. For this purpose, we designed a new experimental paradigm in which participants engaged in dual (N-back, visual search) tasks under different levels of cognitive load while an attentional cue (visual, tactile, or visuotactile) was presented to facilitate search performance. The results showed that multi-sensory attentional cues are generally more effective than uni-sensory cues in enhancing task performance, but the benefit of multi-sensory cues changes according to the level of cognitive load and the individual's cognitive capacity; the amount of benefit increases as the cognitive load is higher and the cognitive capacity is lower. The findings of this study provide practical implications for designing attentional cues to enhance VR task performance, considering both the complexity of the VR context and users' internal characteristics.  © 1995-2012 IEEE.",Attentional Cue; Cognitive Capacity; Cognitive Load; Multisensory; Performance; Virtual Reality,Keywords,TRUE,
Scopus,journalPaper,2024,Instant Segmentation and Fitting of Excavations in Subsurface Utility Engineering,TVCG - Transactions on Visualization and Computer Graphics,A,"Using augmented reality for subsurface utility engineering (SUE) has benefited from recent advances in sensing hardware, enabling the first practical and commercial applications. However, this progress has uncovered a latent problem â the insufficient quality of existing SUE data in terms of completeness and accuracy. In this work, we present a novel approach to automate the process of aligning existing SUE databases with measurements taken during excavation works, with the potential to correct the deviation from the as-planned to as-built documentation, which is still a big challenge for traditional workers at sight. Our segmentation algorithm performs infrastructure segmentation based on the live capture of an excavation on site. Our fitting approach correlates the inferred position and orientation with the existing digital plan and registers the as-planned model into the as-built state. Our approach is the first to circumvent tedious postprocessing, as it corrects data online and on-site. In our experiments, we show the results of our proposed method on both synthetic data and a set of real excavations.  © 1995-2012 IEEE.",3D Models; Augmented Reality; Geometric Constraints; Infrastructure; Localization; Segmentation,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,RedirectedDoors+: Door-Opening Redirection with Dynamic Haptics in Room-Scale VR,TVCG - Transactions on Visualization and Computer Graphics,A,"RedirectedDoors is a visuo-haptic door-opening redirection technique in VR, and it has shown promise in its ability to efficiently compress the physical space required for a room-scale VR experience. However, its previous implementation has only supported laboratory experiments with a single door opening at a fixed location. To significantly expand this technique for room-scale VR, we have developed RedirectedDoors+, a robot-based system that permits consecutive door-opening redirection with haptics. Specifically, our system is mainly achieved with the use of three components: (1) door robots, a small number of wheeled robots equipped with a doorknob-like prop, (2) a robot-positioning algorithm that arbitrarily positions the door robots to provide the user with just-in-time haptic feedback during door opening, and (3) a user-steering algorithm that determines the redirection gain for every instance of door opening to keep the user away from the boundary of the play area. Results of simulated VR exploration in six virtual environments reveal our system's performance relative to user walking speed, paths, and number of door robots, from which we derive its usage guidelines. We then conduct a user study ($N=12$) in which participants experience a walkthrough application using the actual system. The results demonstrate that the system is able to provide haptic feedback while redirecting the user within a limited play area. © 1995-2012 IEEE.",Encounter-type haptic device; Redirected Walking; Virtual reality; Visuo-haptic redirection,Keywords,TRUE,
Scopus,journalPaper,2024,"Assessing Depth Perception in VR and Video See-Through AR: A Comparison on Distance Judgment, Performance, and Preference",TVCG - Transactions on Visualization and Computer Graphics,A,"Spatial User Interfaces along the Reality-Virtuality continuum heavily depend on accurate depth perception. However, current display technologies still exhibit shortcomings in the simulation of accurate depth cues, and these shortcomings also vary between Virtual or Augmented Reality (VR, AR: eXtended Reality (XR) for short). This article compares depth perception between VR and Video See-Through (VST) AR. We developed a digital twin of an existing office room where users had top erform five depth-dependent tasks in VR and VST AR. Thirty-two participants took part in a user study using a 1 Ã - 4 within-subjects design. Our results reveal higher misjudgment rates in VST AR due to conflicting depth cues between virtual and physical content. Increased head movements observed in participants were interpreted as a compensatory response to these conflicting cues. Furthermore, a longer task completion time in the VST AR condition indicates a lower task performance in VST AR. Interestingly, while participants rated the VR condition as easier and contrary to the increased misjudgments and lower performance with the VST AR display, a majority still expressed a preference for the VST AR experience. We discuss and explain these findings with the high visual dominance and referential power of the physical content in the VST AR condition, leading to a higher spatial presence and plausibility. © 1995-2012 IEEE.",AR; Depth perception; egocentric distance judgment; task performance; user preference; video see-through; VR,Abstract,TRUE,
Scopus,journalPaper,2024,Omnidirectional Virtual Visual Acuity: A User-Centric Visual Clarity Metric for Virtual Reality Head-Mounted Displays and Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Users' perceived image quality of virtual reality head-mounted displays (VR HMDs) is determined by multiple factors, including the HMD's structure, optical system, display and render resolution, and users' visual acuity (VA). Existing metrics such as pixels per degree (PPD) have limitations that prevent accurate comparison of different VR HMDs. One of the main limitations is that not all VR HMD manufacturers released the official PPD or details of their HMDs' optical systems. Without these details, developers and users cannot know the precise PPD or calculate it for a given HMD. The other issue is that the visual clarity varies with the VR environment. Our work has identified a gap in having a feasible metric that can measure the visual clarity of VR HMDs. To address this gap, we present an end-to-end and user-centric visual clarity metric, omnidirectional virtual visual acuity (OVVA), for VR HMDs. OVVA extends the physical visual acuity chart into a virtual format to measure the virtual visual acuity of an HMD's central focal area and its degradation in its noncentral area. OVVA provides a new perspective to measure visual clarity and can serve as an intuitive and accurate reference for VR applications sensitive to visual accuracy. Our results show that OVVA is a simple yet effective metric for comparing VR HMDs and environments. © 1995-2012 IEEE.",Frame rate; Head-mounted displays; Measurements; Passthrough; Render resolution; Virtual reality; Visual clarity,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Workspace Guardian: Investigating Awareness of Personal Workspace Between Co-Located Augmented Reality Users,TVCG - Transactions on Visualization and Computer Graphics,A,"As augmented reality (AR) systems proliferate and the technology gets smaller and less intrusive, we imagine a future where many AR users will interact in the same physical locations (e.g., in shared work places and public spaces). While previous research has explored AR collaboration in these spaces, our focus is on co-located but independent work. In this paper, we explore co-located AR user behavior and investigate techniques for promoting awareness of personal workspace boundaries. Specifically, we compare three techniques: showing all virtual content, visualizing bounding box outlines of content, and a self-defined workspace boundary. The findings suggest that a self-defined boundary led to significantly more personal workspace encroachments. © 1995-2012 IEEE.",augmented reality; Mixed reality; three-dimensional displays,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"PLUME: Record, Replay, Analyze and Share User Behavior in 6DoF XR Experiences",TVCG - Transactions on Visualization and Computer Graphics,A,"From education to medicine to entertainment, a wide range of industrial and academic fields now utilize eXtended Reality (XR) technologies. This diversity and growing use are boosting research and leading to an increasing number of XR experiments involving human subjects. The main aim of these studies is to understand the user experience in the broadest sense, such as the user cognitive and emotional states. Behavioral data collected during XR experiments, such as user movements, gaze, actions, and physiological signals constitute precious assets for analyzing and understanding the user experience. While they contribute to overcome the intrinsic flaws of explicit data such as post-experiment questionnaires, the required acquisition and analysis tools are costly and challenging to develop, especially for 6DoF (Degrees of Freedom) XR experiments. Moreover, there is no common format for XR behavioral data, which restrains data-sharing, and thus hinders wide usages across the community, replicability of studies, and the constitution of large datasets or meta-analysis. In this context, we present PLUME, an open-source software toolbox (PLUME Recorder, PLUME Viewer, PLUME Python) that allows for the exhaustive record of XR behavioral data (including synchronous physiological signals), their offline interactive replay and analysis (with a standalone application), and their easy sharing due to our compact and interoperable data format. We believe that PLUME can greatly benefit the scientific community by making the use of behavioral and physiological data available for the greatest, contributing to the reproducibility and replicability of XR user studies, enabling the creation of large datasets, and contributing to a deeper understanding of user experience. © 1995-2012 IEEE.",Data Collection; Extended Reality; Human-Computer Interaction; Physiological Signals; Quality of Experience; User Behavior; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Investigating the Effects of Avatarization and Interaction Techniques on Near-field Mixed Reality Interactions with Physical Components,TVCG - Transactions on Visualization and Computer Graphics,A,"Mixed reality (MR) interactions feature users interacting with a combination of virtual and physical components. Inspired by research investigating aspects associated with near-field interactions in augmented and virtual reality (AR & VR), we investigated how avatarization, the physicality of the interacting components, and the interaction technique used to manipulate a virtual object affected performance and perceptions of user experience in a mixed reality fundamentals of laparoscopic peg-transfer task wherein users had to transfer a virtual ring from one peg to another for a number of trials. We employed a 3 (Physicality of pegs) X 3 (Augmented Avatar Representation) X 2 (Interaction Technique) multi-factorial design, manipulating the physicality of the pegs as a between-subjects factor, the type of augmented self-avatar representation, and the type of interaction technique used for object-manipulation as within-subjects factors. Results indicated that users were significantly more accurate when the pegs were virtual rather than physical because of the increased salience of the task-relevant visual information. From an avatar perspective, providing users with a reach envelope-extending representation, though useful, was found to worsen performance, while co-located avatarization significantly improved performance. Choosing an interaction technique to manipulate objects depends on whether accuracy or efficiency is a priority. Finally, the relationship between the avatar representation and interaction technique dictates just how usable mixed reality interactions are deemed to be.  © 1995-2012 IEEE.",Interactions in MR; Mixed Reality; Self-Avatars; Tangible entities,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Molecular Docking Improved with Human Spatial Perception Using Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Adaptive steered molecular dynamics (ASMD) is a computational biophysics method in which an external force is applied to a selected set of atoms or a specific reaction coordinate to induce a particular molecular motion. Virtual reality (VR) based methods for protein-ligand docking are beneficial for visualizing on-the-fly interactive molecular dynamics and performing promising docking trajectories. In this paper, we propose a novel method to guide ASMD with optimal trajectories collected from human experiences using interactive molecular dynamics in virtual reality (iMD-VR). We also explain the benefits of using VR as a tool for expediting the process of ligand binding, outlining an experimental protocol that enables iMD-VR users to guide Amprenavir into and out of the binding pockets of HIV-1 protease and recreate their respective crystallographic binding poses within 5 minutes. Later, we discuss our analysis of the results from iMD-VR-assisted ASMD simulation and assess its performance compared to a standard ASMD simulation. From the accuracy point of view, our proposed method calculates higher Potential Mean Force (PMF) values consistently relative to a standard ASMD simulation with an almost twofold increase in all the experiments. Finally, we describe the novelty of the research and discuss results showcasing a faster and more effective convergence of the ligand to the protein's binding site as compared to a standard molecular dynamics simulation, proving the effectiveness of VR in the field of drug discovery. Future work includes the development of an artificial intelligence algorithm capable of predicting optimal binding trajectories for many protein-ligand pairs, as well as the required force needed to steer the ligand to follow the said trajectory. © 1995-2012 IEEE.",Molecular Docking; Molecular Dynamics Simulation; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Virtual Reality Self Co-Embodiment: An Alternative to Mirror Therapy for Post-Stroke Upper Limb Rehabilitation,TVCG - Transactions on Visualization and Computer Graphics,A,"We present Virtual Reality Self Co-embodiment, a new method for post-stroke upper limb rehabilitation. It is inspired by mirror therapy, where the patient's healthy arm is involved in recovering the affected arm's motion. By tracking the user's head, wrists, and fingers' positions, our new approach allows the handicapped arm to control a digital avatar in order to pursue a reaching task. We apply the concept of virtual co-embodiment to use the information from the unaffected arm and complete the affected limb's impaired motion, which is our added unique feature. This requires users to mechanically involve the incapacitated area as much as possible, prioritizing actual movement rather than the sole imagination of it. As a result, subjects will see a seemingly normally functional virtual arm primarily controlled by their handicapped extremity, but with the constant support of their healthy limb's motion. Our experiment compares the task execution performance and embodiment perceived when interacting with both mirror therapy and our proposed technique. We found that our approach's provided sense of ownership is mildly impacted by users' motion planning response times, which mirror therapy does not exhibit. We also observed that mirror therapy's sense of ownership is moderately affected by the subject's proficiency while executing the assigned task, which our new method did not display. The results indicate that our proposed method provides similar embodiment and rehabilitation capabilities to those perceived from existing mirror therapy. This experiment was performed in healthy individuals to have an unbiased comparison of how mirror therapy's and VRSelfCo's task performance and degree of virtual embodiment compare, but future work explores the possibility of applying this new approach to actual post-stroke patients. © 1995-2012 IEEE.",Human-centered computing; motor imagery; rehabilitation techniques; stroke recovery; user interfaces; virtual embodiment; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via Transformer-Based 360° Image Outpainting,TVCG - Transactions on Visualization and Computer Graphics,A,"360° images, with a field-of-view (FoV) of 180°× 360°, provide immersive and realistic environments for emerging virtual reality (VR) applications, such as virtual tourism, where users desire to create diverse panoramic scenes from a narrow FoV photo they take from a viewpoint via portable devices. It thus brings us to a technical challenge: 'How to allow the users to freely create diverse and immersive virtual scenes from a narrow FoV image with a specified viewport?' To this end, we propose a transformer-based 360° image outpainting framework called Dream360, which can generate diverse, high-fidelity, and high-resolution panoramas from user-selected viewports, considering the spherical properties of 360° images. Compared with existing methods, e.g., [3], which primarily focus on inputs with rectangular masks and central locations while overlooking the spherical property of 360° images, our Dream360 offers higher outpainting flexibility and fidelity based on the spherical representation. Dream360 comprises two key learning stages: (I) codebook-based panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN learns a sphere-specific codebook from spherical harmonic (SH) values, providing a better representation of spherical data distribution for scene modeling. The frequency-aware refinement matches the resolution and further improves the semantic consistency and visual fidelity of the generated results. Our Dream360 achieves significantly lower Frechet Inception Distance (FID) scores and better visual fidelity than existing methods. We also conducted a user study involving 15 participants to interactively evaluate the quality of the generated results in VR, demonstrating the flexibility and superiority of our Dream360 framework. © 1995-2012 IEEE.",360 image outpainting; virtual scene creation; vision transformer,Abstract,TRUE,
Scopus,journalPaper,2024,Swift-Eye: Towards Anti-blink Pupil Tracking for Precise and Robust High-Frequency Near-Eye Movement Analysis with Event Cameras,TVCG - Transactions on Visualization and Computer Graphics,A,"Eye tracking has shown great promise in many scientific fields and daily applications, ranging from the early detection of mental health disorders to foveated rendering in virtual reality (VR). These applications all call for a robust system for high-frequency near-eye movement sensing and analysis in high precision, which cannot be guaranteed by the existing eye tracking solutions with CCD/CMOS cameras. To bridge the gap, in this paper, we propose Swift-Eye, an offline precise and robust pupil estimation and tracking framework to support high-frequency near-eye movement analysis, especially when the pupil region is partially occluded. Swift-Eye is built upon the emerging event cameras to capture the high-speed movement of eyes in high temporal resolution. Then, a series of bespoke components are designed to generate high-quality near-eye movement video at a high frame rate over kilohertz and deal with the occlusion over the pupil caused by involuntary eye blinks. According to our extensive evaluations on EV-Eye, a large-scale public dataset for eye tracking using event cameras, Swift-Eye shows high robustness against significant occlusion. It can improve the IoU and F1-score of the pupil estimation by 20% and 12.5% respectively, compared with the second-best competing approach, when over 80% of the pupil region is occluded by the eyelid. Lastly, it provides continuous and smooth traces of pupils in extremely high temporal resolution and can support high-frequency eye movement analysis and a number of potential applications, such as mental health diagnosis, behaviour-brain association, etc. The implementation details and source codes can be found at https://github.com/ztysdu/Swift-Eye.  © 1995-2012 IEEE.",event camera; Eye tracking; feature fusion,Abstract,TRUE,
Scopus,journalPaper,2024,Research Trends in Virtual Reality Music Concert Technology: A Systematic Literature Review,TVCG - Transactions on Visualization and Computer Graphics,A,"Advances in virtual reality (VR) technology have sparked novel avenues of growth in the musical domain. Following the COVID-19 pandemic, the rise of VR technology has led to growing interest in VR music concerts as an alternative to traditional live concerts. These virtual settings can provide immersion like attending real concerts for physically distant audiences and performers, and also can offer new creative possibilities. VR music concert research is still in its infancy, and advances in technologies such as multimodal devices are rapidly expanding the diversity of research, requiring a unified understanding of the field. To identify trends in VR music concert technology, we conducted a PRISMA-based systematic literature review covering the period from 2018 to 2023. After a thorough screening process, a total of 27 papers were selected for review. The studies were classified and analyzed based on the research topic (audience, performer, concert venue), interaction type (user-environment, user-user), and hardware used (head-mounted display, additional hardware). Furthermore, we categorized the evaluation metrics into user experience, usability, and performance. Our review contributes to advancing the understanding of recent developments in VR music concert technology, shedding light on the diversification and potential of this emerging field. © 1995-2012 IEEE.",Evaluation Metric; Interaction; Music Concert; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Designing and Evaluating a VR Lobby for a Socially Enriching Remote Opera Watching Experience,TVCG - Transactions on Visualization and Computer Graphics,A,"The latest social VR technologies have enabled users to attend traditional media and arts performances together while being geographically removed, making such experiences accessible despite budget, distance, and other restrictions. In this work, we aim at improving the way remote performances are shared by designing and evaluating a VR theatre lobby which serves as a space for users to gather, interact, and relive the common experience of watching a virtual opera. We conducted an initial test with experts ($\mathrm{N}=10$, i.e., designers and opera enthusiasts) in pairs using our VR lobby prototype, developed based on the theoretical lobby design concept. A unique aspect of our experience is its highly realistic representation of users in the virtual space. The test results guided refinements to the VR lobby structure and implementation, aiming to improve the user experience and align it more closely with the social VR lobby's intended purpose. With the enhanced prototype, we ran a between-subject controlled study ($\mathrm{N}=40$) to compare the user experience in the social VR lobby between individuals and paired participants. To do so, we designed and validated a questionnaire to measure the user experience in the VR lobby. Results of our mixed-methods analysis, including interviews, questionnaire results, and user behavior, reveal the strength of our social VR lobby in connecting with other users, consuming the opera in a deeper manner, and exploring new possibilities beyond what is common in real life. All supplemental materials are available at https://github.com/cwi-dis/IEEEVR2024-VRLobby.  © 1995-2012 IEEE.",Collaborative interaction; Empirical studies in HCI; Perfoming arts; User studies; Virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,Embodying a self-avatar with a larger leg: its impacts on motor control and dynamic stability,TVCG - Transactions on Visualization and Computer Graphics,A,"Several studies have shown that users of immersive virtual reality can feel high levels of embodiment in self-avatars that have different morphological proportions than those of their actual bodies. Deformed and unrealistic morphological modifications are accepted by embodied users, underlying the adaptability of one's mental map of their body (body schema) in response to incoming sensory feedback. Before initiating a motor action, the brain uses the body schema to plan and sequence the necessary movements. Therefore, embodiment in a self-avatar with a different morphology, such as one with deformed proportions, could lead to changes in motor planning and execution. In this study, we aimed to measure the effects on movement planning and execution of embodying a self-avatar with an enlarged lower leg on one side. Thirty participants embodied an avatar without any deformations, and with an enlarged dominant or non-dominant leg, in randomized order. Two different levels of embodiment were induced, using synchronous or asynchronous visuotactile stimuli. In each condition, participants performed a gait initiation task. Their center of mass and center of pressure were measured, and the margin of stability (MoS) was computed from these values. Their perceived level of embodiment was also measured, using a validated questionnaire. Results show no significant changes on the biomechenical variables related to dynamic stability. Embodiment scores decreased with asynchronous stimuli, without impacting the measures related to stability. The body schema may not have been impacted by the larger virtual leg. However, deforming the self-avatar's morphology could have important implications when addressing individuals with impaired physical mobility by subtly influencing action execution during a rehabilitation protocol. © 1995-2012 IEEE.","H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, Augmented, and Virtual Realities; H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/Methodology",Abstract,TRUE,
Scopus,journalPaper,2024,Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual Reality: Robustness and User Experience,TVCG - Transactions on Visualization and Computer Graphics,A,"Eye tracking is routinely being incorporated into virtual reality (VR) systems. Prior research has shown that eye tracking data, if exposed, can be used for re-identification attacks [14]. The state of our knowledge about currently existing privacy mechanisms is limited to privacy-utility trade-off curves based on data-centric metrics of utility, such as prediction error, and black-box threat models. We propose that for interactive VR applications, it is essential to consider user-centric notions of utility and a variety of threat models. We develop a methodology to evaluate real-time privacy mechanisms for interactive VR applications that incorporate subjective user experience and task performance metrics. We evaluate selected privacy mechanisms using this methodology and find that re-identification accuracy can be decreased to as low as 14% while maintaining a high usability score and reasonable task performance. Finally, we elucidate three threat scenarios (black-box, black-box with exemplars, and white-box) and assess how well the different privacy mechanisms hold up to these adversarial scenarios. This work advances the state of the art in VR privacy by providing a methodology for end-to-end assessment of the risk of re-identification attacks and potential mitigating solutions. f  © 1995-2012 IEEE.",eye tracking; privacy; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Exploring Audio Interfaces for Vertical Guidance in Augmented Reality via Hand-Based Feedback,TVCG - Transactions on Visualization and Computer Graphics,A,"This research proposes an evaluation of pitch-based sonification methods via user experiments in real-life scenarios, specifically vertical guidance, with the aim of standardizing the use of audio interfaces in AR in guidance tasks. Using literature on assistive technology for people who are blind or visually impaired, we aim to generalize their applicability to a broader population and for different use cases. We propose and test sonification methods for vertical guidance in a series of hand-navigation assessments with users without visual feedback. Including feedback from a visually impaired expert in digital accessibility, results (N=19) outlined that methods that do not rely on memorizing pitch had the most promising accuracy and self-reported workload performances. Ultimately, we argue for audio AR's ability to enhance user performance in different scenarios, from video games to finding objects in a pantry. © 1995-2012 IEEE.",assistive technologies; Audio interfaces; augmented and mixed reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,Exploring Bimanual Haptic Feedback for Spatial Search in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Spatial search tasks are common and crucial in many Virtual Reality (VR) applications. Traditional methods to enhance the performance of spatial search often employ sensory cues such as visual, auditory, or haptic feedback. However, the design and use of bimanual haptic feedback with two VR controllers for spatial search in VR remains largely unexplored. In this work, we explored bimanual haptic feedback with various combinations of haptic properties, where four types of bimanual haptic feedback were designed, for spatial search tasks in VR. Two experiments were designed to evaluate the effectiveness of bimanual haptic feedback on spatial direction guidance and search in VR. The results from the first experiment reveal that our proposed bimanual haptic schemes significantly enhanced the recognition of spatial directions in terms of accuracy and speed compared to spatial audio feedback. The second experiment's findings suggest that the performance of bimanual haptic feedback was comparable to or even better than the visual arrow, especially in reducing the angle of head movement and enhancing searching targets behind the participants, which was supported by subjective feedback as well. Based on these findings, we have derived a set of design recommendations for spatial search using bimanual haptic feedback in VR.  © 1995-2012 IEEE.",Bimanual Haptic Feedback; Controllers; Spatial Search; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,PetPresence: Investigating the Integration of Real-World Pet Activities in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"For VR interaction, the home environment with complicated spatial setup and dynamics may hinder the VR user experience. In particular, pets' movement may be more unpredictable. In this paper, we investigate the integration of real-world pet activities into immersive VR interaction. Our pilot study showed that the active pet movements, especially dogs, could negatively impact users' performance and experience in immersive VR. We proposed three different types of pet integration, namely semitransparent real-world portal, non-interactive object in VR, and interactive object in VR. We conducted the user study with 16 pet owners and their pets. The results showed that compared to the baseline condition without any pet-integration technique, the approach of integrating the pet as interactive objects in VR yielded significantly higher participant ratings in perceived realism, joy, multisensory engagement, and connection with their pets in VR. © 1995-2012 IEEE.",Distractions; Haptics; Pet; Presence; Virtual Reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,Spatial Contraction Based on Velocity Variation for Natural Walking in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) offers an immersive 3D digital environment, but enabling natural walking sensations without the constraints of physical space remains a technological challenge. Previous VR locomotion methods, including game controller, teleportation, treadmills, walking-in-place, and redirected walking (RDW), have made strides towards overcoming this challenge. However, these methods also face limitations such as possible unnaturalness, additional hardware requirements, or motion sickness risks. This paper introduces Spatial Contraction (SC), an innovative VR locomotion method inspired by the phenomenon of Lorentz contraction in Special Relativity. Similar to the Lorentz contraction, our SC contracts the virtual space along the user's velocity direction in response to velocity variation. The virtual space contracts more when the user's speed is high, whereas minimal or no contraction happens at low speeds. We provide a virtual space transformation method for spatial contraction and optimize the user experience in smoothness and stability. Through SC, VR users can effectively traverse a longer virtual distance with a shorter physical walking. Different from locomotion gains, the spatial contraction effect is observable by the user and aligns with their intentions, so there is no inconsistency between the user's proprioception and visual perception. SC is a general locomotion method that has no special requirements for VR scenes. The experimental results of our live user studies in various virtual scenarios demonstrate that SC has a significant effect in reducing both the number of resets and the physical walking distance users need to cover. Furthermore, experiments have also demonstrated that SC has the potential for integration with existing locomotion techniques such as RDW. © 1995-2012 IEEE.",locomotion; redirected walking; spatial contraction; velocity; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Learning Middle-Latitude Cyclone Formation up in the Air: Student Learning Experience, Outcomes, and Perceptions in a CAVE-Enabled Meteorology Class",TVCG - Transactions on Visualization and Computer Graphics,A,"Cave Automatic Virtual Environment (CAVE) is a virtual reality (VR) environment that has not been fully studied due to its high cost and complexity in system integration. Previous CAVE-related studies mainly focused on comparing its effectiveness with other learning media, such as textbooks, desktop VR, or head-mounted display (HMD) VR. In this study, through the utilization of CAVE in a meteorology class, we concentrated on CAVE itself, measured how CAVE impacted learners' learning outcomes before and after using CAVE in an actual ongoing undergraduate-level class, and investigated how learners perceived their learning experiences. Quantitative data were collected to examine the students' knowledge acquisition and learning experience. We also triangulated the quantitative results with qualitative data from the interviews regarding learners' perceptions of the CAVE-enabled class and their knowledge mastery. The results indicated that their learning outcomes increased through learning with CAVE and that their perceptions of immersion, presence, and engagement significantly correlated with each other. The interview results showed a great fondness of and satisfaction with the learning experience, group collaboration, and effectiveness of the CAVE-enabled class from the learners. We also learned that the learners' learning experiences in CAVE could be further improved if we provided them with more learner-environment interaction, offered them a better sense of immersion, and reduced cybersickness. Implications of these findings are discussed. © 1995-2012 IEEE.",CAVE; learning expereince; learning outcomes; meteorology; student perceptions,Abstract,TRUE,
Scopus,journalPaper,2024,Empowering Persons with Autism Through Cross-Reality and Conversational Agents,TVCG - Transactions on Visualization and Computer Graphics,A,"Autism Spectrum Disorder is a neurodevelopmental condition that can affect autonomy and independence. Our research explores the integration of Cross-Reality and Conversational Agents for Autistic persons to improve ability and confidence in everyday life situations. We combine two technologies of the Virtual-Real continuum. User experiences unfold from the simulation of tasks in VR to the execution of similar tasks supported by AR in the real world. A speech-based Conversational Agent is integrated with both VR and AR. It provides contextualized help, promotes generalization, and stimulates users to apply what they learned in the virtual space. The paper presents the approach and describes an empirical study involving 17 young Autistic persons. © 1995-2012 IEEE.",Accessibility technologies; Augmented reality; Conversational agents; Human computer interaction; Interactive learning environments; Virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,Expressive Talking Avatars,TVCG - Transactions on Visualization and Computer Graphics,A,"Stylized avatars are common virtual representations used in VR to support interaction and communication between remote collaborators. However, explicit expressions are notoriously difficult to create, mainly because most current methods rely on geometric markers and features modeled for human faces, not stylized avatar faces. To cope with the challenge of emotional and expressive generating talking avatars, we build the Emotional Talking Avatar Dataset which is a talking-face video corpus featuring 6 different stylized characters talking with 7 different emotions. Together with the dataset, we also release an emotional talking avatar generation method which enables the manipulation of emotion. We validated the effectiveness of our dataset and our method in generating audio based puppetry examples, including comparisons to state-of-the-art techniques and a user study. Finally, various applications of this method are discussed in the context of animating avatars in VR. © 1995-2012 IEEE.",Computer graphics; Graphics systems and interfaces; HCI design and evaluation methods; Human computer interaction (HCI); Human-centered computing; User studies; Virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,"In Case of Doubt, One Follows One's Self: The Implicit Guidance of the Embodied Self-Avatar",TVCG - Transactions on Visualization and Computer Graphics,A,"The sense of embodiment in virtual reality (VR) is commonly understood as the subjective experience that one's physical body is substituted by a virtual counterpart, and is typically achieved when the avatar's body, seen from a first-person view, moves like one's physical body. Embodiment can also be experienced in other circumstances (e.g., in third-person view) or with imprecise or distorted visuo-motor coupling. It was moreover observed, in various cases of small or progressive temporal and spatial manipulations of avatars' movements, that participants may spontaneously follow the movement shown by the avatar. The present work investigates whether, in some specific contexts, participants would follow what their avatar does even when large movement discrepancies occur, thereby extending the scope of understanding of the self-avatar follower effect beyond subtle changes of motion or speed manipulations. We conducted an experimental study in which we introduced uncertainty about which movement to perform at specific times and analyzed participants' movements and subjective feedback after their avatar showed them an incorrect movement. Results show that, when in doubt, participants were influenced by their avatar's movements, leading them to perform that particular error twice more often than normal. Importantly, results of the embodiment score indicate that participants experienced a dissociation with their avatar at those times. Overall, these observations not only demonstrate the possibility of provoking situations in which participants follow the guidance of their avatar for large motor distortions, despite their awareness about the avatar movement disruption and on the possible influence it had on their choice, and, importantly, exemplify how the cognitive mechanism of embodiment is deeply rooted in the necessity of having a body. © 1995-2012 IEEE.",self-avatar follower effect; sense of agency; sense of body ownership; virtual embodiment; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Stepping into the Right Shoes: The Effects of User-Matched Avatar Ethnicity and Gender on Sense of Embodiment in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"In many consumer virtual reality (VR) applications, users embody predefined characters that offer minimal customization options, frequently emphasizing storytelling over user choice. We explore whether matching a user's physical characteristics, specifically ethnicity and gender, with their virtual self-avatar affects their sense of embodiment in VR. We conducted a $2\times 2$ within-subjects experiment ($\mathrm{n}=32$) with a diverse user population to explore the impact of matching or not matching a user's self-avatar to their ethnicity and gender on their sense of embodiment. Our results indicate that matching the ethnicity of the user and their self-avatar significantly enhances sense of embodiment regardless of gender, extending across various aspects, including appearance, response, and ownership. We also found that matching gender significantly enhanced ownership, suggesting that this aspect is influenced by matching both ethnicity and gender. Interestingly, we found that matching ethnicity specifically affects self-location while matching gender specifically affects one's body ownership.  © 1995-2012 IEEE.",avatars; diversity; sense of embodiment; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Revisiting Walking-in-Place by Introducing Step-Height Control, Elastic Input, and Pseudo-Haptic Feedback",TVCG - Transactions on Visualization and Computer Graphics,A,"Walking-in-place (WIP) is a locomotion technique that enables users to 'walk infinitely' through vast virtual environments using walking-like gestures within a limited physical space. This article investigates alternative interaction schemes for WIP, addressing successively the control, input, and output of WIP. First, we introduce a novel height-based control to increase advanced speed. Second, we introduce a novel input system for WIP based on elastic and passive strips. Third, we introduce the use of pseudo-haptic feedback as a novel output for WIP meant to alter walking sensations. The results of a series of user studies show that height and frequency based control of WIP can facilitate higher virtual speed with greater efficacy and ease than in frequency-based WIP. Second, using an upward elastic input system can result in a stable virtual speed control, although excessively strong elastic forces may impact the usability and user experience. Finally, using a pseudo-haptic approach can improve the perceived realism of virtual slopes. Taken together, our results suggest that, for future VR applications, there is value in further research into the use of alternative interaction schemes for walking-in-place. © 1995-2012 IEEE.",elastic input; locomotion; passive haptics; pseudo-haptics; virtual reality; Walking-in-place,Keywords,TRUE,
Scopus,journalPaper,2024,BiRD: Using Bidirectional Rotation Gain Differences to Redirect Users during Back-and-forth Head Turns in Walking,TVCG - Transactions on Visualization and Computer Graphics,A,"Redirected walking (RDW) facilitates user navigation within expansive virtual spaces despite the constraints of limited physical spaces. It employs discrepancies between human visual-proprioceptive sensations, known as gains, to enable the remapping of virtual and physical environments. In this paper, we explore how to apply rotation gain while the user is walking. We propose to apply a rotation gain to let the user rotate by a different angle when reciprocating from a previous head rotation, to achieve the aim of steering the user to a desired direction. To apply the gains imperceptibly based on such a Bidirectional Rotation gain Difference (BiRD), we conduct both measurement and verification experiments on the detection thresholds of the rotation gain for reciprocating head rotations during walking. Unlike previous rotation gains which are measured when users are turning around in place (standing or sitting), BiRD is measured during users' walking. Our study offers a critical assessment of the acceptable range of rotational mapping differences for different rotational orientations across the user's walking experience, contributing to an effective tool for redirecting users in virtual environments. © 1995-2012 IEEE.",detection thresholds; Redirected walking; rotation gain; simulator sickness; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,Investigating Personalization Techniques for Improved Cybersickness Prediction in Virtual Reality Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"In recent cybersickness research, there has been a growing interest in predicting cybersickness using real-time physiological data such as heart rate, galvanic skin response, eye tracking, postural sway, and electroencephalogram. However, the impact of individual factors such as age and gender, which are pivotal in determining cybersickness susceptibility, remains unknown in predictive models. Our research seeks to address this gap, underscoring the necessity for a more personalized approach to cybersickness prediction to ensure a better, more inclusive virtual reality experience. We hypothesize that a personalized cybersickness prediction model would outperform non-personalized models in predicting cybersickness. Evaluating this, we explored four personalization techniques: 1) data grouping, 2) transfer learning, 3) early shaping, and 4) sample weighing using an open-source cybersickness dataset. Our empirical results indicate that personalized models significantly improve prediction accuracy. For instance, with early shaping, the Deep Temporal Convolutional Neural Network (DeepTCN) model achieved a 69.7% reduction in RMSE compared to its non-personalized version. Our study provides evidence of personalization techniques' benefits in improving cybersickness prediction. These findings have implications for developing personalized cybersickness prediction models tailored to individual differences, which can be used to develop personalized cybersickness reduction techniques in the future. © 1995-2012 IEEE.",Cybersickness; Cybersickness Personalization; Cybersickness Prediction; Deep Learning; Early Shaping; Machine Learning; Transfer Learning,Title_Abstract,TRUE,
Scopus,journalPaper,2024,CAEVR: Biosignals-Driven Context-Aware Empathy in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"There is little research on how Virtual Reality (VR) applications can identify and respond meaningfully to users' emotional changes. In this paper, we investigate the impact of Context-Aware Empathic VR (CAEVR) on the emotional and cognitive aspects of user experience in VR. We developed a real-time emotion prediction model using electroencephalography (EEG), electrodermal activity (EDA), and heart rate variability (HRV) and used this in personalized and generalized models for emotion recognition. We then explored the application of this model in a context-aware empathic (CAE) virtual agent and an emotion-adaptive (EA) VR environment. We found a significant increase in positive emotions, cognitive load, and empathy toward the CAE agent, suggesting the potential of CAEVR environments to refine user-agent interactions. We identify lessons learned from this study and directions for future work. © 1995-2012 IEEE.",context-aware; emotion; empathy; metaverse; physiology; virtual agents; VR,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Evaluating Text Reading Speed in VR Scenes and 3D Particle Visualizations,TVCG - Transactions on Visualization and Computer Graphics,A,"This work reports how text size and other rendering conditions affect reading speeds in a virtual reality environment and a scientific data analysis application. Displaying text legibly yet space-efficiently is a challenging problem in immersive displays. Effective text displays that enable users to read at their maximum speed must consider the variety of virtual reality (VR) display hardware and possible visual exploration tasks. We investigate how text size and display parameters affect reading speed and legibility in three state-of-the-art VR displays: two head-mounted displays and one CAVE. In our perception experiments, we establish limits where reading speed declines as the text size approaches the so-called critical print sizes (CPS) of individual displays, which can inform the design of uniform reading experiences across different VR systems. We observe an inverse correlation between display resolution and CPS. Yet, even in high-fidelity VR systems, the measured CPS was larger than in comparable physical text displays, highlighting the value of increased VR display resolutions in certain visualization scenarios. Our findings indicate that CPS can be an effective metric for evaluating VR display usability. Additionally, we evaluate the effects of text panel placement, orientation, and occlusion-reducing rendering methods on reading speeds in generic volumetric particle visualizations. Our study provides insights into the trade-off between text representation and legibility in cluttered immersive environments with specific suggestions for visualization designers and highlight areas for further research. © 1995-2012 IEEE.",Human-Computer Interaction; Perception; Scientific Visualization; Text Representation; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Low-Latency Ocular Parallax Rendering and Investigation of Its Effect on Depth Perception in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"With a demand for an immersive experience in virtual/augmented reality (VR/AR) displays, recent efforts have incorporated eye states, such as focus and fixation, into display graphics. Among these, ocular parallax, a small parallax generated by eye rotation, has received considerable attention for its impact on depth perception. However, the substantial latency of head-mounted displays (HMDs) has made it challenging to accurately assess its true effect during free eye movements. To address this issue, we propose a high-speed (360 Hz) and low-latency (4.8 ms) ocular parallax rendering system with a custom-built eye tracker. Using this proposed system, we conducted an investigation to determine the latency requirements necessary for achieving perceptually stable ocular parallax rendering. Our findings indicate that, in binocular viewing, ocular parallax rendering is perceived as significantly less stable than conventional rendering when the latency exceeds 43.72 ms at 1.3 D and 21.50 ms at 2.0 D. We also evaluated the effects of ocular parallax rendering on binocular fusion and monocular depth perception under free viewing conditions. The results demonstrated that ocular parallax rendering can enhance binocular fusion but has a limited impact on depth perception under monocular viewing conditions when latency is minimized. © 1995-2012 IEEE.",binocular fusion; depth perception; eye's front-nodal-point tracking; low-latency feedback system; Ocular parallax,Title_Abstract,TRUE,
Scopus,journalPaper,2024,"Berkeley Open Extended Reality Recordings 2023 (BOXRR-23): 4.7 Million Motion Capture Recordings from 105,000 XR Users",TVCG - Transactions on Visualization and Computer Graphics,A,"Extended reality (XR) devices such as the Meta Quest and Apple Vision Pro have seen a recent surge in attention, with motion tracking atelemetrya data lying at the core of nearly all XR and metaverse experiences. Researchers are just beginning to understand the implications of this data for security, privacy, usability, and more, but currently lack large-scale human motion datasets to study. The BOXRR-23 dataset contains 4,717,215 motion capture recordings, voluntarily submitted by 105,852 XR device users from over 50 countries. BOXRR-23 is over 200 times larger than the largest existing motion capture research dataset and uses a new, highly efficient and purpose-built XR Open Recording (XROR) file format. © 1995-2012 IEEE.",big data; Dataset; extended reality; motion capture; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,PreVR: Variable-Distance Previews for Higher-Order Disocclusion in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"The paper introduces PreVR, a method for allowing the user of a VR application to preview a virtual environment (VE) around any number of corners. This way the user can gain line of sight to any part of the VE, no matter how distant or how heavily occluded it is. PreVR relies on a multiperspective visualization that implements a higher-order disocclusion effect with piecewise linear rays that bend multiple times as needed to reach the visualization target. PreVR was evaluated in a user study (N = 88) that investigates four points on the VR interface design continuum defined by the maximum disocclusion order d. In a first control condition (CC0), d = 0, corresponds to conventional VR exploration with no preview capability. In a second control condition (CC1), d = 1, corresponds to the prior art approach of giving the user a preview around the first corner. In a first experimental condition (EC3), d = 3, so PreVR provided up to third-order disocclusion. In a second experimental condition (ECN), d was not capped, so PreVR could provide a disocclusion effect of any order, as needed to reach any location in the VE. Participants searched for a stationary target, for a dynamic target moving on a random continuous trajectory, and for a transient dynamic target that appeared at random locations in the maze and disappeared 5s later. The study quantified VE exploration efficiency with four metrics: viewpoint translation, view direction rotation, number of teleportations, and task completion time. Results show that the previews afforded by PreVR bring a significant VE exploration efficiency advantage. ECN outperforms EC3, CC1, and CC0 for all metrics and all tasks, and EC3 frequently outperforms CC1 and CC0. © 1995-2012 IEEE.",Disocclusion; navigation; virtual reality; visualization,Keywords,TRUE,
Scopus,journalPaper,2024,Redirection Strategy Switching: Selective Redirection Controller for Dynamic Environment Adaptation,TVCG - Transactions on Visualization and Computer Graphics,A,"In this paper, we present the Selective Redirection Controller (SRC), which selects the optimal redirection controller based on the physical and virtual environment in Redirected Walking (RDW). The primary advantage of SRC over existing controllers is its dynamic switching among four different redirection controllers (S2C, TAPF, ARC, and SRL) based on the user's environment, as opposed to using a single fixed controller throughout the experience. By switching between redirection controllers based on the context around the user, SRC aims to optimize the advantages of each redirection strategy. The SRC model is trained using reinforcement learning to dynamically and instantaneously switch redirection controllers based on the user's environment. We evaluated the performance of SRC against traditional redirection controllers through simulations and user studies conducted in various physical and virtual environments. The findings indicate that SRC reduces the number of resets significantly compared to traditional redirection controllers. Heat map visualization was utilized during the development process to analyze which redirection controller SRC chooses based on the different environments around the user. SRC alternates between redirection techniques based on the user's environment, maximizing the advantages of each strategy for a superior RDW experience. © 1995-2012 IEEE.",Redirected walking; Reinforcement learning; Virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,IntenSelect+: Enhancing Score-Based Selection in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Object selection in virtual environments is one of the most common and recurring interaction tasks. Therefore, the used technique can critically influence a system's overall efficiency and usability. IntenSelect is a scoring-based selection-by-volume technique that was shown to offer improved selection performance over conventional raycasting in virtual reality. This initial method, however, is most pronounced for small spherical objects that converge to a point-like appearance only, is challenging to parameterize, and has inherent limitations in terms of flexibility. We present an enhanced version of IntenSelect called IntenSelect+ designed to overcome multiple shortcomings of the original IntenSelect approach. In an empirical within-subjects user study with 42 participants, we compared IntenSelect+ to IntenSelect and conventional raycasting on various complex object configurations motivated by prior work. In addition to replicating the previously shown benefits of IntenSelect over raycasting, our results demonstrate significant advantages of IntenSelect+ over IntenSelect regarding selection performance, task load, and user experience. We, therefore, conclude that IntenSelect+ is a promising enhancement of the original approach that enables faster, more precise, and more comfortable object selection in immersive virtual environments.  © 1995-2012 IEEE.",3D Interaction; 3D User Interfaces; IntenSelect; Score-Based Selection; Selection; Temporal Selection; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,The Effects of Secondary Task Demands on Cybersickness in Active Exploration Virtual Reality Experiences,TVCG - Transactions on Visualization and Computer Graphics,A,"Active exploration in virtual reality (VR) involves users navigating immersive virtual environments, going from one place to another. While navigating, users often engage in secondary tasks that require attentional resources, as in the case of distracted driving. Inspired by research generally studying the effects of task demands on cybersickness (CS), we investigated how the attentional demands specifically associated with secondary tasks performed during exploration affect CS. Downstream of this, we studied how increased attentional demands from secondary tasks affect spatial memory and navigational performance. We discuss the results of a multi-factorial between-subjects study, manipulating a secondary task's demand across two levels and studying its effects on CS in two different sickness-inducing levels of an exploration experience. The secondary task's demand was manipulated by parametrically varying n in an aural n-back working memory task and the provocativeness of the experience was manipulated by varying how frequently users experienced a yaw-rotational reorientation effect during the exploration. Results revealed that increases in the secondary task's demand increased sickness levels, also resulting in a higher temporal onset rate, especially when the experience was not already highly sickening. Increased attentional demand from the secondary task also vitiated navigational performance and spatial memory. Overall, increased demands from secondary tasks performed during navigation produce deleterious effects on the VR experience. © 1995-2012 IEEE.",Active Exploration; Cybersickness; Electrodermal Activity; Secondary Task Demand; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Robust Dual-Modal Speech Keyword Spotting for XR Headsets,TVCG - Transactions on Visualization and Computer Graphics,A,"While speech interaction finds widespread utility within the Extended Reality (XR) domain, conventional vocal speech keyword spotting systems continue to grapple with formidable challenges, including suboptimal performance in noisy environments, impracticality in situations requiring silence, and susceptibility to inadvertent activations when others speak nearby. These challenges, however, can potentially be surmounted through the cost-effective fusion of voice and lip movement information. Consequently, we propose a novel vocal-echoic dual-modal keyword spotting system designed for XR headsets. We devise two different modal fusion approches and conduct experiments to test the system's performance across diverse scenarios. The results show that our dual-modal system not only consistently outperforms its single-modal counterparts, demonstrating higher precision in both typical and noisy environments, but also excels in accurately identifying silent utterances. Furthermore, we have successfully applied the system in real-time demonstrations, achieving promising results. The code is available at https://github.com/caizhuojiang/VE-KWS. © 1995-2012 IEEE.",extended reality; keyword spotting; multimodal interaction; Speech interaction,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Exploring the Influence of Virtual Avatar Heads in Mixed Reality on Social Presence, Performance and User Experience in Collaborative Tasks",TVCG - Transactions on Visualization and Computer Graphics,A,"In Mixed Reality (MR), users' heads are largely (if not completely) occluded by the MR Head-Mounted Display (HMD) they are wearing. As a consequence, one cannot see their facial expressions and other communication cues when interacting locally. In this paper, we investigate how displaying virtual avatars' heads on-top of the (HMD-occluded) heads of participants in a Video See-Through (VST) Mixed Reality local collaborative task could improve their collaboration as well as social presence. We hypothesized that virtual heads would convey more communicative cues (such as eye direction or facial expressions) hidden by the MR HMDs and lead to better collaboration and social presence. To do so, we conducted a between-subject study ($\mathrm{n}=88$) with two independent variables: the type of avatar (CartoonAvatar/RealisticAvatar/NoAvatar) and the level of facial expressions provided (HighExpr/LowExpr). The experiment involved two dyadic communication tasks: (i) the 20-question game where one participant asks questions to guess a hidden word known by the other participant and (ii) a urban planning problem where participants have to solve a puzzle by collaborating. Each pair of participants performed both tasks using a specific type of avatar and facial animation. Our results indicate that while adding an avatar's head does not necessarily improve social presence, the amount of facial expressions provided through the social interaction does have an impact. Moreover, participants rated their performance higher when observing a realistic avatar but rated the cartoon avatars as less uncanny. Taken together, our results contribute to a better understanding of the role of partial avatars in local MR collaboration and pave the way for further research exploring collaboration in different scenarios, with different avatar types or MR setups. © 1995-2012 IEEE.",Avatar Representation; Collaboration; Mixed Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Visual Cue Based Corrective Feedback for Motor Skill Training in Mixed Reality: A Survey,TVCG - Transactions on Visualization and Computer Graphics,A,"When learning a motor skill it is helpful to get corrective feedback from an instructor. This will support the learner to execute the movement correctly. With modern technology, it is possible to provide this feedback via mixed reality. In most cases, this involves visual cues to help the user understand the corrective feedback. We analyzed recent research approaches utilizing visual cues for feedback in mixed reality. The scope of this article is visual feedback for motor skill learning, which involves physical therapy, exercise, rehabilitation etc. While some of the surveyed literature discusses therapeutic effects of the training, this article focuses on visualization techniques. We categorized the literature from a visualization standpoint, including visual cues, technology and characteristics of the feedback. This provided insights into how visual feedback in mixed reality is applied in the literature and how different aspects of the feedback are related. The insights obtained can help to better adjust future feedback systems to the target group and their needs. This article also provides a deeper understanding of the characteristics of the visual cues in general and promotes future, more detailed research on this topic. © 1995-2012 IEEE.",Human-centered computing; interaction techniques; virtual and augmented reality; visualization; visualization techniques and methodologies,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Task and Environment-Aware Virtual Scene Rearrangement for Enhanced Safety in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Emerging VR applications have revolutionized user experiences by immersing individuals in digitally crafted environments. However, fully immersive experiences introduce new challenges, notably the risk of physical hazards when users are unaware of their surroundings. Existing solutions, including guardian spaces and locomotion systems, present trade-offs that either disrupt the immersive experience or risk inducing motion sickness. To address these challenges, we propose a novel approach that dynamically rearranges VR scenes according to users' physical spaces, seamlessly embedding physical constraints and interaction tasks into the virtual environment. We design a computational model to optimize the rearranged scene through a cost function, ensuring collision-free interactions while maintaining visual fidelity and the goal of interaction tasks. The experiments demonstrate improvements in user experience and safety, presenting an innovative solution to harmonize physical and virtual environments in VR applications.  © 1995-2012 IEEE.",scene rearrangement; scene synthesis; VR safety,Title,TRUE,
Scopus,journalPaper,2024,'May I Speak?': Multi-Modal Attention Guidance in Social VR Group Conversations,TVCG - Transactions on Visualization and Computer Graphics,A,"In this paper, we present a novel multi-modal attention guidance method designed to address the challenges of turn-taking dynamics in meetings and enhance group conversations within virtual reality (VR) environments. Recognizing the difficulties posed by a confined field of view and the absence of detailed gesture tracking in VR, our proposed method aims to mitigate the challenges of noticing new speakers attempting to join the conversation. This approach tailors attention guidance, providing a nuanced experience for highly engaged participants while offering subtler cues for those less engaged, thereby enriching the overall meeting dynamics. Through group interview studies, we gathered insights to guide our design, resulting in a prototype that employs light as a diegetic guidance mechanism, complemented by spatial audio. The combination creates an intuitive and immersive meeting environment, effectively directing users' attention to new speakers. An evaluation study, comparing our method to state-of-the-art attention guidance approaches, demonstrated significantly faster response times ($p < 0.001$), heightened perceived conversation satisfaction ($p < 0.001$), and preference ($p < 0.001$) for our method. Our findings contribute to the understanding of design implications for VR social attention guidance, opening avenues for future research and development. © 1995-2012 IEEE.",Attention Guidance; Group Conversations; Multi-modal Interaction; Social VR; Turn-taking,Abstract,TRUE,
Scopus,journalPaper,2024,Visuo-Haptic VR and AR Guidance for Dental Nerve Block Education,TVCG - Transactions on Visualization and Computer Graphics,A,"The inferior alveolar nerve block (IANB) is a dental anesthetic injection that is critical to the performance of many dental procedures. Dental students typically learn to administer an IANB through videos and practice on silicone molds and, in many dental schools, on other students. This causes significant stress for both the students and their early patients. To reduce discomfort and improve clinical outcomes, we created an anatomically informed virtual reality headset-based educational system for the IANB. It combines a layered 3D anatomical model, dynamic visual guidance for syringe position and orientation, and active force feedback to emulate syringe interaction with tissue. A companion mobile augmented reality application allows students to step through a visualization of the procedure on a phone or tablet. We conducted a user study to determine the advantages of preclinical training with our IANB simulator. We found that in comparison to dental students who were exposed only to traditional supplementary study materials, dental students who used our IANB simulator were more confident administering their first clinical injections, had less need for syringe readjustments, and had greater success in numbing patients. © 1995-2012 IEEE.",augmented reality; health care information systems; mixed reality; Virtual reality; visualization techniques,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Comparing Synchronous and Asynchronous Task Delivery in Mixed Reality Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Asynchronous digital communication is a widely applied and well-known form of information exchange. Most pieces of technology make use of some variation of asynchronous communication systems, be it messaging or email applications. This allows recipients to process digital messages immediately (synchronous) or whenever they have time (asynchronous), meaning that purely digital interruptions can be mitigated easily. Mixed Reality systems have the potential to not only handle digital interruptions but also interruptions in physical space, e.g., caused by co-workers in workspaces or learning environments. However, the benefits of such systems previously remained untested in the context of Mixed Reality. We conducted a user study (N=26) to investigate the impact that the timing of task delivery has on the participants' performance, workflow, and emotional state. Participants had to perform several cognitively demanding tasks in a Mixed Reality workspace. Inside the virtual workspace, we simulated in-person task delivery either during tasks (i.e., interrupting the participant) or between tasks (i.e., delaying the interruption). Our results show that delaying interruptions has a significant impact on subjective metrics like the perceived performance and workload. © 1995-2012 IEEE.",Evaluation; Interruptions; Mixed Reality; Task focus; Workspaces,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Corr-Track: Category-Level 6D Pose Tracking with Soft-Correspondence Matrix Estimation,TVCG - Transactions on Visualization and Computer Graphics,A,"Category-level pose tracking methods can continuously track the pose of objects without requiring any prior knowledge of the specific shape of the tracked instance. This makes them advantageous in augmented reality and virtual reality applications. The key challenge is how to train neural networks to accurately predict the poses of objects they have never seen before and exhibit strong generalization performance. We propose a novel category-level 6D pose tracking method Corr-Track, which is capable of accurately tracking objects belonging to the same category from depth video streams. Our approach utilizes direct soft correspondence constraints to train a neural network, which estimates bidirectional soft correspondences between sparsely sampled point clouds of objects in two frames. We first introduce a soft correspondence matrix for pose tracking tasks and establish effective constraints through direct spatial point-to-point correspondence representations in the sparse point cloud correspondence matrix. We propose the 'point cloud expansion' strategy to address the 'point cloud shrinkage' problem resulting from soft correspondences. This strategy ensures that the corresponding point cloud accurately reproduces the shape of the target point cloud, leading to precise pose tracking results. We evaluated our approach on the NOCS-REAL275 and Wild6D dataset and observed superior performance compared to previous methods. Additionally, we conducted cross-category experiments that further demonstrated its generalization capability. © 1995-2012 IEEE.",Category-level object pose estimation; object tracking,Abstract,TRUE,
Scopus,journalPaper,2024,Towards Co-Operative Beaming Displays: Dual Steering Projectors for Extended Projection Volume and Head Orientation Range,TVCG - Transactions on Visualization and Computer Graphics,A,"Existing near-eye displays (NEDs) have trade-offs related to size, weight, computational resources, battery life, and body temperature. A recent paradigm, beaming display, addresses these trade-offs by separating the NED into a steering projector (SP) for image presentation and a passive headset worn by the user. However, the beaming display has issues with the projection area of a single SP and has severe limitations on the head orientation and pose that the user can move. In this study, we distribute dual steering projectors in the scene to extend the head orientation and pose of the beaming display by coordinating the dual projections on a passive headset. For cooperative control of each SP, we define a geometric model of the SPs and propose a calibration and projection control method designed for multiple projectors. We present implementations of the system along with evaluations showing that the precision and delay are 1.8 5.7 mm and 14.46 ms, respectively, at a distance of about 1 m from the SPs. From this result, our prototype with multiple SPs can project images in the projection area (formula Presented) of the passive headset while extending the projectable head orientation. Furthermore, as applications of cooperative control by multiple SPs, we show the possibility of multiple users, improving dynamic range and binocular presentation. © 1995-2012 IEEE.",Augmented reality; Near-eye display; Projectors,Keywords,TRUE,
Scopus,journalPaper,2024,Eye-Hand Typing: Eye Gaze Assisted Finger Typing via Bayesian Processes in AR,TVCG - Transactions on Visualization and Computer Graphics,A,"Nowadays, AR HMDs are widely used in scenarios such as intelligent manufacturing and digital factories. In a factory environment, fast and accurate text input is crucial for operators' efficiency and task completion quality. However, the traditional AR keyboard may not meet this requirement, and the noisy environment is unsuitable for voice input. In this article, we introduce Eye-Hand Typing, an intelligent AR keyboard. We leverage the speed advantage of eye gaze and use a Bayesian process based on the information of gaze points to infer users' text input intentions. We improve the underlying keyboard algorithm without changing user input habits, thereby improving factory users' text input speed and accuracy. In real-time applications, when the user's gaze point is on the keyboard, the Bayesian process can predict the most likely characters, vocabulary, or commands that the user will input based on the position and duration of the gaze point and input history. The system can enlarge and highlight recommended text input options based on the predicted results, thereby improving user input efficiency. A user study showed that compared with the current HoloLens 2 system keyboard, Eye-Hand Typing could reduce input error rates by 28.31 % and improve text input speed by 14.5%. It also outperformed a gaze-only technique, being 43.05% more accurate and 39.55% faster. And it was no significant compromise in eye fatigue. Users also showed positive preferences. © 1995-2012 IEEE.",Augmented reality; bayesian process; eye-hand coordination; fitts' law; multi-modal interaction; primacy effect; text entry,Keywords,TRUE,
Scopus,journalPaper,2024,Enhancing Tai Chi Training System: Towards Group-Based and Hyper-Realistic Training Experiences,TVCG - Transactions on Visualization and Computer Graphics,A,"In this article, we propose a lightweight and flexible enhanced Tai Chi training system composed of multiple standalone virtual reality (VR) devices. The system aims to enable a hyper-realistic multi-user action training platform at low cost by displaying real-time action guidance trajectories, providing real-world impossible visual effects and functions, and rapidly enhancing movement precision and communication interest for learners. We objectively evaluate participants' action quality at different levels of immersion, including traditional coach guidance (TCG), VR, and mixed reality (MR), along with subjective measures like motion sickness, quality of interaction, social meaning, presence/immersion to comprehensively explore the system's feasibility. The results indicate VR performs the best in training accuracy, but MR provides superior social experience and relatively high accuracy. Unlike TCG, MR offers hyper-realistic hand movement trajectories and Tai Chi social references. Compared with VR, MR provides more realistic avatar companions and a safer environment. In summary, MR balances accuracy and social experience. © 1995-2012 IEEE.",action guidance trajectories; hand movement trajectories analysis; hyper-realistic; social experience; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,100-Phones: A Large VI-SLAM Dataset for Augmented Reality Towards Mass Deployment on Mobile Phones,TVCG - Transactions on Visualization and Computer Graphics,A,"Visual-inertial SLAM (VI-SLAM) is a key technology for Augmented Reality (AR), which allows the AR device to recover its 6-DoF motion in real-time in order to render the virtual content with the corresponding pose. Nowadays, smartphones are still the mainstream devices for ordinary users to experience AR. However the current VI-SLAM methods, although performing well on high-end phones, still face robustness challenges when deployed on a larger stock of mid- and low-end phones. Existing VI-SLAM datasets use either very ideal sensors or only a limited number of devices for data collection, which cannot reflect the capability gaps that VI-SLAM methods need to solve when deployed on a large variety of phone models. This work proposes 100-Phones. the first VI-SLAM dataset covering a wide range of mainstream phones in the market. The dataset consists of 350 sequences collected by 100 different models of phones. Through analysis and experiments on the collected data, we conclude that the quality of visual-inertial data vary greatly among the mainstream phones, and the current open source VI-SLAM methods still have serious robustness issues when it comes to mass deployment on mobile phones. We release the dataset to facilitate the robustness improvement of VI-SLAM and to promote the mass popularization of AR. Project page: https://github.com/zju3dv/100-Phones.  © 1995-2012 IEEE.",Augmented Reality; Benchmark; Dataset; VI-SLAM,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Self-Guided DMT: Exploring a Novel Paradigm of Dance Movement Therapy in Mixed Reality for Children with ASD,TVCG - Transactions on Visualization and Computer Graphics,A,"Children diagnosed with Autism Spectrum Disorder (ASD) often exhibit motor disorders. Dance Movement Therapy (DMT) has shown great potential for improving the motor control ability of children with ASD. However, traditional DMT methods often lack vividness and are difficult to implement effectively. To address this issue, we propose a Mixed Reality DMT approach, utilizing interactive virtual agents. This approach offers immersive training content and multi-sensory feedback. To improve the training performance of children with ASD, we introduce a novel training paradigm featuring a self-guided mode. This paradigm enables the rapid creation of a virtual twin agent of the child with ASD using a single photo to embody oneself, which can then guide oneself during training. We conducted an experiment with the participation of 24 children diagnosed with ASD (or ASD propensity), recording their training performance under various experimental conditions. Through expert rating, behavior coding of training sessions, and statistical analysis, our findings revealed that the use of the twin agent for self-guidance resulted in noticeable improvements in the training performance of children with ASD. These improvements were particularly evident in terms of enhancing movement quality and refining overall target-related responses. Our study holds clinical potential in the field of medical treatment and rehabilitation for children with ASD. © 1995-2012 IEEE.",Autism spectrum disorder; dance movement therapy; mixed reality; self-guided; virtual agent,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,ViComp: Video Compensation for Projector-Camera Systems,TVCG - Transactions on Visualization and Computer Graphics,A,"Projector video compensation aims to cancel the geometric and photometric distortions caused by non-ideal projection surfaces and environments when projecting videos. Most existing projector compensation methods start by projecting and capturing a set of sampling images, followed by an offline compensation model training step. Thus, abundant user effort is required before the users can watch the video. Moreover, the sampling images have little prior knowledge of the video content and may lead to suboptimal results. To address these issues, this paper builds a video compensation system that can online adapt the compensation parameters. Our approach consists of five threads and can perform compensation, projection, capturing, and short-term and long-term model updates in parallel. Due to the parallel mechanism, rather than projecting and capturing hundreds of sampling images and training the model offline, we can directly use the projected and captured video frames for model updates on the fly. To quickly apply to the new environment, we introduce a deep learning-based compensation model that integrates a fixed transformer-based method and a novel CNN-based network. Moreover, for fast convergence and to reduce error accumulation during fine-tuning, we present a strategy that cooperates with short-term and long-term memory model updates. Experiments show that it significantly outperforms state-of-the-art baselines. © 1995-2012 IEEE.",Continuous projection mapping; Projector compensation; Projector-camera system; Spatial augmented reality; Video compensation,Keywords,TRUE,
Scopus,journalPaper,2024,The Utilitarian Virtual Self - Using Embodied Personalized Avatars to Investigate Moral Decision-Making in Semi-Autonomous Vehicle Dilemmas,TVCG - Transactions on Visualization and Computer Graphics,A,"Embodied personalized avatars are a promising new tool to investigate moral decision-making by transposing the user into the 'middle of the action' in moral dilemmas. Here, we tested whether avatar personalization and motor control could impact moral decision-making, physiological reactions and reaction times, as well as embodiment, presence and avatar perception. Seventeen participants, who had their personalized avatars created in a previous study, took part in a range of incongruent (i.e., harmful action led to better overall outcomes) and congruent (i.e., harmful action led to trivial outcomes) moral dilemmas as the drivers of a semi-autonomous car. They embodied four different avatars (counterbalanced - personalized motor control, personalized no motor control, generic motor control, generic no motor control). Overall, participants took a utilitarian approach by performing harmful actions only to maximize outcomes. We found increased physiological arousal (SCRs and heart rate) for personalized avatars compared to generic avatars, and increased SCRs in motor control conditions compared to no motor control. Participants had slower reaction times when they had motor control over their avatars, possibly hinting at more elaborate decision-making processes. Presence was also higher in motor control compared to no motor control conditions. Embodiment ratings were higher for personalized avatars, and generally, personalization and motor control were perceptually positive features. These findings highlight the utility of personalized avatars and open up a range of future research possibilities that could benefit from the affordances of this technology and simulate, more closely than ever, real-life action. © 1995-2012 IEEE.",Human-centered computing-Empirical studies in HCI; Usability testing; Virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,VR.net: A Real-world Dataset for Virtual Reality Motion Sickness Research,TVCG - Transactions on Visualization and Computer Graphics,A,"Researchers have used machine learning approaches to identify motion sickness in VR experience. These approaches would certainly benefit from an accurately labeled, real-world, diverse dataset that enables the development of generalizable ML models. We introduce 'VR.net', a dataset comprising 165-hour gameplay videos from 100 real-world games spanning ten diverse genres, evaluated by 500 participants. VR.net accurately assigns 24 motion sickness-related labels for each video frame, such as camera/object movement, depth of field, and motion flow. Building such a dataset is challenging since manual labeling would require an infeasible amount of time. Instead, we implement a tool to automatically and precisely extract ground truth data from 3D engines' rendering pipelines without accessing VR games' source code. We illustrate the utility of VR.net through several applications, such as risk factor detection and sickness level prediction. We believe that the scale, accuracy, and diversity of VR.net can offer unparalleled opportunities for VR motion sickness research and beyond.We also provide access to our data collection tool, enabling researchers to contribute to the expansion of VR.net.  © 1995-2012 IEEE.",Machine Learning; Motion Sickness; Virtual Reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,"Handwriting for Text Input and the Impact of XR Displays, Surface Alignments, and Sentence Complexities",TVCG - Transactions on Visualization and Computer Graphics,A,"Text input is desirable across various eXtended Reality (XR) use cases and is particularly crucial for knowledge and office work. This article compares handwriting text input between Virtual Reality (VR) and Video See-Through Augmented Reality (VST AR), facilitated by physically aligned and mid-air surfaces when writing simple and complex sentences. In a $2\times 2\times 2$ experimental design, 72 participants performed two ten-minute handwriting sessions, each including ten simple and ten complex sentences representing text input in real-world scenarios. Our developed handwriting application supports different XR displays, surface alignments, and handwriting recognition based on digital ink. We evaluated usability, user experience, task load, text input performance, and handwriting style. Our results indicate high usability with a successful transfer of handwriting skills to the virtual domain. XR displays and surface alignments did not impact text input speed and error rate. However, sentence complexities did, with participants achieving higher input speeds and fewer errors for simple sentences (17.85 WPM, 0.51% MSD ER) than complex sentences (15.07 WPM, 1.74% MSD ER). Handwriting on physically aligned surfaces showed higher learnability and lower physical demand, making them more suitable for prolonged handwriting sessions. Handwriting on mid-air surfaces yielded higher novelty and stimulation ratings, which might diminish with more experience. Surface alignments and sentence complexities significantly affected handwriting style, leading to enlarged and more connected cursive writing in both mid-air and for simple sentences. The study also demonstrated the benefits of using XR controllers in a pen-like posture to mimic styluses and pressure-sensitive tips on physical surfaces for input detection. We additionally provide a phrase set of simple and complex sentences as a basis for future text input studies, which can be expanded and adapted. © 1995-2012 IEEE.",AR; digital ink; digital twin; handwriting; mid-air; phrase set; physically aligned; recognition; text input; VR; XR,Abstract,TRUE,
Scopus,journalPaper,2024,Measurement of Empathy in Virtual Reality with Head-Mounted Displays: A Systematic Review,TVCG - Transactions on Visualization and Computer Graphics,A,"We present a systematic review of 111 papers that measure the impact of virtual experiences created through head-mounted displays (HMDs) on empathy. Our goal was to analyze the conditions and the extent to which virtual reality (VR) enhances empathy. To achieve this, we categorized the relevant literature according to measurement methods, correlated human factors, viewing experiences, topics, and participants. Meta-analysis was performed based on categorized themes, and under specified conditions, we found that VR can improve empathy. Emotional empathy increased temporarily after the VR experience and returned to its original level over time, whereas cognitive empathy remained enhanced. Furthermore, while VR did not surpass 2D video in improving emotional empathy, it did enhance cognitive empathy, which is associated with embodiment. Our results are consistent with existing research suggesting differentiation between cognitive empathy (influenced by environmental factors and learnable) and emotional empathy (highly heritable and less variable). Interactivity, target of empathy, and point of view were not found to significantly affect empathy, but participants' age and nationality were found to influence empathy levels. It can be concluded that VR enhances cognitive empathy by immersing individuals in the perspective of others and that storytelling and personal characteristics are more important than the composition of the VR scene. Our findings provide guiding information for creating empathy content in VR and designing experiments to measure empathy. © 1995-2012 IEEE.",empathy; measurement; systematic review; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Modeling the Impact of Head-Body Rotations on Audio-Visual Spatial Perception for Virtual Reality Applications,TVCG - Transactions on Visualization and Computer Graphics,A,"Humans perceive the world by integrating multimodal sensory feedback, including visual and auditory stimuli, which holds true in virtual reality (VR) environments. Proper synchronization of these stimuli is crucial for perceiving a coherent and immersive VR experience. In this work, we focus on the interplay between audio and vision during localization tasks involving natural head-body rotations. We explore the impact of audio-visual offsets and rotation velocities on users' directional localization acuity for various viewing modes. Using psychometric functions, we model perceptual disparities between visual and auditory cues and determine offset detection thresholds. Our findings reveal that target localization accuracy is affected by perceptual audio-visual disparities during head-body rotations, but remains consistent in the absence of stimuli-head relative motion. We then showcase the effectiveness of our approach in predicting and enhancing users' localization accuracy within realistic VR gaming applications. To provide additional support for our findings, we implement a natural VR game wherein we apply a compensatory audio-visual offset derived from our measured psychometric functions. As a result, we demonstrate a substantial improvement of up to 40% in participants' target localization accuracy. We additionally provide guidelines for content creation to ensure coherent and seamless VR experiences.  © 1995-2012 IEEE.",Audio-Visual Spatial Perception; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"StainedSweeper: Compact, Variable-Intensity Light-Attenuation Display with Sweeping Tunable Retarders",TVCG - Transactions on Visualization and Computer Graphics,A,"Light Attenuation Displays (LADs) are a type of Optical See-Through Head-Mounted Display (OST-HMD) that present images by attenuating incoming light with a pixel-wise polarizing color filter. Although LADs can display images in bright environments, there is a trade-off between the number of Spatial Light Modulators (SLMs) and the color gamut and contrast that can be expressed, making it difficult to achieve both high-fidelity image display and a small form factor. To address this problem, we propose StainedSweeper, a LAD that achieves both the wide color gamut and the variable intensity with a single SLM. Our system synchronously controls a pixel-wise Digital Micromirror Device (DMD) and a nonpixel polarizing color filter to pass light when each pixel is the desired color. By sweeping this control at high speed, the human eye perceives images in a time-multiplexed, integrated manner. To achieve this, we develop the OST-HMD design using a reflective Solc filter as a polarized color filter and a color reproduction algorithm based on the optimization of the time-multiplexing matrix for the selected primary color filters. Our proof-of-concept prototype showed that our single SLM design can produce subtractive images with variable contrast and a wider color gamut than conventional LADs. © 1995-2012 IEEE.",augmented reality; Light attenuation display; polarized color filter; see-through display; time-multiplexing,Keywords,TRUE,
Scopus,journalPaper,2024,Investigating Whether the Mass of a Tool Replica Influences Virtual Training Learning Outcomes,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) has emerged as a promising solution to address the pressing concern of transferring know-how in the manufacturing industry. Making an immersive training experience often involves designing an instrumented replica of a tool whose use is to be learned through virtual training. The process of making a replica can alter its mass, making it different from that of the original tool. As far as we know, the influence of this difference on learning outcomes has never been evaluated. To investigate this subject, an immersive training experience was designed with pre and post-training phases under real conditions, dedicated to learning the use of a rotary tool. 80 participants took part in this study, split into three groups: a control group performing the virtual training using a replica with the same mass as the original tool ($\mathrm{m}=100\%$), a second group that used a replica with a lighter mass than the original tool ($\mathrm{m}= 50\%$) and a third group using a replica heavier than the original tool ($\mathrm{m}=150\%$). Despite variations in the mass of the replica used for training, this study revealed that the learning outcomes remained comparable across all groups, while also demonstrating significant enhancements in certain performance measures, including task completion time. Overall, these findings provide useful insights regarding the design of tool replicas for immersive training. © 1995-2012 IEEE.",Prop Design; User Study; Virtual Reality; Virtual Training; Weight Perception,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Try This for Size: Multi-Scale Teleportation in Immersive Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"The ability of a user to adjust their own scale while traveling through virtual environments enables them to inspect tiny features being ant-sized and to gain an overview of the surroundings as a giant. While prior work has almost exclusively focused on steering-based interfaces for multi-scale travel, we present three novel teleportation-based techniques that avoid continuous motion flow to reduce the risk of cybersickness. Our approaches build on the extension of known teleportation workflows and suggest specifying scale adjustments either simultaneously with, as a connected second step after, or separately from the user's new horizontal position. The results of a two-part user study with 30 participants indicate that the simultaneous and connected specification paradigms are both suitable candidates for effective and comfortable multi-scale teleportation with nuanced individual benefits. Scale specification as a separate mode, on the other hand, was considered less beneficial. We compare our findings to prior research and publish the executable of our user study to facilitate replication and further analyses.  © 1995-2012 IEEE.",3D Navigation; 3D User Interfaces; Head-Mounted Display; Multi-Scale; Teleportation; Virtual Reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,A Study on Collaborative Visual Data Analysis in Augmented Reality with Asymmetric Display Types,TVCG - Transactions on Visualization and Computer Graphics,A,"Collaboration is a key aspect of immersive visual data analysis. Due to its inherent benefit of seeing co-located collaborators, augmented reality is often useful in such collaborative scenarios. However, to enable the augmentation of the real environment, there are different types of technology available. While there are constant developments in specific devices, each of these device types provide different premises for collaborative visual data analysis. In our work we combine handheld, optical see-through and video see-through displays to explore and understand the impact of these different device types in collaborative immersive analytics. We conducted a mixed-methods collaborative user study where groups of three performed a shared data analysis task in augmented reality with each user working on a different device, to explore differences in collaborative behaviour, user experience and usage patterns. Both quantitative and qualitative data revealed differences in user experience and usage patterns. For collaboration, the different display types influenced how well participants could participate in the collaborative data analysis, nevertheless, there was no measurable effect in verbal communication.  © 1995-2012 IEEE.",Augmented Reality; Collaboration; Empirical Studies,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,APF-S2T: Steering to Target Redirection Walking Based on Artificial Potential Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"Redirected walking (RDW) enables users to walk naturally within a virtual environment that is larger than the physical environment. Recently, several artificial potential field (APF) and alignment-based redirected controllers have been developed and have been demonstrated to significantly outperform conventional controllers. APF Steer-to-Gradient (APF-S2G) and APF Redirected Walking (APF-RDW) utilize the negative gradient and the total force vector, respectively, which are localized to the user's position. These vectors usually point towards the opposite wall when the user is in corridors, resulting in frequent resets within those regions. This paper introduces the APF Steer-to-Target (APF-S2T), a redirected controller that first finds the target sample point with the lowest score in the user's walkable area in both physical and virtual environments. The score of a sample point is determined by the APF value at the point and the distance from the user's position. The direction from the user's position to the target point is then used as the steering direction for setting RDW gains. We conducted a simulation-based evaluation to compare APF-S2T, APF-S2G, APF-RDW, Visibility Polygon-based alignment (Vis.-Poly.) and Alignment-Optimized controllers in terms of the number of resets and the average distance between resets. The results indicated that APF-S2T significantly outperformed the state-of-the-art controllers. © 1995-2012 IEEE.",artificial potential field; locomotion; Redirected walking; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,"Who says you are so sick? An investigation on individual susceptibility to cybersickness triggers using EEG, EGG and ECG",TVCG - Transactions on Visualization and Computer Graphics,A,"In this research paper, we conducted a study to investigate the connection between three objective measures: Electrocardio-gram(EGG), Electrogastrogram (EGG), and Electroencephalogram (EEG), and individuals' susceptibility to cybersickness. Our primary objective was to identify which of these factors plays a central role in causing discomfort when experiencing rotations along three different axes: Roll, Pitch, and Yaw. This study involved 35 participants who were tasked with destroying asteroids using their eye gaze while undergoing passive rotations in four separate sessions. The results, when combined with subjective measurements (specifically, Fast motion sickness questionnaire (FMS) and Simulator sickness questionnaire (SSQ) score), demonstrated that EGG measurements were superior in detecting symptoms associated with nausea. As for ECG measurements, our observations did reveal significant changes in Heart Rate Variability (HRV) parameters. However, we caution against relying solely on ECG as a dependable indicator for assessing the extent of cybersickness. Most notably, EEG signals emerged as a crucial resource for discerning individual differences related to these rotational axes. Our findings were significant not only in the context of periodic activities but also underscored the potential of aperiodic activities in detecting the severity of cybersickness and an individual's susceptibility to rotational triggers.  © 1995-2012 IEEE.",Electroencephalogram; Electrogastrogram; Gybersickness; Individual Susceptibility.Electrocardiogram; Virtual Reality,Keywords,TRUE,
Scopus,journalPaper,2024,Cybersickness Abatement from Repeated Exposure to VR with Reduced Discomfort,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness, or sickness induced by virtual reality (VR), negatively impacts the enjoyment and adoption of the technology. One method that has been used to reduce sickness is repeated exposure to VR, herein Cybersickness Abatement from Repeated Exposure (CARE). However, high sickness levels during repeated exposure may discourage some users from returning. Field of view (FOV) restriction reduces cybersickness by minimizing visual motion in the periphery, but also negatively affects the user's visual experience. This study explored whether CARE that occurs with FOV restriction generalizes to a full FOV experience. Participants played a VR game for up to 20 minutes. Those in the Repeated Exposure Condition played the same VR game on four separate days, experiencing FOV restriction during the first three days and no FOV restriction on the fourth day. Results indicated significant CARE with FOV restriction (Days 1-3). Further, cybersickness on Day 4, without FOV restriction, was significantly lower than that of participants in the Single Exposure Condition, who experienced the game without FOV restriction only on one day. The current findings show that significant CARE can occur while experiencing minimal cybersickness. Results are considered in the context of multiple theoretical explanations for CARE, including sensory rearrangement, adaptation, habituation, and postural control.  © 1995-2012 IEEE.",Adaptation; Cybersickness; Field of View Restriction; Habituation; Repeated Exposure; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Beyond the Wizard of Oz: Negative Effects of Imperfect Machine Learning to Examine the Impact of Reliability of Augmented Reality Cues on Visual Search Performance,TVCG - Transactions on Visualization and Computer Graphics,A,"Despite knowing exactly what an object looks like, searching for it in a person's visual field is a time-consuming and error-prone experience. In Augmented Reality systems, new algorithms are proposed to speed up search time and reduce human errors. However, these algorithms might not always provide 100% accurate visual cues, which might affect users' perceived reliability of the algorithm and, thus, search performance. Here, we examined the detrimental effects of automation bias caused by imperfect cues presented in the Augmented Reality head-mounted display using the YOLOv5 machine learning model. 53 participants in the two groups received either 100% accurate visual cues or 88.9% accurate visual cues. Their performance was compared with the control condition, which did not include any additional cues. The results show how cueing may increase performance and shorten search times. The results also showed that performance with imperfect automation was much worse than perfect automation and that, consistent with automation bias, participants were frequently enticed by incorrect cues. © 1995-2012 IEEE.",Augmented Reality; Automation Bias; Imperfect Cues; Visual Search,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,With or Without You: Effect of Contextual and Responsive Crowds on VR-based Crowd Motion Capture,TVCG - Transactions on Visualization and Computer Graphics,A,"While data is vital to better understand and model interactions within human crowds, capturing real crowd motions is extremely challenging. Virtual Reality (VR) demonstrated its potential to help, by immersing users into either simulated virtual crowds based on autonomous agents, or within motion-capture-based crowds. In the latter case, users' own captured motion can be used to progressively extend the size of the crowd, a paradigm called Record-and-Replay (2R). However, both approaches demonstrated several limitations which impact the quality of the acquired crowd data. In this paper, we propose the new concept of contextual crowds to leverage both crowd simulation and the 2R paradigm towards more consistent crowd data. We evaluate two different strategies to implement it, namely a Replace-Record-Replay (3R) paradigm where users are initially immersed into a simulated crowd whose agents are successively replaced by the user's captured-data, and a Replace-Record-Replay-Responsive (4R) paradigm where the pre-recorded agents are additionally endowed with responsive capabilities. These two paradigms are evaluated through two real-world-based scenarios replicated in VR. Our results suggest that the behaviors observed in VR users with surrounding agents from the beginning of the recording process are made much more natural, enabling 3R or 4R paradigms to improve the consistency of captured crowd datasets.  © 1995-2012 IEEE.",Crowd Simulation; Human Interaction; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,'Where Did My Apps Go?' Supporting Scalable and Transition-Aware Access to Everyday Applications in Head-Worn Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Future augmented reality (AR) glasses empower users to view personal applications and services anytime and anywhere without being restricted by physical locations and the availability of physical screens. In typical everyday activities, people move around to carry out different tasks and need a variety of information on the go. Existing interfaces in AR do not support these use cases well, especially when the number of applications increases. We explore the usability of three world-referenced approaches that move AR applications with users as they transition among different locations, featuring different levels of AR app availability: (1) always using a menu to manually open an app when needed; (2) automatically suggesting a relevant subset of all apps; and (3) carrying all apps with the users to the new location. Through a controlled study and a relatively more ecologically-valid study in AR, we reached better understandings on the performance trade-offs and observed the impact of various everyday contextual factors on these interfaces in more realistic AR settings. Our results shed light on how to better support the mobile information needs of users in everyday life in future AR interfaces.  © 1995-2012 IEEE.",Adaptive interface; augmented reality; automation; glanceable interface; mobile computing,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Projection Mapping under Environmental Lighting by Replacing Room Lights with Heterogeneous Projectors,TVCG - Transactions on Visualization and Computer Graphics,A,"Projection mapping (PM) is a technique that enhances the appearance of real-world surfaces using projected images, enabling multiple people to view augmentations simultaneously, thereby facilitating communication and collaboration. However, PM typically requires a dark environment to achieve high-quality projections, limiting its practicality. In this paper, we overcome this limitation by replacing conventional room lighting with heterogeneous projectors. These projectors replicate environmental lighting by selectively illuminating the scene, excluding the projection target. Our contributions include a distributed projector optimization framework designed to effectively replicate environmental lighting and the incorporation of a large-aperture projector, in addition to standard projectors, to reduce high-luminance emitted rays and hard shadows-undesirable factors for collaborative tasks in PM. We conducted a series of quantitative and qualitative experiments, including user studies, to validate our approach. Our findings demonstrate t hat our projector-based lighting system significantly enhancesthe contrast and realism of PM results even under e nvironmental lighting compared to typical lights. Furthermore, our method facilitates a substantial shift in the perceived color mode from the undesirable aperture-color mode, where observers perceive the projected object as self-luminous, to the surface-color mode in PM. © 1995-2012 IEEE.",Augmented reality; cooperative distributed projector optimization; large-aperture projector; projection mapping,Keywords,TRUE,
Scopus,journalPaper,2024,Real-and-Present: Investigating the Use of Life-Size 2D Video Avatars in HMD-Based AR Teleconferencing,TVCG - Transactions on Visualization and Computer Graphics,A,"Augmented Reality (AR) teleconferencing allows spatially distributed users to interact with each other in 3D through agents in their own physical environments. Existing methods leveraging volumetric capturing and reconstruction can provide a high-fidelity experience but are often too complex and expensive for everyday use. Other solutions target mobile and effortless-to-setup teleconferencing on AR Head Mounted Displays (HMD). They directly transplant the conventional video conferencing onto an AR-HMD platform or use avatars to represent remote participants. However, they can only support either a high fidelity or a high level of co-presence. Moreover, the limited Field of View (FoV) of HMDs could further degrade users' immersive experience. To achieve a balance between fidelity and co-presence, we explore using life-size 2D video-based avatars (video avatars for short) in AR teleconferencing. Specifically, with the potential effect of FoV on users' perception of proximity, we first conducted a pilot study to explore the local-user-centered optimal placement of video avatars in small-group AR conversations. With the placement results, we then implement a proof-of-concept prototype of video-avatar-based teleconferencing. We conduct user evaluations with our prototype to verify its effectiveness in balancing fidelity and co-presence. Following the indication in the pilot study, we further quantitatively explore the effect of FoV size on the video avatar's optimal placement through a user study involving more FoV conditions in a VR-simulated environment. We regress placement models to serve as references for computationally determining video avatar placements in such teleconferencing applications on various existing AR HMDs and future ones with bigger FoVs.  © 1995-2012 IEEE.",AR Teleconference; Co-presence; Fidelity; Field of View; Video-Based Avatar,Abstract,TRUE,
Scopus,journalPaper,2024,Inserting Objects into Any Background Images via Implicit Parametric Representation,TVCG - Transactions on Visualization and Computer Graphics,A,"Inserting an object into a background scene has wide applications in image editing and mixed reality. However, existing methods still struggle to seamlessly adapt the object to the background while maintaining its individual characteristics. In this paper, we propose to fine-tune a pre-trained diffusion-based insertion model such that it learns to establish a unique correspondence between a few weights and the target object, given as input few-shot images of an object. A novel individualized feature extraction (IFE) module is designed to extract the individual detail features from few-shot object images. Then, the individual features of the target object, together with the semantic features of the target object and the background context features extracted by the pre-trained image encoders are injected into the cross-attention modules of the latent diffusion model, enabling it to learn the correlation information of the target object and the background scene through the attention mechanism. The weights obtained by fine-tuning implicitly serve as an alternative representation of the target object, with which the object can be easily inserted into any background images. Extensive comparative experiments validate the superiority of the proposed method to the state-of-the-art insertion methods in maintaining the individual details of the inserted object and adapting it to background scenes, including allowing the interaction between the inserted object and the background scene, correctly handling their occlusion relationship, maintaining the consistency of their viewpoints and poses. © 1995-2012 IEEE.",implicit parametric representation; insertion strategy; latent diffusion model; Object insertion,Abstract,TRUE,
Scopus,journalPaper,2024,Multi-User Redirected Walking in Separate Physical Spaces for Online VR Scenarios,TVCG - Transactions on Visualization and Computer Graphics,A,"With the recent rise of Metaverse, online multiplayer VR applications are becoming increasingly prevalent worldwide. However, as multiple users are located in different physical environments, different reset frequencies and timings can lead to serious fairness issues for online collaborative/competitive VR applications. For the fairness of online VR apps/games, an ideal online RDW strategy must make the locomotion opportunities of different users equal, regardless of different physical environment layouts. The existing RDW methods lack the scheme to coordinate multiple users in different PEs, and thus have the issue of triggering too many resets for all the users under the locomotion fairness constraint. We propose a novel multi-user RDW method that is able to significantly reduce the overall reset number and give users a better immersive experience by providing a fair exploration. Our key idea is to first find out the 'bottleneck' user that may cause all users to be reset and estimate the time to reset given the users' next targets, and then redirect all the users to favorable poses during that maximized bottleneck time to ensure the subsequent resets can be postponed as much as possible. More particularly, we develop methods to estimate the time of possibly encountering obstacles and the reachable area for a specific pose to enable the prediction of the next reset caused by any user. Our experiments and user study found that our method outperforms existing RDW methods in online VR applications.  © 1995-2012 IEEE.",fairness; multi-user; online VR; Redirected walking,Abstract,TRUE,
Scopus,journalPaper,2024,Adaptive Complementary Filter for Hybrid Inside-Out Outside-In HMD Tracking With Smooth Transitions,TVCG - Transactions on Visualization and Computer Graphics,A,"Head-mounted displays (HMDs) in room-scale virtual reality are usually tracked using inside-out visual SLAM algorithms. Alternatively, to track the motion of the HMD with respect to a fixed real-world reference frame, an outside-in instrumentation like a motion capture system can be adopted. However, outside-in tracking systems may temporarily lose tracking as they suffer by occlusion and blind spots. A possible solution is to adopt a hybrid approach where the inside-out tracker of the HMD is augmented with an outside-in sensing system. On the other hand, when the tracking signal of the outside-in system is recovered after a loss of tracking the transition from inside-out tracking to hybrid tracking may generate a discontinuity, i.e a sudden change of the virtual viewpoint, that can be uncomfortable for the user. Therefore, hybrid tracking solutions for HMDs require advanced sensor fusion algorithms to obtain a smooth transition. This work proposes a method for hybrid tracking of a HMD with smooth transitions based on an adaptive complementary filter. The proposed approach can be configured with several parameters that determine a trade-off between user experience and tracking error. A user study was carried out in a room-scale virtual reality environment, where users carried out two different tasks while multiple signal tracking losses of the outside-in sensor system occurred. The results show that the proposed approach improves user experience compared to a standard Extended Kalman Filter, and that tracking error is lower compared to a state-of-the-art complementary filter when configured for the same quality of user experience.  © 1995-2012 IEEE.",Head mounted displays; room-scale virtual reality; sensor fusion; tracking technologies,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Fumos: Neural Compression and Progressive Refinement for Continuous Point Cloud Video Streaming,TVCG - Transactions on Visualization and Computer Graphics,A,"Point cloud video (PCV) offers watching experiences in photorealistic 3D scenes with six-degree-of-freedom (6-DoF), enabling a variety of VR and AR applications. The user's Field of View (FoV) is more fickle with 6-DoF movement than 3-DoF movement in 360-degree video. PCV streaming is extremely bandwidth-intensive. However, current streaming systems require hundreds of Mbps bandwidth, exceeding the bandwidth capabilities of commodity devices. To save bandwidth, FoV-adaptive streaming predicts a user's FoV and only downloads point cloud data falling in the predicted FoV. But it is difficult to accurately predict the user's FoV even 2a3 seconds before playback due to 6-DoF. Misprediction of FoV or network bandwidth dips results in frequent stalls. To avoid rebuffering, existing systems would cause incomplete FoV and degraded experience, deteriorating the user's quality of experience (QoE). In this paper, we describe Fumos, a novel system that preserves interactive experience by avoiding playback stalls while maintaining high perceptual quality and high compression rate. We find a research gap in inter-frame redundant utilization and progressive mechaism. Fumos has three crucial designs, including (1) Neural compression framework with inter-frame coding, namely N-PCC, which achieves both bandwidth efficiency and high fidelity. (2) Progressive refinement streaming framework that enables continuous playback by incrementally upgrading a fetched portion to a higher quality (3) System-level adaptation that employs Lyapunov optimization to jointly optimize the long-term user QoE. Experimental results demonstrate that Fumos significantly outperforms Draco, achieving an average decoding rate acceleration of over 260Ã- . Moreover, the proposed compression framework N-PCC attains remarkable BD-Rate gains, averaging 91.7% and 51.7% against the state-of-the-art point cloud compression methods G-PCC and V-PCC, respectively.  © 1995-2012 IEEE.",Deep learning; Point cloud compression; Streaming media; User Experience; Virtual Reality (VR),Keywords,TRUE,
Scopus,journalPaper,2024,MusiKeys: Exploring Haptic-to-Auditory Sensory Substitution to Improve Mid-Air Text-Entry,TVCG - Transactions on Visualization and Computer Graphics,A,"Physical QWERTY keyboards are the current standard for performing precision text-entry with extended reality devices. Ideally, there would exist a comparable, self-contained solution that works anywhere, without requiring external keyboards. Unfortunately, when physical keyboards are recreated virtually, we currently lose critical haptic feedback information from the sense of touch, which impedes typing. In this paper, we introduce the MusiKeys Technique, which uses auditory feedback in virtual reality to communicate missing haptic feedback information typists normally receive when using a physical keyboard. To examine this concept, we conducted a user study with 24 participants which encompassed four mid-air virtual keyboards augmented with increasing amounts of feedback information, along with a fifth physical keyboard for reference. Results suggest that providing clicking feedback on key-press and key-release improves typing performance compared to not providing auditory feedback, which is consistent with the literature. We also found that audio can serve as a substitute for information contained in haptic feedback, in that users can accurately perceive the presented information. However, under our specific study conditions, this awareness of the feedback information did not yield significant differences in typing performance. Our results suggest this kind of feedback replacement can be perceived by users but needs more research to tune and improve the specific techniques.  © 1995-2012 IEEE.",Extended reality; Human-computer interaction (HCI); Mid-air text-entry; Sensory substitution; Spatial computing; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,ProtoColVR: Requirements Gathering and Collaborative Rapid Prototyping of VR Training Simulators for Multidisciplinary Teams,TVCG - Transactions on Visualization and Computer Graphics,A,"We present ProtoColVR, a methodology and a plugin designed for gathering requirements and collaborative rapid prototyping of virtual reality training simulators. Our methodology outlines the utilization of current technologies, the involvement of stakeholders during design and development, and the implementation of simulator creation through multiple iterations. We incorporate open-source tools and freely available environments like Twine and Unity to establish a reference implementation for requirements gathering and rapid prototyping. ProtoColVR is the outcome of our collaboration with a hospital and our Navy, and it has undergone testing in a development Jam. From these tests, we have gained valuable insights, including the ability to create functional prototypes within multidisciplinary teams, enhance communication among different roles, and streamline requirements gathering while improving our understanding of the virtualized environment.  © 1995-2012 IEEE.",Methodology; Multidisciplinary; Prototyping; Simulation; Training; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Human Factors at Play: Understanding the Impact of Conditioning on Presence and Reaction Time in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"A prerequisite to improving the presence of a user in mixed reality (MR) is the ability to measure and quantify presence. Traditionally, subjective questionnaires have been used to assess the level of presence. However, recent studies have shown that presence is correlated with objective and systemic human performance measures such as reaction time. These studies analyze the correlation between presence and reaction time when technical factors such as object realism and plausibility of the object's behavior change. However, additional psychological and physiological human factors can also impact presence. It is unclear if presence can be mapped to and correlated with reaction time when human factors such as conditioning are involved. To answer this question, we conducted an exploratory study ($N=60$) where the relationship between presence and reaction time was assessed under three different conditioning scenarios: control, positive, and negative. We demonstrated that human factors impact presence. We found that presence scores and reaction times are significantly correlated (correlation coefficient of 0.64), suggesting that the impact of human factors on reaction time correlates with its effect on presence. In demonstrating that, our study takes another important step toward using objective and systemic measures like reaction time as a presence measure. © 1995-2012 IEEE.",Conditioning; Mixed Reality; Presence,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Field of View Restriction and Snap Turning as Cybersickness Mitigation Tools,TVCG - Transactions on Visualization and Computer Graphics,A,"Multiple tools are available to reduce cybersickness (sickness caused by virtual reality), but past research has not investigated the combined effects of multiple mitigation tools. Field of view (FOV) restriction limits peripheral vision during self-motion, and ample evidence supports its effectiveness for reducing cybersickness. Snap turning involves discrete rotations of the user's perspective without presenting intermediate views, although reports on its effectiveness at reducing cybersickness are limited and equivocal. Both mitigation tools reduce the visual motion that can cause cybersickness. The current study (N = 201) investigated the individual and combined effects of FOV restriction and snap turning on cybersickness when playing a consumer virtual reality game. FOV restriction and snap turning in isolation reduced cybersickness compared to a control condition without mitigation tools. Yet, the combination of FOV restriction and snap turning did not further reduce cybersickness beyond the individual tools in isolation, and in some cases the combination of tools led to cybersickness similar to that in the no mitigation control. These results indicate that caution is warranted when combining multiple cybersickness mitigation tools, which can interact in unexpected ways.  © 1995-2012 IEEE.",Cybersickness; field of view restriction; motion sickness; snap turning; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Examining Effects of Technique Awareness on the Detection of Remapped Hands in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Input remapping techniques have been widely explored to allow users in virtual reality to exceed both their own physical abilities, the limitations of physical space, or to facilitate interactions with real-world objects. Often considered is how these techniques can be applied to achieve maximum utility, but still be undetectable to users to maintain a sense of immersion and presence. Existing psychophysical methods used to determine these detection thresholds have known limitations: they are highly conservative lower bounds for detection and do not account for complex usage of the technique. Our work describes and evaluates a method for estimating detection that reduces these limitations and yields meaningful upper bounds. We present the findings of our work where we apply this method to a well-explored hand motion scaling technique. In wholly unaware cases, we determined that users may detect their hand speed as abnormal at around 3.37 times the normal speed, compared to a scale factor of 1.47 that was estimated using traditional methods when users knew the motion scaling was occurring. A considerable number of participants in unaware cases (12 of 56) never detected their hand speed increasing at all, even at the maximum scale factor of 5.0. The study demonstrates just how conservative the thresholds generated by traditional psychophysical methods can be compared to detection during naive usage, and our method can be modified and applied easily to other techniques. © 1995-2012 IEEE.",human perception; interaction techniques; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,A Sense of Urgency on the Sense of Agency: Challenges in Evaluating Agency and Embodiment in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Control over an avatar in virtual reality can improve one's perceived sense of agency and embodiment towards their avatar. Yet, the relationship between control on agency and embodiment remains unclear. This work aims to investigate two main ideas: (1) the effectiveness of currently used metrics in measuring agency and embodiment and (2) the relationship between different levels of control on agency, embodiment, and cognitive performance. To do this, we conducted a between-participants user study with three conditions on agency (n=57). Participants embodied an avatar with one of three types of control (i.e., Low - control over head only, Medium - control over head and torso, or High - control over head, torso, and arms) and completed a Stroop test. Our results indicate that the degree of control afforded to participants impacted their embodiment and cognitive performance but, as expected, could not be detected in the self-reported agency scores. Furthermore, our results elucidated further insights into the relationship between control and embodiment, suggesting potential uncanny valley-like effects. Future work should aim to refine agency measures to better capture the effect of differing levels of control and consider other methodologies to measure agency. © 1995-2012 IEEE.",,Title_Abstract,TRUE,
Scopus,journalPaper,2024,SmoothRide: A Versatile Solution to Combat Cybersickness in Elevation-Altering Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness continues to bar many individuals from taking full advantage of virtual reality (VR) technology. Previous work has established that navigating virtual terrain with elevation changes poses a significant risk in this regard. In this paper, we investigate the effectiveness of three cybersickness reduction strategies on users performing a navigation task across virtual elevation-altering terrain. These strategies include static field of view (FOV) reduction, a flat surface approach that disables terrain collision and maintains constant elevation for users, and SmoothRide, a novel technique designed to dampen a user's perception of vertical motion as they travel. To assess the impact of these strategies, we conducted a within-subjects study involving 61 participants. Each strategy was compared against a control condition, where users navigated across terrain without any cybersickness reduction measures in place. Cybersickness data were collected using the Fast Motion Sickness Scale (FMS) and Simulator Sickness Questionnaire (SSQ), along with galvanic skin response (GSR) data. We measured user presence using the IGroup Presence questionnaire (IPQ) and a Single-Item Presence Scale (SIP). Our findings reveal that users experienced significantly lower levels of cybersickness using SmoothRide or FOV reduction. Presence scores reported on the IPQ were statistically similar between SmoothRide and the control condition. Conversely, terrain flattening had adverse effects on user presence scores, and we could not identify a significant effect on cybersickness compared to the control. We demonstrate that SmoothRide is an effective, lightweight, configurable, and easy-to-integrate tool for reducing cybersickness in simulations featuring elevation-altering terrain. © 1995-2012 IEEE.",Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI; Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,HOIMotion: Forecasting Human Motion during Human-Object Interactions Using Egocentric 3D Object Bounding Boxes,TVCG - Transactions on Visualization and Computer Graphics,A,"We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information. © 1995-2012 IEEE.",augmented reality; graph convolutional network; Human motion forecasting; human-object interaction,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Scene-aware Foveated Rendering,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose a new scene-aware foveated rendering method, which incorporates the scene awareness and characteristics of the human visual system into the mapping-based foveated rendering framework. First, we generate the conservative visual importance map that encodes the visual features of the scene, visual acuity, and gaze motion. Second, we construct the pixel size control map using a convolution kernel method. Third, we utilize the pixel size control map to guide the foveated rendering. At last, a temporal coherent refinement strategy is used to maintain the smooth foveated rendering for the adjacent frames. Compared to the state-of-the-art mapping-based foveated rendering methods using the same compression ratio, our method achieves smaller MSE, higher PSNR, and SSIM in the fovea, periphery, salient regions, and the whole image. We also conducted user studies, and the results proved that the perceptual quality of our method has a high visual similarity with the around truth rendered with the full resolution. © 1995-2012 IEEE.",Foveated rendering; Pixel size control; Virtual reality; Visual importance,Keywords,TRUE,
Scopus,journalPaper,2024,Efficient and Accurate Semi-automatic Neuron Tracing with Extended Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Neuron tracing, alternately referred to as neuron reconstruction, is the procedure for extracting the digital representation of the three-dimensional neuronal morphology from stacks of microscopic images. Achieving accurate neuron tracing is critical for profiling the neuroanatomical structure at single-cell level and analyzing the neuronal circuits and projections at whole-brain scale. However, the process often demands substantial human involvement and represents a nontrivial task. Conventional solutions towards neuron tracing often contend with challenges such as non-intuitive user interactions, suboptimal data generation throughput, and ambiguous visualization. In this paper, we introduce a novel method that leverages the power of extended reality (XR) for intuitive and progressive semi-automatic neuron tracing in real time. In our method, we have defined a set of interactors for controllable and efficient interactions for neuron tracing in an immersive environment. We have also developed a GPU-accelerated automatic tracing algorithm that can generate updated neuron reconstruction in real time. In addition, we have built a visualizer for fast and improved visual experience, particularly when working with both volumetric images and 3D objects. Our method has been successfully implemented with one virtual reality (VR) headset and one augmented reality (AR) headset with satisfying results achieved. We also conducted two user studies and proved the effectiveness of the interactors and the efficiency of our method in comparison with other approaches for neuron tracing. © 2024 IEEE.",Extended Reality; Eye Tracking; Human-centered Computing; Neuron Tracing; Visualization,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,The Effects of Depth of Knowledge of a Virtual Agent,TVCG - Transactions on Visualization and Computer Graphics,A,"We explored the impact of depth of knowledge on conversational agents and human perceptions in a virtual reality (VR) environment. We designed experimental conditions with low, medium, and high depths of knowledge in the domain of game development and tested them among 27 game development students. We aimed to understand how the agent's predefined knowledge levels affected the participants' perceptions of the agent and its knowledge. Our findings showed that participants could distinguish between different knowledge levels of the virtual agent. Moreover, the agent's depth of knowledge significantly impacted participants' perceptions of intelligence, rapport, factuality, the uncanny valley effect, anthropomorphism, and willingness for future interaction. We also found strong correlations between perceived knowledge, perceived intelligence, factuality, and willingness for future interactions. We developed design guidelines for creating conversational agents from our data and observations. This study contributes to the human-agent interaction field in VR settings by providing empirical evidence on the importance of tailoring virtual agents' depth of knowledge to improve user experience, offering insights into designing more engaging and effective conversational agents. © 1995-2012 IEEE.",conversational AI; depth of knowledge; knowledge bank; prompt engineering; virtual agent; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Precise Tool to Target Positioning Widgets (TOTTA) in Spatial Environments: A Systematic Review,TVCG - Transactions on Visualization and Computer Graphics,A,"TOTTA outlines the spatial position and rotation guidance of a real/virtual tool (TO) towards a real/virtual target (TA), which is a key task in Mixed reality applications. The task error can have critical consequences regarding safety, performance, and quality, such as surgical implantology or industrial maintenance scenarios. The TOTTA problem lacks a dedicated study and it is scattered in different domains with isolated designs. This work contributes to a systematic review of the TOTTA visual widgets, studying 70 unique designs from 24 papers. TOTTA is commonly guided by the visual overlap -an intuitive, pre-attentive ""collimation""feedback- of simple shaped widgets: Box, 3D Axes, 3D Model, 2D Crosshair, Globe, Tetrahedron, Line, Plane. Our research discovers that TO and TA are often represented with the same shape. They are distinguished by topological elements (e.g. edges/vertices/faces), colors, transparency levels, and added. shapes, widget quantity, and size. Meanwhile some designs provide continuous ""during manipulation feedback""relative to the distance between TO and TA by text, dynamic color, sonification, and amplified graphical visualization. Some approaches trigger discrete ""TA reached feedback""such as color alteration, added sound, TA shape change, and added text. We found the lack of golden standards, including in testing procedures, as current ones are limited to partial sets with different and incomparable setups (different target configurations, avatar, background, etc.). We also found a bias in participants: right-handed, young male, non-color impaired. © 1995-2012 IEEE.",3D positioning; 3D user interface; tool to target manipulation; Virtual environments; widgets,Abstract,TRUE,
Scopus,journalPaper,2024,Frankenstein's Monster in the Metaverse: User Interaction with Customized Virtual Agents,TVCG - Transactions on Visualization and Computer Graphics,A,"Enabled by the latest achievements in artificial intelligence (AI), computer graphics as well as virtual, augmented, and mixed reality (VR/AR/MR), virtual agents are increasingly resembling humans in both their appearance and intelligent behavior. This results in enormous potential for agents to support users in their daily lives, for example in customer service, healthcare, education or the envisioned all-encompassing metaverse. Today's technology would allow users to customize their conversation partners in the metaverse - as opposed to reality - according to their preferences, potentially improving the user experience. On the other hand, there is little research on how reshaping the head of a communication partner might affect the immediate interaction with them. In this paper, we investigate the user requirements for and the effects of agent customization. In a two-stage user study ($N=30$), we collected both self-reported evaluations (e.g., intrinsic motivation) and interaction metrics (e.g., interaction duration and number of tried out items) for the process of agent customization itself as well as data on how users perceived the subsequent human-agent interaction in VR. Our results indicate that users only wish to have full customization for agents in their personal social circle, while for general services, a selection or even a definite assignment of pre-configured agents is sufficient. When customization is offered, attributes such as gender, clothing or hair are subjectively more relevant to users than facial features such as skin or eye color. Although the customization of human interaction partners is beyond our control, customization of virtual agents significantly increases perceived social presence as well as rapport and trust. Further findings on user motivation and agent diversity are discussed in the paper. © 1995-2012 IEEE.",Intelligent virtual agents; personalization; user study,Title_Abstract,TRUE,
Scopus,journalPaper,2024,NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene Understanding,TVCG - Transactions on Visualization and Computer Graphics,A,"In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM). However, a notable gap exists in the existing approaches when it comes to scene understanding. In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations. Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations. Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning. Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking. Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches. Finally, we also show that our approach can be used in augmented reality applications. Project page: https://zju3dv.github.io/nis_slam. © 1995-2012 IEEE.",Implicit representation; Neural dense SLAM; Scene understanding; Semantic segmentation,Abstract,TRUE,
Scopus,journalPaper,2024,Searching Across Realities: Investigating ERPs and Eye-Tracking Correlates of Visual Search in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Mixed Reality allows us to integrate virtual and physical content into users' environments seamlessly. Yet, how this fusion affects perceptual and cognitive resources and our ability to find virtual or physical objects remains uncertain. Displaying virtual and physical information simultaneously might lead to divided attention and increased visual complexity, impacting users' visual processing, performance, and workload. In a visual search task, we asked participants to locate virtual and physical objects in Augmented Reality and Augmented Virtuality to understand the effects on performance. We evaluated search efficiency and attention allocation for virtual and physical objects using event-related potentials, fixation and saccade metrics, and behavioral measures. We found that users were more efficient in identifying objects in Augmented Virtuality, while virtual objects gained saliency in Augmented Virtuality. This suggests that visual fidelity might increase the perceptual load of the scene. Reduced amplitude in distractor positivity ERP, and fixation patterns supported improved distractor suppression and search efficiency in Augmented Virtuality. We discuss design implications for mixed reality adaptive systems based on physiological inputs for interaction. © 1995-2012 IEEE.",Augmented Reality; Augmented Virtuality; EEG; Event-Related Potentials; Eye Tracking; Mixed Reality; Visual Search,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Expressive 3D Facial Animation Generation Based on Local-to-Global Latent Diffusion,TVCG - Transactions on Visualization and Computer Graphics,A,"3D Facial animations, crucial to augmented and mixed reality digital media, have evolved from mere aesthetic elements to potent storytelling media. Despite considerable progress in facial animation of neutral emotions, existing methods still struggle to capture the authenticity of emotions. This paper introduces a novel approach to capture fine facial expressions and generate facial animations using audio synchronization. Our method consists of two key components: First, the Local-to-global Latent Diffusion Model (LG-LDM) tailored for authentic facial expressions, which can integrate audio, time step, facial expressions, and other conditions towards possible encoding of emotionally rich yet latent features in response to possibly noisy raw audio signals. The core of LG-LDM is our carefully designed Facial Denoiser Model (FDM) for aligning the local-to-global animation feature with audio. Second, we redesign an Emotion-centric Vector Quantized-Variational AutoEncoder framework (EVQ-VAE) to finely decode the subtle differences under different emotions and reconstruct the final 3D facial geometry. Our work significantly contributes to the key challenges of emotionally realistic 3D facial animation for audio synchronization and enhances the immersive experience and emotional depth in augmented and mixed reality applications. We provide a reproducibility kit including our code, dataset, and detailed instructions for running the experiments. This kit is available at https://github.com/wangxuanx/Face-Diffusion-Model. © 1995-2012 IEEE.",3D facial animation; diffusion model; speech-driven,Abstract,TRUE,
Scopus,journalPaper,2024,Measuring and Predicting Multisensory Reaction Latency: A Probabilistic Model for Visual-Auditory Integration,TVCG - Transactions on Visualization and Computer Graphics,A,"— Virtual/augmented reality (VR/AR) devices offer both immersive imagery and sound. With those wide-field cues, we can simultaneously acquire and process visual and auditory signals to quickly identify objects, make decisions, and take action. While vision often takes precedence in perception, our visual sensitivity degrades in the periphery. In contrast, auditory sensitivity can exhibit an opposite trend due to the elevated interaural time difference. What occurs when these senses are simultaneously integrated, as is common in VR applications such as 360◦ video watching and immersive gaming? We present a computational and probabilistic model to predict VR users’ reaction latency to visual-auditory multisensory targets. To this aim, we first conducted a psychophysical experiment in VR to measure the reaction latency by tracking the onset of eye movements. Experiments with numerical metrics and user studies with naturalistic scenarios showcase the model’s accuracy and generalizability. Lastly, we discuss the potential applications, such as measuring the sufficiency of target appearance duration in immersive video playback, and suggesting the optimal spatial layouts for AR interface design. © 2024 IEEE.",augmented reality; human perception; reaction latency; Virtual reality; visual-audio,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Analysis and Design of Efficient Authentication Techniques for Password Entry with the Qwerty Keyboard for VR Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Authentication in digital security relies heavily on text-based passwords, even with other available methods like biometrics and graphical passwords. While virtual reality (VR) keyboards are typically invisible to onlookers, the presence of inconspicuous sensors, including accelerometers, gyroscopes, and barometers, poses a potential risk of unauthorized observation and recording. Traditional defense shoulder-surfing attack methods typically involve breaking apart the Qwerty layout, which destroys the user's inherent familiarity with the layout. This research addresses the need for secure password entry in VR environments while retaining the Qwerty layout. We explore three keyboard-related position alteration strategies to ensure security while mitigating the decline in user experience. These strategies involve moving the entire keyboard, cursor, and keys. Our theoretical study assesses the effectiveness of these strategies against shoulder-surfing attacks. Two user studies, employing ray-based and position-based text entry methods, respectively, evaluate the practical effectiveness of the three strategies in resisting shoulder-surfing attacks, as well as their impact on typing performance and user experience. Our findings demonstrate that the three strategies achieve shoulder-surfing attack resistance comparable to a random layout keyboard. Moreover, compared to a random layout, the two strategies involving the movement of the entire keyboard and the repositioning of keys support faster entry rates and enhanced user experience. © 1995-2012 IEEE.",keyboard layout; password; text entry; user study; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,HaptoFloater: Visuo-Haptic Augmented Reality by Embedding Imperceptible Color Vibration Signals for Tactile Display Control in a Mid-Air Image,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose HaptoFloater, a low-latency mid-air visuo-haptic augmented reality (VHAR) system that utilizes imperceptible color vibrations. When adding tactile stimuli to the visual information of a mid-air image, the user should not perceive the latency between the tactile and visual information. However, conventional tactile presentation methods for mid-air images, based on camera-detected fingertip positioning, introduce latency due to image processing and communication. To mitigate this latency, we use a color vibration technique; humans cannot perceive the vibration when the display alternates between two different color stimuli at a frequency of 25 Hz or higher. In our system, we embed this imperceptible color vibration into the mid-air image formed by a micromirror array plate, and a photodiode on the fingertip device directly detects this color vibration to provide tactile stimulation. Thus, our system allows for the tactile perception of multiple patterns on a mid-air image in 59.5 ms. In addition, we evaluate the visual-haptic delay tolerance on a mid-air display using our VHAR system and a tactile actuator with a single pattern and faster response time. The results of our user study indicate a visual-haptic delay tolerance of 110.6 ms, which is considerably larger than the latency associated with systems using multiple tactile patterns. © 1995-2012 IEEE.",imperceptible color vibration; LCD displays; mid-air images; Visuo-haptic displays,Title_Abstract,TRUE,
Scopus,journalPaper,2024,A Framework for Multimodal Medical Image Interaction,TVCG - Transactions on Visualization and Computer Graphics,A,"Medical doctors rely on images of the human anatomy, such as magnetic resonance imaging (MRI), to localize regions of interest in the patient during diagnosis and treatment. Despite advances in medical imaging technology, the information conveyance remains unimodal. This visual representation fails to capture the complexity of the real, multisensory interaction with human tissue. However, perceiving multimodal information about the patient's anatomy and disease in real-time is critical for the success of medical procedures and patient outcome. We introduce a Multimodal Medical Image Interaction (MMII) framework to allow medical experts a dynamic, audiovisual interaction with human tissue in three-dimensional space. In a virtual reality environment, the user receives physically informed audiovisual feedback to improve the spatial perception of anatomical structures. MMII uses a model-based sonification approach to generate sounds derived from the geometry and physical properties of tissue, thereby eliminating the need for hand-crafted sound design. Two user studies involving 34 general and nine clinical experts were conducted to evaluate the proposed interaction framework's learnability, usability, and accuracy. Our results showed excellent learnability of audiovisual correspondence as the rate of correct associations significantly improved ($p < 0.001$) over the course of the study. MMII resulted in superior brain tumor localization accuracy ($p < 0.05$) compared to conventional medical image interaction. Our findings substantiate the potential of this novel framework to enhance interaction with medical images, for example, during surgical procedures where immediate and precise feedback is needed. © 1995-2012 IEEE.",Audiovisual feedback; Augmented reality; Brain surgery; Brain tumor; HCI; Human-centered design; Human-computer interaction; Medical image interaction; Medical images; Multimodal interaction; Physical modeling synthesis; Sonification; Surgical navigation; Tumor localization; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Robust Collaborative Visual-Inertial SLAM for Mobile Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Achieving precise real-time localization and ensuring robustness are critical challenges in multi-user mobile AR applications. Leveraging collaborative information to augment tracking accuracy on lightweight devices and fortify overall system robustness emerges as a crucial necessity. In this paper, we propose a robust centralized collaborative multi-agent VI-SLAM system for mobile AR interaction and server-side efficient consistent mapping. The system deploys a lightweight VIO frontend on mobile devices for real-time tracking, and a backend running on a remote server to update multiple submaps. When overlapping areas between submaps across agents are detected, the system performs submap fusion to establish a globally consistent map. Additionally, we propose a map registration and fusion strategy based on covisibility areas for online registration and fusion in multi-agent scenarios. To improve the tracking accuracy of the frontend on agent, we introduce a strategy for updating the global map to the local map at a moderate frequency between the camera-rate pose estimation of the frontend VIO and the low-frequency global map optimization, using a tightly coupled strategy to achieve consistency of the multi-agent frontend poses estimation in the global map. The effectiveness of the proposed method is further confirmed by executing backend mapping on the server and deploying VIO frontends on multiple mobile devices for AR demostration. Additionally, we discuss the scalability of the proposed system by analyzing network traffic, synchronization frequency, and other factors at both the agent and server ends. © 2024 IEEE.",Collaborative; Map fusion; SLAM; Tightly coupled; VIO,Title,TRUE,
Scopus,journalPaper,2024,"Investigating Object Translation in Room-scale, Handheld Virtual Reality",TVCG - Transactions on Visualization and Computer Graphics,A,"— Handheld devices have become an inclusive alternative to head-mounted displays in virtual reality (VR) environments, enhancing accessibility and allowing cross-device collaboration. Object manipulation techniques in 3D space with handheld devices, such as those in handheld augmented reality (AR), have been typically evaluated in table-top scale and we currently lack an understanding of how these techniques perform in larger scale environments. We conducted two studies, each with 30 participants, to investigate how different techniques impact usability and performance for room-scale handheld VR object translations. We compared three translation techniques that are similar to commonly studied techniques in handheld AR: 3DSlide, VirtualGrasp, and Joystick. We also examined the effects of target size, target distance, and user mobility conditions (stationary vs. moving). Results indicated that the Joystick technique, which allowed translation in relation to the user’s perspective, was the fastest and most preferred, without difference in precision. Our findings provide insights for designing room-scale handheld VR systems, with potential implications for mixed reality systems involving handheld devices. © 2024 IEEE.",3D manipulation; Augmented Reality; Handheld VR; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Gesture2Text: A Generalizable Decoder for Word-Gesture Keyboards in XR Through Trajectory Coarse Discretization and Pre-Training,TVCG - Transactions on Visualization and Computer Graphics,A,"Text entry with word-gesture keyboards (WGK) is emerging as a popular method and becoming a key interaction for Extended Reality (XR). However, the diversity of interaction modes, keyboard sizes, and visual feedback in these environments introduces divergent word-gesture trajectory data patterns, thus leading to complexity in decoding trajectories into text. Template-matching decoding methods, such as SHARK2 [32], are commonly used for these WGK systems because they are easy to implement and configure. However, these methods are susceptible to decoding inaccuracies for noisy trajectories. While conventional neural-network-based decoders (neural decoders) trained on word-gesture trajectory data have been proposed to improve accuracy, they have their own limitations: they require extensive data for training and deep-learning expertise for implementation. To address these challenges, we propose a novel solution that combines ease of implementation with high decoding accuracy: a generalizable neural decoder enabled by pre-training on large-scale coarsely discretized word-gesture trajectories. This approach produces a ready-to-use WGK decoder that is generalizable across mid-air and on-surface WGK systems in augmented reality (AR) and virtual reality (VR), which is evident by a robust average Top-4 accuracy of 90.4% on four diverse datasets. It significantly outperforms SHARK2 with a 37.2% enhancement and surpasses the conventional neural decoder by 7.4%. Moreover, the Pre-trained Neural Decoder's size is only 4 MB after quantization, without sacrificing accuracy, and it can operate in real-time, executing in just 97 milliseconds on Quest 3. © 1995-2012 IEEE.",discretization; Pre-trained models; text entry; word-gesture keyboard,Abstract,TRUE,
Scopus,journalPaper,2024,Filtering on the Go: Effect of Filters on Gaze Pointing Accuracy during Physical Locomotion in Extended Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Eye tracking filters have been shown to improve accuracy of gaze estimation and input for stationary settings. However, their effectiveness during physical movement remains underexplored. In this work, we compare common online filters in the context of physical locomotion in extended reality and propose alterations to improve them for on-the-go settings. We conducted a computational experiment where we simulate performance of the online filters using data on participants attending visual targets located in world-, path-, and two head-based reference frames while standing, walking, and jogging. Our results provide insights into the filters' effectiveness and factors that affect it, such as the amount of noise caused by locomotion and differences in compensatory eye movements, and demonstrate that filters with saccade detection prove most useful for on-the-go settings. We discuss the implications of our findings and conclude with guidance on gaze data filtering for interaction in extended reality. © 1995-2012 IEEE.",extended reality; eye tracking; gaze filters; gaze-based pointing; physical locomotion; spatial reference frames,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Should I Evaluate my Augmented Reality System in an Industrial Environment? Investigating the Effects of Classroom and Shop Floor Settings on Guided Assembly,TVCG - Transactions on Visualization and Computer Graphics,A,"—Numerous prior studies have investigated real-time assembly instructions using Augmented Reality (AR). However, most such experiments were conducted in laboratory settings with simplistic assembly tasks, failing to represent real-world industrial conditions. To ascertain to what extent results obtained in a laboratory environment may differ from studies in actual industrial environments, we carried out a user study with 32 manufacturing apprentices. We compared assembly task execution results in two settings, a classroom and an industrial workshop environment. To facilitate the experiments, we developed AR-guided manual assembly systems for simple and more complex assets. Our findings reveal a significantly improved task performance in the industrial workshop, reflected in faster task completion times, fewer errors, and subjectively perceived higher flow. This contradicted participants’ subjective ratings, as they expected to perform better in the classroom environment. Our results suggest that the actual manufacturing environment is critical in evaluating AR systems for real-world industrial applications. © 2024 IEEE Computer Society. All rights reserved.",Augmented reality; augmented reality guidelines; classroom setting; manual assembly; shop floor setting; user study,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,RingGesture: A Ring-Based Mid-Air Gesture Typing System Powered by a Deep-Learning Word Prediction Framework,TVCG - Transactions on Visualization and Computer Graphics,A,"—Text entry is a critical capability for any modern computing experience, with lightweight augmented reality (AR) glasses being no exception. Designed for all-day wearability, a limitation of lightweight AR glass is the restriction to the inclusion of multiple cameras for extensive field of view in hand tracking. This constraint underscores the need for an additional input device. We propose a system to address this gap: a ring-based mid-air gesture typing technique, RingGesture, utilizing electrodes to mark the start and end of gesture trajectories and inertial measurement units (IMU) sensors for hand tracking. This method offers an intuitive experience similar to raycast-based mid-air gesture typing found in VR headsets, allowing for a seamless translation of hand movements into cursor navigation. To enhance both accuracy and input speed, we propose a novel deep-learning word prediction framework, Score Fusion, comprised of three key components: a) a word-gesture decoding model, b) a spatial spelling correction model, and c) a lightweight contextual language model. In contrast, this framework fuses the scores from the three models to predict the most likely words with higher precision. We conduct comparative and longitudinal studies to demonstrate two key findings: firstly, the overall effectiveness of RingGesture, which achieves an average text entry speed of 27.3 words per minute (WPM) and a peak performance of 47.9 WPM. Secondly, we highlight the superior performance of the Score Fusion framework, which offers a 28.2% improvement in uncorrected Character Error Rate over a conventional word prediction framework, Naive Correction, leading to a 55.2% improvement in text entry speed for RingGesture. Additionally, RingGesture received a System Usability Score of 83 signifying its excellent usability. © 2024 IEEE.",augmented reality; language models; Text entry; word prediction,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Evaluating Force-based Haptics for Immersive Tangible Interactions with Surface Visualizations,TVCG - Transactions on Visualization and Computer Graphics,A,"Haptic feedback provides an essential sensory stimulus crucial for interaction and analyzing three-dimensional spatio-temporal phenomena on surface visualizations. Given its ability to provide enhanced spatial perception and scene maneuverability, virtual reality (VR) catalyzes haptic interactions on surface visualizations. Various interaction modes, encompassing both mid-air and on-surface interactions - with or without the application of assisting force stimuli - have been explored using haptic force feedback devices. In this paper, we evaluate the use of on-surface and assisted on-surface haptic modes of interaction compared to a no-haptic interaction mode. A force-based haptic stylus is used for all three modalities; the on-surface mode uses collision based forces, whereas the assisted on-surface mode is accompanied by an additional snapping force. We conducted a within-subjects user study involving fundamental interaction tasks performed on surface visualizations. Keeping a consistent visual design across all three modes, our study incorporates tasks that require the localization of the highest, lowest, and random points on surfaces; and tasks that focus on brushing curves on surfaces with varying complexity and occlusion levels. Our findings show that participants took almost the same time to brush curves using all the interaction modes. They could draw smoother curves using the on-surface interaction modes compared to the no-haptic mode. However, the assisted on-surface mode provided better accuracy than the on-surface mode. The on-surface mode was slower in point localization, but the accuracy depended on the visual cues and occlusions associated with the tasks. Finally, we discuss participant feedback on using haptic force feedback as a tangible input modality and share takeaways to aid the design of haptics-based tangible interactions for surface visualizations. © 1995-2012 IEEE.",AR/VR/Immersive; Computer Graphics Techniques; Domain Agnostic; Guidelines; Human-Subjects Quantitative Studies; Interaction Design; Isosurface Techniques; Scalar Field Data; Specialized Input/Display Hardware,Abstract,TRUE,
Scopus,journalPaper,2024,MobiTangibles: Enabling Physical Manipulation Experiences of Virtual Precision Hand-Held Tools' Miniature Control in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"Realistic simulation for miniature control interactions, typically identified by precise and confined motions, commonly found in precision hand-held tools, like calipers, powered engravers, retractable knives, etc., are beneficial for skill training associated with these kinds of tools in virtual reality (VR) environments. However, existing approaches aiming to simulate hand-held tools' miniature control manipulation experiences in VR entail prototyping complexity and require expertise, posing challenges for novice users and individuals with limited resources. Addressing this challenge, we introduce MobiTangibles-proxies for precision hand-held tools' miniature control interactions utilizing smartphone-based magnetic field sensing. MobiTangibles passively replicate fundamental miniature control experiences associated with hand-held tools, such as single-axis translation and rotation, enabling quick and easy use for diverse VR scenarios without requiring extensive technical knowledge. We conducted a comprehensive technical evaluation to validate the functionality of MobiTangibles across diverse settings, including evaluations for electromagnetic interference within indoor environments. In a user-centric evaluation involving 15 participants across bare hands, VR controllers, and MobiTangibles conditions, we further assessed the quality of miniaturized manipulation experiences in VR. Our findings indicate that MobiTangibles outperformed conventional methods in realism and fatigue, receiving positive feedback. © 1995-2012 IEEE.",Hand-held tool; Miniature control; Physical manipulation; Skill training; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,A Multi-aperture Coaxial Projector Balancing Shadow Suppression and Deblurring,TVCG - Transactions on Visualization and Computer Graphics,A,"This paper proposes a projection system that optically removes the cast shadow in projection mapping. Specifically, we realize the large-aperture (LA) projection using a large-format Fresnel lens to suppress cast shadows by condensing the projection light from a wide viewing angle. However, the resolution and contrast of the projected results are significantly degraded by defocus blur, veiling glare, and stray light caused by the aberration of an LA Fresnel lens. To solve the technical problems, we employ two different approaches: optical and digital image processing methods. First, we introduce a residual projector with a typical aperture lens on the same optical axis as the LA projector, projecting the residual (i.e., high-frequency) components attenuated in the LA projection. These projectors play different roles in shadow suppression and blur compensation, both achieved by projecting simultaneously. Secondly, we optimize the pair of projection images that can balance the shadow suppression and deblurring performance of our projection system. We implemented a proof-of-concept prototype and validated the above-mentioned techniques through projection experiments and a user study. © 1995-2012 IEEE.",Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality; Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers,Keywords,TRUE,
Scopus,journalPaper,2024,Exploring the Effect of Viewing Attributes of Mobile AR Interfaces on Remote Collaborative and Competitive Tasks,TVCG - Transactions on Visualization and Computer Graphics,A,"— Mobile devices have the potential to facilitate remote tasks through Augmented Reality (AR) solutions by integrating digital information into the real world. Although prior studies have explored Mobile Augmented Reality (MAR) for co-located collaboration, none have investigated the impact of various viewing attributes that can influence remote task performance, such as target object viewing angles, synchronization styles, or having a secondary small screen showing other users current view in the MAR environment. In this paper, we explore five techniques considering these attributes, specifically designed for two modes of remote tasks: collaborative and competitive. We conducted a user study employing various combinations of those attributes for both tasks. In both instances, results indicate users’ optimal performance and preference for the technique that allows asynchronous viewing of object manipulations on the small screen. Overall, this paper contributes novel techniques for remote tasks in MAR, addressing aspects such as viewing angle and synchronization in object manipulation alongside secondary small-screen interfaces. Additionally, it presents the results of a user study evaluating the effectiveness, usability, and user preference of these techniques in remote settings and offers a set of recommendations for designing and implementing MAR solutions to enhance remote activities. © 2024 IEEE.",Collaborative Task; Competitive Task; Mobile Augmented Reality; Remote Collaboration,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Multimodal Feedback Methods for Advancing the Accessibility of Immersive Virtual Reality for People With Balance Impairments Due to Multiple Sclerosis,TVCG - Transactions on Visualization and Computer Graphics,A,"Maintaining balance in immersive virtual reality (VR) environments poses a significant challenge for users, particularly affecting those with pre-existing balance disorders. This study investigates the efficacy of multimodal feedback—comprising auditory, vibrotactile, and visual stimuli—in mitigating balance issues within VR. A sample of 68 participants, divided equally between individuals with balance deficits related to multiple sclerosis and those without, was evaluated. The research explored the impact of various feedback conditions on balance performance. The results demonstrated that the multimodal feedback condition significantly enhanced balance control compared to other conditions, with statistical analysis confirming this improvement (p<.001). These findings underscore the potential of integrated sensory feedback in addressing balance-related difficulties in VR, thereby improving the overall accessibility and user experience for individuals affected by balance impairments. This research contributes valuable insights into optimizing VR environments for enhanced stability and user comfort. © 2024 IEEE.",HMDs; immersive VR; Multimodal feedback; Multiple Sclerosis,Title_Abstract,TRUE,
Scopus,journalPaper,2024,From Avatars to Agents: Self-Related Cues Through Embodiment and Personalization Affect Body Perception in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Our work investigates the influence of self-related cues in the design of virtual humans on body perception in virtual reality. In a $2\times 2$ mixed design, 64 participants faced photorealistic virtual humans either as a motion-synchronized embodied avatar or as an autonomous moving agent, appearing subsequently with a personalized and generic texture. Our results unveil that self-related cues through embodiment and personalization yield an individual and complemented increase in participants' sense of embodiment and self-identification towards the virtual human. Different body weight modification and estimation tasks further showed an impact of both factors on participants' body weight perception. Additional analyses revealed that the participant's body mass index predicted body weight estimations in all conditions and that participants' self-esteem and body shape concerns correlated with different body weight perception results. Hence, we have demonstrated the occurrence of double standards through induced self-related cues in virtual human perception, especially through embodiment. © 1995-2012 IEEE.",agency; body image; body weight perception; self-location; virtual body ownership; Virtual human,Title_Abstract,TRUE,
Scopus,journalPaper,2024,Touching the Ground: Evaluating the Effectiveness of Data Physicalizations for Spatial Data Analysis Tasks,TVCG - Transactions on Visualization and Computer Graphics,A,"Inspired by recent advances in digital fabrication, artists and scientists have demonstrated that physical data encodings (i.e., data physicalizations) can increase engagement with data, foster collaboration, and in some cases, improve data legibility and analysis relative to digital alternatives. However, prior empirical studies have only investigated abstract data encoded in physical form (e.g., laser cut bar charts) and not continuously sampled spatial data fields relevant to climate and medical science (e.g., heights, temperatures, densities, and velocities sampled on a spatial grid). This paper presents the design and results of the first study to characterize human performance in 3D spatial data analysis tasks across analogous physical and digital visualizations. Participants analyzed continuous spatial elevation data with three visualization modalities: (1) 2D digital visualization; (2) perspective-tracked, stereoscopic 'fishtank' virtual reality; and (3) 3D printed data physicalization. Their tasks included tracing paths downhill, looking up spatial locations and comparing their relative heights, and identifying and reporting the minimum and maximum heights within certain spatial regions. As hypothesized, in most cases, participants performed the tasks just as well or better in the physical modality (based on time and error metrics). Additional results include an analysis of open-ended feedback from participants and discussion of implications for further research on the value of data physicalization. All data and supplemental materials are available at https://osf.io/7xdq4/.  © 1995-2012 IEEE.",Data physicalization; evaluation; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,SpatialTouch: Exploring Spatial Data Visualizations in Cross-reality,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose and study a novel cross-reality environment that seamlessly integrates a monoscopic 2D surface (an interactive screen with touch and pen input) with a stereoscopic 3D space (an augmented reality HMD) to jointly host spatial data visualizations. This innovative approach combines the best of two conventional methods of displaying and manipulating spatial 3D data, enabling users to fluidly explore diverse visual forms using tailored interaction techniques. Providing such effective 3D data exploration techniques is pivotal for conveying its intricate spatial structures - often at multiple spatial or semantic scales - across various application domains and requiring diverse visual representations for effective visualization. To understand user reactions to our new environment, we began with an elicitation user study, in which we captured their responses and interactions. We observed that users adapted their interaction approaches based on perceived visual representations, with natural transitions in spatial awareness and actions while navigating across the physical surface. Our findings then informed the development of a design space for spatial data exploration in cross-reality. We thus developed cross-reality environments tailored to three distinct domains: for 3D molecular structure data, for 3D point cloud data, and for 3D anatomical data. In particular, we designed interaction techniques that account for the inherent features of interactions in both spaces, facilitating various forms of interaction, including mid-air gestures, touch interactions, pen interactions, and combinations thereof, to enhance the users' sense of presence and engagement. We assessed the usability of our environment with biologists, focusing on its use for domain research. In addition, we evaluated our interaction transition designs with virtual and mixed-reality experts to gather further insights. As a result, we provide our design suggestions for the cross-reality environment, emphasizing the interaction with diverse visual representations and seamless interaction transitions between 2D and 3D spaces  © 1995-2012 IEEE.",cross reality; immersive visualization; interaction techniques; Spatial data,Abstract,TRUE,
Scopus,journalPaper,2024,Collaborative Forensic Autopsy Documentation and Supervised Report Generation using a Hybrid Mixed-Reality Environment and Generative AI,TVCG - Transactions on Visualization and Computer Graphics,A,"—Forensic investigation is a complex procedure involving experts working together to establish cause of death and report findings to legal authorities. While new technologies are being developed to provide better post-mortem imaging capabilities—including mixed-reality (MR) tools to support 3D visualisation of such data—these tools do not integrate seamlessly into their existing collaborative workflow and report authoring process, requiring extra steps, e.g. to extract imagery from the MR tool and combine with physical autopsy findings for inclusion in the report. Therefore, in this work we design and evaluate a new forensic autopsy report generation workflow and present a novel documentation system using hybrid mixed-reality approaches to integrate visualisation, voice and hand interaction, as well as collaboration and procedure recording. Our preliminary findings indicate that this approach has the potential to improve data management, aid reviewability, and thus, achieve more robust standards. Further, it potentially streamlines report generation and minimise dependency on external tools and assistance, reducing autopsy time and related costs. This system also offers significant potential for education. A free copy of this paper and all supplemental materials are available at https://osf.io/ygfzx. © 2024 IEEE.",documentation; Forensic autopsy; generative AI; mixed reality; report generation,Keywords,TRUE,
Scopus,journalPaper,2024,Is Video Gaming a Cure for Cybersickness? Gamers Experience Less Cybersickness Than Non-Gamers in a VR Self-Motion Task,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness remains a major drawback of Virtual Reality (VR) headsets, as a breadth of stationary experiences with visual self-motion can result in visually-induced motion sickness. However, not everybody experiences the same intensity or type of adverse symptoms. Here we propose that prior experience with virtual environments can predict ones degree of cybersickness. Video gaming can enhance visuospatial abilities, which in-turn relate negatively to cybersickness - meaning that consistently engaging in virtual environments can result in protective habituation effects. In a controlled stationary VR experiment, we found that 'VR-naive' video gamers experienced significantly less cybersickness in a virtual tunnel-travel task and outperformed 'VR-naive' non-video gamers on a visual attention task. These findings strongly motivate the use of non-VR games for training VR cybersickness resilience, with future research needed to further understand the mechanism(s) by which gamers become cybersickness resilient - potentially expanding access to VR for even the most susceptible participants. © 1995-2012 IEEE.",Cybersickness; Habituation; Video Gamers; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Virtual Crowds Rheology: Evaluating the Effect of Character Representation on User Locomotion in Crowds,TVCG - Transactions on Visualization and Computer Graphics,A,"Crowd data is a crucial element in the modeling of collective behaviors, and opens the way to simulation for their study or prediction. Given the difficulty of acquiring such data, virtual reality is useful for simplifying experimental processes and opening up new experimental opportunities. This comes at the cost of the need to assess the biases introduced by the use of this technology. Our paper is part of this effort, and investigates the effect of the graphical representation of a crowd on the behavior of a user immersed within. More specifically, we inspect the virtual navigation through virtual crowds, in terms of travel speeds and local navigation choices as a function of the visual representation of the virtual agents that make up the crowd (simple geometric model, anthropomorphic model or realistic model). Through an experiment in which we ask a user to navigate virtual crowds of varying densities, we show that the effect of the visual representation is limited, but that an anthropomorphic representation offers the best trade-off between computational complexity and ecological validity, even though a more realistic representation can be preferred when user behaviour is studied in more details. Our work leads to clear recommendations on the design of immersive simulations for the study of crowd behavior. © 2024 IEEE. P.",Crowd; Human Interaction; Navigation; Virtual Reality; Visual Representation,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Visual Perceptual Confidence: Exploring Discrepancies between Self-reported and Actual Distance Perception in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) systems are widely used, and it is essential to know if spatial perception in virtual environments (VEs) is similar to reality. Research indicates that users tend to underestimate distances in VR. Prior work suggests that actual distance judgments in VR may not always match the users self-reported preference of where they think they most accurately estimated distances. However, no explicit investigation evaluated whether user preferences match actual performance in a spatial judgment task. We used blind walking to explore potential dissimilarities between actual distance estimates and user-selected preferences of visual complexities, VE conditions, and targets. Our findings show a gap between user preferences and actual performance when visual complexities were varied, which has implications for better visual perception understanding, VR applications design, and research in spatial perception, indicating the need to calibrate and align user preferences and true spatial perception abilities in VR. © 1995-2012 IEEE.",distance perception; spatial perception; understanding people; Virtual reality; visual complexity,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Pseudo-walking Sensation by Anteroposterior or Lateral Galvanic Vestibular Stimulation and Synchronous Foot-sole Vibrations,TVCG - Transactions on Visualization and Computer Graphics,A,"The walking sensation is a result of the synthesis of multisensory inputs from various systems. The vestibular system, typically used for detecting acceleration, is a crucial component of the walking sensation. This study investigated the use of galvanic vestibular stimulation(GVS) to enhance the sensation of walking in virtual reality (VR) environments, particularly when users are seated and not engaged in active movements. GVS is a transcutaneous electric stimulation technique to evoke vestibular sensory responses and involves the application of a penetrating current to vestibular afferents. This study revealed that the pseudo-walking sensation can be intensified by applying lateral GVS. However, no difference was observed when it was synchronized with the walking rhythm represented by foot-sole vibration patterns. Furthermore, the study compares the effectiveness of lateral versus anterior-posterior GVS in enhancing walking sensations in VR. The findings provide novel perspectives on enhancing the VR walking experience through vestibular stimulation, even in scenarios in which the user is seated. Authors",Cybersickness; Electrodes; Foot; GVS; Legged locomotion; peripersonal space; Rhythm; Synchronization; Vibrations; walking sensation,Abstract,TRUE,
Scopus,journalPaper,2024,A Real-Time and Interactive Fluid Modeling System for Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Within the realm of mixed reality, the capability to dynamically render environmental effects with high realism plays a crucial role in amplifying user engagement and interaction. Fluid dynamics, in particular, stand out as essential elements for crafting immersive virtual settings. This includes the simulation of phenomena like smoke, fire, and clouds, which are instrumental in enriching the virtual experience. This work showcases a cutting-edge system developed to produce dynamic and interactive fluid effects that mirror real captured data in real-time for mixed reality applications. This innovative system seamlessly incorporates fluid reconstruction alongside velocity estimation processes within the Unity engine environment. Our approach leverages a novel physics-based differentiable rendering technique, grounded in the principles of light transport in participating media, to simulate the intricate behaviors of fluid while ensuring high fidelity in visual appearance. To further enhance realism, we have expanded our framework to include the estimation of velocity fields, addressing the critical need for fluid motion simulation. The practical application of these techniques demonstrates the system’s capacity to offer a robust platform for fluid modeling in mixed reality environments. Through extensive evaluations, we illustrate the effectiveness of our approach in various scenes, underscoring its potential to transform mixed reality content creation by providing developers with the tools to incorporate highly realistic and interactive fluid seamlessly. © 2024 IEEE.",differentiable rendering; fluid modeling; mixed reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,TouchMark: Partial Tactile Feedback Design for Upper Limb Rehabilitation in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"The use of Virtual Reality (VR) technology, especially in medical rehabilitation, has expanded to include tactile cues along with visual stimuli. For patients with upper limb hemiplegia, tangible handles with haptic stimuli could improve their ability to perform daily activities. Traditional VR controllers are unsuitable for patient rehabilitation in VR, necessitating the design of specialized tangible handles with integrated tracking devices. Besides, matching tactile stimulation with corresponding virtual visuals could strengthen users' embodiment (i.e., owning and controlling virtual bodies) in VR, which is crucial for patients' training with virtual hands. Haptic stimuli have been shown to amplify the embodiment in VR, whereas the effect of partial tactile stimulation from tangible handles on embodiment remains to be clarified. This research, including three experiments, aims to investigate how partial tactile feedback of tangible handles impacts users' embodiment, and we proposed a design concept called TouchMark for partial tactile stimuli that could help users quickly connect the physical and virtual worlds. To evaluate users' tactile and comfort perceptions when grasping tangible handles in a non-VR setting, various handles with three partial tactile factors were manipulated in Study 1. In Study 2, we explored the effects of partial feedback using three forms of TouchMark on the embodiment of healthy users in VR, with various tangible handles, while Study 3 focused on similar investigations with patients. These handles were utilized to complete virtual food preparation tasks. The tactile and comfort perceptions of tangible handles and users' embodiment were evaluated in this research using questionnaires and interviews. The results indicate that TouchMark with haptic line and ring forms over no stimulation would significantly enhance users' embodiment, especially for patients. The low-cost and innovative TouchMark approach may assist users, particularly those with limited VR experience, in achieving the embodiment and enhancing their virtual interactive experience. © 1995-2012 IEEE.",agency; body ownership; embodiment; self-location; tactile sensation; Virtual rehabilitation,Title_Abstract,TRUE,
Scopus,journalPaper,2024,Co-Designing Dynamic Mixed Reality Drill Positioning Widgets: A Collaborative Approach with Dentists in a Realistic Setup,TVCG - Transactions on Visualization and Computer Graphics,A,"—Mixed Reality (MR) is proven in the literature to support precise spatial dental drill positioning by superimposing 3D widgets. Despite this, the related knowledge about widget’s visual design and interactive user feedback is still limited. Therefore, this study is contributed to by co-designed MR drill tool positioning widgets with two expert dentists and three MR experts. The results of co-design are two static widgets (SWs): a simple entry point, a target axis, and two dynamic widgets (DWs), variants of dynamic error visualization with and without a target axis (DWTA and DWEP). We evaluated the co-designed widgets in a virtual reality simulation supported by a realistic setup with a tracked phantom patient, a virtual magnifying loupe, and a dentist’s foot pedal. The user study involved 35 dentists with various backgrounds and years of experience. The findings demonstrated significant results; DWs outperform SWs in positional and rotational precision, especially with younger generations and subjects with gaming experiences. The user preference remains for DWs (19) instead of SWs (16). However, findings indicated that the precision positively correlates with the time trade-off. The post-experience questionnaire (NASA-TLX) showed that DWs increase mental and physical demand, effort, and frustration more than SWs. Comparisons between DWEP and DWTA show that the DW’s complexity level influences time, physical and mental demands. The DWs are extensible to diverse medical and industrial scenarios that demand precision. © 2024 IEEE.",co-design; dentistry; Dynamic widgets; mixed reality; precise tool positioning; usability testing,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,HuBar: A Visual Analytics Tool to Explore Human Behavior based on fNIRS in AR Guidance Systems,TVCG - Transactions on Visualization and Computer Graphics,A,"The concept of an intelligent augmented reality (AR) assistant has significant, wide-ranging applications, with potential uses in medicine, military, and mechanics domains. Such an assistant must be able to perceive the environment and actions, reason about the environment state in relation to a given task, and seamlessly interact with the task performer. These interactions typically involve an AR headset equipped with sensors which capture video, audio, and haptic feedback. Previous works have sought to facilitate the development of intelligent AR assistants by visualizing these sensor data streams in conjunction with the assistant's perception and reasoning model outputs. However, existing visual analytics systems do not focus on user modeling or include biometric data, and are only capable of visualizing a single task session for a single performer at a time. Moreover, they typically assume a task involves linear progression from one step to the next. We propose a visual analytics system that allows users to compare performance during multiple task sessions, focusing on non-linear tasks where different step sequences can lead to success. In particular, we design visualizations for understanding user behavior through functional near-infrared spectroscopy (fNIRS) data as a proxy for perception, attention, and memory as well as corresponding motion data (acceleration, angular velocity, and gaze). We distill these insights into embedding representations that allow users to easily select groups of sessions with similar behaviors. We provide two case studies that demonstrate how to use these visualizations to gain insights about task performance using data collected during helicopter copilot training tasks. Finally, we evaluate our approach by conducting an in-depth examination of a think-aloud experiment with five domain experts. © 1995-2012 IEEE.",Application Motivated Visualization; AR/VR/Immersive; Image and Video Data; Mobile; Perception & Cognition; Specialized Input/Display Hardware; Temporal Data,Abstract,TRUE,
Scopus,journalPaper,2024,A Testbed for Studying Cybersickness and its Mitigation in Immersive Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Cybersickness (CS) represents one of the oldest problems affecting Virtual Reality (VR) technology. In an attempt to resolve or at least limit this form of discomfort, an increasing number of mitigation techniques have been proposed by academic and industrial researchers. However, the validation of such techniques is often carried out without grounding on a common methodology, making the comparison between the various works in the state of the art difficult. To address this issue, the present article proposes a novel testbed for studying CS in immersive VR and, in particular, methods to mitigate it. The testbed consists of four virtual scenarios, which have been designed to elicit CS in a targeted and predictable manner. The scenarios, grounded on available literature, support the extraction of objective metrics about user's performance. The testbed additionally integrates an experimental protocol that employs standard questionnaires as well as measurements typically adopted in state-of-the-art practice to assess levels of CS and other subjective aspects regarding User Experience. The article shows a possible use case of the testbed, concerning the evaluation of a CS mitigation technique that is compared with the absence of mitigation as baseline condition.  © 1995-2012 IEEE.",Cybersickness; evaluation; simulator sickness; taxonomy; testbed; virtual environments; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Towards 360 VR Sickness Mitigation: From Virtual Reality Eye-tracking to Visual Communication,TVCG - Transactions on Visualization and Computer Graphics,A,"Most 360 virtual reality (VR) contents have been developed without considering that users could be affected by VR sickness. Accordingly, users&#x0027; <italic>viewing safety</italic> has been steadily highlighted as a critical problem in the VR market. In this study, we investigate a novel VR sickness mitigation framework based on human visual characteristics for the rendered VR content. First, we build a large-scale 360 VR content database termed VRSP360 (VR Sickness and Presence 360) dedicated to the analysis of VR sickness and thoroughly conduct eye-tracking experiments to measure human perception. In the experiment, we observe that the users&#x0027; gaze distribution is highly center-biased when they experience excessive VR sickness. From this observation, we design a foveated filtering framework that limits high-frequency textures in the peripheral view to mitigate VR sickness. Particularly, given the human visual system&#x0027;s (HVS) non-uniform resolution with respect to the fovea, we also adopt the foveation-based filtering method using the trade-off between sickness mitigation and presence conservation, which reduces any loss in perceptual quality despite the filtering. We further demonstrate that our framework can effectively compress visual information by applying foveated compression. In addition, we develop two metrics (visual texture index and perceptual information index) to measure the effective preservation of user-perceived information despite the filtration of peripheral vision textures by our proposed mitigation method. Through rigorous subjective evaluation on both original content and its VR-sickness-mitigated version, we demonstrate that the proposed framework successfully mitigates VR sickness with a reduction rate of <inline-formula><tex-math notation=""LaTeX"">$\sim$</tex-math></inline-formula>19&#x0025; on the proposed dataset. IEEE",360 Saliency; Databases; Filtering; Foveation; Gaze tracking; Human Visual Perception (HVS); Indexes; Motion sickness; Prevention and mitigation; Visualization; VR sickness mitigation,Title_Abstract,TRUE,
Scopus,journalPaper,2024,An Immersive and Interactive VR Dataset to Elicit Emotions,TVCG - Transactions on Visualization and Computer Graphics,A,"Images and videos are widely used to elicit emotions; however, their visual appeal differs from real-world experiences. With virtual reality becoming more realistic, immersive, and interactive, we envision virtual environments to elicit emotions effectively, rapidly, and with high ecological validity. This work presents the first interactive virtual reality dataset to elicit emotions. We created five interactive virtual environments based on corresponding validated 360Â° videos and validated their effectiveness with 160 participants. Our results show that our virtual environments successfully elicit targeted emotions. Compared with the existing methods using images or videos, our dataset allows virtual reality researchers and practitioners to integrate their designs effectively with emotion elicitation settings in an immersive and interactive way. © 1995-2012 IEEE.",Dataset (https//github.com/HighTemplar-wjiang/VR-Dataset-Emotions); Emotions; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Gaze-Contingent Layered Optical See-Through Displays with a Confidence-Driven View Volume,TVCG - Transactions on Visualization and Computer Graphics,A,"The vergence-accommodation conflict (VAC) presents a major perceptual challenge for head-mounted displays with a fixed image plane. Varifocal and layered display designs can mitigate the VAC. However, the image quality of varifocal displays is affected by imprecise eye tracking, whereas layered displays suffer from reduced image contrast as the distance between layers increases. Combined designs support a larger workspace and tolerate some eye-tracking error. However, any layered design with a fixed layer spacing restricts the amount of error compensation and limits the in-focus contrast. We extend previous hybrid designs by introducing confidence-driven volume control, which adjusts the size of the view volume at runtime. We use the eye tracker's confidence to control the spacing of display layers and optimize the trade-off between the display's view volume and the amount of eye tracking error the display can compensate. In the case of high-quality focus point estimation, our approach provides high in-focus contrast, whereas low-quality eye tracking increases the view volume to tolerate the error. We describe our design, present its implementation as an optical-see head-mounted display using a multiplicative layer combination, and present an evaluation comparing our design with previous approaches. © 1995-2012 IEEE.",Gaze-Contingent Layered Display; Optical See-Through Mixed Reality; Vergence-Accommodation Conflict,Keywords,TRUE,
Scopus,journalPaper,2024,Tasks Reflected in the Eyes: Egocentric Gaze-Aware Visual Task Type Recognition in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"With eye tracking finding widespread utility in augmented reality and virtual reality headsets, eye gaze has the potential to recognize users' visual tasks and adaptively adjust virtual content displays, thereby enhancing the intelligence of these headsets. However, current studies on visual task recognition often focus on scene-specific tasks, like copying tasks for office environments, which lack applicability to new scenarios, e.g., museums. In this paper, we propose four scene-agnostic task types for facilitating task type recognition across a broader range of scenarios. We present a new dataset that includes eye and head movement data recorded from 20 participants while they engaged in four task types across 15 360-degree VR videos. Using this dataset, we propose an egocentric gaze-aware task type recognition method, TRCLP, which achieves promising results. Additionally, we illustrate the practical applications of task type recognition with three examples. Our work offers valuable insights for content developers in designing task-aware intelligent applications. Our dataset and source code are available at zhimin-wang.github.io/TaskTypeRecognition.html. © 1995-2012 IEEE.",deep learning; eye tracking; intelligent application; Virtual reality; visual task type recognition,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Depth Perception in Optical See-Through Augmented Reality: Investigating the Impact of Texture Density, Luminance Contrast, and Color Contrast",TVCG - Transactions on Visualization and Computer Graphics,A,"The immersive augmented reality (AR) system necessitates precise depth registration between virtual objects and the real scene. Prior studies have emphasized the efficacy of surface texture in providing depth cues to enhance depth perception across various media, including the real scene, virtual reality, and AR. However, these studies predominantly focus on black-and-white textures, leaving a gap in understanding the effectiveness of colored textures. To address this gap and further explore texture-related factors in AR, a series of experiments were conducted to investigate the effects of different texture cues on depth perception using the perceptual matching method. Findings indicate that the absolute depth error increases with decreasing contrast under black-and-white texture. Moreover, textures with higher color contrast also contribute to enhanced accuracy of depth judgments in AR. However, no significant effect of texture density on depth perception was observed. The findings serve as a theoretical reference for texture design in AR, aiding in the optimization of virtual-real registration processes. © 1995-2012 IEEE.",Augmented reality; depth perception; OST AR; texture contrast; texture density,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,An Exploratory Expert-Study for Multi-Type Haptic Feedback for Automotive Virtual Reality Tasks,TVCG - Transactions on Visualization and Computer Graphics,A,"Previous research has shown that integrating haptic feedback can improve immersion and realism in automotive VR applications. However, current haptic feedback approaches primarily focus on a single feedback type. This means users must switch between devices to experience haptic stimuli for different feedback types, such as grabbing, collision, or weight simulation. This restriction limits the ability to simulate haptics realistically for complex tasks such as maintenance. To address this issue, we evaluated existing feedback devices based on our requirements analysis to determine which devices are most suitable for simulating these three feedback types. Since no suitable haptic feedback system can simulate all three feedback types simultaneously, we evaluated which devices can be combined. Based on that, we devised a new multi-type haptic feedback system combining three haptic feedback devices. We evaluated the system with different feedback-type combinations through a qualitative expert study involving twelve automotive VR experts. The results showed that combining weight and collision feedback yielded the best and most realistic experience. The study also highlighted technical limitations in current grabbing devices. Our findings provide insights into the effectiveness of haptic device combinations and practical boundaries for automotive virtual reality tasks. © 1995-2012 IEEE.",Haptics; Human Computer Interaction; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"""As if it were my own hand"": inducing the rubber hand illusion through virtual reality for motor imagery enhancement",TVCG - Transactions on Visualization and Computer Graphics,A,"—Brain-computer interfaces (BCI) are widely used in the field of disability assistance and rehabilitation, and virtual reality (VR) is increasingly used for visual guidance of BCI-MI (motor imagery). Therefore, how to improve the quality of electroencephalogram (EEG) signals for MI in VR has emerged as a critical issue. People can perform MI more easily when they visualize the hand used for visual guidance as their own, and the Rubber Hand Illusion (RHI) can increase people’s ownership of the prosthetic hand. We proposed to induce RHI in VR to enhance participants’ MI ability and designed five methods of inducing RHI, namely active movement, haptic stimulation, passive movement, active movement mixed with haptic stimulation, and passive movement mixed with haptic stimulation, respectively. We constructed a first-person training scenario to train participants’ MI ability through the five induction methods. The experimental results showed that through the training, the participants’ feeling of ownership of the virtual hand in VR was enhanced, and the MI ability was improved. Among them, the method of mixing active movement and tactile stimulation proved to have a good effect on enhancing MI. Finally, we developed a BCI system in VR utilizing the above training method, and the performance of the participants improved after the training. This also suggests that our proposed method is promising for future application in BCI rehabilitation systems. © 2024 IEEE.",Assistive technologies; Brain-computer interface; Rubber hand illusion; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Cultural Reflections in Virtual Reality: The Effects of User Ethnicity in Avatar Matching Experiences on Sense of Embodiment,TVCG - Transactions on Visualization and Computer Graphics,A,"—Matching avatar characteristics to a user can impact sense of embodiment (SoE) in VR. However, few studies have examined how participant demographics may interact with these matching effects. We recruited a diverse and racially balanced sample of 78 participants to investigate the differences among participant groups when embodying both demographically matched and unmatched avatars. We found that participant ethnicity emerged as a significant factor, with Asian and Black participants reporting lower total SoE compared to Hispanic participants. Furthermore, we found that user ethnicity significantly influences ownership (a subscale of SoE), with Asian and Black participants exhibiting stronger effects of matched avatar ethnicity compared to White participants. Additionally, Hispanic participants showed no significant differences, suggesting complex dynamics in ethnic-racial identity. Our results also reveal significant main effects of matched avatar ethnicity and gender on SoE, indicating the importance of considering these factors in VR experiences. These findings contribute valuable insights into understanding the complex dynamics shaping VR experiences across different demographic groups. © 2024 IEEE.",avatars; diversity; sense of embodiment; Virtual reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,Exploring and Modeling Directional Effects on Steering Behavior in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"— Steering is a fundamental task in interactive Virtual Reality (VR) systems. Prior work has demonstrated that movement direction can significantly influence user behavior in the steering task, and different interactive environments (VEs) can lead to various behavioral patterns, such as tablets and PCs. However, its impact on VR environments remains unexplored. Given the widespread use of steering tasks in VEs, including menu adjustment and object manipulation, this work seeks to understand and model the directional effect with a focus on barehand interaction, which is typical in VEs. This paper presents the results of two studies. The first study was conducted to collect behavioral data with four categories: movement time, average movement speed, success rate, and reenter times. According to the results, we examined the effect of movement direction and built the SθModel. We then empirically evaluated the model through the data collected from the first study. The results proved that our proposed model achieved the best performance across all the metrics (r2 > 0.95), with more than 15% improvement over the original Steering Law in terms of prediction accuracy. Next, we further validated the SθModel by another study with the change of device and steering direction. Consistent with previous assessments, the model continues to exhibit optimal performance in both predicting movement time and speed. Finally, based on the results, we formulated design recommendations for steering tasks in VEs to enhance user experience and interaction efficiency. © 2024 IEEE.",barehand interaction; head-mounted display; human performance modeling; steering law; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,To use or not to use viewpoint oscillations when walking in VR &#x003F; State of the art and perspectives,TVCG - Transactions on Visualization and Computer Graphics,A,"Viewpoint oscillations are periodic changes in the position and&#x002F;or orientation of the point of view in a virtual environment. They can be implemented in Virtual Reality (VR) walking simulations to make them feel closer to real walking. This is especially useful in simulations where users remain in place because of space or hardware constraints. As for today, it remains unclear what exact benefit they bring to user experience during walking simulations, and with what characteristics they should be implemented. To answer these questions, we conduct a systematic literature review focusing on five main dimensions of user experience (walking sensation, vection, cybersickness, presence and embodiment) and discuss 44 articles from the fields of VR, Vision, and Human-Computer Interaction. Overall, the literature suggests that viewpoint oscillations benefit vection, and with less evidence, walking sensation and presence. As for cybersickness, the literature contains contrasted results. Based on these results, we recommend using viewpoint oscillations in applications that require accurate distance or speed perception, or that aim to provide compelling walking simulations without a walking avatar, and a particular attention should be paid to cybersickness. Taken together, this work gives recommendations for enhancing walking simulations in VR, which may be applied to entertainment, virtual visits, and medical rehabilitation. IEEE",Cybersickness; cybersickness; embodiment; Legged locomotion; Oscillators; presence; Solid modeling; User experience; vection; Viewpoint oscillations; Virtual environments; virtual reality; Visualization; walking sensation,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Multiple Self-Avatar Effect: Effects of Using Diverse Self-Avatars on Memory Acquisition and Retention of Sign-Language Gestures,TVCG - Transactions on Visualization and Computer Graphics,A,"This study proposes a new learning method that employs multiple embodied self-avatars during learning, to use the potential benefit of virtual reality (VR) for effective learning and training. In this study, by taking advantage of the benefit of virtual reality (VR), we propose a new learning method that employs multiple embodied self-avatars during learning. Based on the multiple-context effect, which posits that learning in diverse situations can prevent forgetting and enhance memory retention, we conducted a between-participants study under two conditions: the varied avatar condition, in which participants learned sign languages with different self-avatars in six iterations, and the constant avatar condition, in which the same self-avatar was used consistently. We employed sign language as a learning material that naturally draws attention to self-avatars and is suitable for investigating the effects of varying self-avatars. Initially, the varied avatar condition performed worse than the constant avatar condition. However, in a test conducted after one week in the real world, the varied avatar condition showed significantly less forgetting and better retention than the constant avatar condition. Furthermore, our results suggested a positive correlation between the degree of embodiment toward the avatars and the effectiveness of the proposed method. This study presents an innovative design approach for the use of self-avatars in VR-based education. Authors",Avatar; Avatars; context-dependent memory; Cybersickness; embodiment; learning; Task analysis; Testing; Training; Virtual environments; virtual reality; Visualization,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,VPRF: Visual Perceptual Radiance Fields for Foveated Image Synthesis,TVCG - Transactions on Visualization and Computer Graphics,A,"Neural radiance fields (NeRF) has achieved revolutionary breakthrough in the novel view synthesis task for complex 3D scenes. However, this new paradigm struggles to meet the requirements for real-time rendering and high perceptual quality in virtual reality. In this paper, we propose VPRF, a novel visual perceptual based radiance fields representation method, which for the first time integrates the visual acuity and contrast sensitivity models of human visual system (HVS) into the radiance field rendering framework. Initially, we encode both the appearance and visual sensitivity information of the scene into our radiance field representation. Then, we propose a visual perceptual sampling strategy, allocating computational resources according to the HVS sensitivity of different regions. Finally, we propose a sampling weight-constrained training scheme to ensure the effectiveness of our sampling strategy and improve the representation of the radiance field based on the scene content. Experimental results demonstrate that our method renders more efficiently, with higher PSNR and SSIM in the foveal and salient regions compared to the state-of-the-art FoV-NeRF. The results of the user study confirm that our rendering results exhibit high-fidelity visual perception. © 1995-2012 IEEE.",Contrast sensitivity; Foveated rendering; Virtual reality; Visual perceptual,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Classification of Internal and External Distractions in an Educational VR Environment Using Multimodal Features,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual reality (VR) can potentially enhance student engagement and memory retention in the classroom. However, distraction among participants in a VR-based classroom is a significant concern. Several factors, including mind wandering, external noise, stress, etc., can cause students to become internally and/or externally distracted while learning. To detect distractions, single or multi-modal features can be used. A single modality is found to be insufficient to detect both internal and external distractions, mainly because of individual variability. In this work, we investigated multi-modal features: eye tracking and EEG data, to classify the internal and external distractions in an educational VR environment. We set up our educational VR environment and equipped it for multi-modal data collection. We implemented different machine learning (ML) methods, including k-nearest-neighbors (kNN), Random Forest (RF), one-dimensional convolutional neural network - long short-term memory (1 D-CNN-LSTM), and two-dimensional convolutional neural networks (2D-CNN) to classify participants' internal and external distraction states using the multi-modal features. We performed cross-subject, cross-session, and gender-based grouping tests to evaluate our models. We found that the RF classifier achieves the highest accuracy over 83% in the cross-subject test, around 68% to 78% in the cross-session test, and around 90% in the gender-based grouping test compared to other models. SHAP analysis of the extracted features illustrated greater contributions from the occipital and prefrontal regions of the brain, as well as gaze angle, gaze origin, and head rotation features from the eye tracking data. © 1995-2012 IEEE.",EEG; Eye-tracking; Human-centered computing; Machine Learning,Abstract,TRUE,
Scopus,journalPaper,2024,Evaluating and Modeling the Effect of Frame Rate on Steering Performance in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Prior work has shown that frame rate significantly influences user behavior in fast-response tasks in 2D and 3D contexts. However, its impact on a steering task, which involves navigating an object along a path from the start to the end, remains relatively unexplored, especially in the context of virtual reality (VR). This task is considered a typical non-fast-response activity, as it does not demand rapid reactions within a limited time frame. Our work aims to understand and model users&#x0027; steering behavior and predict movement time with different task complexities and frame rates in VR environments. We first conducted a user study to collect user behavior in a steering task with four factors: frame rate, path length, width, and radius of curvature. Based on the results, we then quantified the effects of frame rate and built two predictive models. Our models exhibited the best fit (<inline-formula><tex-math notation=""LaTeX"">$r^{2}\gt 0.957$</tex-math></inline-formula>) and over 17&#x0025; improvement in prediction accuracy for movement time compared to existing models. Our models&#x0027; robustness was further validated by applying them to predict steering performance with different VR tasks and frame rates. The two models keep the best predictability for both movement time and speed. IEEE",Adaptation models; Behavioral sciences; Computational modeling; frame rate; head-mounted display; human performance modeling; Predictive models; Solid modeling; steering law; Task analysis; Three-dimensional displays; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Scene-aware Foveated Neural Radiance Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"Foveated rendering provides an idea for improving the image synthesis performance of neural radiance fields (NeRF) methods. In this paper, we propose a scene-aware foveated neural radiance fields method to synthesize high-quality foveated images in complex VR scenes at high frame rates. Firstly, we construct a multi-ellipsoidal neural representation to enhance the neural radiance field&#x0027;s representation capability in salient regions of complex VR scenes based on the scene content. Then, we introduce a uniform sampling based foveated neural radiance field framework to improve the foveated image synthesis performance with one-pass color inference, and improve the synthesis quality by leveraging the foveated scene-aware objective function. Our method synthesizes high-quality binocular foveated images at the average frame rate of 66 frames per second (FPS) in complex scenes with high occlusion, intricate textures, and sophisticated geometries. Compared with the state-of-the-art foveated NeRF method, our method achieves significantly higher synthesis quality in both the foveal and peripheral regions with 1.41-1.46&#x00D7; speedup. We also conduct a user study to prove that the perceived quality of our method has a high visual similarity with the ground truth IEEE",Foveated Rendering; Geometry; Neural networks; Neural radiance field; Neural Radiance Fields; Real-time systems; Rendering (computer graphics); Streaming media; Three-dimensional displays; Virtual Reality,Keywords,TRUE,
Scopus,journalPaper,2024,Immersive Study Analyzer: Collaborative Immersive Analysis of Recorded Social VR Studies,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual Reality (VR) has become an important tool for conducting behavioral studies in realistic, reproducible environments. In this paper, we present ISA, an Immersive Study Analyzer system designed for the comprehensive analysis of social VR studies. For in-depth analysis of participant behavior, ISA records all user actions, speech, and the contextual environment of social VR studies. A key feature is the ability to review and analyze such immersive recordings collaboratively in VR, through support of behavioral coding and user-defined analysis queries for efficient identification of complex behavior. Respatialization of the recorded audio streams enables analysts to follow study participants' conversations in a natural and intuitive way. To support phases of close and loosely coupled collaboration, ISA allows joint and individual temporal navigation, and provides tools to facilitate collaboration among users at different temporal positions. An expert review confirms that ISA effectively supports collaborative immersive analysis, providing a novel and effective tool for nuanced understanding of user behavior in social VR studies. © 1995-2012 IEEE.",Behavior Analysis; Collaborative Immersive Analytics; Social VR Studies,Abstract,TRUE,
Scopus,journalPaper,2024,CompositingVis: Exploring Interactions for Creating Composite Visualizations in Immersive Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Composite visualization represents a widely embraced design that combines multiple visual representations to create an integrated view. However, the traditional approach of creating composite visualizations in immersive environments typically occurs asynchronously outside of the immersive space and is carried out by experienced experts. In this work, we aim to empower users to participate in the creation of composite visualization within immersive environments through embodied interactions. This could provide a flexible and fluid experience with immersive visualization and has the potential to facilitate understanding of the relationship between visualization views. We begin with developing a design space of embodied interactions to create various types of composite visualizations with the consideration of data relationships. Drawing inspiration from people's natural experience of manipulating physical objects, we design interactions based on the combination of 3D manipulations in immersive environments. Building upon the design space, we present a series of case studies showcasing the interaction to create different kinds of composite visualizations in virtual reality. Subsequently, we conduct a user study to evaluate the usability of the derived interaction techniques and user experience of creating composite visualizations through embodied interactions. We find that empowering users to participate in composite visualizations through embodied interactions enables them to flexibly leverage different visualization views for understanding and communicating the relationships between different views, which underscores the potential of several future application scenarios  © 1995-2012 IEEE.",Composite Visualization; Embodied Interaction; Empowerment; Immersive Analytics,Abstract,TRUE,
Scopus,journalPaper,2024,The Least Increasing Aversion (LIA) Protocol: Illustration on Identifying Individual Susceptibility to Cybersickness Triggers,TVCG - Transactions on Visualization and Computer Graphics,A,"This paper introduces the Least Increase aversion (LIA) protocol to investigate the relative impact of factors that may trigger cybersickness. The protocol is inspired by the Subjective Matching methodology (SMT) from which it borrows the incremental construction of a richer VR experience, except that the full-blown target experience may cause undesired discomfort. In the first session, the participant briefly encounter all factors at the maximum level. Then in the second session they start with the minimum level of all factors as a Baseline. Subsequently, we expect the participant to minimize their exposure to the most adverse factors. This approach ranks the factors from mildest to worst and helps detect individual susceptibility to cybersickness triggers.To validate the applicability of LIA protocol, we further evaluate it with an experiment to identify individual susceptibility to three rotational axes (Yaw, Pitch, and Roll). The findings not only confirm the protocol&#x0027;s capability to accurately discern individual rankings of various factors to cybersickness but also indicate that individual susceptibility is more intricate and multifaceted than initially anticipated. Authors",Avatars; Cybersickness; Cybersickness; individual susceptibility; Motion sickness; Protocols; Sensitivity; virtual reality; Visual perception; Visualization,Keywords,TRUE,
Scopus,journalPaper,2024,Eye-body Coordination during Daily Activities for Gaze Prediction from Full-body Poses,TVCG - Transactions on Visualization and Computer Graphics,A,"Human eye gaze plays a significant role in many virtual and augmented reality (VR&#x002F;AR) applications, such as gaze-contingent rendering, gaze-based interaction, or eye-based activity recognition. However, prior works on gaze analysis and prediction have only explored eye-head coordination and were limited to human-object interactions. We first report a comprehensive analysis of eye-body coordination in various humanobject and human-human interaction activities based on four public datasets collected in real-world (MoGaze), VR (ADT), as well as AR (GIMO and EgoBody) environments. We show that in human-object interactions, e.g. pick and place, eye gaze exhibits strong correlations with full-body motion while in human-human interactions, e.g. chat and teach, a person&#x0027;s gaze direction is correlated with the body orientation towards the interaction partner. Informed by these analyses we then present Pose2Gaze &#x2013; a novel eye-body coordination model that uses a convolutional neural network and a spatio-temporal graph convolutional neural network to extract features from head direction and full-body poses, respectively, and then uses a convolutional neural network to predict eye gaze. We compare our method with state-of-theart methods that predict eye gaze only from head movements and show that Pose2Gaze outperforms these baselines with an average improvement of 24.0&#x0025; on MoGaze, 10.1&#x0025; on ADT, 21.3&#x0025; on GIMO, and 28.6&#x0025; on EgoBody in mean angular error, respectively. We also show that our method significantly outperforms prior methods in the sample downstream task of eyebased activity recognition. These results underline the significant information content available in eye-body coordination during daily activities and open up a new direction for gaze prediction. IEEE",Activity recognition; activity recognition; augmented reality; Convolutional neural networks; Correlation; Eye-body coordination; Feature extraction; gaze prediction; Head; human-human interaction; human-object interaction; Task analysis; Virtual environments; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Effects of Eye Vergence and Accommodation on Interactions with Content on an AR Magic-lens Display and its Surroundings,TVCG - Transactions on Visualization and Computer Graphics,A,"Augmented reality (AR) magic-lens (ML) displays, such as handheld devices, offer a convenient and accessible way to enrich our environment using virtual imagery. Several display technologies, including conventional monocular, less common stereoscopic, and varifocal displays, are currently being used. Vergence and accommodation effects on depth perception, as well as vergence&#x2013;accommodation conflict, have been studied, where users interact only with the content on the display. However, little research exists on how vergence and accommodation influence user performance and cognitive-task load when users interact with the content on a display and its surroundings in a short timeframe. Examples of this are validating augmented instructions before making an incision andperforming general hand-eye coordinated tasks such as grasping augmented objects. To improve interactions with future AR displays in such scenarios, we must improve our understanding of this influence. To this end, we conducted two fundamental visual-acuity user studies with 28 and 27 participants, while investigating eye vergence and accommodation distances on four ML displays. Our findings show that minimizing the accommodation difference between the display and its surroundings is crucial when the gaze between the display and its surroundings shifts rapidly. Minimizing the difference in vergence is more important when viewing the display and its surroundings as a single context without shifting the gaze. Interestingly, the vergence&#x2013;accommodation conflict did not significantly affect the cognitive-task load nor play a pivotal role in the accuracy of interactions with AR ML content and its physical surroundings Authors",Augmented Reality—Human-computer interaction— Video see-through display—Vergence-accommodation; Fatigue; Lenses; Optical imaging; Smart phones; Stereo image processing; Task analysis; Visualization,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering Perceived Texture Roughness in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Extensive research has been done in haptic feedback for texture simulation in virtual reality (VR). However, it is challenging to modify the perceived tactile texture of existing physical objects which usually serve as anchors for virtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a finger-worn haptic device that uses vibratory-pneumatic feedback to modulate (i.e., increase and decrease) the perceived roughness of the material surface contacted by the user&#x0027;s fingerpad while supporting the perceived sensation of other haptic properties (e.g., temperature or stickiness) in MR. Our device includes a silicone-based pneumatic actuator that can lift the user&#x0027;s fingerpad on the physical surface to reduce the contact area for roughness decreasing, and an on-finger vibrator for roughness increasing. The results of our perceptual study showed that the participants could perceive changes in roughness, both increasing and decreasing, compared to the original material surface. We also observed the overlapping roughness ratings among certain haptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived roughness of some materials without any haptic feedback. This suggests the potential to alter the perceived texture of one type of material to another in terms of roughness (e.g., modifying the perceived texture of ceramics as glass). Lastly, a user study of MR experience showed that ViboPneumo could significantly improve the MR user experience, particularly for visual-haptic matching, compared to the condition of a bare finger. We also demonstrated a few application scenarios for ViboPneumo. IEEE",AR; Haptic interfaces; haptic perception; Modulation; pneumatic devices; Rough surfaces; roughness; Surface roughness; Surface texture; Vibrations; Virtual reality; VR; wearable haptics,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,A novel AR recoloring technique to enhance operator performance on inspection tasks in Industry 4.0 environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Over the past few years, the manufacturing industry has increasingly embraced Augmented Reality (AR) for inspecting real products, yet faces challenges in visualization modalities. In fact, AR content presentation significantly impacts user performance, especially when virtual object colors lack real-world context. Additionally, the lack of studies in this area compounds uncertainty about visualization effects on user performance in inspection tasks. This study introduces a novel AR recoloring technique to enhance user performance during industrial assembly inspection tasks. This technique automatically recolors virtual components based on their physical counterparts, improving distinctiveness. Experimental comparisons with AR experts and representative users, using objective and subjective metrics, demonstrate the proposed AR recoloring technique enhances task performance and reduces mental burden during inspection activities. This innovative approach outperforms established methods like CAD and random modes, showcasing its potential for advancing AR applications in manufacturing, particularly in the inspection of products. Authors",AR recoloring techniques; Assembly; Augmented Reality; Cognitive load; Color; Image color analysis; Industrial assembly inspection; Industry 4.0; Inspection; Three-dimensional displays; Videos; Visualization; Visualization modalities,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Enabling Predictive Redirection Reset Based on Virtual-Real Spatial Probability Density Distributions,TVCG - Transactions on Visualization and Computer Graphics,A,"Redirected walking (RDW) allows users to explore vast virtual spaces by walking in confined real spaces, yet suffers from frequent boundary collisions due to physical constraints. The major solution is to use the reset strategy to steer users away from boundaries. However, most reset methods guide users to fixed spots or follow constant patterns, neglecting spatial features and users&#x0027; movement trends. In this paper, we propose an innovative predictive reset method based on spatial probability density distribution to jointly involve impacts of spatial feature and walking intention for forecasting the user&#x0027;s possible positional distribution, and thereby determines the optimal reset direction by maximizing walking expectation. Given a space, we calculate the stationary layout energy to indicate traveling difficulties of all positions. Meanwhile, we exploit a novel intention inference model to anticipate the probability distribution of the user&#x0027;s presence across adjacent positions. Furthermore, we incorporate the obstacle energy attenuation to predict the obstacle avoidance behaviors. All aforementioned factors are amalgamated into a potential region energy map, and then we integrate energy maps of virtual and real spaces into a fusion energy map to enable the prediction considering both spaces simultaneously. Thus, the optimal reset direction is derived by maximizing the fusion energy. Simulation and user studies are conducted on a broad dataset containing plentiful virtual and real spaces. The results demonstrate that our method effectively reduces the physical collisions and increase the continuous walking distance compared to prevalent reset methods, while exhibiting superior applicability when combined with various RDW controllers. The source code and dataset are available at https:&#x002F;&#x002F;github.com&#x002F;huiyuroy&#x002F;Return2MaxPotentialEnergy. IEEE",Aerospace electronics; Layout; Legged locomotion; Market research; Motion Planning; Obstacle Avoidance; Predictive models; Probability Density; Redirected Walking; Space exploration; Trajectory; Virtual Reality,Keywords,TRUE,
Scopus,journalPaper,2024,Every &#x201C;Body&#x201D; Gets a Say: An Augmented Optimization Metric to Preserve Body Pose during Avatar Adaptation in Mixed/Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"User-Avatar interaction within augmented reality applications is rapidly increasing in frequency. Applications routinely place users in rooms with other, remote users embodied by photorealistic avatars, or require users to work with an avatar of a remote user to complete a task. During these types of interactions, it is often required to modify or redirect the posture of an avatar to achieve goals such as contact with or pointing at an object or maintaining eye gaze with the local user. A key limitation of modern redirection techniques is successfully preserving body posture, a critical component of nonverbal communication. This paper presents a new pose-preserving objective function to be used in the multi-objective optimization of an avatar&#x0027;s kinematic configuration. This objective function not only mimics the correct placement of body joints, but also preserves their orientation in space. We have tested this approach against several commonly used and current state-of-the-art redirection techniques and have found that our new approach achieves a significant reduction in targeted redirection error while simultaneously reducing body posture error. Additionally, human subject testing has shown that our new technique provides both a significantly more natural looking redirection and a significantly more realistic and believable overall body posture. Authors",Avatars; Collaboration; Collaborative Augmented Reality (CAR); MIMICs; Motion capture; Multi-Objective Optimization (MOO); Optimization; Performance evaluation; Pose Preservation; Social Presence; Task analysis,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Proxy Importance Based Haptic Retargeting With Multiple Props in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"In virtual reality applications, in addition to visual feedback, real objects can be used as props for virtual objects to provide passive haptic feedback, which greatly enhances user immersion. Usually, real object props are not one-to-one correspondence with virtual objects. Haptic retargeting technique is proposed to establish the virtual-real correspondence by introducing an offset between the virtual hand and the real hand. Sometimes, the offset is too large to cause user discomfort, and it is necessary to introduce a reset between two haptic retargeting operations to force the virtual hand and the real hand to coincide in order to eliminate the offset. However, too many resets can interfere with this immersion. To address this problem, we propose a haptic retargeting method based on proxy importance calculation using multiple props in virtual reality. The concept of proxy importance for props is introduced first, and then a proxy importance based prop selection and placement method for moving virtual objects are proposed. We also improve the performance of our method by using the props&#x0027; weighted proxy importance strategy for multi-user collaboration. Compared to the state-of-the-art methods, our method significantly reduces the number of resets, the task completion time, hand movement distances, and task load without the cost of cybersickness in the single-user task. In the multi-user collaborative task, our method also achieves significant improvement using the strategy that weights the proxy importance of the props. IEEE",Avatars; Collaboration; Hand redirection; Haptic interfaces; haptic retargeting; perception; reset techniques; Shape; Task analysis; Virtual environments; virtual reality; Visualization,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,3D Gaussian Splatting as New Era: A Survey,TVCG - Transactions on Visualization and Computer Graphics,A,"3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the field of Computer Graphics, offering explicit scene representation and novel view synthesis without the reliance on neural networks, such as Neural Radiance Fields (NeRF). This technique has found diverse applications in areas such as robotics, urban mapping, autonomous navigation, and virtual reality/augmented reality, just name a few. Given the growing popularity and expanding research in 3D Gaussian Splatting, this paper presents a comprehensive survey of relevant papers from the past year. We organize the survey into taxonomies based on characteristics and applications, providing an introduction to the theoretical underpinnings of 3D Gaussian Splatting. Our goal through this survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a valuable reference for seminal works in the field, and inspire future research directions, as discussed in our concluding section. IEEE",3D Gaussian Splatting; computer graphics; generation; Image color analysis; Image reconstruction; manipulation; perception; reconstruction; rendering; Rendering (computer graphics); Reviews; Surveys; Three-dimensional displays; Videos; virtual humans,Abstract,TRUE,
Scopus,journalPaper,2024,Designing a Virtual Toolkit for Analysing Planetary Science Data in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"To understand the surface evolution and potential habitability of other planets we must analyse their geology &#x2013; the 3D structure and chemistry of the rocks that are exposed at the surface. Although rovers capture this 3D structure using stereo camera systems and other instruments, when we present this information to mission scientists for analysis it is generally confined to the 2D plane of a computer screen, and the spatial information is lost at the point when it is needed most. To address this problem, we design, develop, and evaluate a prototype Virtual Environment to present geological data in the 3D form in which it was originally captured, and users are supplied with a toolkit for measurement and annotation of data. We observed that users were inspired by the environment and felt more connected to it because they could move within the data; they valued the tools but did not trust the scale and therefore did not always trust the results. We conclude with recommendations for others working in this application area, and pose a series of questions for future research. IEEE",3D perception; Extraterrestrial measurements; Geology; Instruments; Mars; mixed-method study; Prototypes; Spatial databases; Three-dimensional displays; virtual reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,A Real-time Method for Inserting Virtual Objects into Neural Radiance Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"We present the first real-time method for inserting a rigid virtual object into a neural radiance field (NeRF), which produces realistic lighting and shadowing effects, as well as allows interactive manipulation of the object. By exploiting the rich information about lighting and geometry in a NeRF, our method overcomes several challenges of object insertion in augmented reality. For lighting estimation, we produce accurate and robust incident lighting that combines the 3D spatially-varying lighting from NeRF and an environment lighting to account for sources not covered by the NeRF. For occlusion, we blend the rendered virtual object with the background scene using an opacity map integrated from the NeRF. For shadows, with a precomputed field of spherical signed distance fields, we query the visibility term for any point around the virtual object, and cast soft, detailed shadows onto 3D surfaces. Compared with state-of-the-art techniques, our approach can insert virtual objects into scenes with superior fidelity, and has great potential to be further applied to augmented reality systems. IEEE",all-frequency rendering; augmented reality; Estimation; Geometry; Lighting; Neural radiance field; Real-time systems; Rendering (computer graphics); Runtime; shadow; Task analysis,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Multisource Differential Fusion Driven Monocular Endoscope Hybrid 3-D Tracking for Advanced Endoscopic Navigation Surgery,TVCG - Transactions on Visualization and Computer Graphics,A,"Surgical navigation systems involve various technologies of segmentation, calibration, registration, tracking, and visualization. These systems aim to superimpose multisource information in the surgical field and provide surgeons with a composite overlay (augmented-reality) view, improving the operative precision and experience. Surgical 3-D tracking is the key to build these systems. Unfortunately, surgical 3-D tracking is still a challenge to endoscopic and robotic navigation systems and easily gets trapped in image artifacts, tissue deformation, and inaccurate positional (e.g., electromagnetic) sensor measurements. This work explores a new monocular endoscope hybrid 3-D tracking method called spatially constrained adaptive differential evolution that combines two spatial constraints with observation-recall adaptive propagation and observation-based fitness computing for stochastic optimization. Specifically, we spatially constraint inaccurate electromagnetic sensor measurements to the centerline of anatomical tubular structures to keep them physically locating inside the tubes, as well as interpolate these measurements to reduce jitter errors for smooth 3-D tracking. We then propose observation-recall adaptive propagation with fitness computing to precisely fuse the constrained sensor measurements, preoperative images, and endoscopic video sequences for accurate hybrid 3-D tracking. Additionally, we also propose a new marker-free hybrid registration strategy to precisely align positional sensor measurements to preoperative images. Our new framework was evaluated on a large amount of clinical data acquired from various surgical endoscopic procedures, with the experimental results showing that it certainly outperforms current surgical 3-D approaches. In particular, the position and rotation errors were significantly reduced from (6.55, 11.4) to (3.02 mm, 8.54<inline-formula><tex-math notation=""LaTeX"">$^\circ$</tex-math></inline-formula>). IEEE",augmented reality; differential evolution; Endoscopes; endoscopy; Monocular 3-D tracking; multisource fusion; Navigation; Optical imaging; Optical sensors; Robot sensing systems; Surgery; surgical navigation; Tracking,Keywords,TRUE,
Scopus,journalPaper,2024,Effects of Focal Distance on Near-Field Depth Perception and Accommodative Response in a VariFocal Optical See-Through Augmented Reality Display,TVCG - Transactions on Visualization and Computer Graphics,A,"Through a human-subject experiment, we investigated the effects of focal distance on depth perception and accommodative response in an optical see-through augmented reality (AR) display. The display was able to provide focus cues and was rigorously calibrated. The near-field distances ranging between 3 diopters and 1 diopter were considered as target distance. In the experiment, it was found that the perceived depth of a virtual object was significantly biased along with the focal distance of virtual image plane of the display. In addition, the experimental results implied that the perceived depth of a virtual object would be potentially more accurate in the condition where the focal distance of virtual image plane was consistent with the target distance than in the conditions where it could deviate from the target distance. Regarding accommodative response, it was found that the response to a virtual object changed along with the focal distance of virtual image plane as well as the target distance. However, the changing rate depending on target distance was less steep in the conditions where the focal distance could be mismatched with the target distance than in the condition where it was consistent with the target distance. In the consistent condition, the changing rate of accommodative responses to virtual objects were similar to that for their physical counterparts. IEEE",Accommodative response; Adaptive optics; augmented reality; Calibration; depth perception; focal distance; focus cue; Optical imaging; optical see-through head-mounted display; Stereo image processing; stereoscopic display; Task analysis; Three-dimensional displays; vergenceaccommodation conflict; Visualization,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Reduction of Forgetting by Contextual Variation During Encoding Using 360-Degree Video-Based Immersive Virtual Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Recall impairment in a different environmental context from learning is called context-dependent forgetting. Two learning methods have been proposed to prevent context-dependent forgetting: reinstatement and decontextualization. Reinstatement matches the environmental context between learning and retrieval, whereas decontextualization involves repeated learning in various environmental contexts and eliminates the context dependency of memory. Conventionally, these methods have been validated by switching between physical rooms. However, in this study, we use immersive virtual environments (IVEs) as the environmental context assisted by virtual reality (VR), which is known for its low cost and high reproducibility compared to traditional manipulation. Whereas most existing studies using VR have failed to reveal the reinstatement effect, we test its occurrence using a 360-degree video-based IVE with improved familiarity and realism instead of a computer graphics-based IVE. Furthermore, we are the first to address decontextualization using VR. Our experiment showed that repeated learning in the same constant IVE as retrieval did not significantly reduce forgetting compared to repeated learning in different constant IVEs. Conversely, repeated learning in various IVEs significantly reduced forgetting than repeated learning in constant IVEs. These findings contribute to the design of IVEs for VR-based applications, particularly in educational settings. Authors",360-degree video; decontextualization effect; Encoding; environmental context-dependent memory; forgetting; Memory management; reinstatement effect; Task analysis; Testing; Time measurement; Urban areas; Virtual environments; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Augmented Reality-based Contextual Guidance through Surgical Tool Tracking in Neurosurgery,TVCG - Transactions on Visualization and Computer Graphics,A,"External ventricular drain (EVD) is a common, yet challenging neurosurgical procedure of placing a catheter into the brain ventricular system that requires prolonged training for surgeons to improve the catheter placement accuracy. In this paper, we introduce NeuroLens, an Augmented Reality (AR) system that provides neurosurgeons with guidance that aides them in completing an EVD catheter placement. NeuroLens builds on prior work in AR-assisted EVD to present a registered hologram of a patient&#x0027;s ventricles to the surgeons, and uniquely incorporates guidance on the EVD catheter&#x0027;s trajectory, angle of insertion, and distance to the target. The guidance is enabled by tracking the EVD catheter. We evaluate NeuroLens via a study with 33 medical students and 9 neurosurgeons, in which we analyzed participants&#x0027; EVD catheter insertion accuracy and completion time, eye gaze patterns, and qualitative responses. Our study, in which NeuroLens was used to aid students and surgeons in inserting an EVD catheter into a realistic phantom model of a human head, demonstrated the potential of NeuroLens as a tool that will aid and educate novice neurosurgeons. On average, the use of NeuroLens improved the EVD placement accuracy of the year 1 students by 39.4&#x0025;, of the year 2<inline-formula><tex-math notation=""LaTeX"">$-$</tex-math></inline-formula>4 students by 45.7&#x0025;, and of the neurosurgeons by 16.7&#x0025;. Furthermore, students who focused more on NeuroLens-provided contextual guidance achieved better results, and novice surgeons improved more than the expert surgeons with NeuroLens&#x0027;s assistance. IEEE",Augmented reality; Biomedical optical imaging; Catheters; contextual guidance; image registration; Image registration; Neurosurgery; neurosurgery; Optical imaging; Phantoms; tool tracking; Visualization,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Impact of Background, Foreground, and Manipulated Object Rendering on Egocentric Depth Perception in Virtual and Augmented Indoor Environments",TVCG - Transactions on Visualization and Computer Graphics,A,"This research investigated how the similarity of the rendering parameters of background and foreground objects affected egocentric depth perception in indoor virtual and augmented environments. We refer to the similarity of the rendering parameters as visual &#x2018;congruence&#x2019;. Study participants manipulated the depth of a sphere to match the depth of a designated target peg. In the first experiment, the sphere and peg were both virtual, while in the second experiment, the sphere is virtual and the peg is real. In both experiments, depth perception accuracy was found to depend on the levels of realism and congruence between the sphere, pegs, and background. In Experiment 1, realistic backgrounds lead to overestimation of depth, but resulted in underestimation when the background was virtual, and when depth cues were applied to the sphere and target peg. In Experiment 2, background and target pegs were real but matched with the virtual sphere; in comparison to Experiment 1, realistically rendered targets prompted an underestimation and more accuracy with the manipulated object. These findings suggest that congruence can affect distance estimation and the underestimation effect in the AR environment resulted from increased graphical fidelity of the foreground target and background. IEEE",Augmented reality; Biomedical imaging; depth perception; egocentric; Estimation; Legged locomotion; perceptual matching; rendering; Rendering (computer graphics); Task analysis; Virtual environments; virtual reality; Visualization,Keywords,TRUE,
Scopus,journalPaper,2024,Come Look at This: Supporting Fluent Transitions between Tightly and Loosely Coupled Collaboration in Social Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Collaborative work in social virtual reality often requires an interplay of loosely coupled collaboration from different virtual locations and tightly coupled face-to-face collaboration. Without appropriate system mediation, however, transitioning between these phases requires high navigation and coordination efforts. In this paper, we present an interaction system that allows collaborators in virtual reality to seamlessly switch between different collaboration models known from related work. To this end, we present collaborators with functionalities that let them work on individual sub-tasks in different virtual locations, consult each other using asymmetric interaction patterns while keeping their current location, and temporarily or permanently join each other for face-to-face interaction. We evaluated our methods in a user study with 32 participants working in teams of two. Our quantitative results indicate that delegating the target selection process for a long-distance teleport significantly improves placement accuracy and decreases task load within the team. Our qualitative user feedback shows that our system can be applied to support flexible collaboration. In addition, the proposed interaction sequence received positive evaluations from teams with varying VR experiences. IEEE",3D User Interfaces; Collaboration; Collaborative Interfaces; Groupwork; Multi-User Environments; Navigation; Social VR; Task analysis; Teleportation; Three-dimensional displays; Virtual environments; Virtual reality; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Development and Evaluation of a Treadmill-based Video-see-through and Optical-see-through Mixed Reality Systems for Obstacle Negotiation Training,TVCG - Transactions on Visualization and Computer Graphics,A,"Mixed reality (MR) technologies have a high potential to enhance obstacle negotiation training beyond the capabilities of existing physical systems. Despite such potential, the feasibility of using MR for obstacle negotiation on typical training treadmill systems and its effects on obstacle negotiation performance remains largely unknown. This research bridges this gap by developing an MR obstacle negotiation training system deployed on a treadmill, and implementing two MR systems with a video see-through (VST) and an optical see-through (OST) Head Mounted Displays (HMDs). We investigated the obstacle negotiation performance with virtual and real obstacles. The main outcomes show that the VST MR system significantly changed the parameters of the leading foot in cases of Box obstacle (approximately 22 cm to 30 cm for stepping over 7cm-box), which we believe was mainly attributed to the latency difference between the HMDs. In the condition of OST MR HMD, users tended to not lift their trailing foot for virtual obstacles (approximately 30 cm to 25 cm for stepping over 7cm-box). Our findings indicate that the low-latency visual contact with the world and the user&#x0027;s body is a critical factor for visuo-motor integration to elicit obstacle negotiation. IEEE",Belts; Foot; Legged locomotion; Resists; Training; Virtual environments; Visualization,Title_Abstract,TRUE,
Scopus,journalPaper,2024,Unified Cross-Structural Motion Retargeting for Humanoid Characters,TVCG - Transactions on Visualization and Computer Graphics,A,"Motion retargeting for animation characters has potential applications in fields such as animation production and virtual reality. However, current methods either assume that the source and target characters have the same skeletal structure, or require designing and training specific model architectures for each structure. In this paper, we aim to address the challenge of motion retargeting across previously unseen skeletal structures with a unified dynamic graph network. The proposed approach utilizes a dynamic graph transformation module to dynamically transfer latent motion features to different structures. We also take into consideration for intricate hand movements and model both torso and hand joints as graphs in a unified manner for whole-body motion retargeting. Our model allows the use of motion data from different structures to train a unified model and learns cross-structural motion retargeting in an unsupervised manner with unpaired data. Experimental results demonstrate the superiority of the proposed method in terms of data efficiency and performance on both seen and unseen structures. IEEE",Animation; Character animation; Decoding; deep learning; Dynamics; Kinematics; motion retargeting; Optimization; Task analysis; Training,Abstract,TRUE,
Scopus,journalPaper,2024,"A Comparison of Virtual Reality Menu Archetypes: Raycasting, Direct Input, and Marking Menus",TVCG - Transactions on Visualization and Computer Graphics,A,"We contribute an analysis of the prevalence and relative performance of archetypal VR menu techniques. An initial survey of 108 menu interfaces in 84 popular commercial VR applications establishes common design characteristics. These characteristics motivate the design of raycast, direct, and marking menu archetypes, and a two-experiment comparison of their relative performance with one and two levels of hierarchy using 8 or 24 items. With a single-level menu, direct input is the fastest interaction technique in general, and is unaffected by number of items. With a two-level hierarchical menu, marking is fastest regardless of item number. Menus using raycasting, the most common menu interaction technique, were among the slowest of the tested menus but were rated most consistently usable. Using the combined results, we provide design and implementation recommendations with applications to general VR menu design. IEEE",direct input; interaction techniques; marking menu; menus; raycasting; virtual reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,AudioGest: Gesture-based Interaction for Virtual Reality using Audio Devices,TVCG - Transactions on Visualization and Computer Graphics,A,"Current virtual reality (VR) system takes gesture interaction based on camera, handle and touch screen as one of the mainstream interaction methods, which can provide accurate gesture input for it. However, limited by application forms and the volume of devices, these methods cannot extend the interaction area to such surfaces as walls and tables. To address the above challenge, we propose AudioGest, a portable, plug-and-play system that detects the audio signal generated by finger tapping and sliding on the surface through a set of microphone devices without extensive calibration. First, an audio synthesis-recognition pipeline based on micro-contact dynamics simulation is constructed to generate modal audio synthesis from different materials and physical properties. Then the accuracy and effectiveness of the synthetic audio are verified by mixing the synthetic audio with real audio proportionally as the training sets. Finally, a series of desktop office applications are developed to demonstrate the application potential of AudioGest&#x0027;s scalability and versatility in VR scenarios. IEEE",audio synthesis; Data acquisition; Dynamics; gesture interaction; Human computer interaction; Microphones; Pipelines; Rough surfaces; Surface roughness; Training; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"Memory Recall for Data Visualizations in Mixed Reality, Virtual Reality, 3D and 2D",TVCG - Transactions on Visualization and Computer Graphics,A,"This article explores how the ability to recall information in data visualizations depends on the presentation technology. Participants viewed 10 Isotype visualizations on a 2D screen, in 3D, in Virtual Reality (VR) and in Mixed Reality (MR). To provide a fair comparison between the three 3D conditions, we used LIDAR to capture the details of the physical rooms, and used this information to create our textured 3D models. For all environments, we measured the number of visualizations recalled and their order (2D) or spatial location (3D, VR, MR). We also measured the number of syntactic and semantic features recalled. Results of our study show increased recall and greater richness of data understanding in the MR condition. Not only did participants recall more visualizations and ordinal/spatial positions in MR, but they also remembered more details about graph axes and data mappings, and more information about the shape of the data. We discuss how differences in the spatial and kinesthetic cues provided in these different environments could contribute to these results, and reasons why we did not observe comparable performance in the 3D and VR conditions.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Data visualisation; human memory; locomotion; mixed reality; perception; recall; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Stress Assessment for Augmented Reality Applications Based on Head Movement Features,TVCG - Transactions on Visualization and Computer Graphics,A,"Augmented reality is one of the enabling technologies of the upcoming future. Its usage in working and learning scenarios may lead to a better quality of work and training by helping the operators during the most crucial stages of processes. Therefore, the automatic detection of stress during augmented reality experiences can be a valuable support to prevent consequences on people's health and foster the spreading of this technology. In this work, we present the design of a non-invasive stress assessment approach. The proposed system is based on the analysis of the head movements of people wearing a Head Mounted Display while performing stress-inducing tasks. First, we designed a subjective experiment consisting of two stress-related tests for data acquisition. Then, a statistical analysis of head movements has been performed to determine which features are representative of the presence of stress. Finally, a stress classifier based on a combination of Support Vector Machines has been designed and trained. The proposed approach achieved promising performances thus paving the way for further studies in this research direction.  © 1995-2012 IEEE.",Augmented reality; machine learning classifier; stress detection,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Bimanual Ultrasound Mid-Air Haptics for Virtual Reality Manipulation,TVCG - Transactions on Visualization and Computer Graphics,A,"The ability to manipulate and physically feel virtual objects without any real object being present and without equipping the user has been a long-standing goal in virtual reality (VR). Emerging ultrasound mid-air haptics (UMH) technology could potentially address this challenge, as it enables remote tactile stimulation of unequipped users. However, to date, UMH has received limited attention in the field of haptic exploration and manipulation in virtual environments. Existing work has primarily focused on interactions requiring a single hand and thus the delivery of unimanual haptic feedback. Despite being fundamental to a large part of haptic interactions with our environments, bimanual tasks have rarely been studied in the field of UMH interaction in VR. In this paper, we propose the use of non-coplanar mid-air haptic devices for providing simultaneous tactile feedback to both hands during bimanual VR manipulation. We discuss coupling schemes and haptic rendering algorithms for providing bimanual haptic feedback in bimanual interactions with virtual environments. We then present two human participant studies, assessing the benefits of bimanual ultrasound haptic feedback in a two-handed grasping and holding task and in a shape exploration task. Results suggest that the use of multiple non-coplanar UMH devices could be an interesting approach for enriching unencumbered haptic manipulation in virtual environments. IEEE",bimanual; Grasping; Haptic interfaces; Mid-air haptics; Shape; Task analysis; Three-dimensional displays; Ultrasonic imaging; ultrasound; Virtual environments; VR,Title_Abstract,TRUE,
Scopus,journalPaper,2024,F-RDW: Redirected Walking With Forecasting Future Position,TVCG - Transactions on Visualization and Computer Graphics,A,"In order to serve better VR experiences to users, existing predictive methods of Redirected Walking (RDW) exploit future information to reduce the number of reset occurrences. However, such methods often impose a precondition during deployment, either in the virtual environment&#x0027;s layout or the user&#x0027;s walking direction, which constrains its universal applications. To tackle this challenge, we propose a mechanism <italic>F-RDW</italic> that is twofold: (1) forecasts the future information of a user in the virtual space without any assumptions by using the conventional method, and (2) fuse this information while maneuvering existing RDW methods. The backbone of the first step is an LSTM-based model that ingests the user&#x0027;s spatial and eye-tracking data to predict the user&#x0027;s future position in the virtual space, and the following step feeds those predicted values into existing RDW methods (such as MPCRed, S2C, TAPF, and ARC) while respecting their internal mechanism in applicable ways. The results of our simulation test and user study demonstrate the significance of future information when using RDW in small physical spaces or complex environments. We prove that the proposed mechanism significantly reduces the number of resets and increases the traveled distance between resets, hence augmenting the redirection performance of all RDW methods explored in this work. Our project and dataset are available at <uri>https://github.com/YonseiCGnA-VR/F-RDW.</uri> IEEE",Eye tracking; Gaze tracking; Layout; Legged locomotion; path planning; path prediction; Prediction algorithms; Predictive models; redirected walking; Spatial databases; Virtual environments; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,Visual Guidance for User Placement in Avatar-Mediated Telepresence between Dissimilar Spaces,TVCG - Transactions on Visualization and Computer Graphics,A,"Rapid advances in technology gradually realize immersive mixed-reality (MR) telepresence between distant spaces. This paper presents a novel visual guidance system for avatar-mediated telepresence, directing users to optimal placements that facilitate the clear transfer of gaze and pointing contexts through remote avatars in dissimilar spaces, where the spatial relationship between the remote avatar and the interaction targets may differ from that of the local user. Representing the spatial relationship between the user/avatar and interaction targets with angle-based interaction features, we assign recommendation scores of sampled local placements as their maximum feature similarity with remote placements. These scores are visualized as color-coded 2D sectors to inform the users of better placements for interaction with selected targets. In addition, virtual objects of the remote space are overlapped with the local space for the user to better understand the recommendations. We examine whether the proposed score measure agrees with the actual user perception of the partner's interaction context and find a score threshold for recommendation through user experiments in virtual reality (VR). A subsequent user study in VR investigates the effectiveness and perceptual overload of different combinations of visualizations. Finally, we conduct a user study in an MR telepresence scenario to evaluate the effectiveness of our method in real-world applications.  © 1995-2012 IEEE.",Mixed reality; telepresence; virtual avatar; visualization,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in Dynamic Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"It is typically challenging for visual or visual-inertial odometry systems to handle the problems of dynamic scenes and pure rotation. In this work, we design a novel visual-inertial odometry (VIO) system called RD-VIO to handle both of these two problems. First, we propose an IMU-PARSAC algorithm which can robustly detect and match keypoints in a two-stage process. In the first state, landmarks are matched with new keypoints using visual and IMU measurements. We collect statistical information from the matching and then guide the intra-keypoint matching in the second stage. Second, to handle the problem of pure rotation, we detect the motion type and adapt the deferred-triangulation technique during the data-association process. We make the pure-rotational frames into the special subframes. When solving the visual-inertial bundle adjustment, they provide additional constraints to the pure-rotational motion. We evaluate the proposed VIO system on public datasets and online comparison. Experiments show the proposed RD-VIO has obvious advantages over other methods in dynamic environments.  © 2024 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",Degenerate motion; dynamic environment; RANSAC; SLAM; VIO,Title,TRUE,
Scopus,journalPaper,2024,Impact of Socio-Demographic Attributes and Mutual Gaze of Virtual Humans on Users' Visual Attention and Collision Avoidance in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"This study investigated the extent that the non-verbal behaviors of virtual humans (VHs) and their socio-demographic attributes altered users' collision avoidance behaviors in Virtual Reality (VR). Users interacted with VHs representing different levels of ethnicities and gender, exhibiting different conditions of physical movement, and gaze behaviors. The VHs were depicted in three major ethnic conditions namely Asian, Caucasian, and Black. The physical movement states of the VHs were either static in the path of the user or walking toward the user in the opposite direction. The non-verbal gaze behavior of the VHs was either direct gaze or averted gaze. We used an HTC Vive tracking system to track users' performing real walking while we collected objective measures (i,e., continuous gaze, fixation gaze, clearance distance, and travel length), and subjective variables (i.e., game experiences and social presence). The results showed that the ethnicity of the VHs significantly impacted the gaze behavior of the users, and the gender of the VHs affected the user avoidance movement and their reciprocal gaze behavior. Our results revealed that users' physical movement, gaze behaviors, and collision avoidance were moderated by the VHs' perceived ethnicity, gender, and gaze behaviors. Understanding the impact of the socio-demographics attributes of VHs and their gaze behavior on users' collision avoidance is critical for applications in which users are navigating through virtual traffic, crowd, and other inter-personal simulations.  © 1995-2012 IEEE.",Collision avoidance; human-computer interaction; user studies; virtual humans and crowds; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Comparative Analysis of Interactive Modalities for Intuitive Endovascular Interventions,TVCG - Transactions on Visualization and Computer Graphics,A,"Endovascular intervention is a minimally invasive method for treating cardiovascular diseases. Although fluoroscopy, known for real-time catheter visualization, is commonly used, it exposes patients and physicians to ionizing radiation and lacks depth perception due to its 2D nature. To address these limitations, a study was conducted using teleoperation and 3D visualization techniques. This <italic>in-vitro</italic> study involved the use of a robotic catheter system and aimed to evaluate user performance through both subjective and objective measures. The focus was on determining the most effective modes of interaction. Three interactive modes for guiding robotic catheters were compared in the study: 1) Mode GM, using a gamepad for control and a standard 2D monitor for visual feedback; 2) Mode GH, with a gamepad for control and HoloLens providing 3D visualization; and 3) Mode HH, where HoloLens serves as both control input and visualization device. Mode GH outperformed other modalities in subjective metrics, except for mental demand. It exhibited a median tracking error of 4.72 mm, a median targeting error of 1.01 mm, a median duration of 82.34 s, and a median natural logarithm of dimensionless squared jerk of 40.38 in the <italic>in-vitro</italic> study. Mode GH showed 8.5&#x0025;, 4.7&#x0025;, 6.5&#x0025;, and 3.9&#x0025; improvements over Mode GM and 1.5&#x0025;, 33.6&#x0025;, 34.9&#x0025;, and 8.1&#x0025; over Mode HH for tracking error, targeting error, duration, and dimensionless squared jerk, respectively. To sum up, the user study emphasizes the potential benefits of employing HoloLens for enhanced 3D visualization in catheterization. The user study also illustrates the advantages of using a gamepad for catheter teleoperation, including user-friendliness and passive haptic feedback, compared to HoloLens. To further gauge the potential of using a more traditional joystick as a control input device, an additional study utilizing the Haption VirtuoseTM robot was conducted. It reveals the potential for achieving smoother trajectories, with a 38.9&#x0025; reduction in total path length compared to a gamepad, potentially due to its larger range of motion and single-handed control. Authors",Augmented Reality; Catheter Navigation; Catheters; Endovascular Intervention; Input devices; Navigation; Resists; Robotic Catheter; Robots; Three-dimensional displays; User study; Visualization,Keywords,TRUE,
Scopus,journalPaper,2024,Simple and Efficient? Evaluation of Transitions for Task-Driven Cross-Reality Experiences,TVCG - Transactions on Visualization and Computer Graphics,A,"The inquiry into the impact of diverse transitions between cross-reality environments on user experience remains a compelling research endeavor. Existing work often offers fragmented perspectives on various techniques or confines itself to a singular segment of the reality-virtuality spectrum, be it virtual reality or augmented reality. This study embarks on bridging this knowledge gap by systematically assessing the effects of six prevalent transitions while users remain immersed in tasks spanning both virtual and physical domains. In particular, we investigate the effect of different transitions while the user is continuously engaged in a demanding task instead of purely focusing on a given transition. As a preliminary step, we evaluate these six transitions within the realm of pure virtual reality to establish a baseline. Our findings reveal a clear preference among participants for brief and efficient transitions in a task-driven experience, instead of transitions that prioritize interactivity and continuity. Subsequently, we extend our investigation into a cross-reality context, encompassing transitions between virtual and physical environments. Once again, our results underscore the prevailing preference for concise and effective transitions. Furthermore, our research offers intriguing insights about the potential mitigation of visual incoherence between virtual and augmented reality environments by utilizing different transitions.  © 1995-2012 IEEE.",Artificial; augmented; graphical user interfaces; user interfaces; virtual realities,Abstract,TRUE,
Scopus,journalPaper,2024,DRCmpVis: Visual Comparison of Physical Targets in Mobile Diminished and Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Numerous physical objects in our daily lives are grouped or ranked according to a stereotyped presentation style. For example, in a library, books are typically grouped and ranked based on classification numbers. However, for better comparison, we often need to re-group or re-rank the books using additional attributes such as ratings, publishers, comments, publication years, keywords, prices, etc., or a combination of these factors. In this article, we propose a novel mobile DR/MR-based application framework named DRCmpVis to achieve in-context multi-attribute comparisons of physical objects with text labels or textual information. The physical objects are scanned in the real world using mobile cameras. All scanned objects are then segmented and labeled by a convolutional neural network and replaced (diminished) by their virtual avatars in a DR environment. We formulate three visual comparison strategies, including filtering, re-grouping, and re-ranking, which can be intuitively, flexibly, and seamlessly performed on their avatars. This approach avoids breaking the original layouts of the physical objects. The computation resources in virtual space can be fully utilized to support efficient object searching and multi-attribute visual comparisons. We demonstrate the usability, expressiveness, and efficiency of DRCmpVis through a user study, NASA TLX assessment, quantitative evaluation, and case studies involving different scenarios.  © 1995-2012 IEEE.",Diminished reality; mixed reality; mobile environment; visual comparison,Title_Keywords,TRUE,
Scopus,journalPaper,2024,MARR : A Multi-Agent Reinforcement Resetter for Redirected Walking,TVCG - Transactions on Visualization and Computer Graphics,A,"The reset technique of Redirected Walking (RDW) forcibly reorients the user&#x0027;s direction overtly to avoid collisions with boundaries, obstacles, or other users in the physical space. However, excessive resetting can decrease the user&#x0027;s sense of immersion and presence. Several RDW studies have been conducted to address this issue. Among them, much research has been done on reset techniques that reduce the number of resets by devising reset direction rules or optimizing them for a given environment. However, existing optimization studies on reset techniques have mainly focused on a single-user environment. In a multi-user environment, the dynamic movement of other users and static obstacles in the physical space increase the possibility of resetting. In this study, we propose Multi-Agent Reinforcement Resetter (MARR), which resets the user taking into account both physical obstacles and multi-user movement to minimize the number of resets. MARR is trained using multi-agent reinforcement learning to determine the optimal reset direction in different environments. This approach allows MARR to effectively account for different environmental contexts, including arbitrary physical obstacles and the dynamic movements of other users in the same physical space. We compared MARR to other reset technologies through simulation tests and user studies, and found that MARR outperformed the existing methods. MARR improved performance by learning the optimal reset direction for each subtle technique used in training. MARR has the potential to be applied to new subtle techniques proposed in the future. Overall, our study confirmed that MARR is an effective reset technique in multi-user environments. Authors",Aerospace electronics; Legged locomotion; Optimization; Redirected walking; Reinforcement learning; Reinforcement learning; Resetting; Resists; Space exploration; Training; Virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,360° Stereo Image Composition With Depth Adaption,TVCG - Transactions on Visualization and Computer Graphics,A,"360° images and videos have become an economic and popular way to provide VR experiences using real-world content. However, the manipulation of the stereo panoramic content remains less explored. In this article, we focus on the 360° image composition problem, and develop a solution that can take an object from a stereo image pair and insert it at a given 3D position in a target stereo panorama, with well-preserved geometry information. Our method uses recovered 3D point clouds to guide the composited image generation. More specifically, we observe that using only a one-off operation to insert objects into equirectangular images will never produce satisfactory depth perception and generate ghost artifacts when users are watching the result from different view directions. Therefore, we propose a novel per-view projection method that segments the object in 3D spherical space with the stereo camera pair facing in that direction. A deep depth densification network is proposed to generate depth guidance for the stereo image generation of each view segment according to the desired position and pose of the inserted object. We finally combine the synthesized view segments and blend the objects into the target stereo 360° scene. A user study demonstrates that our method can provide good depth perception and removes ghost artifacts. The per-view solution is a potential paradigm for other content manipulation methods for 360° images and videos.  © 1995-2012 IEEE.",Image composition; image synthesis; stereoscopic panoramic image; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,iMetaTown: A Metaverse System with Multiple Interactive Functions Based on Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"This work aims to pioneer the development of a real-time interactive and immersive Metaverse Human-Computer Interaction (HCI) system leveraging Virtual Reality (VR). The system incorporates a three-dimensional (3D) face reconstruction method, grounded in weakly supervised learning, to enhance player-player interactions within the Metaverse. The proposed method, two-dimensional (2D) face images, are effectively employed in a 2D Self-Supervised Learning (2DASL) approach, significantly optimizing 3D model learning outcomes and improving the quality of 3D face alignment in HCI systems. The work outlines the functional modules of the system, encompassing user interactions such as hugs and handshakes and communication through voice and text via blockchain. Solutions for managing multiple simultaneous online users are presented. Performance evaluation of the HCI system in a 3D reconstruction scene indicates that the 2DASL face reconstruction method achieves noteworthy results, enhancing the system&#x0027;s interaction capabilities by aiding 3D face modeling through 2D face images. The experimental system achieves a maximum processing speed of 18 frames of image data on a personal computer, meeting real-time processing requirements. User feedback regarding social acceptance, action interaction usability, emotions, and satisfaction with the VR interactive system reveals consistently high scores. The designed VR HCI system exhibits outstanding performance across diverse applications. IEEE",3D Interaction; Animation; Avatar; Blockcha; Blockchains; Face recognition; Faces; Human-Computer Interaction; Image reconstruction; Metaverse; Metaverse; Three-dimensional displays; Vectors; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Multiple Monitors or Single Canvas&#x003F; Evaluating Window Management and Layout Strategies on Virtual Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual displays enabled through head-worn augmented reality have unique characteristics that can yield extensive amounts of screen space. Existing research has shown that increasing the space on a computer screen can enhance usability. Since virtual displays offer the unique ability to present content without rigid physical space constraints, they provide various new design possibilities. Therefore, we must understand the trade-offs of layout choices when structuring that space. We propose a single Canvas approach that eliminates boundaries from traditional multi-monitor approaches and instead places windows in one large, unified space. Our user study compared this approach against a multi-monitor setup, and we considered both purely virtual systems and hybrid systems that included a physical monitor. We looked into usability factors such as performance, accuracy, and overall window management. Results show that Canvas displays can cause users to compact window layouts more than multiple monitors with snapping behavior, even though such optimizations may not lead to longer window management times. We did not find conclusive evidence of either setup providing a better user experience. Multi-Monitor displays offer quick window management with snapping and a structured layout through subdivisions. However, Canvas displays allow for more control in placement and size, lowering the amount of space used and, thus, head rotation. Multi-Monitor benefits were more prominent in the hybrid configuration, while the Canvas display was more beneficial in the purely virtual configuration. IEEE",Augmented Reality; Canvas; Layout; Layout; Monitoring; Multiple Monitors; Operating systems; Task analysis; Two-dimensional displays; User experience; Virtual Displays; Visualization; Window Management,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Design and Evaluation of Controller-Based Raycasting Methods for Efficient Alphanumeric and Special Character Entry in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Alphanumeric and special characters are essential during text entry. Text entry in virtual reality (VR) is usually performed on a virtual Qwerty keyboard to minimize the need to learn new layouts. As such, entering capitals, symbols, and numbers in VR is often a direct migration from a physical/touchscreen Qwerty keyboard-that is, using the mode-switching keys to switch between different types of characters and symbols. However, there are inherent differences between a keyboard in VR and a physical/touchscreen keyboard, and as such, a direct adaptation of mode-switching via switch keys may not be suitable for VR. The high flexibility afforded by VR opens up more possibilities for entering alphanumeric and special characters using the Qwerty layout. In this work, we designed two controller-based raycasting text entry methods for alphanumeric and special characters input (Layer-ButtonSwitch and Key-ButtonSwitch) and compared them with two other methods (Standard Qwerty Keyboard and Layer-PointSwitch) that were derived from physical and soft Qwerty keyboards. We explored the performance and user preference of these four methods via two user studies (one short-Term and one prolonged use), where participants were instructed to input text containing alphanumeric and special characters. Our results show that Layer-ButtonSwitch led to the highest statistically significant performance, followed by Key-ButtonSwitch and Standard Qwerty Keyboard, while Layer-PointSwitch had the slowest speed. With continuous practice, participants' performance using Key-ButtonSwitch reached that of Layer-ButtonSwitch. Further, the results show that the key-level layout used in Key-ButtonSwitch led users to parallel mode switching and character input operations because this layout showed all characters on one layer. We distill three recommendations from the results that can help guide the design of text entry techniques for alphanumeric and special characters in VR.  © 1995-2012 IEEE.",alphanumeric and special character entry; keyboard layout; mode-switching; text entry; user study; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields,TVCG - Transactions on Visualization and Computer Graphics,A,"Text-driven 3D scene generation is widely applicable to video gaming, film industry, and metaverse applications that have a large demand for 3D scenes. However, existing text-to-3D generation methods are limited to producing 3D objects with simple geometries and dreamlike styles that lack realism. In this work, we present Text2NeRF, which is able to generate a wide range of 3D scenes with complicated geometric structures and high-fidelity textures purely from a text prompt. To this end, we adopt NeRF as the 3D representation and leverage a pre-trained text-to-image diffusion model to constrain the 3D reconstruction of the NeRF to reflect the scene description. Specifically, we employ the diffusion model to infer the text-related image as the content prior and use a monocular depth estimation method to offer the geometric prior. Both content and geometric priors are utilized to update the NeRF model. To guarantee textured and geometric consistency between different views, we introduce a progressive scene inpainting and updating strategy for novel view synthesis of the scene. Our method requires no additional training data but only a natural language description of the scene as the input. Extensive experiments demonstrate that our Text2NeRF outperforms existing methods in producing photo-realistic, multi-view consistent, and diverse 3D scenes from a variety of natural language prompts. Our code and model are available at https://github.com/eckertzhang/Text2NeRF.  © 1995-2012 IEEE.",3D scene generation; depth alignment; NeRF; scene inpainting; Text-to-3D,Abstract,TRUE,
Scopus,journalPaper,2024,VisTellAR: Embedding Data Visualization to Short-form Videos Using Mobile Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"With the rise of short-form video platforms and the increasing availability of data, we see the potential for people to share short-form videos embedded with data in situ (e.g., daily steps when running) to increase the credibility and expressiveness of their stories. However, creating and sharing such videos in situ is challenging since it involves multiple steps and skills (e.g., data visualization creation and video editing), especially for amateurs. By conducting a formative study (N=10) using three design probes, we collected the motivations and design requirements. We then built VisTellAR, a mobile AR authoring tool, to help amateur video creators embed data visualizations in short-form videos in situ. A two-day user study shows that participants (N=12) successfully created various videos with data visualizations in situ and they confirmed the ease of use and learning. AR pre-stage authoring was useful to assist people in setting up data visualizations in reality with more designs in camera movements and interaction with gestures and physical objects to storytelling. IEEE",augmented reality; data visualization; Personal data; short-form video; storytelling,Title_Keywords,TRUE,
Scopus,journalPaper,2024,Point Cloud Completion: A Survey,TVCG - Transactions on Visualization and Computer Graphics,A,"Point cloud completion is the task of producing a complete 3D shape given an input of a partial point cloud. It has become a vital process in 3D computer graphics, vision and applications such as autonomous driving, robotics, and augmented reality. These applications often rely on the presence of a complete 3D representation of the environment. Over the past few years, many completion algorithms have been proposed and a substantial amount of research has been carried out. However, there are not many in-depth surveys that summarise the research progress in such a way that allows users to make an informed choice of what algorithms to employ given the type of data they have, the end result they want, the challenges they may face and the possible strategies they could use. In this study, we present a comprehensive survey and classification of articles on point cloud completion untill August 2023 based on the strategies, techniques, inputs, outputs, and network architectures. We will also cover datasets, evaluation methods, and application areas in point cloud completion. Finally, we discuss challenges faced by the research community and future research directions. © 1995-2012 IEEE.",computer vision; deep learning; Point cloud completion,Abstract,TRUE,
Scopus,journalPaper,2024,ARGUS: Visualization of AI-Assisted Task Guidance in AR,TVCG - Transactions on Visualization and Computer Graphics,A,"The concept of augmented reality (AR) assistants has captured the human imagination for decades, becoming a staple of modern science fiction. To pursue this goal, it is necessary to develop artificial intelligence (AI)-based methods that simultaneously perceive the 3D environment, reason about physical tasks, and model the performer, all in real-time. Within this framework, a wide variety of sensors are needed to generate data across different modalities, such as audio, video, depth, speech, and time-of-flight. The required sensors are typically part of the AR headset, providing performer sensing and interaction through visual, audio, and haptic feedback. AI assistants not only record the performer as they perform activities, but also require machine learning (ML) models to understand and assist the performer as they interact with the physical world. Therefore, developing such assistants is a challenging task. We propose ARGUS, a visual analytics system to support the development of intelligent AR assistants. Our system was designed as part of a multi-year-long collaboration between visualization researchers and ML and AR experts. This co-design process has led to advances in the visualization of ML in AR. Our system allows for online visualization of object, action, and step detection as well as offline analysis of previously recorded AR sessions. It visualizes not only the multimodal sensor data streams but also the output of the ML models. This allows developers to gain insights into the performer activities as well as the ML models, helping them troubleshoot, improve, and fine-tune the components of the AR assistant. © 1995-2012 IEEE.",Application Motivated Visualization; AR/VR/Immersive; Data Models; Image and Video Data; Temporal Data,Abstract,TRUE,
Scopus,journalPaper,2024,Influence of Scenarios and Player Traits on Flow in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Many studies have investigated how interpersonal differences between users influence their experience in Virtual Reality (VR) and it is now well recognized that user's subjective experiences and responses to the same VR environment can vary widely. In this study, we focus on player traits, which correspond to users' preferences for game mechanics, arguing that players react differently when experiencing VR scenarios. We developed three scenarios in the same VR environment that rely on different game mechanics, and evaluate the influence of the scenarios, the player traits and the time of practice of the VR environment on users' perceived flow. Our results show that 1) the type of scenario has an impact on specific dimensions of flow; 2) the scenarios have different effects on flow depending on the order they are performed, the flow preconditions being stronger when performed at last; 3) almost all dimensions of flow are influenced by the player traits, these influences depending on the scenario, 4) the Aesthetic trait has the most influences in the three scenarios. We finally discuss the findings and limitations of the present study that we believe have strong implications for the design of scenarios in VR experiences.  © 1995-2012 IEEE.",flow; game mechanics; player traits; scenario; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,MeTACAST: Target- and Context-Aware Spatial Selection in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"We propose three novel spatial data selection techniques for particle data in VR visualization environments. They are designed to be target- and context-aware and be suitable for a wide range of data features and complex scenarios. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters - with all of them facilitating post-selection threshold adjustment. These techniques allow users to precisely select those regions of space for further exploration - with simple and approximate 3D pointing, brushing, or drawing input - using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes. These new techniques are evaluated in a controlled experiment and compared with the Baseline method, a region-based 3D painting selection. Our results indicate that our techniques are effective in handling a wide range of scenarios and allow users to select data based on their comprehension of crucial features. Furthermore, we analyze the attributes, requirements, and strategies of our spatial selection methods and compare them with existing state-of-the-art selection methods to handle diverse data features and situations. Based on this analysis we provide guidelines for choosing the most suitable 3D spatial selection techniques based on the interaction environment, the given data characteristics, or the need for interactive post-selection threshold adjustment. © 1995-2012 IEEE.",immersive analytics; Spatial selection; target-aware and context-aware interaction for visualization; virtual reality (VR),Keywords,TRUE,
Scopus,journalPaper,2024,"2D, 2.5D, or 3D? An Exploratory Study on Multilayer Network Visualisations in Virtual Reality",TVCG - Transactions on Visualization and Computer Graphics,A,"Relational information between different types of entities is often modelled by a multilayer network (MLN) - a network with subnetworks represented by layers. The layers of an MLN can be arranged in different ways in a visual representation, however, the impact of the arrangement on the readability of the network is an open question. Therefore, we studied this impact for several commonly occurring tasks related to MLN analysis. Additionally, layer arrangements with a dimensionality beyond 2D, which are common in this scenario, motivate the use of stereoscopic displays. We ran a human subject study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer arrangements. The study employs six analysis tasks that cover the spectrum of an MLN task taxonomy, from path finding and pattern identification to comparisons between and across layers. We found no clear overall winner. However, we explore the task-to-arrangement space and derive empirical-based recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for MLNs. © 1995-2012 IEEE.",CompSystems; Guidelines; HumanQuant; Network; VisDesign,Title_Abstract,TRUE,
Scopus,journalPaper,2024,Design Patterns for Situated Visualization in Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Situated visualization has become an increasingly popular research area in the visualization community, fueled by advancements in augmented reality (AR) technology and immersive analytics. Visualizing data in spatial proximity to their physical referents affords new design opportunities and considerations not present in traditional visualization, which researchers are now beginning to explore. However, the AR research community has an extensive history of designing graphics that are displayed in highly physical contexts. In this work, we leverage the richness of AR research and apply it to situated visualization. We derive design patterns which summarize common approaches of visualizing data in situ. The design patterns are based on a survey of 293 papers published in the AR and visualization communities, as well as our own expertise. We discuss design dimensions that help to describe both our patterns and previous work in the literature. This discussion is accompanied by several guidelines which explain how to apply the patterns given the constraints imposed by the real world. We conclude by discussing future research directions that will help establish a complete understanding of the design of situated visualization, including the role of interactivity, tasks, and workflows. © 1995-2012 IEEE.",Augmented reality; design patterns; design space; immersive analytics; situated visualization,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,HazARdSnap: Gazed-Based Augmentation Delivery for Safe Information Access While Cycling,TVCG - Transactions on Visualization and Computer Graphics,A,"During cycling activities, cyclists often monitor a variety of information such as heart rate, distance, and navigation using a bike-mounted phone or cyclocomputer. In many cases, cyclists also ride on sidewalks or paths that contain pedestrians and other obstructions such as potholes, so monitoring information on a bike-mounted interface can slow the cyclist down or cause accidents and injury. In this article, we present HazARdSnap, an augmented reality-based information delivery approach that improves the ease of access to cycling information and at the same time preserves the user's awareness of hazards. To do so, we implemented real-time outdoor hazard detection using a combination of computer vision and motion and position data from a head mounted display (HMD). We then developed an algorithm that snaps information to detected hazards when they are also viewed so that users can simultaneously view both rendered virtual cycling information and the real-world cues such as depth, position, time to hazard, and speed that are needed to assess and avoid hazards. Results from a study with 24 participants that made use of real-world cycling and virtual hazards showed that both HazARdSnap and forward-fixed augmented reality (AR) user interfaces (UIs) can effectively help cyclists access virtual information without having to look down, which resulted in fewer collisions (51% and 43% reduction compared to baseline, respectively) with virtual hazards.  © 1995-2012 IEEE.",Augmented reality; cycling; eye tracking; object detection and user interfaces; safety,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,VisionCoach: Design and Effectiveness Study on VR Vision Training for Basketball Passing,TVCG - Transactions on Visualization and Computer Graphics,A,"Vision Training is important for basketball players to effectively search for teammates who has wide-open opportunities to shoot, observe the defenders around the wide-open teammates and quickly choose a proper way to pass the ball to the most suitable one. We develop an immersive virtual reality (VR) system called VisionCoach to simulate the player's viewing perspective and generate three designed systematic vision training tasks to benefit the cultivating procedure. By recording the player's eye gazing and dribbling video sequence, the proposed system can analyze the vision-related behavior to understand the training effectiveness. To demonstrate the proposed VR training system can facilitate the cultivation of vision ability, we recruited 14 experienced players to participate in a 6-week between-subject study, and conducted a study by comparing the most frequently used 2D vision training method called Vision Performance Enhancement (VPE) program with the proposed system. Qualitative experiences and quantitative training results are reported to show that the proposed immersive VR training system can effectively improve player's vision ability in terms of gaze behavior and dribbling stability. Furthermore, training in the VR-VisionCoach Condition can transfer the learned abilities to real scenario more easily than training in the 2D-VPE Condition.  © 1995-2012 IEEE.",basketball VR; computer-aided training; Sports VR; vision training,Abstract,TRUE,
Scopus,journalPaper,2024,Submerse: Visualizing Storm Surge Flooding Simulations in Immersive Display Ecologies,TVCG - Transactions on Visualization and Computer Graphics,A,"We present Submerse, an end-to-end framework for visualizing flooding scenarios on large and immersive display ecologies. Specifically, we reconstruct a surface mesh from input flood simulation data and generate a to-scale 3D virtual scene by incorporating geographical data such as terrain, textures, buildings, and additional scene objects. To optimize computation and memory performance for large simulation datasets, we discretize the data on an adaptive grid using dynamic quadtrees and support level-of-detail based rendering. Moreover, to provide a perception of flooding direction for a time instance, we animate the surface mesh by synthesizing water waves. As interaction is key for effective decision-making and analysis, we introduce two novel techniques for flood visualization in immersive systems: (1) an automatic scene-navigation method using optimal camera viewpoints generated for marked points-of-interest based on the display layout, and (2) an AR-based focus+context technique using an aux display system. Submerse is developed in collaboration between computer scientists and atmospheric scientists. We evaluate the effectiveness of our system and application by conducting workshops with emergency managers, domain experts, and concerned stakeholders in the Stony Brook Reality Deck, an immersive gigapixel facility, to visualize a superstorm flooding scenario in New York City.  © 1995-2012 IEEE.",Camera navigation; flooding simulation; immersive visualization; mixed reality,Keywords,TRUE,
Scopus,journalPaper,2024,Improving Depth Perception in Immersive Media Devices by Addressing Vergence-Accommodation Conflict,TVCG - Transactions on Visualization and Computer Graphics,A,"Recently, immersive media devices have seen a boost in popularity. However, many problems still remain. Depth perception is a crucial part of how humans behave and interact with their environment. Convergence and accommodation are two physiological mechanisms that provide important depth cues. However, when humans are immersed in virtual environments, they experience a mismatch between these cues. This mismatch causes users to feel discomfort while also hindering their ability to fully perceive object distances. To address the conflict, we have developed a technique that encompasses inverse blurring into immersive media devices. For the inverse blurring, we utilize the classical Wiener deconvolution approach by proposing a novel technique that is applied without the need for an eye-tracker and implemented in a commercial immersive media device. The technique's ability to compensate for the vergence-accommodation conflict was verified through two user studies aimed at reaching and spatial awareness, respectively. The two studies yielded a statistically significant 36% and 48% error reduction in user performance to estimate distances, respectively. Overall, the work done demonstrates how visual stimuli can be modified to allow users to achieve a more natural perception and interaction with the virtual environment.  © 1995-2012 IEEE.",Depth perception; depth-of-field; immersive media; inverse blurring; reaching task; space-variant technique; vergence-accommodation conflict; virtual reality; wiener deconvolution,Keywords,TRUE,
Scopus,journalPaper,2024,VisTA-LIVE: A Visualization Tool for Assessment of Laboratories in Virtual Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"A Virtual Reality Laboratory (VR Lab) experiment refers to an experiment session that is being conducted in the virtual environment through Virtual Reality (VR) and aims to deliver procedural knowledge to students similar to that in a physical lab environment. While VR Lab is becoming more popular among education institutes as a learning tool for students, existing designs are mostly considered from a student's perspective. Instructors could only receive limited information on how the students are performing and could not provide useful feedback to aid the students' learning and evaluate their performance. This motivated us to create VisTA-LIVE: a Visualization Tool for Assessment of Laboratories In Virtual Environments. In this article, we present in detail the design thinking approach that was applied to create VisTA-LIVE. The tool is deployed in an Extended Reality (XR) environment, and we report the evaluation results with domain experts and discuss issues related to monitoring and assessing a live VR lab session which lay potential directions for future work. We also describe how the resulting design of the tool could be used as a reference for other education developers who wish to develop similar applications.  © 1995-2012 IEEE.",computer-assisted instruction; Education technology; extended reality; virtual laboratory,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,VoxAR: Adaptive Visualization of Volume Rendered Objects in Optical See-Through Augmented Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"We present VoxAR, a method to facilitate an effective visualization of volume-rendered objects in optical see-through head-mounted displays (OST-HMDs). The potential of augmented reality (AR) to integrate digital information into the physical world provides new opportunities for visualizing and interpreting scientific data. However, a limitation of OST-HMD technology is that rendered pixels of a virtual object can interfere with the colors of the real-world, making it challenging to perceive the augmented virtual information accurately. We address this challenge in a two-step approach. First, VoxAR determines an appropriate placement of the volume-rendered object in the real-world scene by evaluating a set of spatial and environmental objectives, managed as user-selected preferences and pre-defined constraints. We achieve a real-time solution by implementing the objectives using a GPU shader language. Next, VoxAR adjusts the colors of the input transfer function (TF) based on the real-world placement region. Specifically, we introduce a novel optimization method that adjusts the TF colors such that the resulting volume-rendered pixels are discernible against the background and the TF maintains the perceptual mapping between the colors and data intensity values. Finally, we present an assessment of our approach through objective evaluations and subjective user studies. © 1995-2012 IEEE.",Adaptive visualization; augmented reality; situated visualization; volume rendering,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Handling Non-Visible Referents in Situated Visualizations,TVCG - Transactions on Visualization and Computer Graphics,A,"Situated visualizations are a type of visualization where data is presented next to its physical referent (i.e., the physical object, space, or person it refers to), often using augmented-reality displays. While situated visualizations can be beneficial in various contexts and have received research attention, they are typically designed with the assumption that the physical referent is visible. However, in practice, a physical referent may be obscured by another object, such as a wall, or may be outside the user's visual field. In this paper, we propose a conceptual framework and a design space to help researchers and user interface designers handle non-visible referents in situated visualizations. We first provide an overview of techniques proposed in the past for dealing with non-visible objects in the areas of 3D user interfaces, 3D visualization, and mixed reality. From this overview, we derive a design space that applies to situated visualizations and employ it to examine various trade-offs, challenges, and opportunities for future research in this area. © 1995-2012 IEEE.",AR/VR/Immersive; Frameworks; Mobile; Models; Specialized Input/Display Hardware; Taxonomy; Theory,Abstract,TRUE,
Scopus,journalPaper,2024,RL-L: A Deep Reinforcement Learning Approach Intended for AR Label Placement in Dynamic Scenarios,TVCG - Transactions on Visualization and Computer Graphics,A,"Labels are widely used in augmented reality (AR) to display digital information. Ensuring the readability of AR labels requires placing them in an occlusion-free manner while keeping visual links legible, especially when multiple labels exist in the scene. Although existing optimization-based methods, such as force-based methods, are effective in managing AR labels in static scenarios, they often struggle in dynamic scenarios with constantly moving objects. This is due to their focus on generating layouts optimal for the current moment, neglecting future moments and leading to sub-optimal or unstable layouts over time. In this work, we present RL-Label, a deep reinforcement learning-based method intended for managing the placement of AR labels in scenarios involving moving objects. RL-Label considers both the current and predicted future states of objects and labels, such as positions and velocities, as well as the user's viewpoint, to make informed decisions about label placement. It balances the trade-offs between immediate and long-term objectives. We tested RL-Label in simulated AR scenarios on two real-world datasets, showing that it effectively learns the decision-making process for long-term optimization, outperforming two baselines (i.e., no view management and a force-based method) by minimizing label occlusions, line intersections, and label movement distance. Additionally, a user study involving 18 participants indicates that, within our simulated environment, RL-Label excels over the baselines in aiding users to identify, compare, and summarize data on labels in dynamic scenes. © 1995-2012 IEEE.",Augmented Reality; Dynamic Scenarios; Label Placement; Reinforcement Learning,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Efficient Binocular Rendering of Volumetric Density Fields With Coupled Adaptive Cube-Map Ray Marching for Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Creating visualizations of multiple volumetric density fields is demanding in virtual reality (VR) applications, which often include divergent volumetric density distributions mixed with geometric models and physics-based simulations. Real-time rendering of such complex environments poses significant challenges for rendering quality and performance. This article presents a novel scheme for efficient real-time rendering of varying translucent volumetric density fields with global illumination (GI) effects on high-resolution binocular VR displays. Our scheme proposes creative solutions to address three challenges involved in the target problem. First, to tackle the doubled heavy workloads of binocular ray marching, we explore the anti-aliasing principles and more advanced potentials of ray marching on interior cube-map faces, and propose a coupled ray-marching technique that converges to multi-resolution cube maps with interleaved adaptive sampling. Second, we devise a fully dynamic ambient GI approximation method that leverages spherical-harmonics (SH) transform information of the phase function to reduce the huge amount of ray sampling required for GI while ensuring fidelity. The method catalyzes spatial ray-marching reuse and adaptive temporal accumulation. Third, we deploy a two-phase ray-tracing algorithm with a tiled k-buffer to achieve fast processing of order-independent transparency (OIT) for multiple volume instances. Consequently, high-quality and high-performance real-time dynamic volume rendering can be achieved under constrained budgets controlled by developers. As our solution supports mixed mesh-volume rendering, the test results prove the practical usefulness of our approach for high-resolution binocular VR rendering on hybrid multi-volumetric and geometric environments. © 1995-2012 IEEE.",Binocular views; density field; global illumination; ray marching; real-time rendering; virtual reality; volume rendering,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Text Entry Performance and Situation Awareness of a Joint Optical See-Through Head-Mounted Display and Smartphone System,TVCG - Transactions on Visualization and Computer Graphics,A,"Optical see-through head-mounted displays (OST HMDs) are a popular output medium for mobile Augmented Reality (AR) applications. To date, they lack efficient text entry techniques. Smartphones are a major text entry medium in mobile contexts but attentional demands can contribute to accidents while typing on the go. Mobile multi-display ecologies, such as combined OST HMD-smartphone systems, promise performance and situation awareness benefits over single-device use. We study the joint performance of text entry on mobile phones with text output on optical see-through head-mounted displays. A series of five experiments with a total of 86 participants indicate that, as of today, the challenges in such a joint interactive system outweigh the potential benefits. 1077-2626  © 2023 IEEE.",Augmented reality; cross-device; head-mounted display; mobile; multi-display; optical see-through; text entry,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Leveraging Tendon Vibration to Enhance Pseudo-Haptic Perceptions in VR,TVCG - Transactions on Visualization and Computer Graphics,A,"Pseudo-haptic techniques are used to modify haptic perception by appropriately changing visual feedback to body movements. Based on the knowledge that tendon vibration can affect our somatosensory perception, this article proposes a method for leveraging tendon vibration to enhance pseudo-haptics during free arm motion. Three experiments were performed to examine the impact of tendon vibration on the range and resolution of pseudo-haptics. The first experiment investigated the effect of tendon vibration on the detection threshold of the discrepancy between visual and physical motion. The results indicated that vibrations applied to the inner tendons of the wrist and elbow increased the threshold, suggesting that tendon vibration can augment the applicable visual motion gain by approximately 13% without users detecting the visual/physical discrepancy. Furthermore, the results demonstrate that tendon vibration acts as noise on haptic motion cues. The second experiment assessed the impact of tendon vibration on the resolution of pseudo-haptics by determining the just noticeable difference in pseudo-weight perception. The results suggested that the tendon vibration does not largely compromise the resolution of pseudo-haptics. The third experiment evaluated the equivalence between the weight perception triggered by tendon vibration and that by visual motion gain, that is, the point of subjective equality. The results revealed that vibration amplifies the weight perception and its effect was equivalent to that obtained using a gain of 0.64 without vibration, implying that the tendon vibration also functions as an additional haptic cue. Our results provide design guidelines and future work for enhancing pseudo-haptics with tendon vibration. 1077-2626  © 2023 IEEE.",Cross-modal integration; maximum likelyhood estimation; pseudo-haptics; tendon vibration; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,This is the Table I Want! Interactive Data Transformation on Desktop and in Virtual Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"—Data transformation is an essential step in data science. While experts primarily use programming to transform their data, there is an increasing need to support non-programmers with user interface-based tools. With the rapid development in interaction techniques and computing environments, we report our empirical findings about the effects of interaction techniques and environments on performing data transformation tasks. Specifically, we studied the potential benefits of direct interaction and virtual reality (VR) for data transformation. We compared gesture interaction versus a standard WIMP user interface, each on the desktop and in VR. With the tested data and tasks, we found time performance was similar between desktop and VR. Meanwhile, VR demonstrates preliminary evidence to better support provenance and sense-making throughout the data transformation process. Our exploration of performing data transformation in VR also provides initial affirmation for enabling an iterative and fully immersive data science workflow. © 2023 IEEE.",Data science; data transformation; empirical study; immersive analytics; interaction; virtual/augmented/mixed reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,MeshWGAN: Mesh-to-Mesh Wasserstein GAN with Multi-Task Gradient Penalty for 3D Facial Geometric Age Transformation,TVCG - Transactions on Visualization and Computer Graphics,A,"As the metaverse develops rapidly, 3D facial age transformation is attracting increasing attention, which may bring many potential benefits to a wide variety of users, e.g., 3D aging figures creation, 3D facial data augmentation and editing. Compared with 2D methods, 3D face aging is an underexplored problem. To fill this gap, we propose a new mesh-to-mesh Wasserstein generative adversarial network (MeshWGAN) with a multi-task gradient penalty to model a continuous bi-directional 3D facial geometric aging process. To the best of our knowledge, this is the first architecture to achieve 3D facial geometric age transformation via real 3D scans. As previous image-to-image translation methods cannot be directly applied to the 3D facial mesh, which is totally different from 2D images, we built a mesh encoder, decoder, and multi-task discriminator to facilitate mesh-to-mesh transformations. To mitigate the lack of 3D datasets containing children's faces, we collected scans from 765 subjects aged 5-17 in combination with existing 3D face databases, which provided a large training dataset. Experiments have shown that our architecture can predict 3D facial aging geometries with better identity preservation and age closeness compared to 3D trivial baselines. We also demonstrated the advantages of our approach via various 3D face-related graphics applications.  © 1995-2012 IEEE.",3D face geometry; Age transformation; mesh generative adversarial networks; MeshWGAN; multi-task gradient penalty,Abstract,TRUE,
Scopus,journalPaper,2024,Unraveling the Design Space of Immersive Analytics: A Systematic Review,TVCG - Transactions on Visualization and Computer Graphics,A,"Immersive analytics has emerged as a promising research area, leveraging advances in immersive display technologies and techniques, such as virtual and augmented reality, to facilitate data exploration and decision-making. This paper presents a systematic literature review of 73 studies published between 2013-2022 on immersive analytics systems and visualizations, aiming to identify and categorize the primary dimensions influencing their design. We identified five key dimensions: Academic Theory and Contribution, Immersive Technology, Data, Spatial Presentation, and Visual Presentation. Academic Theory and Contribution assess the motivations behind the works and their theoretical frameworks. Immersive Technology examines the display and input modalities, while Data dimension focuses on dataset types and generation. Spatial Presentation discusses the environment, space, embodiment, and collaboration aspects in IA, and Visual Presentation explores the visual elements, facet and position, and manipulation of views. By examining each dimension individually and cross-referencing them, this review uncovers trends and relationships that help inform the design of immersive systems visualizations. This analysis provides valuable insights for researchers and practitioners, offering guidance in designing future immersive analytics systems and shaping the trajectory of this rapidly evolving field. A free copy of this paper and all supplemental materials are available at osf.io/5ewaj. © 1995-2012 IEEE.",Augmented Reality; Design Space; Immersive Analytics; Survey; Systematic Review; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Investigating the Correlation Between Presence and Reaction Time in Mixed Reality,TVCG - Transactions on Visualization and Computer Graphics,A,"Measuring presence is critical to improving user involvement and performance in Mixed Reality (MR). Presence, a crucial aspect of MR, is traditionally gauged using subjective questionnaires, leading to a lack of time-varying responses and susceptibility to user bias. Inspired by the existing literature on the relationship between presence and human performance, the proposed methodology systematically measures a user's reaction time to a visual stimulus as they interact within a manipulated MR environment. We explore the user reaction time as a quantity that can be easily measured using the systemic tools available in modern MR devices. We conducted an exploratory study (N = 40) with two experiments designed to alter the users' sense of presence by manipulating place illusion and plausibility illusion. We found a significant correlation between presence scores and reaction times with a correlation coefficient -0.65, suggesting that users with a higher sense of presence responded more swiftly to stimuli. We develop a model that estimates a user's presence level using the reaction time values with high accuracy of up to 80%. While our study suggests that reaction time can be used as a measure of presence, further investigation is needed to improve the accuracy of the model.  © 1995-2012 IEEE.",Mixed reality; presence,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Modeling the Intent to Interact with VR Using Physiological Features,TVCG - Transactions on Visualization and Computer Graphics,A,"Objective: Mixed-Reality (XR) technologies promise a user experience (UX) that rivals the interactive experience with the real-world. The key facilitators in the design of such a natural UX are that the interaction has zero lag and that users experience no excess mental load. This is difficult to achieve due to technical constraints such as motion-to-photon latency as well as false-positives during gesture-based interaction. Methods: In this paper, we explored the use of physiological features to model the user's intent to interact with a virtual reality (VR) environment. Accurate predictions about when users want to express an interaction intent could overcome the limitations of an interactive device that lags behind the intention of a user. We computed time-domain features from electroencephalography (EEG) and electromyography (EMG) recordings during a grab-and-drop task in VR and cross-validated a Linear Discriminant Analysis (LDA) for three different combinations of (1) EEG, (2) EMG and (3) EEG-EMG features. Results & Conclusion: We found the classifiers to detect the presence of a pre-movement state from background idle activity reflecting the users' intent to interact with the virtual objects (EEG: 62 ± 10, EMG: 72 ± 9, EEG-EMG: 69 ± 10) above simulated chance level. The features leveraged in our classification scheme have a low computational cost and are especially useful for fast decoding of users' mental states. Our work is a further step towards a useful classification of users' intent to interact, as a high temporal resolution and speed of detection is crucial. This facilitates natural experiences through zero-lag adaptive interfaces. 1077-2626  © 2023 IEEE.",Brain-computer interfaces; electroencephalography; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,ViewR: Architectural-Scale Multi-User Mixed Reality With Mobile Head-Mounted Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"—The emergence of mobile head-mounted displays with robust “inside-out” markerless tracking and video-passthrough permits the creation of novel mixed reality (MR) experiences in which architectural spaces of arbitrary size can be transformed into immersive multi-user visualisation arenas. Here we outline ViewR, an open-source framework for rapidly constructing and deploying architectural-scale multi-user MR experiences. ViewR includes tools for rapid alignment of real and virtual worlds, tracking loss detection and recovery, user trajectory visualisation and world state synchronisation between users with persistence across sessions. ViewR also provides control over the blending of the real and the virtual, specification of site-specific blending zones, and video-passthrough avatars, allowing users to see and interact with one another directly. Using ViewR, we explore the transformation of large architectural structures into immersive arenas by creating a range of experiences in various locations, with a particular focus on architectural affordances such as mezzanines, stairs, gangways and elevators. Our tests reveal that ViewR allows for experiences that would not be possible with pure virtual reality, and indicate that, with certain strategies for recovering from tracking errors, it is possible to construct large scale multi-user MR experiences using contemporary consumer virtual reality head-mounted displays. © 2023 The Authors.",co-located systems; collaborative systems; mixed / augmented reality; virtual reality; Visualization systems,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,"LoCoMoTe - A Framework for Classification of Natural Locomotion in VR by Task, Technique and Modality",TVCG - Transactions on Visualization and Computer Graphics,A,"Virtual reality (VR) research has provided overviews of locomotion techniques, how they work, their strengths and overall user experience. Considerable research has investigated new methodologies, particularly machine learning to develop redirection algorithms. To best support the development of redirection algorithms through machine learning, we must understand how best to replicate human navigation and behaviour in VR, which can be supported by the accumulation of results produced through live-user experiments. However, it can be difficult to identify, select and compare relevant research without a pre-existing framework in an ever-growing research field. Therefore, this work aimed to facilitate the ongoing structuring and comparison of the VR-based natural walking literature by providing a standardised framework for researchers to utilise. We applied thematic analysis to study methodology descriptions from 140 VR-based papers that contained live-user experiments. From this analysis, we developed the LoCoMoTe framework with three themes: navigational decisions, technique implementation, and modalities. The LoCoMoTe framework provides a standardised approach to structuring and comparing experimental conditions. The framework should be continually updated to categorise and systematise knowledge and aid in identifying research gaps and discussions. © 1995-2012 IEEE.",Human-computer interaction; machine learning; navigation; redirected walking; virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,Wizualization: A 'Hard Magic' Visualization System for Immersive and Ubiquitous Analytics,TVCG - Transactions on Visualization and Computer Graphics,A,"What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (Spellbook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space. © 1995-2012 IEEE.",gestural interaction; Immersive analytics; situated analytics; ubiquitous analytics; voice interaction,Abstract,TRUE,
Scopus,journalPaper,2024,Survey of Annotations in Extended Reality Systems,TVCG - Transactions on Visualization and Computer Graphics,A,"Annotation in 3D user interfaces such as Augmented Reality (AR) and Virtual Reality (VR) is a challenging and promising area; however, there are not currently surveys reviewing these contributions. In order to provide a survey of annotations for Extended Reality (XR) environments, we conducted a structured literature review of papers that used annotation in their AR/VR systems from the period between 2001 and 2021. Our literature review process consists of several filtering steps which resulted in 103 XR publications with a focus on annotation. We classified these papers based on the display technologies, input devices, annotation types, target object under annotation, collaboration type, modalities, and collaborative technologies. A survey of annotation in XR is an invaluable resource for researchers and newcomers. Finally, we provide a database of the collected information for each reviewed paper. This information includes applications, the display technologies and its annotator, input devices, modalities, annotation types, interaction techniques, collaboration types, and tasks for each paper. This database provides a rapid access to collected data and gives users the ability to search or filter the required information. This survey provides a starting point for anyone interested in researching annotation in XR environments.  © 1995-2012 IEEE.",Annotation; augmented reality; extended reality; immersive technologies; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,SceneFusion: Room-Scale Environmental Fusion for Efficient Traveling between Separate Virtual Environments,TVCG - Transactions on Visualization and Computer Graphics,A,"Traveling between scenes has become a major requirement for navigation in numerous virtual reality (VR) social platforms and game applications, allowing users to efficiently explore multiple virtual environments (VEs). To facilitate scene transition, prevalent techniques such as instant teleportation and virtual portals have been extensively adopted. However, these techniques exhibit limitations when there is a need for frequent travel between separate VEs, particularly within indoor environments, resulting in low efficiency. In this article, we first analyze the design rationale for a novel navigation method supporting efficient travel between virtual indoor scenes. Based on the analysis, we introduce the SceneFusion technique that fuses separate virtual rooms into an integrated environment. SceneFusion enables users to perceive rich visual information from both rooms simultaneously, achieving high visual continuity and spatial awareness. While existing teleportation techniques passively transport users, SceneFusion allows users to actively access the fused environment using short-range locomotion techniques. User experiments confirmed that SceneFusion outperforms instant teleportation and virtual portal techniques in terms of efficiency, workload, and preference for both single-user exploration and multi-user collaboration tasks in separate VEs. Thus, SceneFusion presents an effective solution for seamless traveling between virtual indoor scenes.  © 1995-2012 IEEE.",collaborative virtual environments; scene transition; Virtual reality,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,The Reality of the Situation: A Survey of Situated Analytics,TVCG - Transactions on Visualization and Computer Graphics,A,"The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user's physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.  © 1995-2012 IEEE.",Augmented reality; data visualization; immersive analytics; situated analytics; situated visualization,Abstract_Keywords,TRUE,
Scopus,journalPaper,2024,MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction,TVCG - Transactions on Visualization and Computer Graphics,A,"High-accuracy, high-efficiency physics-based fluid-solid interaction is essential for reality modeling and computer animation in online games or real-time Virtual Reality (VR) systems. However, the large-scale simulation of incompressible fluid and its interaction with the surrounding solid environment is either time-consuming or suffering from the reduced time/space resolution due to the complicated iterative nature pertinent to numerical computations of involved Partial Differential Equations (PDEs). In recent years, we have witnessed significant growth in exploring a different, alternative data-driven approach to addressing some of the existing technical challenges in conventional model-centric graphics and animation methods. This article showcases some of our exploratory efforts in this direction. One technical concern of our research is to address the central key challenge of how to best construct the numerical solver effectively and how to best integrate spatiotemporal/dimensional neural networks with the available MPM's pressure solvers. In particular, we devise the MPMNet, a hybrid data-driven framework supporting the popular and powerful MPM, to combine the comprehensive properties of MPM in numerically handling physical behaviors ranging from fluid to deformable solids and the high efficiency of data-driven models. At the architectural level, our MPMNet comprises three primary components: A data processing module to describe the physical properties by way of the input fields; A deep neural network group to learn the spatiotemporal features; And an iterative refinement process to continue to reduce possible numerical errors. The goal of these special technical developments is to aim at involved numerical acceleration while preserving physical accuracy, realizing efficient and accurate fluid-solid interactions in a data-driven fashion. The extensive experimental results verify that our MPMNet can tremendously speed up the computation compared with the popular numerical methods as the complexity of interaction scenes increases while better retaining the numerical accuracy.  © 1995-2012 IEEE.",Data-driven simulation; fluid-solid interaction; neural networks; physics-based simulation,Abstract,TRUE,
Scopus,journalPaper,2024,An Overview of Enhancing Distance Learning Through Emerging Augmented and Virtual Reality Technologies,TVCG - Transactions on Visualization and Computer Graphics,A,"Although distance learning presents a number of interesting educational advantages as compared to in-person instruction, it is not without its downsides. We first assess the educational challenges presented by distance learning as a whole and identify 4 main challenges that distance learning currently presents as compared to in-person instruction: the lack of social interaction, reduced student engagement and focus, reduced comprehension and information retention, and the lack of flexible and customizable instructor resources. After assessing each of these challenges in-depth, we examine how AR/VR technologies might serve to address each challenge along with their current shortcomings, and finally outline the further research that is required to fully understand the potential of AR/VR technologies as they apply to distance learning.  © 1995-2012 IEEE.",Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality; Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality,Title_Keywords,TRUE,
Scopus,journalPaper,2024,Leaning-Based Interfaces Improve Simultaneous Locomotion and Object Interaction in VR Compared to the Handheld Controller,TVCG - Transactions on Visualization and Computer Graphics,A,"Physical walking is often considered the gold standard for VR travel whenever feasible. However, limited free-space walking areas in the real-world do not allow exploring larger-scale virtual environments by actual walking. Therefore, users often require handheld controllers for navigation, which can reduce believability, interfere with simultaneous interaction tasks, and exacerbate adverse effects such as motion sickness and disorientation. To investigate alternative locomotion options, we compared handheld Controller (thumbstick-based) and physical walking versus a seated (HeadJoystick) and standing/stepping (NaviBoard) leaning-based locomotion interface, where seated/standing users travel by moving their head toward the target direction. Rotations were always physically performed. To compare these interfaces, we designed a novel simultaneous locomotion and object interaction task, where users needed to keep touching the center of upward moving target balloons with their virtual lightsaber, while simultaneously staying inside a horizontally moving enclosure. Walking resulted in the best locomotion, interaction, and combined performances while the controller performed worst. Leaning-based interfaces improved user experience and performance compared to Controller, especially when standing/stepping using NaviBoard, but did not reach walking performance. That is, leaning-based interfaces HeadJoystick (sitting) and NaviBoard (standing) that provided additional physical self-motion cues compared to controller improved enjoyment, preference, spatial presence, vection intensity, motion sickness, as well as performance for locomotion, object interaction, and combined locomotion and object interaction. Our results also showed that less embodied interfaces (and in particular the controller) caused a more pronounced performance deterioration when increasing locomotion speed. Moreover, observed differences between our interfaces were not affected by repeated interface usage.  © 1995-2012 IEEE.",3D user interface; continuous interaction; cybersickness; dual task; locomotion; motion sickness; travel techniques; virtual reality,Keywords,TRUE,
Scopus,journalPaper,2024,Does Multi-Actuator Vibrotactile Feedback Within Tangible Objects Enrich VR Manipulation?,TVCG - Transactions on Visualization and Computer Graphics,A,"Rich, informative and realistic haptic feedback is key to enhancing Virtual Reality (VR) manipulation. Tangible objects provide convincing grasping and manipulation interactions with haptic feedback of e.g., shape, mass and texture properties. But these properties are static, and cannot respond to interactions in the virtual environment. On the other hand, vibrotactile feedback provides the opportunity for delivering dynamic cues rendering many different contact properties, such as impacts, object vibrations or textures. Handheld objects or controllers in VR are usually restricted to vibrating in a monolithic fashion. In this article, we investigate how spatialiazing vibrotactile cues within handheld tangibles could enable a wider range of sensations and interactions. We conduct a set of perception studies, investigating the extent to which spatialization of vibrotactile feedback within tangible objects is possible as well as the benefits of proposed rendering schemes leveraging multiple actuators in VR. Results show that vibrotactile cues from localized actuators can be discriminated and are beneficial for certain rendering schemes.  © 1995-2012 IEEE.",Haptics; manipulation; tangible; vibrotactile; VR,Abstract,TRUE,
Scopus,journalPaper,2024,3D Gamut Morphing for Non-Rectangular Multi-Projector Displays,TVCG - Transactions on Visualization and Computer Graphics,A,"In a spatially augmented reality system, multiple projectors are tiled on a complex shaped surface to create a seamless display on it. This has several applications in visualization, gaming, education and entertainment. The main challenges in creating seamless and undistorted imagery on such complex shaped surfaces are geometric registration and color correction. Prior methods that provide solutions for the spatial color variation in multi-projector displays assume rectangular overlap regions across the projectors that is possible only on flat surfaces with extremely constrained projector placement. In this article, we present a novel and fully automated method for removing color variations in a multi-projector display on arbitrary shaped smooth surfaces using a general color gamut morphing algorithm that can handle any arbitrarily shaped overlap between the projectors and assures imperceptible color variations across the display surface.  © 1995-2012 IEEE.",Color-correction; gamut morphing; multi-projector displays; photometric correction,Abstract,TRUE,
Scopus,conferencePaper,2013,Towards hand-eye coordination training in virtual knee arthroscopy,VRST - Virtual Reality Software and Technology,A,"Minimally invasive arthroscopic surgery has replaced the common orthopaedic surgery procedures on joints. However it demands from surgeons to acquire very different motor-skills for using special miniature pencil-like instruments and cameras inserted through little incisions on the body while observing the surgical field on a video monitor. Training in virtual reality is becoming an alternative to traditional surgical training based on either real patients or increasingly difficult to procure cadavers. In this paper we propose solutions for simulation in virtual environments a few basic arthroscopic procedures including incision of the arthroscopic camera, positioning of the instrument in front of it, as well as using scissors and graspers. Our approach is based on both full 3D simulation and haptic interaction as well as image-based visualization and haptic interaction.",arthroscopy; haptics; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,A two-arm coordination model for phantom limb pain rehabilitation,VRST - Virtual Reality Software and Technology,A,"Following limb loss, patients usually continue having sensations on their missing limbs as if they were still present. Significant amount of such sensations are painful and referred as Phantom Limb Pain (PLP). Previous research has shown that providing the patient with the visual feedback of a limb at the place of the missing one in Virtual Reality (VR) can reduce PLP. In this paper we introduce a model to coordinate the arms allowing the exercising of a much broader range of reach tasks for alleviating the PLP more efficiently. Our Two-Arm Coordination Model (TACM) synthesizes the missing limb pose from the instantaneous variations of the intact opposite limb for a given reach task. Moreover, we propose a setup that makes use of a virtual mirror to enhance the full-body awareness of the patient in the virtual space.",bimanual reach; phantom limb pain; virtual reality based rehabilitation,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,A methodology to assess the acceptability of human-robot collaboration using virtual reality,VRST - Virtual Reality Software and Technology,A,"Robots are becoming more and more present in our everyday life: they are already used for domestic tasks, for companionship activities, and soon they will be used to assist humans and collaborate with them in their work. Human-robot collaboration has already been studied in the industry, for ergonomics and efficiency purposes, but more from a safety than from an acceptability point of view. In this work, we focused on how people perceive robots in a collaboration task and we proposed to use virtual reality as a simulation environment to test different parameters, by making users collaborate with virtual robots. A simple use case was implemented to compare different robot appearances and different robot movements. Questionnaires and physiological measures were used to assess the acceptability level of each condition with a user study. The results showed that the perception of robot movements depended on robot appearance and that a more anthropomorphic robot, both in its appearance and movements, was not necessarily better accepted by the users in a collaboration task. Finally, this preliminary use case was also the opportunity to guarantee the relevance of using such a methodology — based on virtual reality, questionnaires and physiological measures — for future studies.",acceptability; human factors; human-robot collaboration; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,Can we use a brain-computer interface and manipulate a mouse at the same time?,VRST - Virtual Reality Software and Technology,A,"Brain-Computer Interfaces (BCI) introduce a novel way of interacting with real and virtual environments by directly exploiting cerebral activity. However in most setups using a BCI, the user is explicitly asked to remain as motionless as possible, since muscular activity is commonly admitted to add noise and artifacts in brain electrical signals. Thus, as for today, people have been rarely let using other classical input devices such as mice or joysticks simultaneously to a BCI-based interaction. In this paper, we present an experimental study on the influence of manipulating an input device such as a standard computer mouse on the performance of a BCI system. We have designed a 2-class BCI which relies on Alpha brainwaves to discriminate between focused versus relaxed mental activities. The study uses a simple virtual environment inspired by the well-known Pac-Man videogame and based on BCI and mouse controls. The control of mental activity enables to eat pellets in a simple 2D virtual maze. Different levels of motor activity achieved with the mouse are progressively introduced in the gameplay: 1) no motor activity (control condition), 2) a semi-automatic motor activity, and 3) a highly-demanding motor activity. As expected the BCI performance was found to slightly decrease in presence of motor activity. However, we found that the BCI could still be successfully used in all conditions, and that relaxed versus focused mental activities could still be significantly discriminated even in presence of a highly-demanding mouse manipulation. These promising results pave the way to future experimental studies with more complex mental and motor activities, but also to novel 3D interaction paradigms that could mix BCI and other input devices for virtual reality and videogame applications.",brain-computer interface; EEG; mental activity; motor activity,Abstract,TRUE,
Scopus,conferencePaper,2013,Impact of graphical fidelity on physiological responses in virtual environments,VRST - Virtual Reality Software and Technology,A,"Higher quality computer graphics in interactive applications in the areas of virtual reality and games is generally assumed to create a more immersive experience for the end user. In this study we examined this assumption by testing to what degree graphical fidelity was associated with physiological arousal as measured by a galvanic skin response (GSR) sensor. Thirty-six subjects played two different video games at the highest and lowest graphical quality settings while their GSR activity was measured. No significant difference in GSR was observed that was associated with graphical quality. We conclude that, for applications in which an emotional response is desired, increased graphical quality alone does not predict a physiological arousal response.",electrodermal activity; emotional measurement; fidelity; galvanic skin response; games; immersion; quality; resolution; virtual environments; visual perception,Abstract,TRUE,
Scopus,conferencePaper,2013,Facetons: face primitives with adaptive bounds for building 3D architectural models in virtual environment,VRST - Virtual Reality Software and Technology,A,"We present faceton, a geometric modeling primitive designed for building architectural models, using a six degrees of freedom (DoF) input device in a virtual environment (VE). A faceton is given as an oriented point floating in the air and defines a plane of infinite extent passing through the point. The polygonal mesh model is constructed by taking the intersection of the planes associated with the facetons. With the simple drag-and-drop and group interaction of faceton, users can easily create 3D architecture models in the VE. The faceton primitive and its interaction reduce the overhead associated with standard polygonal mesh modeling in VE, where users have to manually specify vertexes and edges which could be far away. The faceton representation is inspired by the research on boundary representations (B-rep) and constructive solid geometry (CSG), but it is driven by a novel adaptive bounding algorithm and is specifically designed for the 3D modeling activities in an immersive virtual environment.",geometric modeling; polygonal mesh; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2013,Interacting with danger in an immersive environment: issues on cognitive load and risk perception,VRST - Virtual Reality Software and Technology,A,"Any human-computer interface imposes a certain level of cognitive load to the user task. Analogously, the task itself also imposes different levels of cognitive load. It is common sense in 3D user interfaces research that a higher number of degrees of freedom increases the interface cognitive load. If the cognitive load is significant, it might compromise the user performance and undermine the evaluation of user skills in a virtual environment. In this paper, we propose an assessment of two immersive VR interfaces with varying degrees of freedom in two VR tasks: risk perception and basic object selection. We examine the effectiveness of both interfaces in these two different tasks. Results show that the number of degrees of freedom does not significantly affect a basic selection task, but it affects risk perception task in an unexpected way.",3D interaction; presence; risk perception; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2013,ShoeSoleSense: proof of concept for a wearable foot interface for virtual and real environments,VRST - Virtual Reality Software and Technology,A,"ShoeSoleSense is a proof of concept, novel body worn interface - an insole that enables location independent hands-free interaction through the feet. Forgoing hand or finger interaction is especially beneficial when the user is engaged in real world tasks. In virtual environments as moving through safety training applications is often conducted via finger input, which is not very suitable. To enable a more intuitive interaction, alternative control concepts utilize gesture control, which is usually tracked by statically installed cameras in CAVE-like-installations. Since tracking coverage is limited, problems may also occur. The introduced prototype provides a novel control concept for virtual reality as well as real life applications. Demonstrated functions include movement control in a virtual reality installation such as moving straight, turning and jumping. Furthermore the prototype provides additional feedback by heating up the feet and vibrating in dedicated areas on the surface of the insole.",eyes-free; foot; hands-free; insole; mobile; physical interface; shoe; tactile feedback; virtual reality; wearable,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,"Bubble bee, an alternative to arrow for pointing out directions",VRST - Virtual Reality Software and Technology,A,"We present Bubble Bee - an extension for the 3D bubble cursor in Virtual Environments (VEs). This technique provides an alternative to arrows for pointing out a direction in a 3D scene.Bubble Bee is based on a ring concept. A circular ring in 3D appears like an ellipse, according to its orientation. This orientation is easy to infer by comparing the minor radius which varies with the view angle, to the reference major radius which is constant and equal to the radius of the ring. Bubble Bee is a sphere with several rings oriented towards the same direction. The rings give a natural axis to the sphere. A color gradient sets the direction of this axis.We compared the performance of Bubble Bee and a 3D arrow through an experiment. The participants were asked to indicate which object was pointed by the two competing techniques. No significant differences on decision time were found, while Bubble Bee was shown to be nearly as accurate as a 3D arrow.",3D interaction; pointing aid; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2013,Robust prediction of auditory step feedback for forward walking,VRST - Virtual Reality Software and Technology,A,"Virtual reality systems supporting real walking as a navigation interface usually lack auditory step feedback, although this could give additional information to the user e.g. about the ground he is walking on. In order to add matching auditory step feedback to virtual environments, we propose a calibration-free and easy to use system that can predict the occurrence time of stepping sounds based on human gait data.Our system is based on the timing of reliably occurring characteristic events in the gait cycle which are detected using foot mounted accelerometers and gyroscopes. This approach not only allows us to detect but to predict the time of an upcoming step sound in realtime. Based on data gathered in an experiment, we compare different suitable events that allow a tradeoff between the maximum precision of the prediction and the maximum time by which the sound can be predicted.",auditory feedback; gait; human walking; step prediction; step sound; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,Persuading people in a remote destination to sing by beaming there,VRST - Virtual Reality Software and Technology,A,"We built a Collaborative Virtual Environment (CVE) allowing one person, the 'visitor' to be digitally transported to a remote destination to interact with local people there. This included full body tracking, vibrotactile feedback and voice. This allowed interactions in the same CVE between multiple people situated in different physical remote locations. This system was used for an experiment to study whether the conveyance of touch has an impact on the willingness of participants embodied in the CVE to sing in public.In a first experimental condition, the experimenter virtually touched the avatar of the participants on the shoulder, producing vibrotactile feedback. In another condition using the identical physical setup, the vibrotactile displays were not activated, so that they would not feel the touch. Our hypothesis was that the tactile touch condition would produce a greater likelihood of compliance with the request to sing. In a second part we examined the hypothesis that people might be more willing to sing (execute an embarrassing task) in a CVE, because of the anonymity provided by virtual reality. Hence we carried out a similar study in physical reality.The results suggest that the tactile intervention had no effect on the sensations of body ownership, presence or the behaviours of the participants, in spite of the finding that the sensation of touch itself was effectively realised. Moreover we found an overall similarity in responses between the VR and real conditions.",collaborative virtual environments; embodiment; haptic interaction; presence; social touch,Abstract,TRUE,
Scopus,conferencePaper,2013,Supporting interoperability and presence awareness in collaborative mixed reality environments,VRST - Virtual Reality Software and Technology,A,"In the BEAMING project we have been extending the scope of collaborative mixed reality to include the representation of users in multiple modalities, including augmented reality, situated displays and robots. A single user (a visitor) uses a high-end virtual reality system (the transporter) to be virtually teleported to a real remote location (the destination). The visitor may be tracked in several ways including emotion and motion capture. We reconstruct the destination and the people within it (the locals). In achieving this scenario, BEAMING has integrated many heterogeneous systems. In this paper, we describe the design and key implementation choices in the Beaming Scene Service (BSS), which allows the various processes to coordinate their behaviour. The core of the system is a light-weight shared object repository that allows loose coupling between processes with very different requirements (e.g. embedded control systems through to mobile apps). The system was also extended to support the notion of presence awareness. We demonstrate two complex applications built with the BSS.",interoperability; mixed reality; presence; telepresence; telerobotics; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,Real time whole body motion mapping for avatars and robots,VRST - Virtual Reality Software and Technology,A,"We describe a system that allows for controlling different robots and avatars from a real time motion stream. The underlying problem is that motion data from tracking systems is usually represented differently to the motion data required to drive an avatar or a robot: there may be different joints, motion may be represented by absolute joint positions and rotations or by a root position, bone lengths and relative rotations in the skeletal hierarchy. Our system resolves these issues by remapping in real time the tracked motion so that the avatar or robot performs motions that are visually close to those of the tracked person. The mapping can also be reconfigured interactively at run-time. We demonstrate the effectiveness of our system by case studies in which a tracked person is embodied as an avatar in immersive virtual reality or as a robot in a remote location. We show this with a variety of tracking systems, humanoid avatars and robots.",avatars; motion capture; robots; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,Color correction for optical see-through displays using display color profiles,VRST - Virtual Reality Software and Technology,A,"In optical see-through displays, light coming from background objects mixes with the light originating from the display, causing what is known as the color blending problem. Color blending negatively affects the usability of such displays as it impacts the legibility and color encodings of digital content. Color correction aims at reducing the impact of color blending by finding an alternative display color which, once mixed with the background, results in the color originally intended.In this paper we model color blending based on two distortions induced by the optical see-through display. The render distortion explains how the display renders colors. The material distortion explains how background colors are changed by the display material. We show the render distortion has a higher impact on color blending and propose binned-profiles (BP) - descriptors of how a display renders colors - to address it. Results show that color blending predictions using BP have a low error rate - within nine just noticeable differences (JND) in the worst case. We introduce a color correction algorithm based on predictions using BP and measure its correction capacity. Results show light display colors can be better corrected for all backgrounds. For high intensity backgrounds light colors in the neutral and CyanBlue regions perform better. Finally, we elaborate on the applicability, design and hardware implications of our approach.",augmented reality; color blending; color correction; display binned-profile; interface design; optical see-through displays,Keywords,TRUE,
Scopus,conferencePaper,2013,Intercept tags: enhancing intercept-based systems,VRST - Virtual Reality Software and Technology,A,"In some virtual reality (VR) systems, OpenGL intercept methods are used to capture and render a desktop application's OpenGL calls within an immersive display. These systems often suffer from lower frame rates due to network bandwidth limitations, implementation of the intercept routine, and in some cases, the intercepted application's frame rate. To mitigate these issues and to enhance intercept-based systems in other ways, we present intercept tags, which are OpenGL geometries that are interpreted instead of rendered. We have identified and developed several uses for intercept tags, including hand-off interactions, display techniques, and visual enhancements. To demonstrate the value of intercept tags, we conducted a user study to compare a simple virtual hand technique implemented with and without intercept tags. Our results show that intercept tags significantly improve user performance and experience.",intercept tags; intercept-based systems; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2013,Are virtual patients effective to train diagnostic skills? a study with bulimia nervosa virtual patients,VRST - Virtual Reality Software and Technology,A,"Differential diagnosis is carried out early during the diagnostic interview, and this process requires a series of abilities that must be developed through sound training. The use of virtual reality in interactive simulations with Virtual Patients (VPs) enables students to learn by doing, through first-person experience, without interaction with real patients. VPs are interactive computer simulations of patient encounters used in health care education for learning and assessment. They typically include interactive features for illness history taking, explorations, tests, and features for suggesting diagnosis and treatment plans [Fors, Muntean, Botezatu and Zary, 2009]. VPs have been shown to have a great educational value especially for training clinical reasoning [Cook and Triola, 2009] and differential diagnosis [Peñaloza-Salazar et al. 2011]. The present study tested an application for teaching healthcare professionals the skills required to perform the differential diagnosis of bulimia nervosa.",,Abstract,TRUE,
Scopus,conferencePaper,2013,Cue-elicited craving for food in virtual reality,VRST - Virtual Reality Software and Technology,A,"This study explores the use of virtual reality technology as an alternative to in vivo exposure in cue-exposure therapy for bingeing behavior, and assesses the ability of different virtual environments to elicit craving for food in a non-clinical sample. Previous research has indicated that craving for food can be elicited by exposure to food cues [Ferriday and Brunstrom 2011; Sobik, Hutchinson and Craighead 2005]. Given that craving for food is considered a trigger of bingeing, cue-exposure therapy with response prevention of bingeing may be effective in extinguishing the craving response in patients with eating disorders and obesity. However, the application of the in vivo cue exposure technique in the therapist's office faces logistical difficulties and is hampered by a lack of ecological validity [Koskina, Campbell and Schmidt 2013]. The use of Virtual reality (VR) technology may overcome the difficulties described. Nevertheless, before VR-based cue-exposure can be used for therapeutic purposes, the ability of VR scenarios to elicit craving responses in participants must be assessed. This is the objective of the present study.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2014,DigiTap: an eyes-free VR/AR symbolic input device,VRST - Virtual Reality Software and Technology,A,"In this paper we present DigiTap—a wrist-worn device specially designed for symbolic input in virtual and augmented reality (VR/AR) environments. DigiTap is able to robustly sense thumb-to-finger taps on the four fingertips and the eight minor knuckles. These taps are detected by an accelerometer, which triggers capturing of an image sequence with a small wrist-mounted camera. The tap position is then extracted with low computational effort from the images by an image processing pipeline. Thus, the device is very energy efficient and may potentially be integrated in a smartwatch-like device, allowing an unobtrusive, always available, eyes-free input. To demonstrate the feasibility of our approach an initial user study with our prototype device was conducted. In this study the suitability of the twelve tapping locations was evaluated, and the most prominent sources of error were identified. Our prototype system was able to correctly classify 92% of the input locations.",character input; optical glove; system control; virtual/augmented reality; wrist camera,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,Third person view and guidance for more natural motor behaviour in immersive basketball playing,VRST - Virtual Reality Software and Technology,A,"The use of Virtual Reality (VR) in sports training is now widely studied with the perspective to transfer motor skills learned in virtual environments (VEs) to real practice. However precision motor tasks that require high accuracy have been rarely studied in the context of VE, especially in Large Screen Image Display (LSID) platforms. An example of such a motor task is the basketball free throw, where the player has to throw a ball in a 46cm wide basket placed at 4.2m away from her. In order to determine the best VE training conditions for this type of skill, we proposed and compared three training paradigms. These training conditions were used to compare the combinations of different user perspectives: first (1PP) and third-person (3PP) perspectives, and the effectiveness of visual guidance. We analysed the performance of eleven amateur subjects who performed series of free throws in a real and immersive 1:1 scale environment under the proposed conditions. The results show that ball speed at the moment of the release in 1PP was significantly lower compared to real world, supporting the hypothesis that distance is underestimated in large screen VEs. However ball speed in 3PP condition was more similar to the real condition, especially if combined with guidance feedback. Moreover, when guidance information was proposed, the subjects released the ball at higher - and closer to optimal - position (5-7% higher compared to no-guidance conditions). This type of information contributes to better understand the impact of visual feedback on the motor performance of users who wish to train motor skills using immersive environments. Moreover, this information can be used by exergames designers who wish to develop coaching systems to transfer motor skills learned in VEs to real practice.",basketball training; immersive room; perception of distance in VR; performance; visual feedback,Abstract,TRUE,
Scopus,conferencePaper,2014,Illumination independent marker tracking using cross-ratio invariance,VRST - Virtual Reality Software and Technology,A,"Marker tracking is used in numerous applications. Depending on the context and its constraints, tracking accuracy can be a crucial component of the application. In this paper, we firstly highlight that the tracking accuracy depends on the illumination, which is usually not controlled in most applications. Particularly, we show how corner detection can shift of several pixels when light power or background context change, even if the camera and the marker are static in the scene. Then, we propose a method, based on the cross ratio invariance, that allows to re-estimate the corner extraction so that the cross ratio of the marker model corresponds to the one computed from the extracted corners in the image. Finally, we show on real data that our approach improves the tracking accuracy, particularly along the camera depth axis, up to several millimeters, depending on the marker depth.",augmented reality; cross-ratio; illumination conditions; marker tracking,Keywords,TRUE,
Scopus,conferencePaper,2014,I'm in VR! using your own hands in a fully immersive MR system,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel fully immersive Mixed Reality system that we have recently developed where the user freely walks in a life-size virtual scenario wearing an HMD and can see and use her/his own body when interacting with objects. This form of natural interaction is made possible in our system because the user's hands are real-time captured by means of a RGBD camera on the HMD. This allow the system to have in real-time a texturized geometric mesh of the hands and body (as seen from her/his own perspective) that can be rendered like any other polygonal model in the scene. Our hypothesis is that by presenting to the users an egocentric view of the virtual environment ""populated"" by their own bodies, a very strong feeling of presence is developed as well.",hand gestures; mixed reality; natural interaction,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,User-perspective augmented reality magic lens from gradients,VRST - Virtual Reality Software and Technology,A,"In this paper we present a new approach to creating a geometrically-correct user-perspective magic lens and a prototype device implementing the approach. Our prototype uses just standard color cameras, with no active depth sensing. We achieve this by pairing a recent gradient domain image-based rendering method with a novel semi-dense stereo matching algorithm inspired by PatchMatch. Our stereo algorithm is simple but fast and accurate within its search area. The resulting system is a real-time magic lens that displays the correct user perspective with a high-quality rendering, despite the lack of a dense disparity map.",augmented reality; gradient domain; image based rendering; magic lens; semi-dense stereo; user-perspective,Title_Keywords,TRUE,
Scopus,conferencePaper,2014,Simulator sickness and presence using HMDs: comparing use of a game controller and a position estimation system,VRST - Virtual Reality Software and Technology,A,"Consumer-grade head-mounted displays (HMD) such as the Oculus Rift have become increasingly available for Virtual Reality recently. Their high degree of immersion and presence provokes usually amazement when first used. Nevertheless, HMDs also have been reported to cause adverse reactions such as simulator sickness. As their impact is growing, it is important to understand such side effects. This paper presents the results of a relatively large scale user experiment which compares using a conventional game controller versus positioning in the virtual world based upon the signal of the internal Inertial Measurement Unit (IMU) using Oculus Rift DK1. We show that simulator sickness is significantly reduced when using a position estimation system rather than using the more traditional game controller for navigation. However the sense of presence was not enhanced by the possibility of 'real walking'. We also show the impact of other factors, such as prior experience or motion history, and discuss the results.",locomotion; oculus rift; presence; simulator sickness; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,Desktop virtual reality for emergency preparedness: user evaluation of an aircraft ditching experience under different fear arousal conditions,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR), in the form of 3D interactive simulations of emergency scenarios, is increasingly used for emergency preparedness training. This paper advances knowledge about different aspects of such virtual emergency experiences, showing that: (i) the designs we propose in the paper are effective in improving emergency preparedness of common citizens, considering aviation safety as a relevant case study, (ii) changing specific visual and auditory features is effective to create emotionally different versions of the same experience, increasing the level of fear aroused in users, and (iii) the protection motivation role of fear highlighted by psychological studies of traditional media applies to desktop VR too.",aviation safety; emergency preparedness; fear arousal; training systems; user studies; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,Performance improvement using data tags for handheld spatial augmented reality,VRST - Virtual Reality Software and Technology,A,"Mobile devices such as some recent phones are now fitted with projection capabilities that support Spatial Augmented Reality (SAR) and require investigation to uncover new interaction possibilities. This paper presents a study measuring user performance in a search and select task using a tracked handheld projector and data tags, a 3D physical cue. This physical cue is used to mark the location of hidden SAR information. The experiment required participants to search for virtual symbols presented on two 5ft, multi-sided control panels. Two methods of presenting AR information were employed, SAR alone and SAR with the inclusion of physical cues to indicate the location of the information. The results showed that attaching data tags, compared to virtual content alone lowered the overall task completion time and reduced handheld projector movement. Subjectively, participants also preferred the combination of virtual data with data tags across both task variations",asynchronous collaboration; handheld projector; physical cues; spatial augmented reality; tangible user interface,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,A usability scale for handheld augmented reality,VRST - Virtual Reality Software and Technology,A,"Handheld augmented reality (HAR) applications must be carefully designed and improved based on user feedback to sustain commercial use. However, no standard questionnaire considers perceptual and ergonomic issues found in HAR. We address this issue by creating a HAR Usability Scale (HARUS).To create HARUS, we performed a systematic literature review to enumerate user-reported issues in HAR applications. Based on these issues, we created a questionnaire measuring manipulability – the ease of handling the HAR system, and comprehensibility – the ease of understanding the information presented by HAR. We then provide evidences of validity and reliability of the HARUS questionnaire by applying it to three experiments. The results show that HARUS consistently correlates with other subjective and objective measures of usability, thereby supporting its concurrent validity. Moreover, HARUS obtained a good Cronbach's alpha in all three experiments, thereby demonstrating internally consistency.HARUS, as well as its decomposition into individual manipulability and comprehensibility scores, are evaluation tools that researchers and professionals can use to analyze their HAR applications. By providing such a tool, they can gain quality feedback from users to improve their HAR applications towards commercial success.",augmented reality; evaluation method; handheld devices; usability; user studies,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,The influence of step frequency on the range of perceptually natural visual walking speeds during walking-in-place and treadmill locomotion,VRST - Virtual Reality Software and Technology,A,"Walking-In-Place (WIP) techniques make relatively natural walking experiences within immersive virtual environments possible when the physical interaction space is limited in size. In order to facilitate such experiences it is necessary to establish a natural connection between steps in place and virtual walking speeds. This paper details a study investigating the effects of movement type (treadmill walking and WIP) and step frequency (1.4, 1.8 and 2.2 steps per second) on the range of perceptually natural visual walking speeds. The results suggests statistically significant main effects of both movement type and step frequency but no significant interaction between the two variables.",locomotion; perceived naturalness; speed perception; virtual reality; walking-in-place,Keywords,TRUE,
Scopus,conferencePaper,2014,Displaying shapes with various types of surfaces using visuo-haptic interaction,VRST - Virtual Reality Software and Technology,A,"In this paper, we proposed a visuo-haptic system for displaying various shapes which have curve, edge, and inclined surfaces, using a simple transmutative physical device and the effect of visuo-haptic interaction. We aim to construct a perception-based shape display system to provide users with the sensation of touching virtual objects of varying shapes using only a simple mechanism. We have confirmed that the perception of each primitive shape such as curvature and angle could be modified by displacing a user's hand image on the monitor as if s/he were touching the visual shape while actually touching another shape. In this study, we constructed the method to merge these findings for displaying more various shapes, including angular ones. We built a transmutative device, which the user touches. The device does not undergo significant transformation, but its surface can be slightly bumped in and out, and displayed various shapes with various angles, length and curvature. The results of experimental trials confirmed that our method for displaying each primitive shape can also worked as designed when we combine these findings to display more complex objects using this device which transforms slightly.",perception-based shape display; virtual reality; visuo-haptics,Keywords,TRUE,
Scopus,conferencePaper,2014,In touch with the remote world: remote collaboration with augmented reality drawings and virtual navigation,VRST - Virtual Reality Software and Technology,A,"Augmented reality annotations and virtual scene navigation add new dimensions to remote collaboration. In this paper, we present a touchscreen interface for creating freehand drawings as world-stabilized annotations and for virtually navigating a scene reconstructed live in 3D, all in the context of live remote collaboration. Two main focuses of this work are (1) automatically inferring depth for 2D drawings in 3D space, for which we evaluate four possible alternatives, and (2) gesture-based virtual navigation designed specifically to incorporate constraints arising from partially modeled remote scenes. We evaluate these elements via qualitative user studies, which in addition provide insights regarding the design of individual visual feedback elements and the need to visualize the direction of drawings.",augmented reality; CSCW; depth interpretation; gesture recognition; telepresence; touch; video-mediated communication,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,A perspective geometry approach to user-perspective rendering in hand-held video see-through augmented reality,VRST - Virtual Reality Software and Technology,A,"Video see-through Augmented Reality (V-AR) displays a video feed overlaid with information, co-registered with the displayed objects. In this paper we consider the type of V-AR that is based on a hand-held device with a fixed camera. In most of the VA-R applications the view displayed on the screen is completely determined by the orientation of the camera, i.e., the device-perspective rendering; the screen displays what the camera sees. The alternative method is to use the relative pose of the user's view and the camera, i.e., the user-perspective rendering. In this paper we present an approach to the user perspective V-AR using 3D projective geometry. The view is adjusted to the user's perspective and rendered on the screen, making it an augmented window. We created and tested a running prototype based on our method.",augmented reality; dynamic frustum; user-perspective; video see-through,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2014,A projection-based mixed-reality display for exterior and interior of a building diorama,VRST - Virtual Reality Software and Technology,A,"This paper proposes an interactive display system that displays both of the exterior and interior construction of a building diorama by using a projection-based Mixed-Reality (MR) technique, which is useful for understanding the complex construction and the spatial relationships between outside and inside. The users can hold and move the diorama model using their hands/body motion, so that they can observe the model from their favorite viewpoint. Our system obtains both of the user's information (the viewpoint and the gesture) and the diorama model's information (the pose) in 3D space by using two RGB-D cameras. The CG image corresponding to the user's viewpoint, gesture and the pose of the diorama is rendered by Dual Rendering algorithm in real time. As the result, the generated CG image is projected onto the diorama to realize MR display. We confirm the effectiveness of our proposed method by developing a pilot system.",diorama interface; interactive display; projection-based mixed reality; RGB-D camera; visualization,Keywords,TRUE,
Scopus,conferencePaper,2014,A view from the hill: where cross reality meets virtual worlds,VRST - Virtual Reality Software and Technology,A,"We present the cross reality [Lifton 2007] system 'Mirrorshades', which enables a user to be present and aware of both a virtual reality environment and the real world at the same time. In so doing the challenge of the vacancy problem is addressed by lightening the cognitive load needed to switch between realities and to navigate the virtual environment. We present a case study in the context of a cultural heritage application wherein users are able to compare a reconstruction of an important 15th century chapel with its present day instantiation, whilst walking through them.",cross reality; head mounted display; indoor positioning,Abstract,TRUE,
Scopus,conferencePaper,2014,Dual sensor filtering for robust tracking of head-mounted displays,VRST - Virtual Reality Software and Technology,A,"We present a low-cost solution for yaw drift in head-mounted display systems that performs better than current commercial solutions and provides a wide capture area for pose tracking. Our method applies an extended Kalman filter to combine marker tracking data from an overhead camera with onboard head-mounted display accelerometer readings. To achieve low latency, we accelerate marker tracking with color blob localisation and perform this computation on the camera server, which only transmits essential pose data over WiFi for an unencumbered virtual reality system.",fast feature tracking; head-mounted display,Abstract,TRUE,
Scopus,conferencePaper,2014,On the benefits of stereo graphics in virtual obstacle avoidance tasks,VRST - Virtual Reality Software and Technology,A,"In virtual reality, stereo graphics is a very common way of increasing the level of perceptual realism in the visual part of the experience. However, stereo graphics comes at cost, both in technical terms and from a user perspective. In this paper, we present the preliminary results of an experiment to see if stereo makes any quantifiable, statistically significant difference in the ability to avoid collisions with virtual obstacles while navigating a 3-D space under constant acceleration. Our results indicate that for this particular application scenario, stereo does provide a significant benefit in terms of the amount of time that participants were able to avoid obstacles.",head-mounted displays; navigation; perception; stereo graphics,Abstract,TRUE,
Scopus,conferencePaper,2014,The collaborative design platform protocol: a protocol for a mixed reality installation for improved incorporation of laypeople in architecture,VRST - Virtual Reality Software and Technology,A,"We present the conceptual design and implementation of the Collaborative Design Platform Protocol (CDPP), a communication protocol that offers the synchronisation of virtual worlds between two mixed reality peers. The CDPP is applied to connect the Collaborative Design Platform (CDP), a design tool which supports the architectural design process in an early stage, with the immersive Cave Automatic Virtual Environment (CAVE) display, where the design is visualised in life-size. This creates a prototype which enables a cost-efficient and easily comprehensible presentation of the early-staged design, and thus significantly simplifies the incorporation of laypeople in the early design process. By this means, the creative capabilities of laymen are exploited to a greater extent.",architecture; collaboration; protocol,Title_Abstract,TRUE,
Scopus,conferencePaper,2014,Virtualized welding: a new paradigm for tele-operated welding,VRST - Virtual Reality Software and Technology,A,"We present a new mixed reality system that supports tele-operation of a welding robot. We create a 3D mockup of the welding pieces and use projector-based displays to visualize the welding process directly on the 3D display. Multi-cameras are used to capture both the welding environment and the operator's motion. The welder can therefore monitor and control the welding process as if the welding is on the mock-up, which provides proper spatial and 3D cues. We evaluated our system with a number of control tasks and the results shows the effectiveness of our system as compared to traditional alternatives.",engineering; human factors; human machine interaction; immersion; mixed reality; tele-operation; welding,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2015,Indoor skydiving in immersive virtual reality with embedded storytelling,VRST - Virtual Reality Software and Technology,A,"We describe the Virtual Jump Simulator, which allows subjects to perform an indoor parachute jump in a virtual environment. The necessity to jump physically off a platform combined with immersive virtual reality and tactile feedback creates an experience with a high amount of presence, as the evaluation of the prototype confirms. The system consists of a steel cube, a mechanical absorber system with stacked eccentric wheels and counterweights that allows subjects in the weight range from 35 to 150kg to jump without the need for individual calibration, a virtual reality setup with high-quality 3D content and tactile stimuli. In the immersive virtual jump experience, we embed a story using rich multimedia content, such as images and sound. We iteratively tested the entire system with users of different backgrounds. Thereby, we gathered user feedback from the very beginning to create a novel virtual reality system that allows for actual physical jumping and flying with free body movement.",entertainment; hybrid physical digital installation; immersive virtual reality; model of interactivity,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2015,Head-mounted display with mid-air tactile feedback,VRST - Virtual Reality Software and Technology,A,"Virtual and physical worlds are merging. Currently users of head-mounted displays cannot have unobtrusive tactile feedback while touching virtual objects. We present a mid-air tactile feedback system for head-mounted displays. Our prototype uses the focus of a modulated ultrasonic phased array for unobtrusive mid-air tactile feedback generation. The array and the hand position sensor are mounted on the front surface of a head-mounted virtual reality display. The presented system can enhance 3D user interfaces and virtual reality in a new way.To evaluate the tactile feedback together with visuals on an Oculus Rift VR headset, we had 13 participants do a simple virtual keypad tapping task with and without tactile feedback. The results indicate that while the measured speed and accuracy differed only a little, the subjects were nearly unanimous in that they preferred to use the tactile feedback. The ""raw"" NASA TLX questionnaires conducted after use revealed that the participants felt slightly less mental, physical and temporal demand with the tactile feedback. The participants' self-assessment of their performance was also higher with the tactile feedback.",3D interaction; 3D user interfaces; augmented reality; gestures; head-mounted display; mid-air tactile feedback; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2015,Virtual reality based laparoscopic surgery simulation,VRST - Virtual Reality Software and Technology,A,"With the development of computer graphic and haptic devices, training surgeons with virtual reality technology has proven to be very effective in surgery simulation. Many successful simulators have been deployed for training medical students. However, due to the various unsolved technical issues, the laparoscopic surgery simulation has not been widely used. Such issues include modeling of complex anatomy structure, large soft tissue deformation, frequent surgical tools interactions, and the rendering of complex material under the illumination of headlight. A successful laparoscopic surgery simulator should integrate all these required components in a balanced and efficient manner to achieve both visual/haptic quality and a satisfactory refreshing rate. In this paper, we propose an efficient framework integrating a set of specially tailored and designed techniques, ranging from deformation simulation, collision detection, soft tissue dissection and rendering. We optimize all the components based on the actual requirement of laparoscopic surgery in order to achieve an improved overall performance of fidelity and responding speed.",collision detection; deformation; dissection; laparoscopic surgery; rendering,Title_Abstract,TRUE,
Scopus,conferencePaper,2015,Embodied interaction using non-planar projections in immersive virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper we evaluate the use of non-planar projections as a means to increase the Field of View (FoV) in embodied Virtual Reality (VR). Our main goal is to bring the virtual body into the user's FoV and to understand how this affects the virtual body/environment relation and quality of interaction. Subjects wore a Head Mounted Display (HMD) and were instructed to perform a selection and docking task while using either Perspective (≈ 106 ° vertical FoV), Hammer or Equirectangular (≈ 180 ° vertical FoV for both) projection. The increased FoV allowed for a shorter search time as well as less head movements. However, quality of interaction was generally inferior, requiring more time to dock, increasing docking error and producing more body/environment collisions. We also assessed cybersickness and the sense of embodiment toward the virtual body through questionnaires, for which the difference between projections seemed to be less pronounced.",immersive interaction; non-planar projections; sense of embodiment,Title_Abstract,TRUE,
Scopus,conferencePaper,2015,Optimal camera placement for motion capture systems in the presence of dynamic occlusion,VRST - Virtual Reality Software and Technology,A,"Optical motion capture is based on estimating the three-dimensional positions of markers by triangulation from multiple cameras. Successful performance depends on points being visible from at least two cameras and on the accuracy of the triangulation. Triangulation accuracy is strongly related to the positions and orientations of the cameras. Thus, the configuration of the camera network has a critical impact on performance. A poor camera configuration may result in a low quality three-dimensional (3D) estimation and consequently low quality of tracking. This paper proposes a camera configuration metric that is based on a probabilistic model of target point visibility from cameras with ""good"" views. An efficient algorithm, based on simulated annealing, is introduced for estimating the optimal configuration of an arbitrary set of cameras for a given distribution of target points. The accuracy and robustness of the algorithm are evaluated through both simulation and empirical measurement. An implementation of the method is available for download as a tool for the community [Rahimian and Kearney 2015] (Figure 1).",augmented reality; camera placement; CAVE; dynamic occlusion; motion capture; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2015,Realizing a low-latency virtual reality environment for motor learning,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) has the potential to support motor learning in ways exceeding beyond the possibilities provided by real world environments. New feedback mechanisms can be implemented that support motor learning during the performance of the trainee and afterwards as a performance review. As a consequence, VR environments excel in controlled evaluations, which has been proven in many other application scenarios.However, in the context of motor learning of complex tasks, including full-body movements, questions regarding the main technical parameters of such a system, in particular that of the required maximum latency, have not been addressed in depth. To fill this gap, we propose a set of requirements towards VR systems for motor learning, with a special focus on motion capturing and rendering. We then assess and evaluate state-of-the-art techniques and technologies for motion capturing and rendering, in order to provide data on latencies for different setups. We focus on the end-to-end latency of the overall system, and present an evaluation of an exemplary system that has been developed to meet these requirements.",low-latency; motor learning; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2015,Dynamic projection mapping onto a deformable object with occlusion based on high-speed tracking of dot marker array,VRST - Virtual Reality Software and Technology,A,"In recent years, projection mapping has attracted much attention in a variety of fields. Generally, however, the objects in projection mapping are limited to rigid and static or quasistatic objects. Dynamic projection mapping onto a deformable object could remarkably expand the possibilities. In order to achieve such a projection mapping, it is necessary to recognize the deformation of the object even when it is occluded. However, it is still a challenging problem to achieve this task in real-time with low latency. In this paper, we propose an efficient, high-speed tracking method utilizing high-frame-rate imaging. Our method is able to track an array of dot markers arranged on a deformable object even when there is external occlusion caused by the user interaction and self-occlusion caused by the deformation of the object itself. Additionally, our method can be applied to a stretchable object. Dynamic projection mapping with our method showed robust and consistent display onto a sheet of paper and cloth with a tracking performance of about 0.2 ms per frame, with the result that the projected pattern appeared to be printed on the deformable object.",augmented reality; deformable object; high-speed vision; projection mapping; tracking,Keywords,TRUE,
Scopus,conferencePaper,2015,Modeling spatial relations of human body parts for indexing and retrieving close character interactions,VRST - Virtual Reality Software and Technology,A,"Retrieving pre-captured human motion for analyzing and synthesizing virtual character movement have been widely used in Virtual Reality (VR) and interactive computer graphics applications. In this paper, we propose a new human pose representation, called Spatial Relations of Human Body Parts (SRBP), to represent spatial relations between body parts of the subject(s), which intuitively describes how much the body parts are interacting with each other. Since SRBP is computed from the local structure (i.e. multiple body parts in proximity) of the pose instead of the information from individual or pairwise joints as in previous approaches, the new representation is robust to minor variations of individual joint location. Experimental results show that SRBP outperforms the existing skeleton-based motion retrieval and classification approaches on benchmark databases.",close interaction; human motion; motion classification; motion retrieval; spatial relations,Abstract,TRUE,
Scopus,conferencePaper,2015,Augmented reality for collision warning and path guide in a vehicle,VRST - Virtual Reality Software and Technology,A,"Today, the automotive industry is focused on human-vehicle interaction related with safety and convenience while driving. One of those is head-up display (HUD) to implement augmented reality (AR) in a vehicle for information offering [Park et al. 2013]. In this study, we discuss a vehicle Augmented Reality Information System (vARIS) to offer information such as collision warning and path guide through augmented reality (AR) in a vehicle. This system realizes AR on windshield using projection type HUD and provides AR information on 50 inch HUD display for safety and convenience of a driver. A currently commercialized HUD has 3 5 inch display.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2015,Local context based recognition + internet of things: complementary infrastructures for future generic mixed reality space,VRST - Virtual Reality Software and Technology,A,"The ""Internet of Things (IoT)"" is one of the most promising emerging technologies today. IoT refers to the concept of, if not all, many everyday objects possessing information processing power and network connectivity from which personal and context based services can be derived collectively [Perera et al, 2014]. Any useful and successful service will require on an effective and usable means for interaction, and Augmented Reality (AR) has been touted as one such ideal method. One difficulty with the proliferation of the AR technology (and associated services) is with the provision of large scale and usable contents. Useful contents and services often require recognition and tracking of a very high number of objects, and as the target objects and respective services increase, so will data redundancy and error.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2015,Resolving view difference between eye and camera for proprioceptive pointing and selection in augmented reality applications,VRST - Virtual Reality Software and Technology,A,"In this poster, we propose ""proprioceptive"" pointing (and selection) in which we use the finger and the sense of proprioception, without focusing on the finger, to point and select an object in the real world for the purpose of further interaction. The assumption is that this will reduce user fatigue since the user will switch one's focus less frequently, while still being able to point effectively through the proprioceptive sense. Figure 1 illustrates the concept and how such a technique could be effectively used e.g. for object query system using a see-through glass. In this typical scenario, the user selects an object in the real world (by proprioceptive pointing/designation), which in turn is captured by the on-glass camera and then identified and recognized for final augmentation. Note the ""blurry"" fingertip used for aiming to the target object.",,Title,TRUE,
Scopus,conferencePaper,2015,Real-time adjustment of contrast saliency for improved information visibility in mobile augmented reality,VRST - Virtual Reality Software and Technology,A,"In this work, we present a technique based on image saliency analysis to improve the conspicuity of the foreground augmentation to the background real world medium by adjusting the local brightness contrast. The proposed technique is implemented on a mobile platform considering the usage nature of AR. The saliency computation is carried out for the augmented object's representative color rather than all the pixels, and searching and adjusting over only a discrete number of brightness levels to produce the highest contrast saliency, thereby making near-real time computation possible. Thus, while the resulting imagery may not be optimal due to such a simplification, our tests showed that the visibility was still significantly improved without much difference to the ""optimal"" ground truth in terms of correctly perceiving and recognizing the augmented information",,Title,TRUE,
Scopus,conferencePaper,2015,Evaluating warped projection cameras,VRST - Virtual Reality Software and Technology,A,"Our primary motivation in this work is creating head-worn virtual reality (VR) and augmented reality (AR) systems that substantially improve human task performance, particularly in searching, counting, and navigation tasks. We focus on both widening the user's field of view, and also providing the user with options for seeing around occluding objects.",,Abstract,TRUE,
Scopus,conferencePaper,2015,Evaluation of factors affecting distance perception in architectural project review in immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"Distances are perceived as being more compressed in immersive virtual environments (IVEs) than in real environments. The goal of this study is to identify the most important factors that influence decision making and accuracy of distance perception in the context of architectural project reviews. Technical factors such as field of view, display devices and motion parallax were widely studied. In this paper, we have investigated other individual and contextual factors. We conducted a between-subject experiment using an immersive large screen display to examine the influence of the three factors: 1) the cognitive profile of the user (visual, auditory and kinesthetic - VAK), 2) the furnishing of the house, and 3) the locomotion speed, on distance perception. Results reveal that participants with visual profile were more accurate in distance estimation. Further, furnished houses were more suitable for virtual visits. The locomotion speed also seems to influence virtual visits which were better with a slow locomotion speed. Based on the results of our study, we finally present guidelines for setting up architectural project review tools which employ similar setup.",architectural project review; cognitive profile; distance estimation; distance perception; immersive virtual environment; virtual reality; virtual visits,Keywords,TRUE,
Scopus,conferencePaper,2015,Augmented reality-aided tele-presence system for robot manipulation in industrial manufacturing,VRST - Virtual Reality Software and Technology,A,"This work investigates the use of a highly immersive telepresence system for industrial robotics. A Robot Operating System integrated framework is presented where a remote robot is controlled through operator's movements and muscle contractions captured with a wearable device. An augmented 3D visual feedback is sent to the user providing the remote environment scenario from the robot's point of view and additional information pertaining to the task execution. The system proposed, using robot mounted RGB-D camera, identifies known objects and relates their pose to robot arm pose and to targets relevant to the task execution. The system is preliminary validated during a pick-and-place task using a Baxter robot. The experiment shows the practicability and the effectiveness of the proposed approach.",augmented and mixed reality; tele-operation and tele-presence; tracking and sensing,Title_Keywords,TRUE,
Scopus,conferencePaper,2016,Localized color correction for optical see-through displays via weighted linear regression,VRST - Virtual Reality Software and Technology,A,"Visual consistency in augmented reality displays requires truthful color reproduction of virtual images. However, the color distortion of Optical See-Through Displays hinders truthful color reproduction. We propose a color correction method for Optical See-Through Displays with three contributions. First, we handle non-linearity of color distortion by localized regression. Second, we model the color distortion in CIE XYZ domain, a device-independent representation of color, based on color measurements. This supports the locally linear modeling of color distortion. Finally, we introduce Hue-constrained gamut mapping for color correction. Experimental results validate the three contributions by showing critically meaningful performance gain.",CIE XYZ color space; color distortion correction; gamut mapping; local linear regression; OST-HMD,Abstract,TRUE,
Scopus,conferencePaper,2016,Perceptual enhancement for stereoscopic videos based on horopter consistency,VRST - Virtual Reality Software and Technology,A,"Audience discomfort, such as eye strain and dizziness, is one of the urgent issues that virtual reality and 3D movie technologies should tackle. Except for inappropriate horizontal and vertical disparity, one major problem is that people's binocular vergence and focal length in the cinema remain inconsistent from normal visual habits. Psychologists discovered the horopter and Panum's fusional area to describe zero-disparity points projected on the retinas based on accommodation-convergence consistency. In this paper, inspired by these concepts, we propose a stereoscopic effect correction system for perceptual enhancement according to fixated region and scene information. As a preprocessing step, tracking and stereo matching algorithms are implemented to prepare cues for further transformation in 3D space. Then in order to accomplish certain visual effects, we describe a geometric framework for disparity refinement and image warping based on parameter adjustment of the virtual stereoscopic rig. For evaluation, subjective experiments have been conducted to prove the effectiveness of our method. Therefore, our work provides a possibility to improve the audience experience from a formerly underexplored perspective.",horopter consistency; image warping; perceptual enhancement; stereoscopic videos; virtual rig modification,Abstract,TRUE,
Scopus,conferencePaper,2016,Eye gaze tracking with google cardboard using purkinje images,VRST - Virtual Reality Software and Technology,A,"Mobile phone-based Virtual Reality (VR) is rapidly growing as a platform for stereoscopic 3D and non-3D digital content and applications. The ability to track eye gaze in these devices would be a tremendous opportunity on two fronts: firstly, as an interaction technique, where interaction is currently awkward and limited, and secondly, for studying human visual behavior. We propose a method to add eye gaze tracking to these existing devices using their on-board display and camera hardware, with a minor modification to the headset enclosure. We present a proof-of-concept implementation of the technique and show results demonstrating its feasibility. The software we have developed will be made available as open source to benefit the research community.",eye tracking; low cost; purkinje images; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Acoustic selfies for extraction of external ear features in mobile audio augmented reality,VRST - Virtual Reality Software and Technology,A,"Virtual and augmented realities are expected to become more and more important in everyday life in the next future; the role of spatial audio technologies over headphones will be pivotal for application scenarios which involve mobility. This paper introduces the SelfEar project, aimed at low-cost acquisition and personalization of Head-Related Transfer Functions (HRTFs) on mobile devices. This first version focuses on capturing individual spectral features which characterize external ear acoustics, through a self-adjustable procedure which guides users in collecting such information: their mobile device must be held with the stretched arm and positioned at several specific elevation points; acoustic data are acquired by an audio augmented reality headset which embeds a pair of microphones at listener ear-canals. A preliminary measurement session assesses the ability of the system to capture spectral features which are crucial for elevation perception. Moreover, a virtual experiment using a computational auditory model predicts clear vertical localization cues in the measured features.",binaural audio; computational auditory model; head-related transfer function; headphones; mobile augmented reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,The impact of latency on perceptual judgments and motor performance in closed-loop interaction in virtual reality,VRST - Virtual Reality Software and Technology,A,"Latency between a user's movement and visual feedback is inevitable in every Virtual Reality application, as signal transmission and processing take time. Unfortunately, a high end-to-end latency impairs perception and motor performance. While it is possible to reduce feedback delay to tens of milliseconds, these delays will never completely vanish. Currently, there is a gap in literature regarding the impact of feedback delays on perception and motor performance as well as on their interplay in virtual environments employing full-body avatars. With the present study at hand, we address this gap by performing a systematic investigation of different levels of delay across a variety of perceptual and motor tasks during full-body action inside a Cave Automatic Virtual Environment. We presented participants with their virtual mirror image, which responded to their actions with feedback delays ranging from 45 to 350 ms. We measured the impact of these delays on motor performance, sense of agency, sense of body ownership and simultaneity perception by means of psychophysical procedures. Furthermore, we looked at interaction effects between these aspects to identify possible dependencies. The results show that motor performance and simultaneity perception are affected by latencies above 75 ms. Although sense of agency and body ownership only decline at a latency higher than 125 ms, and deteriorate for a latency greater than 300 ms, they do not break down completely even at the highest tested delay. Interestingly, participants perceptually infer the presence of delays more from their motor error in the task than from the actual level of delay. Whether or not participants notice a delay in a virtual environment might therefore depend on the motor task and their performance rather than on the actual delay.",body ownership; full-body motion capture; latency; sense of agency; simultaneity perception; virtual mirror,Title_Abstract,TRUE,
Scopus,conferencePaper,2016,The asynchronous time warp for virtual reality on consumer hardware,VRST - Virtual Reality Software and Technology,A,"To help create a true sense of presence in a virtual reality experience, a so called ""time warp"" may be used. This time warp does not only correct for the optical aberration of the lenses used in a virtual reality headset, it also transforms the stereoscopic images based on the very latest head tracking information to significantly reduce the motion-to-photon delay (or end-to-end latency). The time warp operates as close as possible to the display refresh, retrieves updated head tracking information and transforms a stereoscopic pair of images from representing a view at the time it was rendered, to representing the correct view at the time it is displayed. When run asynchronously to the stereoscopic rendering, the time warp can be used to increase the perceived frame rate and to smooth out inconsistent frame rates. Asynchronous operation can also improve the overall graphics hardware utilization by not requiring the stereoscopic rendering to be synchronized with the display refresh cycle. However, on today's consumer hardware it is challenging to implement a high quality time warp that is fast, has predictable latency and throughput, and runs asynchronously. This paper discusses the various challenges and the different trade-offs that need to be considered when implementing an asynchronous time warp on consumer hardware.",image warping; latency; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Towards comparable evaluation methods and measures for timing behavior of virtual reality systems,VRST - Virtual Reality Software and Technology,A,"A low latency is a fundamental timeliness requirement to reduce the potential risks of cyber sickness and to increase effectiveness, efficiency, and user experience of Virtual Reality Systems. The effects of uniform latency degradation based on mean or worst-case values are well researched. In contrast, the effects of latency jitter, the distribution pattern of latency changes over time has largely been ignored so far although today's consumer VR systems are extremely vulnerable in this respect. We investigate the applicability of the Walsh, generalized ESD, and the modified z-score test for the detection of outliers as one central latency distribution aspect. The tests are applied to well defined test cases mimicking typical timing behavior expected from concurrent architectures of today. We introduce accompanying graphical visualization methods to inspect, analyze and communicate the latency behavior of VR systems beyond simple mean or worst-case values. As a result, we propose a stacked modified z-score test for more detailed analysis.",cyber sickness; latency; outlier; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,The effects of cybersickness on persons with multiple sclerosis,VRST - Virtual Reality Software and Technology,A,"Cybersickness is commonly experienced by the users in immersive Virtual Environments (VE). It has symptoms similar to Motion Sickness, such as dizziness, nausea etc. Although there have been many cybersickness experiments conducted with persons without disabilities, persons with disabilities, such as Multiple Sclerosis (MS), have been minimally studied. This is an important area of research because cybersickness could have negative effects on virtual rehabilitation effectiveness and the accessibility of VEs. For this experiment, we recruited 16 participants - 8 persons with MS and 8 persons without MS from similar demographics (e.g. age, race). Two participants from population without MS could not complete the experiment due to severe cybersickness. We asked each participant to experience a VE. We collected Galvanic Skin response (GSR) data before and during VR exposure; GSR is commonly used as an objective measure of cybersickness. Also, Simulator Sickness Questionnaire (SSQ) feedback was recorded before and after the experiment. SSQ results show that the VE induced cybersickness in the participants. The GSR data suggests that the cybersickness may have induced similar physiological changes in participants with MS as participants without MS, albeit with greater variability in participants without MS. However, participants with MS had significantly lower GSR during VR exposure. In this paper, we compare the effects of cybersickness between the people with MS and the people without MS with respect to SSQ score and GSR data.",accessibility; cybersickness; multiple sclerosis; user studies; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,Investigating the process of emotion recognition in immersive and non-immersive virtual technological setups,VRST - Virtual Reality Software and Technology,A,"This paper investigates the use of Immersive Virtual Environment (IVE) to evaluate the process of emotion recognition from faces (ERF). ERF has been mostly probed by using still photographs resembling universal expressions. However, this approach does not reflect the vividness of faces. Virtual Reality (VR) makes use of animated agents, trying to overcome this issue by reproducing the inherent dynamic of facial expressions, but outside a natural environment. We suggest that a setup using IVE technology simulating a real scene in combination with virtual agents (VAs) displaying dynamic facial expressions should improve the study of ERF. To support our claim we carried out an experiment in which two groups of subjects had to recognize VAs facial expression of universal and basic emotions in IVE and No-IVE condition. The goal was to evaluate the impact of the immersion in VE for ERF investigation. Results showed that the level of immersion in IVE does not interfere with the recognition task and a high level of accuracy in facial recognition suggests that IVE can be used to investigate the process of ERF.",ekman basic emotion; emotion recognition; emotional virtual agents; facial expression; immersive virtual environment; virtual environments; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Provision of automated step-by-step procedural guidance in virtual reality surgery simulation,VRST - Virtual Reality Software and Technology,A,"One of the roadblocks to the wide-spread use of virtual reality simulation as a surgical training platform is the need for expert supervision during training to ensure proper skill acquisition. To fully utilize the capacity of virtual reality in surgical training, it is imperative that the guidance process is automated. In this paper, we discuss a method of providing one aspect of performance guidance: advice on the steps of a surgery or procedural guidance. We manually segment the surgical trajectory of an expert surgeon into steps and present them one at a time to guide trainees through a surgical procedure. We show, using a randomized controlled trial, that this form of guidance is effective in moving trainee behavior towards an expert ideal.To support practice variation and different surgical styles adopted by experts, separate guidance templates have to be generated. To enable this, we introduce a method of automatically segmenting a surgical trajectory into steps. We propose a pre-processing step that uses domain knowledge specific to our application to reduce the solution space. We show how this can be incorporated into existing trajectory segmentation methods, as well as a greedy approach that we propose. We compare this segmentation method to existing techniques and show that it is accurate and efficient.",automated guidance; surgery simulation; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Head turn scaling below the threshold of perception in immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"Immersive virtual environments allow to experience presence, the feeling of being present in a virtual environment. When accessing virtual reality with virtual reality goggles, head tracking is used to update the virtual viewpoint according to the user's head movement. While typically used unmodified, the extent to which the virtual viewpoint follows the real head motion can be scaled. In this paper, the effect of scaling below the threshold of perception on presence during a target acquisition task was studied. It was assumed, that presence is reduced when head motion is scaled. No effect on presence, simulator sickness and performance was found. A significant effect on physical task load was found. The results yield information for further work and for the required verification of the used concept of presence. It can be assumed, that load can be modified by the scaling without significantly influencing the quality of presence.",empirical study; head tracking manipulation; immersive virtual environments; perception; presence; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,"Noise-cancelling, steps and soundscapes: the effect of auditory stimulation on presence in virtual realities while walking",VRST - Virtual Reality Software and Technology,A,"This study investigates the influence of different auditory stimuli on perceived presence, the feeling of ""being there"", on a walk through a park-like virtual environment. A single-factorial design with five levels varying the surrounding sound impressions was employed, including the conditions ""No Headphones"", ""Noise-Cancelling"", ""Steps"", ""Soundscape"" and ""Steps &amp; Soundscape"", in order to find out which of the conditions would enhance the feeling of presence most. 36 participants rated their impression of presence using a questionnaire after walking on a treadmill while wearing a head-mounted display and Noise-Cancelling headphones. Statistical analysis of the data showed that the conditions including soundscapes resulted in significantly higher ratings of presence and realism, compared to all other auditory conditions. The results are placed into the context of findings in other studies and point to further research needs regarding the auditory enhancement of virtual environments.",acoustic; head-mounted display; multimodality; noise-cancelling; presence; self-motion; soundscape; treadmill; virtual reality; visual,Keywords,TRUE,
Scopus,conferencePaper,2016,PedVR: simulating gaze-based interactions between a real user and virtual crowds,VRST - Virtual Reality Software and Technology,A,"We present a novel interactive approach, PedVR, to generate plausible behaviors for a large number of virtual humans, and to enable natural interaction between the real user and virtual agents. Our formulation is based on a coupled approach that combines a 2D multi-agent navigation algorithm with 3D human motion synthesis. The coupling can result in plausible movement of virtual agents and can generate gazing behaviors, which can considerably increase the believability. We have integrated our formulation with the DK-2 HMD and demonstrate the benefits of our crowd simulation algorithm over prior decoupled approaches. Our user evaluation suggests that the combination of coupled methods and gazing behavior can considerably increase the behavioral plausibility.",crowds; human agents; multi-agent simulation; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,A real-time x-ray mobile application using augmented reality and google street view,VRST - Virtual Reality Software and Technology,A,"X-ray view can be defined as the ability one has to see through real surfaces. Although this skill is often associated with superheroes and medical examination, there are several researches conducted to employ X-ray view in numerous applications. However, the generation of X-ray visualization includes numerous challenges regarding occlusion, realistic appearance, and depth perception.In this paper, we present a mobile application that uses Augmented Reality and Google Street View to allow users experience real-time X-ray vision. The proposed application was designed to enhance previous Augmented Reality X-ray systems, by introducing a silhouette computation method to provide visual context from the occluder and a perspective estimation system that improves the projection of occluded images into the real scene.Additionally, we implemented two usability studies to assess qualitative aspects of both silhouettes and perspective estimation to generate better X-ray effects. Results indicate good acceptance of the novel X-ray visualization method and a great usability score on the SUS scale for the mobile application.",augmented reality; image and video processing in UI; mobile and embedded devices,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,A study on improving close and distant device movement pose manipulation for hand-held augmented reality,VRST - Virtual Reality Software and Technology,A,"Hand-held smart devices are equipped with powerful processing units, high resolution screens and cameras, that in combination makes them suitable for video see-through Augmented Reality. Many Augmented Reality applications require interaction, such as selection and 3D pose manipulation. One way to perform intuitive, high precision 3D pose manipulation is by direct or indirect mapping of device movement.There are two approaches to device movement interaction; one fixes the virtual object to the device, which therefore becomes the pivot point for the object, thus makes it difficult to rotate without translate. The second approach avoids latter issue by considering rotation and translation separately, relative to the object's center point. The result of this is that the object instead moves out of view for yaw and pitch rotations.In this paper we study these two techniques and compare them with a modification where user perspective rendering is used to solve the rotation issues. The study showed that the modification improves speed as well as both perceived control and intuitiveness among the subjects.",augmented reality; device interaction; device perspective; user study; user-perspective; video see-through,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Multi-view gesture annotations in image-based 3D reconstructed scenes,VRST - Virtual Reality Software and Technology,A,"We present a novel 2D gesture annotation method for use in image-based 3D reconstructed scenes with applications in collaborative virtual and augmented reality. Image-based reconstructions allow users to virtually explore a remote environment using image-based rendering techniques. To collaborate with other users, either synchronously or asynchronously, simple 2D gesture annotations can be used to convey spatial information to another user. Unfortunately, prior methods are either unable to disambiguate such 2D annotations in 3D from novel viewpoints or require relatively dense reconstructions of the environment.In this paper, we propose a simple multi-view annotation method that is useful in a variety of scenarios and applicable to both very sparse and dense 3D reconstructions. Specifically, we employ interactive disambiguation of the 2D gestures via a second annotation drawn from another viewpoint, triangulating two drawings to achieve a 3D result. Our method automatically chooses an appropriate second viewpoint and uses image-based rendering transitions to keep the user oriented while moving to the second viewpoint. User experiments in an asynchronous collaboration scenario demonstrate the usability of the method and its superiority over a baseline method. In addition, we showcase our method running on a variety of image-based reconstruction datasets and highlight its use in a synchronous local-remote user collaboration system.",3D reconstruction; annotations; augmented reality; collaboration; image-based reconstruction; image-based rendering; interactive disambiguation; virtual navigation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Robot gardens: an augmented reality prototype for plant-robot biohybrid systems,VRST - Virtual Reality Software and Technology,A,"Robot Gardens are an augmented reality concept allowing a human user to design a biohybrid, plant-robot system. Plants growing from deliberately placed seeds are directed by robotic units that the user can position, configure and activate. For example, the robotic units may serve as physical shields or frames but they may also guide the plants' growth through emission of light. The biohybrid system evolves over time to redefine architectural spaces. This gives rise to the particular challenge of designing a biohybrid system before its actual implementation and potentially long before its developmental processes unfold. Here, an augmented reality interface featuring according simulation models of plants and robotic units allows one to explore the design space a priori. In this work, we present our first functional augmented reality prototype to design biohybrid systems. We provide details about its workings and elaborate on first empirical studies on its usability.",augmented reality; biohybrids; interactive simulation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,A platform for bimanual virtual assembly training with haptic feedback in large multi-object environments,VRST - Virtual Reality Software and Technology,A,"We present a virtual reality platform which addresses and integrates some of the currently challenging research topics in the field of virtual assembly: realistic and practical scenarios with several complex geometries, bimanual six-DoF haptic interaction for hands and arms, and intuitive navigation in large workspaces. We put an especial focus on our collision computation framework, which is able to display stiff and stable forces in 1 kHz using a combination of penalty- and constraint-based haptic rendering methods. Interaction with multiple arbitrary geometries is supported in realtime simulations, as well as several interfaces, allowing for collaborative training experiences. Performance results for an exemplary car assembly sequence which show the readiness of the system are provided.",haptic devices; haptic rendering; interaction techniques; virtual assembly,Abstract,TRUE,
Scopus,conferencePaper,2016,A fast and robust Six-DoF god object heuristic for haptic rendering of complex models with friction,VRST - Virtual Reality Software and Technology,A,"Collision detection and force computation between complex geometries are essential technologies for virtual reality and robotic applications. Penalty-based haptic rendering algorithms provide a fast collision computation solution, but they cannot avoid the undesired interpenetration between virtual objects, and have difficulties with thin non-watertight geometries. God object methods or constraint-based haptic rendering approaches have shown to solve this problem, but are typically complex to implement and computationally expensive. This paper presents an easy-to-implement god object approach applied to six-DoF penalty-based haptic rendering algorithms. Contact regions are synthesized to penalty force and torque values and these are used to compute the position of the god object on the surface. Then, the pose of this surface proxy is used to render stiff and stable six-DoF contacts with friction. Independently of the complexity of the used geometries, our implementation runs in only around 5 μs and the results show a maximal penetration error of the resolution used in the penalty-based haptic rendering algorithm.",haptic devices; haptic rendering; interaction techniques; virtual assembly,Abstract,TRUE,
Scopus,conferencePaper,2016,VR360HD: a VR360° player with enhanced haptic feedback,VRST - Virtual Reality Software and Technology,A,"We present a VR360° video player with haptic feedback playback. The VR360HD application enhances VR viewing experience by triggering customized haptic effects associated with user's activities, biofeedback, network messages and customizable timeline triggers incorporated in the VR media. The app is developed in the Unity3D game engine and tested using a GearVR headset, therefore allowing users to add animations to VR gameplay and to the VR360° streams. A custom haptic plugin allows users to author and associate animated haptic effects to the triggers, and playback these effects on a custom haptic hardware, the Haptic Chair. We show that the VR360HD app creates rich tactile effects and can be easily adapted to other media types.",haptic feedback; virtual reality; VR viewing,Keywords,TRUE,
Scopus,conferencePaper,2016,Procedurally generated virtual reality from 3D reconstructed physical space,VRST - Virtual Reality Software and Technology,A,"We present a novel system for automatically generating immersive and interactive virtual reality (VR) environments using the real world as a template. The system captures indoor scenes in 3D, detects obstacles like furniture and walls, and maps walkable areas (WA) to enable real-walking in the generated virtual environment (VE). Depth data is additionally used for recognizing and tracking objects during the VR experience. The detected objects are paired with virtual counterparts to leverage the physicality of the real world for a tactile experience. Our approach is new, in that it allows a casual user to easily create virtual reality worlds in any indoor space of arbitrary size and shape without requiring specialized equipment or training. We demonstrate our approach through a fully working system implemented on the Google Project Tango tablet device.",3D reconstruction; computer vision; depth cameras; locomotion; mobile computing; obstacle avoidance; procedural generation; tracking; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Large scale cut plane: an occlusion management technique for immersive dense 3D reconstructions,VRST - Virtual Reality Software and Technology,A,"Dense 3D reconstructions of real-world environments become wide spread and are foreseen to act as data base to solve real world problems, such as remote inspections. Therefore not only scene viewing is required but also the ability to interact with the environment, such as selection of a user-defined part of the reconstruction for later usage. However, inter-object occlusion is inherent to large dense 3D reconstructions, due to scene geometry or reconstruction artifacts that might result in object containment. Since prior art lacks approaches for occlusion management in environments that consist of one or multiple (large) continuous surfaces, we propose the novel technique Large Scale Cut Plane that enables segmentation and subsequent selection of visible, partly or fully occluded patches within a large 3D reconstruction, even at far distance. We combine Large Scale Cut Plane with an immersive virtual reality setup to foster 3D scene understanding and natural user interactions. We furthermore present results from a user study where we investigate performance and usability of our proposed technique compared to a baseline technique. Our results indicate Large Scale Cut Plane to be superior in terms of speed and precision, while we found need of improvement of the user interface. The presented investigations has to the authors' best knowledge not been subject to previous research.",3D selection; dense 3D surface reconstruction; immersive virtual reality; occlusion management,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,6-DOF computation and marker design for magnetic 3D dexterous motion-tracking system,VRST - Virtual Reality Software and Technology,A,"We describe our approach that derives reliable 6-DOF information including the translation and the rotation of a rigid marker in a 3D space from a set of insufficient 5-DOF measurements. As a practical example, we carefully constructed a prototype and its design and evaluated it in our 3D dexterous motion-tracking system, IM6D, which is our novel real-time magnetic 3D motion-tracking system that uses multiple identifiable, tiny, lightweight, wireless, and occlusion-free markers. The system contains two key technologies; a 6-DOF computation algorithm and a marker design for 6D marker. The 6-DOF computation algorithm computes the result of complete 6-DOF information including translation and rotation in 3D space for a single rigid marker that consists of three LC coils. We propose several possible approaches for implementation, including geometric, matrix-based kinematics, and computational approaches. In addition, we introduce workflow to find an optimal marker design for the system to achieve the best compromise between its smallness and accuracy based on the tracking principle. We experimentally compare the performances of some typical marker prototypes with different layouts of LC coils. Finally, we also show another experimental result to prove the effectiveness of the results from the solutions in these two problems.",3D interaction; 3D user interface; augmented reality; input devices; motion capture; sensor; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,Missing the point: an exploration of how to guide users' attention during cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"Recent technological advances have brought Virtual Reality (VR) into the homes of consumers, and there is a growing interest in bringing cinematic experiences from the screen and into VR. However, cinematic VR limits filmmakers' ability to effectively guide the audience's attention. In this paper we present a taxonomy of approaches to guiding users' attention, and present a study comparing two such approaches with a control condition devoid of guidance. One approach guides users by controlling their body's orientation, and the other implicitly directs their attention by encouraging them to follow a firefly with their gaze. The results revealed interesting, albeit statistically insignificant, indications that assuming control of the user's action may negatively influence presence, whereas the firefly was perceived as significantly more helpful.",attention; cinematic VR; film; presence; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Depth information from binocular disparity and familiar size is combined when reaching towards virtual objects,VRST - Virtual Reality Software and Technology,A,"Reaching movements towards stereoscopically presented virtual objects have been reported to be imprecise. This might be a problem for touch interaction with virtual environments. Estimating the distance to an object in personal space relies on binocular disparity and other depth cues but previous work on the influence of familiar size for reaching and grasping has produced conflicting results. We presented a virtual tennis ball and manipulated binocular disparity as well as the size of the tennis ball. The results suggest that depth information from binocular disparity and from familiar size is combined for reaching movements towards virtual objects. However, subjects differed in the weight they assigned to each depth cue.",depth perception; distance estimates; reaching; stereoscopy; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,Perceiving depth: optical versus video see-through,VRST - Virtual Reality Software and Technology,A,"Head-Mounted Displays (HMDs) and similar 3D visualization devices are becoming ubiquitous. Going a step forward, HMD see-through systems bring virtual objects to real world settings, allowing augmented reality to be used in complex engineering scenarios. Of these, optical and video see-through systems differ on how the real world is captured by the device. To provide a seamless integration of real and virtual imagery, the absolute depth and size of both virtual and real objects should match appropriately. However, these technologies are still in their early stages, each featuring different strengths and weaknesses which affect the user experience. In this work we compare optical to video see-through systems, focusing on depth perception via exocentric and egocentric methods. Our study pairs Meta Glasses, an off-the-shelf optical see-through, to a modified Oculus Rift setup with attached video-cameras, for video see-through. Results show that, with the current hardware available, the video see-through configuration provides better overall results. These experiments and our results can help interaction designers for both virtual and augmented reality conditions.",augmented reality; depth perception; see-through system; user evaluation,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Vista widgets: a framework for designing 3D user interfaces from reusable interaction building blocks,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) has been an active field of research for several decades, with 3D interaction and 3D User Interfaces (UIs) as important sub-disciplines. However, the development of 3D interaction techniques and in particular combining several of them to construct complex and usable 3D UIs remains challenging, especially in a VR context. In addition, there is currently only limited reusable software for implementing such techniques in comparison to traditional 2D UIs. To overcome this issue, we present ViSTA Widgets, a software framework for creating 3D UIs for immersive virtual environments. It extends the ViSTA VR framework by providing functionality to create multi-device, multi-focus-strategy interaction building blocks and means to easily combine them into complex 3D UIs. This is realized by introducing a device abstraction layer along sophisticated focus management and functionality to create novel 3D interaction techniques and 3D widgets. We present the framework and illustrate its effectiveness with code and application examples accompanied by performance evaluations.",3D interaction; 3D user interfaces; framework; multi-device; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Improving freehand placement for grasping virtual objects via dual view visual feedback in mixed reality,VRST - Virtual Reality Software and Technology,A,"This paper presents a first study into the use of dual view visual feedback in an exocentric MR environment for assisting freehand grasping of virtual objects. Recent work has highlighted problems associated with user errors in freehand grasping, via an analysis of virtual object type, location and size. Our work presents an extension to this, where 30 participants are recruited for two experiments (one assessing object size and the second object position), giving 1710 grasps in total. We report on results following the same protocol of the aforementioned study using a dual view visual feedback method. Results show that dual view visual feedback significantly increases user z placement accuracy and improves grasp placement in the x and y axes, however completion time is significantly higher. No improvement was found in user grasp aperture using dual view visual feedback for changes in object size and position.",dual view feedback; freehand interaction; grasping; human performance measurement; mixed reality; natural hand interaction; visual feedback,Title_Keywords,TRUE,
Scopus,conferencePaper,2016,Combining bimanual interaction and teleportation for 3D manipulation on multi-touch wall-sized displays,VRST - Virtual Reality Software and Technology,A,"While multi-touch devices are well established in our everyday life, they are currently becoming larger and larger. Large screens such as wall-sized displays are now equipped with multi-touch capabilities. Multi-touch wall-sized displays will become widespread in a near future in various places such as public places or meeting rooms. These new devices are an interesting opportunity to interact with 3D virtual environments: the large display surface offers a good immersion, while the multi-touch capabilities could make interaction with 3D content accessible to the general public.In this paper, we aim to explore touch-based 3D interaction in the situation where users are immersed in a 3D virtual environment and move in front of a vertical wall-sized display. We design In(SITE), a bimanual touch-based technique combined with object teleportation features which enables users to interact on a large wall-sized display. This technique is compared with a standard 3D interaction technique for performing 6 degrees of freedom manipulation tasks on a wall-sized display. The results of two controlled experiments show that participants can reach the same level of performance for completion time and a better precision for fine adjustments of object position with the In(SITE) technique. They also suggest that combining object teleportation with both techniques improves translations in terms of ease of use, fatigue, and user preference.",3D manipulation; multi-touch interaction; virtual reality; wall-sized display,Keywords,TRUE,
Scopus,conferencePaper,2016,"A compact, wide-FOV optical design for head-mounted displays",VRST - Virtual Reality Software and Technology,A,"We present a new optical design for head-mounted displays (HMD) which has an exceptionally wide field of view (FOV). It can cover even the full human FOV. It is based on seamless lenses and screens curved around the eyes. The proof-of-concept prototypes are promising, and one of them far exceeds the human FOV, although the effective FOV is limited by the anatomy of the human head. The presented optical design has advantages such as compactness, light weight, low cost and super-wide FOV with high resolution. Even though this is still work-in-progress and display functionality is not yet implemented, it suggests a feasible way to significantly expand the FOV of HMDs.",field-of-view; head-mounted display; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,"A low-cost, variable, interactive surface for mixed-reality tabletop games",VRST - Virtual Reality Software and Technology,A,"This paper introduces an interactive surface concept for Mixed Reality (MR) tabletop games that combines a variable (LCD and/or projection) screen configuration with the detection of finger touches, in-air gestures, and tangibles. It is low-cost and minimally requires an ordinary table, a TV screen, and a Kinect v2 sensor. Existing applications can easily be connected by being compliant to standards. The concept is intended to foster further research on collaborative tabletop situations, not limited to games, but also including learning, meetings, and social interaction.",interactive surface; mixed reality; tabletop game,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,AR interaction paradigm for closed reduction of long-bone fractures via external fixation,VRST - Virtual Reality Software and Technology,A,"We present an intuitive and ergonomic AR strategy to be coupled with a standard external fixation system aimed at aiding the accurate closed reduction of long-bone shaft fractures. The correct six DOF alignment between the bone fragments can be retrieved by manually repositioning a pair of reference frames constrained to the two extremities of the fixator so as to minimize the geometric distance, on the image plane, between planned/virtual landmarks and their observed/real counterparts. The reduction accuracy was positively validated in vitro in a pilot study that involved an orthopedic surgeon.",augmented reality; content strategy; novel interaction techniques; surgical navigation,Keywords,TRUE,
Scopus,conferencePaper,2016,Are age differences missing in relative and absolute distance perception of stereoscopically presented virtual objects?,VRST - Virtual Reality Software and Technology,A,"Nowadays there is a wide variety of Virtual Reality (VR) applications for users of all age groups. An essential part of most VR systems is stereoscopy. From perceptual research, it is known that stereoscopic perception deteriorates with age [Garnham and Sloper 2006]. As indicator for stereoscopic perception, the authors of this and further studies used stereo acuity, i.e. the smallest disparity difference detected by the visual system. Norman et al. [2000] showed that the declined stereo acuity in older humans can cause these users to perceive less depth in random-dot stereograms. Since VR applications for rehabilitation are advancing, one can expect increasing numbers of elderly users. However, up to now it is unclear whether the age-specific changes in stereoscopic perception might impair the use of VR applications for older people. In our study, we presented stereoscopically rendered virtual objects. The primary aim of this study was to examine possible age differences in fusion range, relative and absolute distance perception, and visual fatigue. Here, we present results of the two tasks concerning distance perception.",binocular eye movements; depth perception; stereoscopy,Abstract,TRUE,
Scopus,conferencePaper,2016,Audio feedback and illusion of virtual body ownership in mixed reality,VRST - Virtual Reality Software and Technology,A,"This paper presents an exploratory experiment measuring the role of audio feedback on the illusion of virtual body ownership (IVBO) under non-immersive mixed reality (MR) settings with Human and Non-Human avatars. Our preliminary results revealed that all avatars elicited a similar level of IVBO, despite the addition of audio feedback.",audio; avatar embodiment; mixed reality; realism,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Augmented reality based remote coaching system,VRST - Virtual Reality Software and Technology,A,"In this paper, we present an AR-based tele-coaching system for fast-paced tasks, applied to the game of tennis, and explore for interface design guidelines through a user study. We have evaluated the player's performance for instruction understanding in three different modalities of augmentation: (1) AR — visual only, (2) Sound — aural only and (3) Multimodal — both visual and aural. The augmented instructions were useful even in the stringent temporal conditions, and most effective with the visual only augmentation due to its modal capability of encoding and presenting several information compactly at once.",augmented reality; multimodal feedback; pre-attentive recognition; tele-coaching,Title_Keywords,TRUE,
Scopus,conferencePaper,2016,Augmented reasoning in the mirror world,VRST - Virtual Reality Software and Technology,A,"In order to enable a social agent to behave in a believable and realistic way, it needs a wide range of information in the form of both low-level value-based data as well as high-level semantic knowledge. In this work we propose a system that puts a virtual reality layer between the real world and an agent's knowledge representation. This mirror world allows the agent to use its abstract representation of the environment and inferred events as an additional source of knowledge when reasoning about the real world. Additionally, users and developers can use the mirror world, with its visualized data and highlighting of the agent's reasoning, for further understanding of the agent's behavior, debugging and testing, or the simulation of additional sensor input.",knowledge representation; mirror world; MR; reasoning; social robots; visualization; VR,Abstract,TRUE,
Scopus,conferencePaper,2016,Avatar anthropomorphism and acrophobia,VRST - Virtual Reality Software and Technology,A,"In this paper, we investigate the impact of avatar anthropomorphism on the fear of heights, when using full body avatar embodiment under an immersive virtual reality (VR) setting. Clear differences could be found in perceived anthropomorphism, but preliminary results do not show differences in stress level between Human and Non-Human avatars, although a high level of perceived secureness was reported with Non-Human avatars.",acrophobia; avatar embodiment; virtual therapy,Abstract,TRUE,
Scopus,conferencePaper,2016,Breaking bad behavior: immersive training of class room management,VRST - Virtual Reality Software and Technology,A,"This article presents a fully immersive portable low-cost Virtual Reality system to train classroom management skills. An instructor controls the simulation of a virtual classroom populated with 24 semi-autonomous virtual agents via a desktop-based graphical user interface (GUI). The GUI provides behavior control and trainee evaluation widgets alongside a non-immersive view of the class and the trainee. The trainee's interface uses an Head-Mounted Display (HMD) and earphones for output. A depth camera and the HMD's built-in motion sensors are used for tracking the trainee and for avatar animation. An initial evaluation of both interfaces confirms the system's usefulness, specifically its capability to successfully simulate critical aspects of classroom management.",class room management; student simulation; virtual agent interaction; virtual reality training,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,"Comparison of gesture, gamepad, and gaze-based locomotion for VR worlds",VRST - Virtual Reality Software and Technology,A,"In this paper we present a VR locomotion technique based on the Leap Motion device and compare it to other often-used locomotion techniques — gaze-directed locomotion and gamepad-based locomotion. We performed a user experiment to evaluate the three techniques based on their performance (time to complete the task), comfort (through the ISO 9241–9 ssessment of comfort questionnaire), and simulation sickness (through the Simulation Sickness Questionnaire). Results indicate that the gamepad technique is both faster and more comfortable than either the Leap Motion-based or the gaze-directed techniques.",HCI; interaction device; leap motion; locomotion; performance measurement; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,"Concept for content-aware, automatic shifting for spherical panoramas",VRST - Virtual Reality Software and Technology,A,"With the adaption of virtual reality in the consumer space, spherical panorama photos are gaining popularity. Through wide-angle head-mounted displays, they can be experienced in a natural way and offer the user an immersive view of the captured scene. While being used in virtual reality, the alignment of the saved image does not matter much. However, when displaying the panorama on a 2D screen, the alignment can make a difference on how pleasant the image looks. We propose an automatic method to do lossless shifting of the image to make it look better on 2D screens.",360 degree panoramas; image processing; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Concept for using eye tracking in a head-mounted display to adapt rendering to the user's current visual field,VRST - Virtual Reality Software and Technology,A,"With increasing spatial and temporal resolution in head-mounted displays (HMDs), using eye trackers to adapt rendering to the user is getting important to handle the rendering workload. Besides using methods like foveated rendering, we propose to use the current visual field for rendering, depending on the eye gaze. We use two effects for performance optimizations. First, we noticed a lens defect in HMDs, where depending on the distance of the eye gaze to the center, certain parts of the screen towards the edges are not visible anymore. Second, if the user looks up, he cannot see the lower parts of the screen anymore. For the invisible areas, we propose to skip rendering and to reuse the pixels colors from the previous frame. We provide a calibration routine to measure these two effects. We apply the current visual field to a renderer and get up to 2x speed-ups.",eye tracking; head-mounted display; rendering; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,Cyber sick but still having fun,VRST - Virtual Reality Software and Technology,A,"In this paper, we present our efforts towards creating a deliberately sickening virtual reality (VR) game. Based on a considerable number of tests, we can show that games can be enjoyable despite experienced adverse effects that arise from frequent acceleration in VR and additional post-processing distortions of the rendered scene. We briefly explain the rationale that drove us to take these measures, details about their realisation and results from a questionary-based user evaluation.",driving; locomotion; motion sickness; simulator sickness; VR; VR sickness,Abstract,TRUE,
Scopus,conferencePaper,2016,Effects of speed and transitions on target-based travel techniques,VRST - Virtual Reality Software and Technology,A,"Travel on Virtual Environments is the simple action where a user moves from a starting point A to a target point B. Choosing an incorrect type of technique could compromise the Virtual Reality experience and cause side effects such as spatial disorientation, fatigue and cybersickness. The design of effective travelling techniques demands to be as natural as possible, thus real walking techniques presents better results, despite their physical limitations. Approaches to surpass these limitations employ techniques that provide an indirect travel metaphor such as point-steering and target-based. In fact, target-based techniques evince a reduction in fatigue and cybersickness against the point-steering techniques, even though providing less control. In this paper we investigate further effects of speed and transition on target-based techniques on factors such as comfort and cybersickness using a Head-Mounted Display setup.",cyber-sickness; navigation; travel techniques; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Exploring floating stereoscopic driver-car interfaces with wide field-of-view in a mixed reality simulation,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose a floating, multi-layered, wide field-of-view user interface for car drivers. It utilizes stereoscopic depth and focus blurring to highlight items with high priority or urgency. Individual layers are additionally used to separate groups of UI elements according to importance or context. Our work is motivated by two main prospects: a fundamentally changing driver-car interaction and ongoing technology advancements for mixed reality devices. A working prototype has been implemented as part of a custom driving simulation and will be further extended. We plan evaluations in contexts ranging from manual to fully automated driving, providing context-specific suggestions. We want to determine user preferences for layout and prioritization of the UI elements, perceived quality of the interface and effects on driving performance.",3DUI; automotive; MR simulation; user-centered,Title_Abstract,TRUE,
Scopus,conferencePaper,2016,Hybrid team interaction in the mixed reality continuum,VRST - Virtual Reality Software and Technology,A,"This paper describes a system, which enables collaboration in a hybrid team consisting of a robot, physically present humans and remote humans, where the latter are connected via Virtual Reality. This setup spans the whole continuum between Physical and Virtual Reality, including Augmented Reality. The work presented herein, describes how such a scattered, hybrid team can interact and cooperate in a virtual representation of a factory, using eye-, head-, hand- and gesture-tracking as multimodal control and communication input.",augmented reality; human-robot interaction; interactive collaboration; multimodal input; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Immersive remote grasping: realtime gripper control by a heterogenous robot control system,VRST - Virtual Reality Software and Technology,A,"Current developments in the field of user interface (UI) technologies as well as robotic systems provide enormous potential to reshape the future of human-robot interaction (HRI) and collaboration. However, the design of reliable, intuitive and comfortable user interfaces is a challenging task. In this paper, we focus on one important aspect of such interfaces, i.e., teleoperation. We explain how to setup a heterogeneous, extendible and immersive system for controlling a distant robotic system via the network. Therefore, we exploit current technologies from the area of virtual reality (VR) and the Unity3D game engine in order to provide natural user interfaces for teleoperation. Regarding robot control, we use the well-known robot operating system (ROS) and apply its freely available modular components. The contribution of this work lies in the implementation of a flexible immersive grasping control system using a network layer (ROSbridge) between Unity3D and ROS for arbitary robotic hardware.",human-robot interaction; tele-operation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,"Inception: a creative coding environment for virtual reality, in virtual reality",VRST - Virtual Reality Software and Technology,A,"In this paper we build and evaluate a live-programming system for digital artists to create artwork within a virtual reality environment. Using the large display space that virtual reality provides, we develop an interaction for artists to see parallel evaluations of their art. A study of the system with ten participants demonstrated that parallel editing and execution is accessible to designers and that designers can leverage these techniques to survey more options faster.",programming; smartwatch interfaces; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Industrial maintenance with augmented reality: two case studies,VRST - Virtual Reality Software and Technology,A,"Remote maintenance of industrial manipulators often is performed via telephone support. Recent approaches in the context of the 'Industry 4.0' consider internet technologies and Augmented Reality (AR) to enhance situation awareness between external experts and local service technicians. We present two AR-based case studies: First, a mobile AR architecture based on optical see through glasses is used for an on-site local repair task. Second, a remote architecture based on a portable tablet PC and a high precision tracking system is used to realize an off-site expert access. The to-be-serviced machine is visualized inside of a large area similar to a machinery hall and can be inspected by the experts walking around this virtual plant using the tablet and perspectively correct rendering to understand the production process and the operation context. Both methods have been evaluated in first user studies.",augmented reality; industrial internet; industry 4.0; maintenance; situation awareness,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Interactive gamified 3D-training of affine transformations,VRST - Virtual Reality Software and Technology,A,"This article presents the Gamified Training Environment for Affine Transformations (GEtiT). GEtiT uses a 3D environment to visualize the effects of object rotation, translation, scaling, reflection, and shearing in 3D space. It encodes the abstract knowledge about homogeneous transformations and their order of application using specific game mechanics encoding 3D movements on different levels of abstraction. Progress in the game requires mastering of the game mechanics of a certain level of abstraction to modify objects in 3D space to a desired goal position and/or shape. Each level increases the abstraction of the representation towards a final 4 × 4 homogeneous matrix representation. Executing the game mechanics during the gameplay results in an effective training of knowledge due to a constant repetition. Evaluation showed a learning effect that is equal to a traditional training method while it achieved a higher enjoyment of use indicating that the learning quality was superior to the traditional training method.",education; gamification; serious games; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,Interference measurement of kinect for xbox one,VRST - Virtual Reality Software and Technology,A,"Microsoft Kinect is widely used for tracking human body in a range of applications. Although Kinect for Xbox One allows for multi-user tracking, it is not possible to use it in large spaces due to its limited range. Hence, using multiple Kinect sensors for large environments seems to be an appropriate solution. Thus, it is important to know if multiple sensors can be used simultaneously for such applications without interfering with each other. In this paper, we investigate the effect of using multiple Kinects on each other by performing multiple measurements in different settings. Our results show that some occasional interference might happen in some specific constellations, when the sensors are facing the same target. Our recommendation is to avoid such constellations, or to perform a simple interference measurement before using multiple sensors in specific settings.",structured light; time of flight; tracking; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,Maintainable management and access of lexical knowledge for multimodal virtual reality interfaces,VRST - Virtual Reality Software and Technology,A,"This poster presents a maintainable method to manage lexical information required for multimodal interfaces. It is tailored for the application in real-time interactive systems, specifically for Virtual Reality, and solves three problems commonly encountered in this context: (1) The lexical information is defined on and grounded in a common knowledge representation layer (KRL) based on OWL. The KRL describes application objects and possible system functions in one place and avoids error-prone redundant data management. (2) The KRL is tightly integrated into the simulator platform using a semantically enriched object model that is auto-generated from the KRL and thus fosters high performance access. (3) A well-defined interface provides application wide access to semantic application state information in general and the lexical information in specific, which greatly contributes to decoupling, maintainability, and reusability.",multimodal platforms; real-time interactive systems; software architecture; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Medical imaging VR: can immersive 3D aid in diagnosis?,VRST - Virtual Reality Software and Technology,A,"In the radiology diagnosis process, medical images are most often visualized slice by slice on 2D screens or printed. At the same time, the visualization based on 3D volumetric rendering of the data is considered useful and has increased its field of application. In this work we present a user study with medical specialists to assess the diagnostic effectiveness of VR usage in fracture identification over 3D volumetric reconstructions. We then performed user experiments to validate the approach in the medical practice. In addition, we assessed the subjects perception of the 3D reconstruction quality and ease of interaction. Among other results, we have found a very high level of effectiveness of the VR interface in identifying superficial fractures on head CTs.",3D images; diagnostic imaging; healthcare; oculus rift; radiology; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2016,Postural stability analysis in virtual reality using the HTC vive,VRST - Virtual Reality Software and Technology,A,"Postural stability is an important measure for many medical diseases such as Parkinson. In the last years, research focused on using inexpensive and portable devices to measure postural stability, while the visual targets were physical objects in the environment. Sensing balancing boards were used to measure stance forces, while movements of the upper body were not taken into account. Within this paper, postural stability was measured using the HTC Vive. A variation of a virtual fixation point's distance was analyzed and compared to a reference condition with closed eyes. It is shown that body sway in the VR conditions is increased in the anterior-posterior and decreased in the medial-lateral direction.",body sway; postural stability; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2016,Shop 'til you hear it drop: influence of interactive auditory feedback in a virtual reality supermarket,VRST - Virtual Reality Software and Technology,A,"In this paper we describe an experiment aiming to investigate the impact of auditory feedback in a virtual reality supermarket scenario. The participants were asked to read a shopping list and collect items one by one and place them into a shopping cart. Three conditions were presented randomly, where audio feedback was (1) absent, (2) had impact sounds for collisions including when grasping, (3) had impact sounds as well as continuous sounds when moving the products. The subjects experience of the simulation during the three experimental conditions were studied using a questionnaire where ratings on presence, body ownership, awareness of own movements, usability and enjoyment were collected. The results are presented and discussed.",auditory feedback; gesture control; interaction; supermarkets; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,SIAMC: a socially immersive avatar mediated communication platform,VRST - Virtual Reality Software and Technology,A,"In this paper, we present a avatar-mediated communication platform for socially immersive interaction in virtual reality (VR). Our approach is based on the combination of body tracking, facial expression tracking and ""fishtank"" VR. Our prototype enables two remote users to communicate via avatars.",avatars; computer-mediated communication; social virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2016,Temporal antialiasing for head mounted displays in virtual reality,VRST - Virtual Reality Software and Technology,A,"This paper identifies a new temporal aliasing problem caused by unintended head movement by users with VR HMDs. The images that users see slightly change even in the case that the users intend to hold and concentrate on a certain part of VR content. The slight change is more perceivable, because the images are magnified by lenses of VR HMDs. We propose the head movement based temporal antialiasing approach which blends colors that users see in the middle of head movement. In our approach, the way to determine locations and weights of colors to be blended is based on head movement and time stamp. Speed of head movement also determines proportions of colors in the past and at present in blending. Our approach is effective to reduce the temporal aliasing caused by unintended head movement.",head mounted display; temporal antialiasing; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2016,Texture analysis and repacking for improved storage efficiency,VRST - Virtual Reality Software and Technology,A,"Textures are widely used in modern computer graphics. Their size, however, is often a limiting factor. Considering the widespread adaptation of mobile virtual and augmented reality applications, efficient storage of textures has become an important factor.We present an approach to analyse textures of a given mesh and compute a new set of textures with the goal of improving storage efficiency and reducing memory requirements. During this process the texture coordinates of the mesh are updated as required. Textures are analysed based on the UV-coordinates of one or more meshes and deconstructed into per-triangle textures. These are further analysed to detect single coloured as well as identical per-triangle textures. Our approach aims to remove these redundancies in order to reduce the amount of memory required to store the texture data. After this analysis, the per-triangle textures are compiled into a new set of texture images of user defined size. Our algorithm aims to pack texture data as tightly as possible in order to reduce the memory requirements.",texture; texture optimization; texture packing,Abstract,TRUE,
Scopus,conferencePaper,2016,The Effects of Indirectly Implied Real Body Cues to Virtual Body Ownership and Presence in a Virtual Reality Environment,VRST - Virtual Reality Software and Technology,A,"While direct associations, such as through visual, audio and tactile senses, play an obvious role in giving a person a perception of body presence in an immersive virtual environment, indirect implied cues can also be effective factors in providing the illusion of reality. Thus, as the direct use of implied association can arouse desired illusions of reality, we believed the indirect associations also has effects that can similarly arouse such illusions in virtual settings. In this paper, we report on an experiment we conducted to explore the effects of indirect implication of a participant's body parts to virtual body ownership and presence.",Presence; Virtual Body Ownership; Virtual Reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2016,TickTockRay: smartwatch-based 3D pointing for smartphone-based virtual reality,VRST - Virtual Reality Software and Technology,A,"TickTockRay is a smartwatch-based raycasting technique designed for smartphone-based head mounted displays. It demonstrates that smartwatch-based raycasting can be reliably implemented on an off-the-shelf smartphone and may provide a feasible alternative for specialized input devices. We release TickTockRay to the research community as an open-source plugin for Unity along with an example application, a Minecraft VR game clone, that shows the utility of the technique for placement and destruction of Minecraft blocks.",3D pointing; freehand pointing; game input; immersive systems; myo; smartphone; smartwatch; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2016,Webizing human interface devices for virtual reality,VRST - Virtual Reality Software and Technology,A,"Recently virtual reality (VR) technology has been widely distributed, but VR interaction devices supported in web environments are limited compared with in the traditional VR environment. In the traditional VR environment, the Virtual-Reality Peripheral Network (VRPN) provides a device-independent and network-transparent interface. To promote the development of WebVR applications with various interaction devices, a method like VRPN is required in the web environment as well. In this paper, we propose a webizing method for human interface devices and related events that serves as either VRPN messages or HTML DOM events to deal with interaction events.",human interface devices; user interaction; VRPN; webizing,Title_Abstract,TRUE,
Scopus,conferencePaper,2017,Beyond cute: exploring user types and design opportunities of virtual reality pet games,VRST - Virtual Reality Software and Technology,A,"Virtual pet games, such as handheld games like Tamagotchi or video games like Petz, provide players with artificial pet companions or entertaining pet-raising simulations. Prior research has found that virtual pets have the potential to promote learning, collaboration, and empathy among users. While virtual reality (VR) has become an increasingly popular game medium, litle is known about users' expectations regarding game avatars, gameplay, and environments for VR-enabled pet games. We surveyed 780 respondents in an online survey and interviewed 30 participants to understand users' motivation, preferences, and game behavior in pet games played on various medium, and their expectations for VR pet games. Based on our findings, we generated three user types that reflect users' preferences and gameplay styles in VR pet games. We use these types to highlight key design opportunities and recommendations for VR pet games.",pet game; user types; virtual pet; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,The matrix has you: realizing slow motion in full-body virtual reality,VRST - Virtual Reality Software and Technology,A,"While we perceive time as a constant factor in the real world, it can be manipulated in media. Being quite easy for linear media, this is used for various aspects of storytelling e.g., by applying slow motion in movies or TV. Interactive media like VR however poses additional challenges, because user interaction speed is independent from media speed. While it is still possible to change the speed of the environment, for interaction it is also necessary to deal with the emerging speed mismatch, e.g., by slowing down visual feedback of user movements. In this paper, we explore the possibility of such manipulations of visual cues, with the goal of enabling the use of slow motion also in immersive interactive media like VR. We conducted a user study to investigate the impact of limiting angular velocity of a virtual character in first person view in VR. Our findings show that it is possible to use slow motion in VR while maintaining the same levels of presence, enjoyment and susceptibility to motion sickness, while users adjust to the maximum speed quickly. Moreover, our results also show an impact of slowing down user movements on their time estimations.",evaluation; slow motion; time perception; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2017,Can you cut it? an exploration of the effects of editing in cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"The advent of affordable virtual reality (VR) displays and 360° video cameras has sparked an interest in bringing cinematic experiences from the screen and into VR. However, it remains uncertain whether traditional approaches to filmmaking can be directly applied to cinematic VR. Historically editing has provided filmmakers with a powerful tool for shaping stories and guiding the attention of audiences. However, will an immersed viewer, experiencing the story from inside the fictional world, find cuts disorienting? This paper details two studies exploring how cut frequency influences viewers' sense of disorientation and their ability to follow the story, during exposure to fictional 360° films experienced using a head-mounted display. The results revealed no effects of increased cut frequency which leads us to conclude that editing need not pose a problem in relation to cinematic VR, as long as the participants' attention is appropriately guided at the point of the cut.",360 degree film; cinematic virtual reality; editing,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,A comparison of head-mounted displays vs. large-screen displays for an interactive pedestrian simulator,VRST - Virtual Reality Software and Technology,A,"This investigation compared how people performed a complex perception-action task - crossing traffic-filled roadways - in a CAVE vs. an HMD virtual environment. Participants physically crossed a virtual roadway with continuous cross traffic in either a CAVE-like or an HTC Vive pedestrian simulator. The 3D model and traffic scenario were identical in both simulators, allowing for a direct comparison between the two display systems. We found that participants in the Vive group accepted smaller gaps for crossing than participants in the CAVE group. They also timed their entry into the gap more precisely and tended to cross somewhat more quickly. As a result, participants in the Vive group had a somewhat larger margin of safety when they exited the roadway than those in the CAVE group. The results provide a foundation for future studies of pedestrian behavior and other tasks involving full-body motion using HMD-based VR.",CAVE; HTC vive; pedestrian road crossing; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2017,Outside-in monocular IR camera based HMD pose estimation via geometric optimization,VRST - Virtual Reality Software and Technology,A,"Accurately tracking a Head Mounted Display (HMD) with a 6 degree of freedom is essential to achieve a comfortable and a nausea free experience in Virtual Reality. Existing commercial HMD systems using synchronized Infrared (IR) camera and blinking IR-LEDs can achieve highly accurate tracking. However, most of the off-the-shelf cameras do not support frame synchronization. In this paper, we propose a novel method for real time HMD pose estimation without using any camera synchronization or LED blinking. We extended over the state of the art pose estimation algorithm by introducing geometrically constrained optimization. In addition, we propose a novel system to increase robustness to the blurred IR-LEDs patterns appearing at high-velocity movements. The quantitative evaluations showed significant improvements in pose stability and accuracy over wide rotational movements as well as a decrease in runtime.",monocular IR camera; perspective-n-point problem; position tracking; vision-based pose estimation,Abstract,TRUE,
Scopus,conferencePaper,2017,A tile based colour picture with hidden QR code for augmented reality and beyond,VRST - Virtual Reality Software and Technology,A,"Most existing Augmented Reality (AR) applications use either template (picture) markers or bar-code markers to overlay computer-generated graphics on the real world surfaces. The use of template markers is computationally expensive and unreliable. On the other hand, bar-code markers display only black and white blocks; thus, they look uninteresting and uninformative. In this short paper, we describe a new way to optically hide a QR code inside a tile based colour picture. Each AR marker is built from hundreds of small tiles (just like tiling a bathroom), and the unique gaps between the tiles are used to determine the elements of the hidden QR Code. This novel type of AR marker presents not only a realistic-looking colour picture but also contains self-Correcting information (stored in QR code). In this article, we demonstrate that this tile based colour picture with hidden QR code is relatively robust under various conditions and scaling. We believe many nowadays' AR challenges could be solved with this type of marker. AR-enabled medias could then be easily generated. For instance, it would be capable of storing and displaying virtual figures of an entire book or magazine. Thus, it provides a promising AR approach to be used in many different AR applications; and beyond, it may even replace the barcodes and QR Codes in some cases.",augmented reality; computer vision; QR code,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Virtual reality studies outside the laboratory,VRST - Virtual Reality Software and Technology,A,"Many user studies are now conducted outside laboratories to increase the number and heterogeneity of participants. These studies are conducted in diverse settings, with the potential to give research greater external validity and statistical power at a lower cost. The feasibility of conducting virtual reality (VR) studies outside laboratories remains unclear because these studies often use expensive equipment, depend critically on the physical context, and sometimes study delicate phenomena concerning body awareness and immersion. To investigate, we explore pointing, 3D tracing, and body-illusions both in-lab and out-of-lab. The in-lab study was carried out as a traditional experiment with state-of-the-art VR equipment; 31 completed the study in our laboratory. The out-of-lab study was conducted by distributing commodity cardboard VR glasses to participants; 57 completed the study anywhere they saw fit. The effects found in-lab were comparable to those found out-of-lab, with much larger variations in the settings in the out-of-lab condition. A follow-up study showed that performance metrics are mostly governed by the technology used, where more complex VR phenomena depend more critically on the internal control of the study. We argue that conducting VR studies outside the laboratory is feasible, and that certain types of VR studies may advantageously be run this way. From the results, we discuss the implications and limitations of running VR studies outside the laboratory.",consumer VR; crowdsourcing; google cardboard; user studies,Title_Abstract,TRUE,
Scopus,conferencePaper,2017,Agent: automatic generation of experimental protocol runtime,VRST - Virtual Reality Software and Technology,A,"Due to the nature of Virtual Reality (VR) research, conducting experiments in order to validate the researcher's hypotheses is a must. However, the development of such experiments is a tedious and time-consuming task. In this work, we propose to make this task easier, more intuitive and faster with a method able to describe and generate the most tedious components of VR experiments. The main objective is to let experiment designers focus on their core tasks: designing, conducting, and reporting experiments. To that end, we propose the use of Domain-Specific Languages (DSLs) to ease the description and generation of VR experiments. An analysis of published VR experiments is used to identify the main properties that characterize VR experiments. This allowed us to design AGENT (Automatic Generation of ExperimeNtal proTocol runtime), a DSL for specifying and generating experimental protocol runtimes. We demonstrated the feasibility of our approach by using AGENT on two experiments published in the VRST'16 proceedings.",automatic generation of experiments; domain-specific language; reusability,Abstract,TRUE,
Scopus,conferencePaper,2017,Accurate real-time occlusion for mixed reality,VRST - Virtual Reality Software and Technology,A,"Properly handling occlusion between real and virtual objects is an important property for any mixed reality (MR) system. Existing methods have typically required known geometry of the real objects in the scene, either specified manually, or reconstructed using a dense mapping algorithm. This limits the situations in which they can be applied. Modern RGBD cameras are cheap and widely available, but the depth information they provide is typically too noisy and incomplete to use directly to provide quality results.In this paper, a method is proposed which makes use of both the colour and depth information provided by an RGBD camera to provide improved occlusion. This method, Cost Volume Filtering Occlusion, is capable of running in real time, and can also handle occlusion of virtual objects by dynamic, moving objects - such as the user's hands. The method operates on individual RGBD frames as they arrive, meaning it can function immediately in unknown environments, and respond appropriately to sudden changes. The accuracy of the presented method is quantified using a novel approach capable of comparing the results of algorithms such as this to dense SLAM-based approaches. The proposed approach is shown to be capable of producing superior results to both previous image-based approaches and dense RGBD reconstruction, at lower computational cost.",image processing; mixed and augmented reality; user interfaces,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Design and exploration of braiding swarms in VR,VRST - Virtual Reality Software and Technology,A,"Swarm-based braiding of structures represents a novel research direction in the domain of building architecture. The idea is that autonomous agents, for instance robots that unroll threads or plants that grow, are programmed or influenced to braid. It is an aspect of biohybrid systems where organisms and robots join forces. In order to harness this idea, we have developed a swarm-based model that allows architects to explore the resulting design spaces in virtual reality. In this paper, we present (1) the model of our swarm-based simulation that aims at growing braided structures, (2) the design elements to guide the otherwise self-organising virtual agents, and (3) the user interface that allows the user to configure, place and grow the swarms of braiding agents. We also present results of a first user study with students and faculty from architecture, in which we tried to capture the usability of our first prototype based on a survey and an analysis of the built results.",agent-based modeling; architecture; braiding; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,A hybrid CRF framework for semantic 3D reconstruction,VRST - Virtual Reality Software and Technology,A,"Nowadays, in order to achieve an immersive experience, virtual reality systems usually require vivid 3D models and a good understanding of particular scenes. The limitations of separately optimizing image segmentation and 3D modeling from images have gradually been seen by more and more researchers, so plenty of novel methods on how to combine them for a better result begin to be put forward widely. In this paper, we propose a new hybrid framework to generate semantic 3D dense models from monocular images. Based on the available hierarchical CRFs model, we make full use of the correlation between voxels and their corresponding pixels from different images. Naturally, valuable information from 3D space can be added as one of the important energy items in the model. Either pixels, segments or voxles are all regarded as a node in the huge graph we build. Our ultimate goal is to realize a joint optimization for both 3D dense reconstruction and image segmentation. Experiments have been done on four real challenging datasets and all of the results prove the efficiency of our proposed hybrid framework.",dense 3D modeling; graphics/3D; image segmentation; semantic reconstruction,Abstract,TRUE,
Scopus,conferencePaper,2017,Collaborators awareness for user cohabitation in co-located collaborative virtual environments,VRST - Virtual Reality Software and Technology,A,"In a co-located collaborative virtual environment, multiple users share the same physical tracked space and the same virtual workspace. When the virtual workspace is larger than the real workspace, navigation interaction techniques must be deployed to let the users explore the entire virtual environment. When a user navigates in the virtual space while remaining static in the real space, his/her position in the physical workspace and in the virtual workspace are no longer the same. Thus, in the context where each user is immersed in the virtual environment with a Head-Mounted-Display, a user can still perceive where his/her collaborators are in the virtual environment but not where they are in real world. In this paper, we propose and compare three methods to warn users about the position of collaborators in the shared physical workspace to ensure a proper cohabitation and safety of the collaborators. The frst one is based on a virtual grid shaped as a cylinder, the second one is based on a ghost representation of the user and the last one displays the physical safe-navigation space on the foor of the virtual environment. We conducted a user-study with two users wearing a Head-Mounted-Display in the context of a collaborative First-Person-Shooter game. Our three methods were compared with a condition where the physical tracked space was separated into two zones, one per user, to evaluate the impact of each condition on safety, displacement freedom and global satisfaction of users. Results suggest that the ghost avatar and the cylinder grid can be good alternatives to the separation of the tracked space.",collaborative virtual environment; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2017,Towards seamless interaction between physical and virtual locations for asymmetric collaboration,VRST - Virtual Reality Software and Technology,A,"Virtual Reality allows rapid prototyping and simulation of physical artefacts, which would be difficult and expensive to perform otherwise. On the other hand, when the design process is complex and involves multiple stakeholders, decisions are taken in meetings hosted in the physical world. In the case of aerospace industrial designs, the process is accelerated by having asymmetric collaboration between the two locations: experts discuss the possibilities in a meeting room while a technician immersed in VR tests the selected alternatives. According to experts, the current approach is not without limitations, and in this work, we present prototypes designed to tackle them. The described artefacts were created to address the main issues: awareness of the remote location, remote interaction and manipulation, and navigation between locations. First feedback from experts regarding the prototypes is also presented. The resulting design considerations can be used in other asymmetric collaborative scenarios.",asymmetric collaboration; head mounted display; mixed reality; spatial augmented reality; tangible user interfaces; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Legomotion: scalable walking-based virtual locomotion,VRST - Virtual Reality Software and Technology,A,"Using real walking for virtual navigation generally delivers the most natural and immersive virtual reality experience, but its usage is generally bounded by available tracking space. To navigate beyond the confines of available tracking space, users need to switch to an artificial locomotion technique, such as controller input. However, having to switch from leg-based input to hand-based input is considered to break presence. We present a hybrid handsfree locomotion technique called legomotion that lets users seamlessly switch between real walking input and walking-in-place input to enable navigation at scale. A user study with 18 participants compared legomotion to full locomotion using a controller. Legomotion led to higher presence as switching to controller input was found to be more tedious. Because controller input is also faster than walking, we observed most users to abandon positional tracking input altogether and primary use a controller for navigation - which then led to a lower presence. This finding could have major implications for the design of VR locomotion.",locomotion; presence; virtual reality; VR sickness; walking-in-place,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,A unified model for interaction in 3D environment,VRST - Virtual Reality Software and Technology,A,"The Virtual (VR), Augmented (AR) and Mixed Reality (MR) devices are currently evolving at a very fast pace. This rapid evolution affects significantly the maintainability and portability of the applications. In this paper, we present a model for designing VR, AR and MR applications independently of any device. To do this, we use degrees of freedom to define an abstraction layer between the tasks to be performed and the interaction device.",input techniques; model-based interactive system development; virtual worlds,Abstract,TRUE,
Scopus,conferencePaper,2017,A study on improving performance in gesture training through visual guidance based on learners' errors,VRST - Virtual Reality Software and Technology,A,"Gesture training, especially for technical gestures, requires supervisors to point out errors made by trainees. Virtual reality (VR) makes it possible to reduce reliance on supervisors (fewer interventions and of shorter duration) and to reduce the length of training, using extrinsic feedback that provides training or learning assistance using different modalities (visual, auditory, and haptic). Visual feedback has received much attention in recent decades. Users can be guided by a metaphor in a virtual environment. This metaphor may be a 3D trace of canonical movements, a visual cue pointing in the right direction, or gestures by an avatar that the trainee must mimic. However, with many kinds of feedback, trainees are not aware of their errors while performing gestures. Our hypothesis is that guiding users with a dynamic metaphor based on the visualization of errors will reduce these errors and improve performance. To this end, in a previous work we designed and implemented a new 3D metaphor called EBAGG to guide users in real time.In the present paper we evaluate EBAGG in relation to two other visual cues: first, a feedforward technique that displays the trace of a reference movement, and, second, a concurrent orientation feedback. The results of the user study show that EBAGG outperformed the others in improving users' performances over a training session. Moreover, the information assimilated during training with this dynamic feedback had a persistent effect when the metaphor was no longer displayed.",gestures; guidance; performance; user study; virtual reality; visual feedback,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Auris: creating affective virtual spaces from music,VRST - Virtual Reality Software and Technology,A,"Affective virtual spaces are of interest in many virtual reality applications such as education, wellbeing, rehabilitation, and entertainment. In this paper we present Auris, a system that attempts to generate affective virtual environments from music. We use music as input because it inherently encodes emotions that listeners readily recognize and respond to. Creating virtual environments is a time consuming and labor-intensive task involving various skills like design, 3D modeling, texturing, animation, and coding. Auris helps make this easier by automating the virtual world generation task using mood and content extracted from song audio and lyrics data respectively. Our user study results indicate virtual spaces created by Auris successfully convey the mood of the songs used to create them and achieve high presence scores with the potential to provide novel experiences of listening to music.",deep neural networks; generative models; music; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Using custom transformation axes for mid-air manipulation of 3D virtual objects,VRST - Virtual Reality Software and Technology,A,"Virtual Reality environments are able to offer natural interaction metaphors. However, it is difficult to accurately place virtual objects in the desired position and orientation using gestures in mid-air. Previous research concluded that the separation of degrees-of-freedom (DOF) can lead to better results, but these benefits come with an increase in time when performing complex tasks, due to the additional number of transformations required. In this work, we assess whether custom transformation axes can be used to achieve the accuracy of DOF separation without sacrificing completion time. For this, we developed a new manipulation technique, MAiOR, which offers translation and rotation separation, supporting both 3-DOF and 1-DOF manipulations, using personalized axes for the latter. Additionally, it also has direct 6-DOF manipulation for coarse transformations, and scaled object translation for increased placement. We compared MAiOR against an exclusively 6-DOF approach and a widget-based approach with explicit DOF separation. Results show that, contrary to previous research suggestions, single DOF manipulations are not appealing to users. Instead, users favored 3-DOF manipulations above all, while keeping translation and rotation independent.",3D user interfaces; custom manipulation axis; DOF separation; mid-air object manipulation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Computational design of hand-held VR controllers using haptic shape illusion,VRST - Virtual Reality Software and Technology,A,"Humans are capable of haptically perceiving the shape of an object by simply wielding it, even without seeing it. On the other hand, typical hand-held controllers for virtual reality (VR) applications are pre-designed for general applications, and thus not capable of providing appropriate haptic shape perception when wielding specific virtual objects. Contradiction between haptic and visual shape perception causes a lack of immersion and leads to inappropriate object handling in VR. To solve this problem, we propose a novel method for designing hand-held VR controllers which illusorily represent haptic equivalent of visual shape in VR. In ecological psychology, it has been suggested that the perceived shape can be modeled using the limited mass properties of wielded objects. Based on this suggestion, we built a shape perception model using a data-driven approach; we aggregated data of perceived shapes against various hand-held VR controllers with different mass properties, and derived the model using regression techniques. We implemented a design system which enables automatic design of hand-held VR controllers whose actual shapes are smaller than target shapes while maintaining their haptic shape perception. We verified that controllers designed with our system can present aimed shape perception irrespective of their actual shapes.",computational design; data-driven; perception; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Occlusion in outdoor augmented reality using geospatial building data,VRST - Virtual Reality Software and Technology,A,"Aligning virtual and real objects in Augmented Reality (AR) is essential for the user experience. Without alignment, the user loses suspension of disbelief and the sense of depth, distance, and size. Occlusion is a key feature to be aligned. Virtual content should be partially or fully occluded if real world objects are in its line-of-sight. The challenge for simulating occlusion is to construct the geometric model of the environment. Earlier studies have aimed to create realistic occlusions, yet most have either required depth-sensing hardware or a static predefined environment. This paper proposes and evaluates an alternative model-based method for dynamic outdoor AR of virtual buildings rendered on non depth-sensing smartphones. It uses geospatial data to construct the geometric model of real buildings surrounding the virtual building. The method removes the target regions from the virtual building using masks constructed from real buildings. While the method is not pixel-perfect, meaning that the simulated occlusion is not fully realistic, results from the user study indicate that it fulfilled its goal. A majority of the participants expressed that their experience and depth perception improved with the method activated. The result from this study has applications to mobile AR since the majority of smartphones are not equipped with depth sensors. Using geospatial data for simulating occlusions is a sufficiently effective solution until depth-sensing AR devices are more widely available.",AR; augmented reality; geospatial data; occlusion; open street maps; physical simulation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Exploring the effects of observed physicality conflicts on real-virtual human interaction in augmented reality,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) enables the illusion of computer-generated virtual objects and humans co-existing with us in the real world. Virtual humans (VHs) in AR can further induce an illusion of physicality in the real world due to their form of presentation and their behavior, such as showing awareness of their surroundings. However, certain behaviors can cause a conflict that breaks this illusion, for example, when we see a VH passing through a physical object.In this paper we describe a human-subject study that we performed to test the hypothesis that participants experience higher copresence in conflict-free circumstances, and we investigate the magnitude of this effect and behavioral manifestations. Participants perceived a social situation in a room that they shared with a VH as seen through a HoloLens head-mounted display. The behavior of the VH either caused conflicts with (occupied the same space as) physical entities, or avoided them. Our results show that the conflicts in physicality significantly reduced subjective reports of copresence. Moreover, we observed that participants were more likely to cause a conflict (occupy the same space as) virtual entities in case the VH had avoided the conflict. We discuss implications for future research and shared AR setups with real-virtual human interactions.",augmented reality; copresence; physicality; virtual humans,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,PanoTrace: interactive 3D modeling of surround-view panoramic images in virtual reality,VRST - Virtual Reality Software and Technology,A,"Full-surround panoramic imagery can provide a viewer with a high-resolution visual impression of a pictured real or realistically rendered environment, but it does not provide as high a level of immersion as modeled 3D geometry can, when viewed with virtual reality (VR) headsets or projection-based setups. In this paper, we demonstrate that augmenting panorama images with geometrical models can be done simply in VR itself and can significantly increase the feeling of immersion a viewer experiences. We propose a novel interactive modeling tool that allows users to model geometry depicted in a surround-panoramic scene directly in VR, utilizing projection mapping of the panorama on top of the evolving geometry. The user interface is intuitive and allows novice users to produce geometry that approximates ground truth models sufficiently to enhance a user's VR viewing experience. We designed a user study that compares users' self-reported levels of immersion, scene realism, and discomfort on a set of created models and comparison cases. Our results indicate that our modeled scenes produce a significantly higher sense of immersion than a basic dome geometry for the panorama when viewed in VR with head orientation and position tracking.",geometric modeling; panorama imaging; virtual reality; VR modeling tools,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Case-based planning for large virtual agent societies,VRST - Virtual Reality Software and Technology,A,"In this paper we discuss building large scale virtual reality reconstructions of historical heritage sites and populating it with crowds of virtual agents. Such agents are capable of performing complex actions, while respecting the cultural and historical accuracy of agent behaviour. In many commercial video games such agents either have very limited range of actions (resulting primitive behaviour) or are manually designed (resulting high development costs). In contrast, we follow the principles of automatic goal generation and automatic planning. Automatic goal generation in our approach is achieved through simulating agent needs and then producing a goal in response to those needs that require satisfaction. Automatic planning refers to techniques that are concerned with producing sequences of actions that can successfully change the state of an agent to the state where its goals are satisfied. Classical planning algorithms are computationally costly and it is difficult to achieve real-time performance for our problem domain with those. We explain how real-time performance can be achieved with Case-Based Planning, where agents build plan libraries and learn how to reuse and combine existing plans to archive their dynamically changing goals. We illustrate the novelty of our approach, its complexity and associated performance gains through a case-study focused on developing a virtual reality reconstruction of an ancient Mesopotamian settlement in 5000 B.C.",case-based planning; social simulations; virtual agents,Abstract,TRUE,
Scopus,conferencePaper,2017,Pulse and vital sign measurement in mixed reality using a HoloLens,VRST - Virtual Reality Software and Technology,A,"Cardiography, quantitative measurement of the functioning of the heart, traditionally requires customized obtrusive contact sensors. Using new methods photoplethysmography and ballistocardiography signals can be captured using ubiquitous sensors, such as webcams and accelerometers. However, these signals are not visible to the unaided eye. We present Cardiolens - a mixed reality system that enables real-time, hands-free measurement and visualization of blood flow and vital signs from multiple people. The system combines a front-facing webcam, imaging ballistocardiography, and remote imaging photoplethysmography methods for recovering pulse signals. A heads up display allows users to view their own heart rate whenever they are wearing the device and the heart rate and heart rate variability of another person simply by looking at them. Cardiolens provides the wearer with a new way to understand physiological signals and has applications in human-computer interaction and in the study of social psychology.",health; interoception; mixed reality; physiology; remote sensing,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Virtual reality navigation system for prostate biopsy,VRST - Virtual Reality Software and Technology,A,"Prostate cancer is the most common non-cutaneous cancer in America. Tumor detection involves non-invasive screening tests, but positive results must be confirmed by a prostate biopsy. About twelve random samples are obtained during the biopsy, which is a systematic procedure traditionally performed with trans-rectal ultrasound (TRUS) guidance to determine prostate location. Recently, methods of fusion between TRUS and preoperative MRI have been introduced in order to perform targeted biopsies aimed to reduce the number of samples to few suspicious areas. Since the TRUS displaces the prostate during the procedure, the preoperative MRI does not match patient anatomy. Therefore, complex MRI deformation algorithms are needed. However, despite the substantial increase in complexity and cost, there is no strong evidence that the TRUS-MRI fusion actually improves accuracy and surgical outcomes.This paper presents an innovative virtual reality surgical navigation system for performing targeted prostate biopsies, without the need of the uncomfortable TRUS. Both biopsy needle and patient anatomy are constantly tracked by an electromagnetic tracking system that provides their 3D position and orientation with respect to the surgical bed. Multiple fiducial markers are placed on the patient skin (at the iliac crest and pubic bone) during MRI scanning. Once in the operative room, the surgeon is presented a stereoscopic 3D volumetric rendering and multiple orthogonal views of the patient anatomy, as well as a virtual representation of the tracked needle. After a simple registration process between the MRI and the tracker coordinate system, the navigation system guides the needle insertion in the patient perineum through several anatomical layers towards the biopsy targets.",graphics/3D; medical and health support; prototyping/implementation,Title_Abstract,TRUE,
Scopus,conferencePaper,2017,Measurement of exceptional motion in VR video contents for VR sickness assessment using deep convolutional autoencoder,VRST - Virtual Reality Software and Technology,A,"This paper proposes a new objective metric of exceptional motion in VR video contents for VR sickness assessment. In VR environment, VR sickness can be caused by several factors which are mismatched motion, field of view, motion parallax, viewing angle, etc. Similar to motion sickness, VR sickness can induce a lot of physical symptoms such as general discomfort, headache, stomach awareness, nausea, vomiting, fatigue, and disorientation. To address the viewing safety issues in virtual environment, it is of great importance to develop an objective VR sickness assessment method that predicts and analyses the degree of VR sickness induced by the VR content. The proposed method takes into account motion information that is one of the most important factors in determining the overall degree of VR sickness. In this paper, we detect the exceptional motion that is likely to induce VR sickness. Spatio-temporal features of the exceptional motion in the VR video content are encoded using a convolutional autoencoder. For objectively assessing the VR sickness, the level of exceptional motion in VR video content is measured by using the convolutional autoencoder as well. The effectiveness of the proposed method has been successfully evaluated by subjective assessment experiment using simulator sickness questionnaires (SSQ) in VR environment.",cybersickness; machine learning; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2017,Information recall in a virtual reality disability simulation,VRST - Virtual Reality Software and Technology,A,"The purpose of this paper is to investigate the effect of the sense of presence on one aspect of learning, information recall, in an immersive virtual reality (VR) disability simulation. Previous research has shown that the use of VR technology in education may facilitate improved learning outcomes, however, it is still an active research topic as the learning outcomes can vary widely. We hypothesized that a higher level of immersion and involvement in a VR disability simulation that leads to a high sense of presence will help the user improve information recall. To investigate this hypothesis, we conducted a between subjects experiment in which participants were presented information about multiple sclerosis in different immersive conditions and afterwards they attempted to recall the information. We also looked into whether there is any adverse effect of cybersickness on the information recall task in our disability simulation. The results from our study suggest that participants who were in immersive conditions were able to recall the information more effectively than the participants who experienced a non-immersive condition.",games for health; information recall; learning; or change (primary keyword); persuasion; user studies; virtual/augmented reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,"Presence and immersion of ""easy"" mobile VR with open flip-on lenses",VRST - Virtual Reality Software and Technology,A,"Mobile virtual reality (M-VR) uses an inexpensive and light headset into which the smartphone is inserted to conveniently experience immersive contents. Even so the headset is bulky and difficult to carry around, and makes the smartphone inaccessible. Recently, an alternative form of M-VR has appeared in the market in which the magnifying lenses are simply clipped on the smartphone (dubbed ""EasyVR""). Despite being open and the user'speripheral view not shut from the outside world, it still gives a good level of immersion with a wide magnified field of view. EasyVR has the added advantages of quick switch between with the regular smartphone usage mode and access to the touch screen for the seamless interaction. In this paper, we examine and compare the level of presence and immersion as provided by three different display configurations of M-VR: (1) EasyVR, (2) the usual headset which completely isolates the user from the outer-world with the vignetted view (ClosedVR), and (3) completely open hand-held smartphone view (OpenVR). We also control the environment condition, static or dynamic, as seen and perceived through the peripheral view and possibly having an effect on the level of presence and immersion in the respective display configuration. Our findings first show the easily expected, namely, both ClosedVR and EasyVR clearly exhibiting a much higher level of presence and immersion than OpenVR. The results also show that even though there is a substantial extent within the peripheral view showing the outer environment, the level of presence and immersion of EasyVR is nearly comparable to that of ClosedVR. Only when the environment was dynamic (as visible in the peripheral view ends), EasyVR showed a lower level of presence and immersion than ClosedVR, but still significantly higher than OpenVR. Therefore, EasyVR is a very attractive alternative to the usual ClosedVR, especially as a ""use-anywhere"" VR considering its clearly improved convenience and sufficient level of immersion beyond just for casual purposes.",distraction; immersion; mobile virtual reality; open/closed VR display; presence,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,The effect of avatar realism in immersive social virtual realities,VRST - Virtual Reality Software and Technology,A,"This paper investigates the effect of avatar realism on embodiment and social interactions in Virtual Reality (VR). We compared abstract avatar representations based on a wooden mannequin with high fidelity avatars generated from photogrammetry 3D scan methods. Both avatar representations were alternately applied to participating users and to the virtual counterpart in dyadic social encounters to examine the impact of avatar realism on self-embodiment and social interaction quality. Users were immersed in a virtual room via a head mounted display (HMD). Their full-body movements were tracked and mapped to respective movements of their avatars. Embodiment was induced by presenting the users' avatars to themselves in a virtual mirror. Afterwards they had to react to a non-verbal behavior of a virtual interaction partner they encountered in the virtual space. Several measures were taken to analyze the effect of the appearance of the users' avatars as well as the effect of the appearance of the others' avatars on the users. The realistic avatars were rated significantly more human-like when used as avatars for the others and evoked a stronger acceptance in terms of virtual body ownership (VBO). There also was some indication of a potential uncanny valley. Additionally, there was an indication that the appearance of the others' avatars impacts the self-perception of the users.",avatars; lifelike; social interaction; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Effects of virtual arm representations on interaction in virtual environments,VRST - Virtual Reality Software and Technology,A,"Many techniques for visualization and interaction that potentially increase user performance have been studied in the growing field of virtual reality. However, the effects of virtual-arm representations on users' performance and perception in selection tasks have not been studied before. This paper presents the results of a user study of three different representations of the virtual arm: ""hand only,"" ""hand+forearm,"" and ""whole arm"" which includes the upper arm. In addition to the representations' effects on performance and perception in selection tasks, we investigate how the users' performance changes depending on whether collisions with objects are allowed or not. The relationship between the virtual-arm representations and the senses of agency and ownership are also explored. Overall, we found that the ""whole arm"" condition performed worst.",3D interaction; natural hand interaction; selection performance; virtual arm; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Dual task based cognitive stress induction and its influence on path integration,VRST - Virtual Reality Software and Technology,A,"Current stress induction methods are often too theoretical and do not reflect to real life scenarios. In this study, we used a dual task paradigm in virtual reality, combining a navigation task with a reaction task. With this setup, we aimed at creating a novel benchmark stress induction approach utilizing modern virtual reality technology. Results show that our paradigm induced small scale physiological and subjective state changes. Lastly, we discuss our paradigm and experimental results from the perspective of ecological validity and present suggestions for improving our stress induction methodology as well as potential areas of use.",cognitive stress; ecological validity; navigation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,A comparative study of 2D and 3D mobile keypad user interaction preferences in virtual reality graphic user interfaces,VRST - Virtual Reality Software and Technology,A,"Graphical User Interfaces (GUI) on mobiles involves user interaction of touch input on a 2D surface. With advances in Augmented/Virtual Reality, possibilities of 3D GUIs will emerge. However, 3D GUIs do not have many design heuristics. This paper reports an experiment by collating quantitative and qualitative responses from 15 users, to explore usability problems that are likely to be encountered when a 2D interface element such as number keypad is replaced with a 3D element interface in Virtual reality. Would an interface with 3D elements perform better than the existing 2D GUIs is a moot research question? The results indicate user motivation towards using the interface inspired from 3D elements. The paper discusses issues of interaction in 2D and 3D virtual spaces with their possible implications for upcoming 3D VR environments.",3D GUI; mobile keypad; user study; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,A virtual reality based internet-of-things (IoT) framework for micro devices assembly,VRST - Virtual Reality Software and Technology,A,"The emergencelof Virtual Reality (VR) based technologies holds the potential to facilitate global collaboration in various fields of engineering. Micro Devices Assembly (MDA) is an emerging domain involving the assembly of micron sized objects and devices. In this paper, the focus of the discussion is the design of a VR based Internet-of-Things (IoT) based framework to support collaborative assembly of micro devices using both cyber and physical resources.At the center of this IoT framework is a Virtual Reality (VR) based simulation environment which serves as the link between cyber resources (which can support design analysis and planning) and physical manufacturing resources (which can accomplish the targeted assembly of micron sized products and parts). The feasibility analysis of proposed assembly plans is supported by stand-alone as well as networked based environments which enable proposing, comparing and modifying assembly sequences and plans. Several algorithms are available to generate near optimal assembly plans; these include Genetic Algorithm and Insertion Algorithm based approaches, which focus on determining near optimal assembly sequences based on target part destinations and part feeder positions. The benefits of such an integrated VR based cyber physical approach is to enable collaborative manufacturing frameworks to be more agile and respond to changing customer designs and requirements. Multiple partner organizations can potentially work together as Virtual Enterprises (VEs) sharing both their cyber and physical resources to accomplish the manufacturing of target designs [Cecil et al. 2017a].",assembly planning; collaborative manufacturing; internet-of-things; virtual reality based simulation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Acoustical manipulation for redirected walking,VRST - Virtual Reality Software and Technology,A,"Redirected Walking (RDW) manipulates a scene that is displayed to VR users so that they unknowingly compensate for scene motion and can thus explore a large virtual world on a limited space. So far, mostly visual manipulation techniques have been studied.This paper shows that users can also be manipulated by means of acoustical signals. In an experiment with a dynamically moving audio source we see deviations of up to 30% from a 20 m long straight-line walk for male participants and of up to 25% for females. Static audio has about two thirds of this impact.",motion perception; redirected walking; spatial audio; virtual locomotion; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2017,Analysis of the user experience in a 3D gesture-based supported mobile VR game,VRST - Virtual Reality Software and Technology,A,"The work presented in this paper, explored the enhancement of User Experience (UX) by introducing a novel gesture-based controller in a mobile multiplayer Virtual Reality (VR) game. Using only the smartphone's RGB camera, the image input was used for both gesture analysis, capable of understanding user actions, as well as segmenting the real hand that was illustrated in the Virtual Environment (VE). Users were also able to share the VR space by cooperating in a survival-strategy scenario. The results from the user studies indicated that both the bare hand controller and the addition of another player in the VR scene, affected the experience for the participants. Users had a stronger feeling of presence in the VE when participated with an other user, and the visual representation of their hand in the VR world made the interactions seem more natural. Even though, there is still a number of limitations, this project nodes this approach capable of offering a natural and engaging solution of VR interaction, capable of rich UX while maintaining a low entry level for the end users.",bare-hand interaction; hand gestures; HCI; multiplayer VR; usability analysis; VR gaming,Abstract,TRUE,
Scopus,conferencePaper,2017,Augmented invaders: a mixed reality multiplayer outdoor game,VRST - Virtual Reality Software and Technology,A,"Many virtual and mixed reality games focus on single player experiences. In this paper, we describe the concept and prototype implementation of a mixed reality multiplayer game that can be played with a smartphone and an HMD in outdoor environments. Players can team up to fight against attacking alien drones. The relative positions between the players are tracked using GPS, and the rear camera of the smartphone is used to augment the environment and teammates with virtual objects. The combination of multiplayer, mixed reality, the use of geographical location and outdoor action together with affordable, mobile equipment enables a novel strategic and social game experience.",AR; augmented reality; games; GPS; mixed reality; multiplayer,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Augmented reality system with collision response simulation using measured coefficient of restitution of real objects,VRST - Virtual Reality Software and Technology,A,"In this paper, we present an Augmented Reality (AR) system for video see-through Head Mounted Displays (HMD) which implements natural physical interaction between augmented virtual balls (which represent real balls) and real world objects. We measure the coefficient of restitution (COR) between real balls and real objects in the environment and use the corresponding COR when the virtual ball collides with a real world object. Experiment result show that subjects prefer the physical behavior of augmented balls using our method over the physical behavior using fixed COR.",augmented reality; coefficient of restitution; collision response simulation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Building a hybrid virtual agent for testing user empathy and arousal in response to avatar (micro-)expressions,VRST - Virtual Reality Software and Technology,A,"This poster paper describes a hybrid (i.e., film and CG) method for capturing and implementing facial expressions for/in VR. A video camera was used to capture an actor's performance. The actor's eyes and mouth were isolated, and footage was processed as movie textures to overlay a static 3D model of a head. Micro-expressions (subtle, rapid movements of muscles in and around the eyes and mouth in particular) are thus captured in a fine-grained, yet low- cost and low-tech alternative to established techniques. A future experiment will compare the emotive efficacy of the hybrid virtual agent with that of a conventional (fully CG) rigged avatar head in a 6DoF scenario that transitions from sympathetic (gauging empathy by self-report) to confrontational (gauging physiological arousal by heart-rate or GSR). The experiment's prospective design is discussed, as well as its significance for the study of the crucial intersection of social plausibility and perceptual realism in VR.",avatar capture; social plausibility; social presence; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2017,Capturing aboriginal heritage in virtual reality,VRST - Virtual Reality Software and Technology,A,"The culture of Aboriginal Australians is unique and diverse, but, unfortunately, it is under threat of extinction. This is the culture that does not rely on written records, as knowledge is predominantly being shared in the form of oral tales and demonstrations. There are also many sensitivities associated with sharing knowledge with outsiders and with viewing pictures or videos of deceased people. To address these limitations we show how the combination of Virtual Reality, Motion Capture and Artificial Intelligence can help with providing interactive facilities for Aboriginal People that allow for preserving their cultural heritage and sharing it with others.",social simulations; virtual agents; virtual heritage,Title_Abstract,TRUE,
Scopus,conferencePaper,2017,Color consistency of specular highlights in consumer cameras,VRST - Virtual Reality Software and Technology,A,"The latest advancements in Augmented Reality (AR) and Diminished Reality (DR) have allowed the development of many consumer-oriented applications (such as sales and driving aid, or education). To increase the realism in rendering, estimating the illumination in the scene is a key element. A lot of works tackle this problem but rarely discuss the color of the reconstructed illumination. The Dichromatic Model indicates that the specular component is not affected in color by the texture underneath and holds the light source's color. Though theoretically sound, in practice consumer cameras are subject to nonlinear behaviors which change RGB ratios and create inconsistencies when estimating the illumination. In this paper, we study the conditioning and limits of inverting local illumination models while relying on the Dichromatic Model. We show that the reconstructed specular component has an inconsistent color because it changes depending on the surface's colors.",augmented reality; color consistency; dichromatic model; local illumination; reflectance; saturation; specular highlight,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Diegetic cues for guiding the viewer in cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"Cinematic Virtual Reality has been increasing in popularity in the last years. Watching 360° movies with a Head Mounted Display, the viewer can freely choose the direction of view, and thus the visible section of the movie. We explored three cinematic methods of guiding the viewers' attention: lights, sounds, and movements. For that, we developed a measurement technique to obtain heat maps of viewing directions and applied statistical analysis methods for spatial data. The results of our work show that the attention of the viewer can be directed by sound and movements. New sound induces the viewer to search for the source of the sound, not all participants paid attention to the direction of the sound. In our experiments, lights without movements did not draw more attention than other objects. However, a moving light cone changed the viewing direction considerably.",cinematic virtual reality; directing gaze; guiding attention; spatial sound,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Direct retinal signals for virtual environments,VRST - Virtual Reality Software and Technology,A,"We present a novel signaling method for head-mounted displays, which surpasses eye (pupil) and delivers guiding light signals directly to retina through tissue near the eyes. This method preserves full visual acuity on the display and does not block view to the scene, while also delivering additional visual signals.",360° video; 3D interaction; HMD; virtual reality; VR viewers,Keywords,TRUE,
Scopus,conferencePaper,2017,Do you feel what you see? Multimodal perception in virtual reality,VRST - Virtual Reality Software and Technology,A,"This paper discusses how different physically existing materials can be mapped on virtual textures in mixed reality environments by carrying out an explorative user study (n=101). For physical materials-in form of 3d trackable and moveable cubes-acrylic, wood and aluminum have been used. The virtual textures convey the impression of ceramic, fabric, glass, leather, paper, wood, acrylic, quartz, granite and aluminum. The study reveals which virtual textures match well with the different virtual textures and which do not match at all.",haptics; mixed reality; perception; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,"""Drop the beat"": virtual reality based mindfulness and cognitive behavioral therapy for panic disorder — a pilot study",VRST - Virtual Reality Software and Technology,A,"In this paper, we present a virtual reality based content/system called the ""Drop the beat"" designed to help the mindfulness and train one to overcome panic disorder. The two main elements of the proposed system are the (1) use of 360-degree video for presenting the panic inducing situation and, (2) facilitation of the mindfulness through a compelling scenario and immersive experience with multimodal feedback. In particular, we hypothesized that the direct observance and tangibly feeling for the beating heart in one's hand would help the user train to rationalize and overcome the situation (and e.g. effectively bring down one's heart rate back to a normal level). We conducted a small pilot study, administering the proposed VR content to five panic disorder patients and report the interim results.",cognitive behavioral therapy; mindfulness; multimodal feedback; panic disorder; psychotherapy; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Emotionally mediated spatial experience with AR,VRST - Virtual Reality Software and Technology,A,"This paper speculates and explores how emotional awareness and communication can be enhanced with the mediation of spatial experience. Based on two exploratory user studies, we designed and prototyped a conceptual system that mediates the spatial attributes of the surroundings according to user's choices and their emotional state. We then conducted user studies with the prototype. We contribute to existing literature by sharing our insights into potential use cases and implications of an emotionally responsive space.",affective computing; augmented reality; emotionscape; mediated reality; prototyping,Keywords,TRUE,
Scopus,conferencePaper,2017,Enjoyable carving with ChiselDevice in mixed reality space,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose a system that can carve virtual objects in Mixed Reality (MR) space. The procedures of real-world carving include sculpting a rough outline, shaping sections, and carving patterns onto an object's surface. Of these, we focus on the procedure for carving patterns. Users of our system stroke a real object directly using ChiselDevice, and the surfaces of 3D virtual objects superimposed on the real object are ""carved."" This paper describes the design and development of the ChiselDevice and the MR carving system and the findings of users' experiences of our system.",carving system; mixed reality; tooldevice,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,EyeExpression: exploring the use of eye expressions as hands-free input for virtual and augmented reality devices,VRST - Virtual Reality Software and Technology,A,"Current virtual reality (VR) and augmented reality (AR) devices rely on handheld devices, hand gestures, head tracking, and voice input. This paper presents an initial exploration of using eye expressions as hands-free input modality for head-mounted AR/VR devices (HMDs). We consulted interaction designers and ophthalmologists and enumerated 12 eye expressions, and conducted a 12-person user study to better understand users' ability to perform them as well as preferences.",eye expression; eye muscle movement; head-mounted display,Title_Abstract,TRUE,
Scopus,conferencePaper,2017,GalVR: a novel collaboration interface using GVS,VRST - Virtual Reality Software and Technology,A,"GalVR is a navigation interface that uses galvanic vestibular stimulation (GVS) during walking to cause users to turn from their planned trajectory. We explore GalVR for collaborative navigation in a two-player virtual reality (VR) game. The interface affords a novel game design that exploits the differences in first and third person perspectives, allowing VR and non-VR users to share a play experience. By introducing interdependence arising from dissimilar points of view, players can uniquely contribute to the shared experience based on their roles. We detail the design of our asymmetrical game, Dark Room and present some insights from a pilot study. Trust emerged as the defining factor for successful play.",collaboration; galvanic vestibular stimulation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Hand-free natural user interface for VR HMD with IR based facial gesture tracking sensor,VRST - Virtual Reality Software and Technology,A,We proposed a hand-free Natural User Interface (NUI) for VR Head-Mounted-Display (HMD) with infra-red (IR) based sensor tracking facial gestures. We have realized NUI based on the real-time recognition of user intuitions for VR HMD without any additional control devices except for a built-in Gyroscope and IR couplers with readout circuitry integrated in the foam interface of an HMD. We implemented seven control commands affordable for 3D interactions with virtual objects. The experimental data show that the proposed system provides a convenient and efficient 2D/3D user interface for both manipulating objects and controlling commands while wearing a VR HMD.,facial gesture recognition; head-mounted-display; natural user interface; virtual reality; wearable sensors,Keywords,TRUE,
Scopus,conferencePaper,2017,Immercity: communicating about virtual and augmented realities,VRST - Virtual Reality Software and Technology,A,"Augmented and Mixed Reality technologies bring often definitions understanding issues for novice users. In this paper, we introduce our work in progress, Immercity, regarding the development of a content curation application which manage the idea of communicating on these technologies by their use.",augmented reality; content curation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Integrating performer into a real-time augmented reality performance spatially by using a multi-sensory prop,VRST - Virtual Reality Software and Technology,A,"The authors developed a system for augmenting live performances where the performer can drive the rendered background image via an infrared LED transmitter prop. A multi-sensory prop was designed to aid the position tracking of performer on large scale stage. An infrared LED and a wireless circuit were integrated into the prop, it could be captured by an infrared camera and host computer. It could also trigger the interaction between performer and virtual objects or visual effects, then the virtual objects or special effects will be matched with the position of real performer and displayed on the background large screen to make a real-time augmented live performance.",augmented reality; circuit; infrared LED; multi-modal interfaces; multi-sensory; performance; wireless,Title_Keywords,TRUE,
Scopus,conferencePaper,2017,Multi-device mixed reality TV: a collaborative experience with joint use of a tablet and a headset,VRST - Virtual Reality Software and Technology,A,"A multi-user experience extending a standard TV content with AR elements is presented. It runs with both a standard tablet and a premium MR headset, the Microsoft HoloLens. A virtual TV mosaic is displayed around the TV screen and used as a GUI to control both TV and MR content. This paper focuses on the collaborative and personalized dimension offered by the experience. Unlike most AR applications, it can be simultaneously run by several users using different devices. The users can share content with others while keeping a personalized display. The added-value of such an extended TV experience has been demonstrated through complementary types of content, and user feedback confirms a real interest in this new kind of home entertainment, at the same time immersive, interactive, collaborative and personalized.",extended TV; GUI; mixed reality; multi-device; multi-user,Title_Keywords,TRUE,
Scopus,conferencePaper,2017,PeriText+: utilizing peripheral vision for reading text on augmented reality smart glasses,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) provides real-time information by super-imposing virtual information onto users' view of the real world. Our work is the first to explore how peripheral vision, instead of central vision, can be used to read text on AR and smart glasses. We present PeriText+, a multiword reading interface using rapid serial visual presentation (RSVP). This enables users to observe the real world using central vision, while using peripheral vision to read text. We conducted a lab study to compare reading efficiency among 40 different conditions of text transformation. We also conducted a field study to evaluate the information transfer while using PeriText+ in a real-world walking scenario.",augmented reality; peripheral vision; reading interface; text transformation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,ProjectDR: augmented reality system for displaying medical images directly onto a patient,VRST - Virtual Reality Software and Technology,A,"The internal anatomy of a patient can be difficult for clinicians and other patients to visualize and analyzed in context. The ability to project medical images (CT and MRI scans) directly onto a patient's body helps those viewing these images to recover missing anatomical context for more accurate interpretation. This paper proposes such a system. Various types of images can be displayed using volume rendering techniques for realistic visualization of the internal anatomy and 3D models from segmented images. Calibration is performed on multiple systems to obtain an accurate common coordinate system, as well as correcting visual distortions from the cameras and the projector. This projected AR system provides a common perspective that is not tied to an individual point-of-view which can be used by others such as a surgical team. The system is easily extendable to other display technology and has many potential applications including education, surgical planning, laparoscopic surgery, and entertainment.",3D tracking; medical display; projected augmented reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2017,Real-time wall outline extraction for redirected walking,VRST - Virtual Reality Software and Technology,A,"Existing redirected walking applications use accurate tracking systems to determine the position and orientation of the user within a designated tracking space. In order to plan redirection and to ensure the user's safety, it is necessary to define the walking area in advance. However, when using ad hoc redirected walking, this is not possible, because the user's surroundings are not known beforehand.This paper introduces an approach to reconstruct the geometry of the available walking area as an outline representing the walls and similar structures. The outline is generated in real-time using a commercial SLAM tracking device and will be used for a wall warner safety mechanism and a redirection planner.",area tracking; redirected walking; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2017,ScatAR: a mobile augmented reality application that uses scattering delay networks for room acoustic synthesis,VRST - Virtual Reality Software and Technology,A,"We present an augmented reality (AR) audio application where scattering delay networks efficiently generate and organize a reverberator, based on room geometry scanned by an AR device. The application allows for real-time processing and updating of reflection path geometry. It provides a proof-of-concept for plausible audio-spatial registration of a virtual object in a real environment, but further tests are needed in perceptual evaluation.",audio; augmented reality; real-time physics-based modeling; real-time rendering,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Searching and exploring software repositories in virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose a new approach to visualization of software repositories that allow users to search and explore software projects in virtual reality. In provided environment, respective information structures are mapped to interactive 3D objects. We assume that such transformation of information space to its visible representation will enable the users to gain problem domain knowledge subliminally during the explorations, and that the acquired knowledge will help them to fulfill future tasks more effectively.",feature location; search results visualization; software repositories; software visualization; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,The impact of avatar-owner visual similarity on body ownership in immersive virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper we report on an investigation of the effects of a self-avatar's visual similarity to a user's actual appearance, on their perceptions of the avatar in an immersive virtual reality (IVR) experience. We conducted a user study to examine the participant's sense of body ownership, presence and visual realism under three levels of avatar-owner visual similarity: (L1) an avatar reconstructed from real imagery of the participant's appearance, (L2) a cartoon-like virtual avatar created by a 3D artist for each participant, where the avatar shoes and clothing mimic that of the participant, but using a low-fidelity model, and (L3) a cartoon-like virtual avatar with a pre-defined appearance for the shoes and clothing. Surprisingly, the results indicate that the participants generally exhibited the highest sense of body ownership and presence when inhabiting the cartoon-like virtual avatar mimicking the outft of the participant (L2), despite the relatively low participant similarity. We present our experiment and main findings, also, discuss the potential impact of a self-avatar's visual differences on human perceptions in IVR.",body ownership; HMD; presence; self-avatar; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Towards multimodal interactions: robot jogging in mixed reality,VRST - Virtual Reality Software and Technology,A,"The recent progress made in the field of Augmented Reality/Mixed Reality (AR/MR) has opened new possibilities and approaches to research areas that can benefit from 3D visualization of digital content in the real world. In fact, human-robot interaction design and the design of user interfaces have very much to gain from MR technologies. Nonetheless, designing the user-robot interaction and processing multimodal feedbacks are very challenging tasks. In this paper we focus in particular on interactions in mixed reality.The main contribution of this paper is the implementation of a control system for an industrial manipulator through the user's interactions with MR content displayed with the Microsoft HoloLens. The system is based on the communication between Unity3D (used to design the user experience) and ROS, therefore extendible to any ROS-compatible robotic hardware.",mixed reality; multimodal interactions; robotics; ROS; unity3D,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Using virtual reality for scaffolding computer programming learning,VRST - Virtual Reality Software and Technology,A,"Learning how to analyze computational problems, to think critically, and to transfer algorithmic logic into language-specific code is central to computer programming. A critical step towards acquiring these skills is analyzing and debugging existing code usually starting with the famous ""Hello World"" program.Even after learning basic structure and syntax of a computer language, new learners struggle to understand algorithmic process, to mentally visualize effects of algorithms on data, and to remain engaged while learning it. We explore the use of virtual reality that teaches introductory concepts of computer programming to students in a 3D interactive space, while scaffolding their progression.",computer programming; interactive learning environments; technology-enhanced learning; virtual reality in education,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,VR wildfire prevention: teaching campfire safety in a gamified immersive environment,VRST - Virtual Reality Software and Technology,A,"Due to an increase in the prevalence and intensity of wildfires worldwide [Liu et al. 2010], it is becoming more important to understand campfire safety in order to prevent human-caused wildfires. In the United States, the most common fire safety advice comes in the form of commercials and posters as a part of the Smokey the Bear campaign [Smo 2017]. Presenting this information through a virtual reality game provides a controlled and engaging environment to practice and learn how to safely control a campfire. This immersive experience guides the user through every step of creating and extinguishing a campfire based on information from the Smokey the Bear campaign. VR Wildfire Prevention aims to engage and educate people in campfire safety by providing a controlled environment to practice the relevant techniques while incentivizing proper behavior through gamification. Players of the game report that the game is an enjoyable experience.",immersion; safety training; serious game; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Cubely: virtual reality block-based programming environment,VRST - Virtual Reality Software and Technology,A,"Block-based programming languages are successfully being used as an alternative way of teaching introductory programming concepts. The success is in part due to the low barrier of entry and the visual game-like appeal fostering experimentation and creativity. Virtual reality (VR) presents a step further to an even more immersive and engaging experience. In this demo, we showcase our project Cubely, an immersive VR programming environment in which novice programmers solve programming puzzles within a virtual world. The puzzles are similar to Code.org exercises and solutions to the exercises are assembled by the programmer within the same virtual world using the cubes representing program instructions. The whole environment is templated to a theme of the popular Minecraft video game.",virtual learning environment; virtual reality; visual programming,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,LabDesignAR: configuring multi-camera motion capture systems in augmented reality,VRST - Virtual Reality Software and Technology,A,"We present LabDesignAR, an augmented reality application to support the planning, setup, and reconfiguration of marker-based motion capture systems with multiple cameras. LabDesignAR runs on the Microsoft HoloLens and allows the user to place an arbitrary number of virtual ""holographic"" motion capture cameras into an arbitrary space, in situ. The holographic cameras can be arbitrarily positioned, and different lens configurations can be selected to visualize the resulting fields of view and their intersections. Lab-DesignAR also demonstrates a hybrid natural gestural interaction technique, implemented through a fusion of the vision-based hand tracking capabilities of an augmented reality headset and instrumented gesture recognition with an electromyography armband. The source code for LabDesignAR and its supporting components can be found online.",augmented reality; gestural interaction; hololens; LabDesignAR; motion capture; natural interaction,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2017,Solving Poisson's equation on the Microsoft HoloLens,VRST - Virtual Reality Software and Technology,A,We present a mixed reality application (HoloFEM) for the Microsoft HoloLens. The application lets a user define and solve a physical problem governed by Poisson's equation with the surrounding real world geometry as input data. Holograms are used to visualise both the problem and the solution. The finite element method is used to solve Poisson's equation. Solving and visualising partial differential equations in mixed reality could have potential usage in areas such as building planning and safety engineering.,FEniCS; finite element method; HoloLens; Poisson's equation,Abstract,TRUE,
Scopus,conferencePaper,2017,The smart pin: a novel object manipulation technique for immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"In this paper we describe a demo setup showing the potential usefulness of a novel single-handed manipulation technique, designed to be used with immersive Virtual Environments. The technique allows manipulation control over objects in the scene through the use of a single 3D widget, allowing easy and separated control of translation, rotation and scaling actions. The goal is to provide an intuitive, easy-to-use and accurate way to perform simple manipulation tasks using only one hand. User tests demonstrated that the widget is intuitive and effective.",3D widgets; mid air manipulation; user evaluation; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2017,Walkable self-overlapping virtual reality maze and map visualization demo: public virtual reality setup for asymmetric collaboration,VRST - Virtual Reality Software and Technology,A,This paper describes our demonstration of a walkable self-overlapping maze and its corresponding map to facilitate asymmetric collaboration for room-scale virtual reality setups in public places.,asymmetric collaboration; computer graphics; demo; public spaces; room-scale virtual reality; self-overlapping maze; virtual reality; visualization,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Sublime: a hands-free virtual reality menu navigation system using a high-frequency SSVEP-based brain-computer interface,VRST - Virtual Reality Software and Technology,A,"In this work we present Sublime, a new concept of Steady-State Visually Evoked Potential (SSVEP) based Brain-Computer Interface (BCI) where brain-computer communication occurs by capturing imperceptible visual stimuli integrated in the virtual scene and effortlessly conveying subliminal information to a computer. The technology was tested in a Virtual Reality (VR) environment, where the subject could navigate between the different menus by just gazing at them. The ratio between the stimuli frequencies and the refresh rate of the VR display creates an undesired perception of beats for which different solutions are proposed. To inform the user of target activation, real-time feedback in the form of loading bars is incorporated under each selectable object. We conducted experiments with several subjects and though the system is slower than a conventional joystick, users reported a satisfactory overall experience, in part due to the unexpected responsiveness of the system, as well as due to the fact that virtual objects flickered at a rate that did not cause annoyance. Since the imperceptible visual stimuli can be integrated unobtrusively to any element of the virtual world, we conclude that the potential applications of Sublime are extensive, especially in situations where knowing user's visual focus can be relevant.",brain-computer interface; electroencephalography; steady-state visually evoked potentials; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Standards-compliant HTTP adaptive streaming of static light fields,VRST - Virtual Reality Software and Technology,A,"Static light fields are an effective technology to precisely visualize complex inanimate objects or scenes, synthetic and real-world alike, in Augmented, Mixed and Virtual Reality contexts. Such light fields are commonly sampled as a collection of 2D images. This sampling methodology inevitably gives rise to large data volumes, which in turn hampers real-time light field streaming over best effort networks, particularly the Internet. This paper advocates the packaging of the source images of a static light field as a segmented video sequence so that the light field can then be interactively network streamed in a quality-variant fashion using MPEG-DASH, the standardized HTTP Adaptive Streaming scheme adopted by leading video streaming services like YouTube and Netflix. We explain how we appropriate MPEG-DASH for the purpose of adaptive static light field streaming and present experimental results that prove the feasibility of our approach, not only from a networking but also a rendering perspective. In particular, real-time rendering performance is achieved by leveraging video decoding hardware included in contemporary consumer-grade GPUs. Important trade-offs are investigated and reported on that impact performance, both network-wise (e.g., applied sequencing order and segmentation scheme for the source images of the static light field) and rendering-wise (e.g., disk-versus-GPU caching of source images). By adopting a standardized transmission scheme and by exclusively relying on commodity graphics hardware, the net result of our work is an interoperable and broadly deployable network streaming solution for static light fields.",experimental evaluation; H.264; HTTP adaptive streaming; IBR; JPEG; MPEG-DASH; static light fields; video compression,Abstract,TRUE,
Scopus,conferencePaper,2018,Design and implementation of a multi-person fish-tank virtual reality display,VRST - Virtual Reality Software and Technology,A,"A mixed reality experience with a physical display, that situates 3D virtual content within the real world, has the potential to help people work and play with 3D information. However, almost all of such ""fish tank virtual reality"" (FTVR) systems have been isolated to a single-person experience, making them unsuitable for collaborative tasks. In this paper, we present a display system that allows two people to have unobstructed 3D perspective views into a spherical display while still being able to see and talk to one another. We evaluated the system through qualitative observation at a four-day exhibition and found it was effective for providing a convincing, shared 3D experience.",3D displays; co-location; collaboration; fish tank virtual reality; spherical displays; stereo,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Comparison of the usability of a car infotainment system in a mixed reality environment and in a real car,VRST - Virtual Reality Software and Technology,A,"Instead of installing new control modes for infotainments systems in a real vehicle for testing, it is an attractive idea (saving time and cost) to evaluate and develop these systems in a mixed reality (MR) environment. The central question of the study is whether the usability evaluation of a car entertainment system within a MR environment provides the same results as the evaluation of the car entertainment system within a real car. For this purpose a prototypical car infotainment system was built and integrated into a real car and into a MR environment. The MR environment represents the interior of the car and uses finger tracking and real haptic control elements of the center console of a car. Two test groups were assigned to the two different test environments. The study shows, that the usability is rated similar in both environments although readability and representation within the infotainment system is problematic.",car infotainment system; mixed reality; study; usability; user experience,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Camera time warp: compensating latency in video see-through head-mounted-displays for reduced cybersickness effects,VRST - Virtual Reality Software and Technology,A,"We introduce Camera Time Warp (CamWarp), a novel reprojection technique for video-see-through augmented reality, which reduces the registration error between captured real-world videos and rendered virtual images. Instead of rendering the image plane locked to the virtual camera, CamWarp renders the image plane at the real-world position it was captured at, and compensates for potential artifacts. We conducted two experiments to evaluate the effectiveness of CamWarp. In the first experiment participants were asked to report subjective discomfort while moving their head in a pattern inspired by the ISO 9241-9 Fitts' Law task at different speeds while the video feed was rendered at varying frame rates. The results show that the technique can significantly reduce subjective levels of discomfort and cybersickness symptoms for all tested configurations. In the second experiment participants were asked to move physical objects on a projected path as quickly and precisely as possible. Results show a positive effect of CamWarp on speed and accuracy.",augmented reality; cybersickness; latency compensation,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Performer vs. observer: whose comfort level should we consider when examining the social acceptability of input modalities for head-worn display?,VRST - Virtual Reality Software and Technology,A,"The popularity of head-worn displays (HWD) technologies such as Virtual Reality (VR) and Augmented Reality (AR) headsets is growing rapidly. To predict their commercial success, it is essential to understand the acceptability of these new technologies, along with new methods to interact with them. In this vein, the evaluation of social acceptability of interactions with these technologies has received significant attention, particularly from the performer's (i.e., user's) viewpoint. However, little work has considered social acceptability concerns from observers' (i.e., spectators') perspective. Although HWDs are designed to be personal devices, interacting with their interfaces are often quite noticeable, making them an ideal platform to contrast performer and observer perspectives on social acceptability. Through two studies, this paper contrasts performers' and observers' perspectives of social acceptability interactions with HWDs under different social contexts. Results indicate similarities as well as differences, in acceptability, and advocate for the importance of including both perspectives when exploring social acceptability of emerging technologies. We provide guidelines for understanding social acceptability specifically from the observers' perspective, thus complementing our current practices used for understanding the acceptability of interacting with these devices.",augmented reality; HWDs; input modalities; social acceptance,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Merging environments for shared spaces in mixed reality,VRST - Virtual Reality Software and Technology,A,"In virtual reality a real walking interface limits the extent of a virtual environment to our local walkable space. As local spaces are specific to each user, sharing a virtual environment with others for collaborative work or games becomes complicated. It is not clear which user's walkable space to prefer, or whether that space will be navigable for both users.This paper presents a technique which allows users to interact in virtual reality while each has a different walkable space. With this method mappings are created between pairs of environments. Remote users are then placed in the local environment as determined by the corresponding mapping.A user study was conducted with 38 participants. Pairs of participants were invited to collaborate on a virtual reality puzzle-solving task while in two different virtual rooms. An avatar representing the remote user was mapped into the local user's space. The results suggest that collaborative systems can be based on local representations that are actually quite different.",augmented reality; computer graphics; head-mounted display; mixed reality; planar map; remote collaboration; virtual co-location; virtual environments; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Perceived weight of a rod under augmented and diminished reality visual effects,VRST - Virtual Reality Software and Technology,A,"We can use augmented reality (AR) and diminished reality (DR) in combination, in practice. However, to the best of our knowledge, there is no research on the validation of the cross-modal effects in AR and DR. Our research interest here is to investigate how this continuous visual changes between AR and DR would change our weight sensation of an object. In this paper, we built a system that can continuously extend and reduce the amount of visual entity of real objects using AR and DR renderings to confirm that users can perceive things heavier and lighter than they actually are in the same manner as SWI. Different from the existing research where either AR or DR visual effects were used, we validated one of cross-modal effects in the context of both continuous AR and DR visuo-haptic. Regarding the weight sensation, we found that such cross-modal effect can be approximated with a continuous linear relationship between the weight and length of real objects. Our experimental results suggested that the weight sensation is closely related to the positions of the center of gravity (CoG) and perceived CoG positions lie within the object's entity under the examined conditions.",augmented reality; diminished reality; sense of ownership; visuo-haptic system; weight sensation,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Tracking projection mosaicing by synchronized high-speed optical axis control,VRST - Virtual Reality Software and Technology,A,"Projectors, as information display devices, have improved substantially and to achieve both the wide range and high resolution is desired for the dynamic human gaze. However, a fixed projector has a trade-off between the angle of projection and a resolution with limited pixels. Conventional methods with dynamic optical axis control lack the potential speed of the devices. We propose a tracking projection mosaicing with a high-speed projector and a high-speed optical axis controller for a randomly moving position, such as the gaze. We also propose a synchronization strategy by queuing and alternating operations to reduce motion-based artifacts, which realize a high-quality static image projection during the dynamic optical axis control. We have experimentally validated the geometric and temporal consistency of the proposed synchronization method and have attempted a demonstration of the tracking projection mosaicing for the dynamically moving bright spot of a laser pointer.",high-speed mirror; high-speed projector; high-speed visual feedback; projection-based augmented reality; synchronization,Keywords,TRUE,
Scopus,conferencePaper,2018,Gaze navigation in the real world by changing visual appearance of objects using projector-camera system,VRST - Virtual Reality Software and Technology,A,"This paper proposes a method for gaze navigation in the real world by projecting an image onto a real object and changing its appearance. In the proposed method, a camera captures an image of objects in the real world. Next all the pixels in the image but those in a specified region are slightly shifted to left and right. Then the obtained image is projected onto the original objects. As a result, the objects not in the specified region looks blurred. We conducted user experiments and showed that the users' gaze were navigated to the specified region.",augmented reality; gaze navigation; procam; shift filter,Keywords,TRUE,
Scopus,conferencePaper,2018,Eyestrain impacts on learning job interview with a serious game in virtual reality: a randomized double-blinded study,VRST - Virtual Reality Software and Technology,A,"Purpose: This study explores eyestrain and its possible impacts on learning performances and quality of experience using different apparatuses and imaging. Materials and Methods: 69 participants played a serious game simulating a job interview with a Samsung Gear VR Head Mounted Display (HMD) or a computer screen. The study was conducted according to a double-blinded protocol. Participants were randomly assigned to 3 groups: PC, HMD biocular and HMD stereoscopy (S3D). Participants played the game twice, allowing between group analyses. Eyestrain was assessed pre- and post-exposure on a chin-head rest with optometric measures. Learning traces were obtained in-game by registering response time and scores. Quality of experience was measured with questionnaires assessing Presence, Flow and Visual Comfort. Results: eyestrain was significantly higher with HMDs than PC based on Punctum Proximum of accommodation and visual acuity variables and tends to be higher with S3D. Learning was more efficient in HMDs conditions based on time for answering but the group with stereoscopy performed lower than the binocular imaging one. Quality of Experience was better based on visual discomfort with the PC condition than with HMDs. Conclusion: learning expected answers from a job interview is more efficient while using HMDs than a computer screen. However, eyestrain tends to be higher while using HMDs and S3D. The quality of experience was also negatively impacted with HMDs compared to computer screen. Not using S3D or lowering its impact should be explored to provide comfortable learning experience.1",eyestrain; head mounted display; learning; serious game; stereoscopy; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2018,Keep my head on my shoulders! why third-person is bad for navigation in VR,VRST - Virtual Reality Software and Technology,A,"Head-Mounted Displays are useful to place users in virtual reality (VR). They do this by totally occluding the physical world, including users' bodies. This can make self-awareness problematic. Indeed, researchers have shown that users' feeling of presence and spatial awareness are highly influenced by their virtual representations, and that self-embodied representations (avatars) of their anatomy can make the experience more engaging. On the other hand, recent user studies show a penchant towards a third-person view of one's own body to seemingly improve spatial awareness. However, due to its unnaturality, we argue that a third-person perspective is not as effective or convenient as a first-person view for task execution in VR. In this paper, we investigate, through a user evaluation, how these perspectives affect task performance and embodiment, focusing on navigation tasks, namely walking while avoiding obstacles. For each perspective, we also compare three different levels of realism for users' representation, specifically a stylized abstract avatar, a mesh-based generic human, and a real-time point-cloud rendering of the users' own body. Our results show that only when a third-person perspective is coupled with a realistic representation, a similar sense of embodiment and spatial awareness is felt. In all other cases, a first-person perspective is still better suited for navigation tasks, regardless of representation.",augmented reality; avatar; embodiment; full-body tracking; travel; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Dynamic HDR environment capture for mixed reality,VRST - Virtual Reality Software and Technology,A,"Rendering accurate and convincing virtual content into mixed reality (MR) scenes requires detailed illumination information about the real environment. In existing MR systems, this information is often captured using light probes [1, 8, 9, 17, 19–21], or by reconstructing the real environment as a preprocess [31, 38, 54]. We present a method for capturing and updating a HDR radiance map of the real environment and tracking camera motion in real time using a self-contained camera system, without prior knowledge about the real scene. The method is capable of producing plausible results immediately and improving in quality as more of the scene is reconstructed. We demonstrate how this can be used to render convincing virtual objects whose illumination changes dynamically to reflect the changing real environment around them.",3D reconstruction; HDR; mixed reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,An evaluation of pupillary light response models for 2D screens and VR HMDs,VRST - Virtual Reality Software and Technology,A,"Pupil diameter changes have been shown to be indicative of user engagement and cognitive load for various tasks and environments. However, it is still not the preferred physiological measure for applied settings. This reluctance to leverage the pupil as an index of user engagement stems from the problem that in scenarios where scene brightness cannot be controlled, the pupil light response confounds the cognitive-emotional response. What if we could predict the light response of an individual's pupil, thus creating the opportunity to factor it out of the measurement? In this work, we lay the groundwork for this research by evaluating three models of pupillary light response in 2D, and in a virtual reality (VR) environment. Our results show that either a linear or an exponential model can be fit to an individual participant with an easy-to-use calibration procedure. This work opens several new research directions in VR relating to performance analysis and inspires the use of eye tracking beyond gaze as a pointer and foveated rendering.",eyetracking; light response; pupil dilation; videos; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Occlusion handling using semantic segmentation and visibility-based rendering for mixed reality,VRST - Virtual Reality Software and Technology,A,"Real-time occlusion handling is a major problem in outdoor mixed reality system because it requires great computational cost mainly due to the complexity of the scene. Using only segmentation, it is difficult to accurately render a virtual object occluded by complex objects such as vegetation. In this paper, we propose a novel occlusion handling method for real-time mixed reality given a monocular image and an inaccurate depth map. We modify the intensity of the overlayed CG object based on the texture of the underlying real scene using visibility-based rendering. To determine the appropriate level of visibility, we use CNN-based semantic segmentation and assign labels to the real scene based on the complexity of object boundary and texture. Then we combine the segmentation results and the foreground probability map from the depth image to solve the appropriate blending parameter for visibility-based rendering. Our results show improvement in handling occlusions for inaccurate foreground segmentation compared to existing blending-based methods.",mixed reality; occlusion handling; semantic segmentation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,A longitudinal study of small group interaction in social virtual reality,VRST - Virtual Reality Software and Technology,A,"Now that high-end consumer phones can support immersive virtual reality, we ask whether social virtual reality is a promising medium for supporting distributed groups of users. We undertook an exploratory in-the-wild study using Samsung Gear VR headsets to see how existing social groups that had become geographically dispersed could use VR for collaborative activities. The study showed a strong propensity for users to feel present and engaged with group members. Users were able to bring group behaviors into the virtual world. To overcome some technical limitations, they had to create novel forms of interaction. Overall, the study found that users experience a range of emotional states in VR that are broadly similar to those that they would experience face-to-face in the same groups. The study highlights the transferability of existing social group dynamics in VR interactions but suggests that more work would need to be done on avatar representations to support some intimate conversations.",affective states; avatar representation; in-the-wild study; social VR; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Human upper-body inverse kinematics for increased embodiment in consumer-grade virtual reality,VRST - Virtual Reality Software and Technology,A,"Having a virtual body can increase embodiment in virtual reality (VR) applications. However, comsumer-grade VR falls short of delivering sufficient sensory information for full-body motion capture. Consequently, most current VR applications do not even show arms, although they are often in the field of view. We address this shortcoming with a novel human upper-body inverse kinematics algorithm specifically targeted at tracking from head and hand sensors only. We present heuristics for elbow positioning depending on the shoulder-to-hand distance and for avoiding reaching unnatural joint limits. Our results show that our method increases the accuracy compared to general inverse kinematics applied to human arms with the same tracking input. In a user study, participants preferred our method over displaying disembodied hands without arms, but also over a more expensive motion capture system. In particular, our study shows that virtual arms animated with our inverse kinematics system can be used for applications involving heavy arm movement. We demonstrate that our method can not only be used to increase embodiment, but can also support interaction involving arms or shoulders, such as holding up a shield.",animation; embodiment; inverse kinematics; motion capture; presence; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Immersion and coherence in a stressful virtual environment,VRST - Virtual Reality Software and Technology,A,"We report on the design and results of two experiments investigating Slater's Place Illusion (PI) and Plausibility Illusion (Psi) in a virtual visual cliff environment. PI (the illusion of being in a place) and Psi (the illusion that the depicted events are actually happening) were proposed by Slater as orthogonal components of virtual experience which contribute to realistic response in a VE. To that end, we identified characteristics of a virtual reality experience that we expected to influence one or the other of PI and Psi. We designed two experiments in which each participant experienced a given VE in one of four conditions chosen from a 2×2 design: high or low levels of PI-eliciting characteristics (that is, immersion) and high or low levels of Psi-eliciting characteristics. Following Skarbez, we use the term ""coherence"" for those characteristics which contribute to Psi, parallel to the use of ""immersion"" for characteristics that contribute to PI. We collected both questionnaire-based and physiological metrics. Several existing presence questionnaires could not reliably distinguish the effects of PI from those of Psi. They did, however, indicate that high levels of PI-eliciting characteristics and Psi-eliciting characteristics together result in higher presence, compared any of the other three conditions. This suggests that ""breaks in PI"" and ""breaks in Psi"" belong to a broader category of ""breaks in experience,"" any of which result in a degraded user experience. Participants' heart rates, however, responded markedly differently in the two Psi conditions; no such difference was observed across the PI conditions. This indicates that a VE that exhibits unusual or confusing behavior can cause stress in a user that affects physiological responses, and that one must take care to eliminate such confusing behaviors if one is using physiological measurement as a proxy for subjective experience in a VE.",coherence; immersion; physiological metrics; place illusion(PI); plausibility illusion (Psi); presence; user studies; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,The physical-virtual table: exploring the effects of a virtual human's physical influence on social interaction,VRST - Virtual Reality Software and Technology,A,"In this paper, we investigate the effects of the physical influence of a virtual human (VH) in the context of face-to-face interaction in augmented reality (AR). In our study, participants played a tabletop game with a VH, in which each player takes a turn and moves their own token along the designated spots on the shared table. We compared two conditions as follows: the VH in the virtual condition moves a virtual token that can only be seen through AR glasses, while the VH in the physical condition moves a physical token as the participants do; therefore the VH's token can be seen even in the periphery of the AR glasses. For the physical condition, we designed an actuator system underneath the table. The actuator moves a magnet under the table which then moves the VH's physical token over the surface of the table. Our results indicate that participants felt higher co-presence with the VH in the physical condition, and participants assessed the VH as a more physical entity compared to the VH in the virtual condition. We further observed transference effects when participants attributed the VH's ability to move physical objects to other elements in the real world. Also, the VH's physical influence improved participants' overall experience with the VH. We discuss potential explanations for the findings and implications for future shared AR tabletop setups.",augmented reality; mediated physicality; virtual humans,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,With a little help from a holographic friend: the OpenIMPRESS mixed reality telepresence toolkit for remote collaboration systems,VRST - Virtual Reality Software and Technology,A,"Remote mixed reality (MR) collaboration systems allow for multimodal, real-time support from remote experts. We present our open toolkit that provides a flexible end-to-end solution for building such systems using off-the-shelf hardware. From related work, three core design aspects have been identified: 1) the independence of the viewpoint that the visitor (the remote expert) can take in relation to position and viewpoint of the visitee, 2) the immersiveness of the presentation technology for visitor and visitee, and 3) the extent to which the visitor's body is represented in the visitee's environment. This paper describes the implementation of our system, which includes these aspects. In a study aimed at validating whether we implemented these core aspects to good effect, conducted with a collaborative puzzle application built with our toolkit, we examine how variations of these aspects contribute to usability, performance and social presence related metrics.",collaboration; embodiment; mixed reality; telepresence,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Training in IVR: investigating the effect of instructor design on social presence and performance of the VR user,VRST - Virtual Reality Software and Technology,A,"We investigate instructor representations (IRs) in the context of virtual trainings with head mounted displays (HMD). Despite the recently increased industry and research focus on virtual training in immersive virtual reality (IVR), the effect of IRs on the performer (VR user) has received little attention. We present the results of a study (N=33), evaluating the effect of three IRs - webcam, avatar and sound-only - on social presence (SP) and performance (PE) of the VR user during task completion. Our results show that instructor representation has an effect on SP and that, contrary to our assumption based on prior work, it affects performance negatively.",immersive virtual reality; instructor design; social presence,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Am I in the theater? usability study of live performance based virtual reality,VRST - Virtual Reality Software and Technology,A,"Duplicating the audience experience of an art performance with VR technology is a promising VR application, which is considered to provide better viewer experience than the conventional video. As various forms of art performances are recorded by the panoramic camera and broadcasted on the Internet, the impact of this new VR-based media to the viewers needs to be systematically studied. In this work, a two-level usability framework is proposed, which combines the traditional concepts of presence and the quality evaluation of art performances, aiming to systematically study the usability of such VR application. Both the conventional video and the panoramic video of a theatre performance were captured simultaneously, and were replayed to two groups of viewers in a cinematic setup and through an HMD respectively. The psychological measurement methods, including the questionnaire and the interview, as well as the psychophysical measurement methods, including the EEG and the motion capture techniques were both used in the study. The results show that the such VR application duplicates the live performance better by providing a higher sense of presence, higher engagement levels, and stronger desire to see live performance. For visual intensive performance contents, the new VR-based media can provide a better user experience. The future development of the new media forms based on the panoramic video technique could benefit from this work.",EEG; live performance; usability evaluation; viewer experience; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2018,Discrete scene rotation during blinks and its effect on redirected walking algorithms,VRST - Virtual Reality Software and Technology,A,"Moving through a virtual environment (VE) by real walking is beneficial to user immersion, feeling of presence and way finding. However, the available physical spaces are of limited size and usually much smaller than the VE. One solution to this problem is using redirection techniques (RDTs). While the focus of existing research has been mostly on continuous RDTs, work on discrete RDTs is still limited.In this paper, we present our research results on the discrete rotation of a virtual scene during walking. A study with 14 subjects was conducted to identify the detection threshold of the scene rotation in two conditions: during blinking and when eyes are open. Results showed that on average, users failed to detect a scene rotation of 9.1 degrees during blinking, as compared to 2.4 degrees when eyes are open. Simulations were then performed to investigate the effects of incorporating discrete scene orientation during blinks into existing algorithms such as steer-to-center and steer-to-orbit when different predefined paths are followed. Results showed that on average the number of resets is reduced by 13%, and the minimum space required for encountering no reset is reduced by 20%. A reset technique was also proposed and shown to give better performance than the existing two-one turn reset technique.",blink; discrete rotation; redirected walking; virtual reality; visual suppression,Keywords,TRUE,
Scopus,conferencePaper,2018,The effect of chair type on users' viewing experience for 360-degree video,VRST - Virtual Reality Software and Technology,A,"The consumption of 360-degree videos with head-mounted displays (HMDs) is increasing rapidly. A large number of HMD users watch 360-degree videos at home, often on non-swivel seats; however videos are frequently designed to require the user to turn around. This work explores how the difference in users' chair type might influence their viewing experience. A between-subject experiment was conducted with 41 participants. Three chair conditions were used: fixed, half-swivel and full-swivel. A variety of measures were explored using eye-tracking, questionnaires, tasks and semi-structured interviews. Results suggest that the fixed and half-swivel chairs discouraged exploration for certain videos compared with the full-swivel chair. Additionally, participants in the fixed chair had worse spatial awareness and greater concern about missing something for certain video than those in the full-swivel chair. No significant differences were found in terms of incidental memory, general engagement and simulator sickness among the three chair conditions. Furthermore, thematic analysis of post-experiment interviews revealed four themes regarding the restrictive chairs: physical discomfort, difficulty following moving objects, reduced orientation and guided attention. Based on the findings, practical implications, limitations and future work are discussed.",cinematic virtual reality; panoramic video; user study,Keywords,TRUE,
Scopus,conferencePaper,2018,Data-driven modeling of group entitativity in virtual environments,VRST - Virtual Reality Software and Technology,A,We present a data-driven algorithm to model and predict the socio-emotional impact of groups on observers. Psychological research finds that highly entitative i.e. cohesive and uniform groups induce threat and unease in observers. Our algorithm models realistic trajectory-level behaviors to classify and map the motion-based entitativity of crowds. This mapping is based on a statistical scheme that dynamically learns pedestrian behavior and computes the resultant entitativity induced emotion through group motion characteristics. We also present a novel interactive multi-agent simulation algorithm to model entitative groups and conduct a VR user study to validate the socio-emotional predictive power of our algorithm. We further show that model-generated high-entitativity groups do induce more negative emotions than low-entitative groups.,crowd simulation; data driven simulation; group dynamics; motion model; pedestrian behavior; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2018,Automatic transfer of musical mood into virtual environments,VRST - Virtual Reality Software and Technology,A,"This paper presents a method that automatically transforms a virtual environment (VE) according to the mood of input music. We use machine learning to extract a mood from the music. We then select images exhibiting the mood and transfer their styles to the textures of objects in the VE photorealistically or artistically. Our user study results indicate that our method is effective in transferring valence-related aspects, but not arousal-related ones. Our method can still provide novel experiences in virtual reality and speed up the production of VEs by automating its procedure.",affect; mood; music; transfer; virtual environment,Abstract,TRUE,
Scopus,conferencePaper,2018,Step aside: an initial exploration of gestural input for lateral movement during walking-in-place locomotion,VRST - Virtual Reality Software and Technology,A,"Walking-in-place (WIP) techniques provide users with a relatively natural way of walking in virtual reality. However, previous research has primarily focused on WIP during forward movement and tasks involving turning. Thus, little is known about what gestures to use in combination with WIP in order to enable sidestepping. This paper presents two user studies comparing three different types of gestures based on movement of the hip, leaning of the torso, and actual sidesteps. The first study focuses on purely lateral movement while the second involves both forward and lateral movement. The results of both studies suggest that leaning yielded significantly more natural walking experiences and this gesture also produced significantly less positional drift.",locomotion; travel; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Haptic around: multiple tactile sensations for immersive environment and interaction in virtual reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we present Haptic Around, a hybrid-haptic feedback system, which utilizes fan, hot air blower, mist creator and heat light to recreate multiple tactile sensations in virtual reality for enhancing the immersive environment and interaction. This system consists of a steerable haptic device rigged on the top of the user head and a handheld device also with haptics feedbacks to simultaneously provide tactile sensations to the users in a 2m x 2m space. The steerable haptic device can enhance the immersive environment for providing full body experience, such as heat in the desert or cold in the snow mountain. Additionally, the handheld device can enhance the immersive interaction for providing partial body experience, such as heating the iron or quenching the hot iron. With our system, the users can perceive visual, auditory and haptic when they are moving around in virtual space and interacting with virtual object. In our study, the result has shown the potential of the hybrid-haptic feedback system, which the participants rated the enjoyment, realism, quality, immersion higher than the other.",haptics; immersive environment; immersive experience; multiple tactile sensation; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Can we perceive changes in our moving speed: a comparison between directly and indirectly powering the locomotion in virtual environments,VRST - Virtual Reality Software and Technology,A,"Many categories of the illusion of self-motion have been widely studied with the potential support of virtual reality. However, the effects of directly and indirectly powering the movement on the possibility of perceiving changes in moving speed and their relationship with sensory feedback on users' speed change perception have not been investigated before. In this paper, we present the results of our user study on the difference in perceiving changes in moving speed between two different movement techniques: ""pedaling"" and ""throttling"". We also explore the effects of different velocity gains, accelerations and speeds of airflow, and their interactions with the movement techniques on users' perception of speed changes in addition to user performance and perception. We built a bike simulator that supports both of the movement techniques and provides sensory feedback. In general, ""pedaling"" gave users more possibility to perceive changes in moving velocity than ""throttling"".",bike simulator; locomotion; speed perception; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,HFX studio: haptic editor for full-body immersive experiences,VRST - Virtual Reality Software and Technology,A,"Current virtual reality systems enable users to explore virtual worlds, fully embodied in avatars. This new type of immersive experience requires specific authoring tools. The traditional ones used in the movie and the video games industries were modified to support immersive visual and audio content. However, few solutions exist to edit haptic content, especially when the whole user's body is involved. To tackle this issue we propose HFX Studio, a haptic editor based on haptic perceptual models. Three models of pressure, vibration and temperature were defined to allow the spatialization of haptic effects on the user's body. These effects can be designed directly on the body (egocentric approach), or specified as objects of the scene (allocentric approach). The perceptual models are also used to describe capabilities of haptic devices. This way the created content is generic, and haptic feedback is rendered on the available devices. The concept has been implemented with the Unity®game engine, a tool already used in VR production. A qualitative pilot user study was conducted to analyze the usability of our tool with expert users. Results shows that the edition of haptic feedback is intuitive for these users.",edition; full body; haptics; immersive experience,Abstract,TRUE,
Scopus,conferencePaper,2018,The impact of fear of the sea on working memory performance: a research based on virtual reality,VRST - Virtual Reality Software and Technology,A,"The sea has been manifested to cause the emotion of fear to people when it comes to a very depth, especially to those who have thalassophobia. Many people have to work in the sea while nearly no research on influence of fear of the sea to cognition has been carried out. This study explores the impact of fear of the sea induced by immersive virtual reality on working memory which is a cognitive system with a limited capacity. Participants were required to complete n-back working memory task of three difficulty levels in the non-emotional environment and the undersea environment respectively by means of virtual reality. Pupil diameter changes were recorded along with the task performance. In addition to reaction times and accuracy (correctly press a button in response to targets) as two task performance indices used in most researches, the commission errors (incorrectly press a button in response to non-targets) and omission errors (incorrectly do not press a button in response to targets) were also differentiated herein. The results of the study indicated that the virtual undersea environment did induce the emotion of fear. As for the task performance, except that the performance of low-level task did not differ much between the two environments, the fear of the sea increased the accuracy of the medium level n-back task but decreased it of high-level n-back task. Result of omission errors was just the opposite and commission errors were increased in both levels of task. The findings, including the positive role of a moderate level of fear of the sea in the performance of working memory task, make a lot of sense for future cognitive work in the sea.",fear of the sea; task performance; virtual reality; working memory,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Investigating the reason for increased postural instability in virtual reality for persons with balance impairments,VRST - Virtual Reality Software and Technology,A,"The objective of this study is to investigate how different visual components of Virtual Reality (VR), such as field of view, frame rate, and display resolution affect postural stability in VR. Although previous studies identified these visual components as some of the primary factors that differ significantly in VR from reality, the effect of each component on postural stability is yet unknown. While most people experience postural instability in VR, it is worse for people with balance impairments (BIs). This may be because they depend more on their visual cues to maintain postural stability. We conducted a study with ten people with balance impairments due to Multiple Sclerosis (MS) and seven people without balance impairments to investigate the effect of different visual components on postural stability. In each condition, we varied one of the visual components and kept all other components fixed. Each participant explored the virtual environment (VE) in a controlled fashion to make sure that the effect of the visual components was consistent for all participants. Results from our study suggest that for people with BIs, decreased field of view and decreased frame rate have significant negative effects on postural stability, but the effect of display resolution is inconclusive. However, for people without BIs, there were no significant differences for any of the visual components. Therefore, VR systems targeting people with balance impairments should focus on improving field of view and frame rate before improving display resolution.",accessibility; balance; head-mounted display; postural stability; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,In-pulse: inducing fear and pain in virtual experiences,VRST - Virtual Reality Software and Technology,A,"Researchers have attempted to increase the realism of virtual reality (VR) applications in many ways. Combinations of the visual, auditory and haptic feedback have successfully simulated experiences in VR, however, multimedia contents may also stimulate emotions. In this paper, we especially paid attention to negative emotions that may be perceived in such experiences (e.g., fear). We hypothesized that volunteering, visual, mechanical, and electrical feedback may induce negative emotional feedback to users. In-Pulse is a novel system and approach to explore the potential of bringing this emotional feedback to users. We designed a head-mounted display (HMD) combined with mechanical and electrical muscle stimulation (EMS) actuators. A user study was performed to explore the effect of our approaches with combinations with VR contents. The results suggest that mechanical actuators and EMS can improve the experience of virtual experiences.",electrical muscle stimulation; emotion; fear; head-mounted display; pain; virtual reality; wearables,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Investigating different modalities of directional cues for multi-task visual-searching scenario in virtual reality,VRST - Virtual Reality Software and Technology,A,"In this study, we investigated and compared the effectiveness of visual, auditory, and vibrotactile directional cues on multiple simultaneous visual-searching tasks in an immersive virtual environment. Effectiveness was determined by the task-completion time, the range of head movement, the accuracy of the identification task, and the perceived workload. Our experiment showed that the on-head vibrotactile display can effectively guide users towards virtual visual targets, without affecting their performance on the other simultaneous tasks, in the immersive VR environment. These results can be applied to numerous applications (e.g. gaming, driving, and piloting) in which there are usually multiple simultaneous tasks, and the user experience and performance could be vulnerable.",auditory; directional cue; multi-task; vibration; virtual reality; visual,Title_Keywords,TRUE,
Scopus,conferencePaper,2018,A lightweight and efficient system for tracking handheld objects in virtual reality,VRST - Virtual Reality Software and Technology,A,"While the content of virtual reality (VR) has grown explosively in recent years, the advance of designing user-friendly control interfaces in VR still remains a slow pace. The most commonly used device, such as gamepad or controller, has fixed shape and weight, and thus can not provide realistic haptic feedback when interacting with virtual objects in VR. In this work, we present a novel and lightweight tracking system in the context of manipulating handheld objects in VR. Specifically, our system can effortlessly synchronize the 3D pose of arbitrary handheld objects between the real world and VR in realtime performance. The tracking algorithm is simple, which delicately leverages the power of Leap Motion and IMU sensor to respectively track object's location and orientation. We demonstrate the effectiveness of our system with three VR applications use pencil, ping-pong paddle, and smartphone as control interfaces to provide users more immersive VR experience.",haptic feedback; object tracking; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,AR DeepCalorieCam V2: food calorie estimation with CNN and AR-based actual size estimation,VRST - Virtual Reality Software and Technology,A,"In most of the cases, the estimated calories are just associated with the estimated food categories, or the relative size compared to the standard size of each food category which are usually provided by a user manually. In addition, in the case of calorie estimation based on the amount of meal, a user conventionally needs to register a size-known reference object in advance and to take a food photo with the registered reference object. In this demo, we propose a new approach for food calorie estimation with CNN and Augmented Reality (AR)-based actual size estimation. By using Apple ARKit framework, we can measure the actual size of the meal area by acquiring the coordinates on the real world as a three-dimensional vector, we implemented this demo app. As a result, it is possible to calculate the size more accurately than in the previous method by measuring the meal area directly, the calorie estimation accuracy has improved.",application; augmented reality; deep learning; food calorie estimation; iOS,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Automatic 3D modeling of artwork and visualizing audio in an augmented reality environment,VRST - Virtual Reality Software and Technology,A,"In recent years, traditional art museums have begun to use AR/VR technology to make visits more engaging and interactive. This paper details an application which provides features designed to be immediately engaging and educational to museum visitors within an AR view. The application superimposes an automatically generated 3D representation over a scanned artwork, along with the work's authorship, title, and date of creation. A GUI allows the user to exaggerate or decrease the depth scale of the 3D representation, as well as to search for related works of music. Given this music as audio input, the generated 3D model will act as an audio visualizer by changing depth scale based on input frequency.",3D model generation; art museum; audio visualization; augmented reality; education; music,Title_Keywords,TRUE,
Scopus,conferencePaper,2018,Design-led 3D visualization of nanomedicines in virtual reality,VRST - Virtual Reality Software and Technology,A,"Nanomedicines are a promising addition to the arsenal of new cancer therapies. During development, scientists must precisely track their distribution in the body, a task that can be severely limited by traditional 2D displays. With its stereoscopic capacity and real-time interactivity, virtual reality (VR) provides an encouraging platform to accurately visualize dynamic 3D volumetric data. In this research, we develop a prototype application to track nanomedicines in VR. This platform has the potential to enhance data assessment, comprehension and communication in preclinical research which may ultimately influence the paradigm of future clinical protocols.",data visualization; education; interface design; medical imaging; nanotechnology; PET-CT; science; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,EXG wearable human-machine interface for natural multimodal interaction in VR environment,VRST - Virtual Reality Software and Technology,A,"Current assistive technologies are complicated, cumbersome, not portable, and users still need to apply extensive fine motor control to operate the device. Brain-Computer Interfaces (BCIs) could provide an alternative approach to solve these problems. However, the current BCIs have low classification accuracy and require tedious human-learning procedures. The use of complicated Electroencephalogram (EEG) caps, where many electrodes must be attached on the user's head to identify imaginary motor commands, brings a lot of inconvenience. In this demonstration, we will showcase EXGbuds, a compact, non-obtrusive, and comfortable wearable device with non-invasive biosensing technology. People can comfortably wear it for long hours without tiring. Under our developed machine learning algorithms, we can identify various eye movements and facial expressions with over 95% accuracy, such that people with motor disabilities could have a fun time to play VR games totally ""Hands-free"".",human-machine interface; machine learning; physiological signal processing; virtual reality; wearable device,Keywords,TRUE,
Scopus,conferencePaper,2018,Future-mine VR as narrative decision making tool,VRST - Virtual Reality Software and Technology,A,"This work presents a narrative story of a Future Mine scenario that uses Virtual Reality as a medium to replace traditional spreadsheet-based policy making framework currently widely used in government agencies for decision making process. The scenario presented envisions user exploring underground mine, where extraction processes had been almost fully automated, and environment is constantly monitored by a variety of modern and futuristic sensors. The use of story-telling using VR is explored to present novel application scenarios for sensing technologies and to facilitate better understanding of the context in which they will be used. Further the experience is translated into informed decision making.",informed decision making; interactive design; storytelling; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,GravityCup: a liquid-based haptics for simulating dynamic weight in virtual reality,VRST - Virtual Reality Software and Technology,A,"During interaction in a virtual environment, haptic displays provide users with sensations such as vibration, texture simulation, and electrical muscle stimulation. However, as humans perceive object weights naturally in daily life, objects picked up in virtual reality feel unrealistically light. To create an immersive experience in virtual reality that includes weight sensation, we propose GravityCup, a liquid-based haptic feedback device that simulates realistic object weights and inertia when moving virtual handheld objects. In different scenarios, GravityCup uses liquid to provide users with a dynamic weight sensation experience that enhances interaction with handheld objects in virtual reality.",haptics; liquid-based; virtual reality; weight simulation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Hand motion prediction for just-in-time thermo-haptic feedback,VRST - Virtual Reality Software and Technology,A,This paper presents two innovative design solutions for thermal feedback displays in virtual environments. First solution is aiming to eliminate or decrease the time delay between the user action and onset of the thermal feedback using Machine Learning for user motion prediction. Second is the design of compact but efficient water cooling system necessary to provide cold sensations using peltier elements. Presented thermal display is wearable and battery powered.,haptic feedback; machine learning; motion prediction; neural networks; thermal feedback; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2018,Indoor AR navigation using tilesets,VRST - Virtual Reality Software and Technology,A,"This paper demonstrates the methodology and findings of creating an augmented reality navigation app that uses tilesets to create the navigation. It illustrates the method in which the app was created - using vector data and uploading it to MapBox, then accessing that data in Unity through the MapBox API and map editor and then overlaying the camera input with the navigation path layer. The application was tested by creating multiple arbitrary navigation scenarios and checking them for various factors. The main finding of this research is that this navigation solution works better than GPS indoor navigation.",augmented reality; indoor navigation; tilesets,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Interactive virtual exhibition: creating custom virtual art galleries using web technologies,VRST - Virtual Reality Software and Technology,A,"This paper presents an immersive 3D virtual reality application accessed through the web that allows users to create their own custom virtual art galleries. The application allows users to select paintings based on a time range or country and then it dynamically generates the 3D virtual exhibit. Various features about the exhibit can be customized, such as the floor texture and wall color. Users can also save their exhibit, so it can be shared with others.",art gallery; immersive world; user interface; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Tap-tap menu: body touching for virtual interactive menus,VRST - Virtual Reality Software and Technology,A,"Virtual and mixed realities make it possible to view and interact with virtual objects in 3D space. However, where to position menus in 3D space and how to interact with them are often problems. Existing studies developed methods of displaying a menu on the hand or arm. In this study, we proposed a menu system that appears at various body parts. By placing the menu on the body, it enables the user to operate the menus comfortably through kinesthesia, and perceive tactile feedback. Furthermore, displaying the menu not only in the hands and arms but also in the upper legs and the abdomen, the menu display area can be expanded. In this study, we developed a modeling application and introduced a proposed menu design for that application.",gestural input; head-mounted display; menu; mixed reality; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2018,TransFork: using olfactory device for augmented tasting experience with video see-through head-mounted display,VRST - Virtual Reality Software and Technology,A,"When people eat, the taste is very complex and be influenced easily by other senses. Such as visual, olfactory, and haptic, even past experiences, can affect the human perception, which in turn creates more taste possibilities. We present TransFork, an eating tool with olfactory feedback, which augments the tasting experience with video see-through head-mounted display. Additionally, we design a recipe via preliminary experiments to find out the taste conversion formula, which could enhance the flavor of foods and change the user perception to recognize the food. In this demonstration, we prepare a mini feast with bite-sized fruit, the participants use the TransFork to eat food A and smell the scent of food B stored at the aromatic box via airflow guiding. Before they deliver the food to their mouth, the head-mounted display augmented the color of food B on food A by the QR code on the aromatic box. With this augmented reality techniques and the recipe, the tasting experience could be augmented or enhanced, which is a potential approach and could be a playful used for eating.",augmented reality; augmented tasting experience; olfactory; video see-through head-mounted display,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Using mixed reality for promoting brand perception,VRST - Virtual Reality Software and Technology,A,"Mixed reality offers an immersive and interactive experience through the use of head mounted displays and in-air gestures. Visitors can discover additional content virtually, on top of existing physical items. For a small-scale exhibition at a cafe, we developed a Microsoft HoloLens application to create an interactive experience on top of a collection of historic physical items. Through public experiences of this exhibition, we received positive feedback of our system, and found that it also helped to promote brand perception. In this demo, visitors can experience a similar mixed reality experience that was shown at the exhibition.",brand perception; exhibition; mixed reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Virtual reality environment to support activity in the real world: a case of working environment using microscope,VRST - Virtual Reality Software and Technology,A,"This manuscript introduces a virtual reality (VR) environment to support research activity in the real world. We constructed a prototype to support intellectual activity in the field of life sciences using VR. In the prototype, the users can operate a real microscope from a virtual space, along with other useful equipment such as huge displays, and analyze images carefully and intuitively using a immersive visualizer seamlessly integrated in the environment. We belive that our prototype is promising for expanding the potential of VR applications.",life science; microscope; virtual laboratory; virtual reality; working environment,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,VirtualHaus: a collaborative mixed reality application with tangible interface,VRST - Virtual Reality Software and Technology,A,"We present VirtualHaus, a collaborative mixed reality application allowing two participants to recreate Mozart's apartment as it used to be by interactively placing furniture. Each participant has a different role and therefore uses a different application: the visitor uses an immersive virtual reality application, while the supervisor uses an augmented reality application. The two applications are wirelessly synchronised and display the same information with distinct viewpoints and tools.",collaborative applications; cultural heritage; mixed reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Visualizing and exploring OSGi-based software architectures in augmented reality,VRST - Virtual Reality Software and Technology,A,"This demo presents an immersive augmented reality solution for visualizing OSGi-based software architectures. By employing an island metaphor, we map abstract software entities to tangible real-world objects. Using advanced input modalities, such as voice and gesture control, our approach allows for interactive exploration and examination of complex software systems.",augmented reality; real-world metaphor; software visualization; user interface design,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,VRTe do: the way of the virtual hand,VRST - Virtual Reality Software and Technology,A,"We are presenting a Virtual Reality training system for Karate kata based on motion capture and Virtual Reality technologies. The system is built as a game, in which the player needs to learn and repeat different kata to progress and reach the next level. Different levels are represented by obi (belts) of different color, corresponding real Karate obi. We capture players' motion with a Kinect camera and enable interaction with game objects. A database is integrated in the game so that different players can use, save and track their training progress.",interactivity; karate; kinect; learning; sport; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,3D model augmentation using depth information in an AR environment,VRST - Virtual Reality Software and Technology,A,"This paper proposes a method for augmenting a 3D CAD model onto an image using a mobile device. An image and its depth map of a target are obtained using a Phab 2 mobile device. The image is processed to extract line segments of the target. Next a rectangular planar region of the target is selected, which is then refined using the depth data. The chosen region is then compared with the CAD model, and a planar face in the CAD model that matches the selected region is obtained using various geometric properties. Using the matching planar faces, a pose of the camera is computed, which is then used for augmenting the CAD model onto the image correctly. The test results demonstrate that the method can be used for real fabrication in a complex environment.",augmented reality(AR); CAD model augmentation; camera pose estimation,Keywords,TRUE,
Scopus,conferencePaper,2018,An AR system for artistic creativity education,VRST - Virtual Reality Software and Technology,A,"Creativity and innovation training is the core of the art education. Modern technology provides more effective tools to help students obtain artistic creativity. In this paper, we propose to employ augmented reality technology to assist artistic creativity education. We first analyze the inefficiency of traditional artistic creation training. We then introduce our AR-based smartphone app with technical detail and explain how it can improve accelerate artistic creativity training. We finally show 3 examples created by our AR app to demonstrate the effectiveness of our proposed method.",artistic creativity education; augmented reality; interaction,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,An evaluation of smartphone-based interaction in AR for constrained object manipulation,VRST - Virtual Reality Software and Technology,A,"In Augmented Reality, interaction with the environment can be achieved with a number of different approaches. In current systems, the most common are hand and gesture inputs. However experimental applications also integrated smartphones as intuitive interaction devices and demonstrated great potential for different tasks. One particular task is constrained object manipulation, for which we conducted a user study. In it we compared standard gesture-based approaches with a touch-based interaction via smartphone. We found that a touch-based interface is significantly more efficient, although gestures are being subjectively more accepted. From these results we draw conclusions on how smartphones can be used to realize modern interfaces in AR.",augmented reality; smartphone; study; user interface design,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Analysis of the R-V dynamics illusion behavior in terms of auditory stimulation,VRST - Virtual Reality Software and Technology,A,"The R-V Dynamics illusion is a phenomenon where weight perception is changed by superimposing a CG case with a movable portion (CG) onto a real object using mixed reality technology. In previous studies, it has been confirmed that weight perception is affected by the size/volume of the CG, and a virtual collision sound between the case and the movable portion could also be a cause of this illusionary phenomenon. However, in previous studies, only one virtual collision sound is applied. Therefore, in this study, we consider the influence of the physical characteristics of virtual collision sound such as the size and weight of the movable object in the phenomenon. As a result, it was confirmed that the weight perception changes according to the virtual collision sound, and participants lightly perceived the real object when a virtual collision sound is played with a smaller and lighter object.",mixed reality; sense of weight; visual stimulation,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,AR navigation solution using vector tiles,VRST - Virtual Reality Software and Technology,A,"This study discusses the results and findings of an augmented reality navigation app that was created using vector data uploaded to an online mapping software for indoor navigation. The main objective of this research is to determine the current issues with a solution of indoor navigation that relies on the use of GPS signals, as these signals are sparse in buildings. The data was uploaded in the form of GeoJSON files to MapBox which relayed the data to the app using an API in the form of Tilesets. The application converted the tilesets to a miniaturized map and calculated the navigation path, and then overlaid that navigation line onto the floor via the camera.Once the project setup was completed, multiple navigation paths have been tested numerous times between the different sync points and destination rooms. At the end, their accuracy, ease of access and several other factors, along with their issues, were recorded. The testing revealed that the navigation system was not only accurate despite the lack of GPS signal, but it also detected the device motion precisely. Furthermore, the navigation system did not take much time to generate the navigation path, as the app processed the data tile by tile. The application was also able to accurately measure the ground plane along with the walls, perfectly overlaying the navigation line. However, a few observations indicated various factors affected the accuracy of the navigation, and testing revealed areas where major improvements can be made to improve both accuracy and ease of access.",augmented reality; indoor navigation; tilesets,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Automatic 3D modeling of artwork and visualizing audio in an augmented reality environment,VRST - Virtual Reality Software and Technology,A,"In recent years, traditional art museums have begun to use AR/VR technology to make visits more engaging and interactive. This paper details an application which provides features designed to be immediately engaging and educational to museum visitors within an AR view. The application superimposes an automatically generated 3D representation over a scanned artwork, along with the work's authorship, title, and date of creation. A GUI allows the user to exaggerate or decrease the depth scale of the 3D representation, as well as to search for related works of music. Given this music as audio input, the generated 3D model will act as an audio visualizer by changing depth scale based on input frequency.",3D model generation; art museum; audio visualization; augmented reality; education; music,Title_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2018,Being them: presence of using non-human avatars in immersive virtual environment,VRST - Virtual Reality Software and Technology,A,"This work examines the differences of the effects between using humanoid and non-humanoid avatars on the user's Illusion of Virtual Body Ownership (IVBO) and experience. We used three kinds of avatars: bipedalism group (human), quadrupedalism group (wolf), and serpentine motion group (snake). The result shows that using non-humanoid avatars feel more sense of change of their body. Users feel more proficient when using the humanoid avatar, but are more pleased with the non-humanoid avatars.",non-humanoid avatar; virtual body ownership; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2018,BoatAR: a multi-user augmented-reality platform for boat,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) allows virtual object projection with an unblocked view of the physical world which provides reference and other people. The mixed scene provides an agile platform for communication and collaboration, especially on a product that would be difficult or expensive to present otherwise. In the boating industry, high customization leaves dealers with a high cost on inventory, financially and spatially. In this work, we present BoatAR, a multi-user AR boat configuration system designed for addressing these issues. A prototype system was implemented using HoloLens with shared experience, and demonstrated to a group of boat dealers and received positive feedback. BoatAR provided an example of how a multi-user AR system could help in the conventional industry.",augmented reality; collaboration,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Designing dynamic aware interiors,VRST - Virtual Reality Software and Technology,A,"We are pursuing a vision of reactive interior spaces that are aware of people's actions and transform according to changing needs. We envision furniture and walls that act as interactive displays and that shapeshift to the correct physical form, and the appropriate interactive visual content and modality. This paper briefly describes our proposal based on our recent efforts on realizing this vision.",3D user interface; augmented reality; human-computer interactions; interactive tabletop and surfaces; robot; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2018,Does automatic game difficulty level adjustment improve acrophobia therapy? differences from baseline.,VRST - Virtual Reality Software and Technology,A,"This paper presents the design and development of a Virtual Reality game for treating acrophobia, as well as a comparative study between the players' performance in the game, under two different conditions - one in which the difficulty levels are adjusted according to the subjects' biophysical data and one in which they are not. The results showed an improvement of the parameters correlated with fear level in the first experiment.",acrophobia; deep learning; fear estimation; game level prediction; gamification; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Dual-MR: interaction with mixed reality using smartphones,VRST - Virtual Reality Software and Technology,A,"Mixed reality (MR) has changed the perspective we see and interact with our world. While the current-generation of MR head-mounted devices (HMDs) are capable of generating high quality visual contents, interation in most MR applications typically relies on in-air hand gestures, gaze, or voice. These interfaces although are intuitive to learn, may easily lead to inaccurate operations due to fatigue or constrained by the environment. In this work, we present Dual-MR, a novel MR interation system that i) synchronizes the MR viewpoints of HMD and handheld smartphone, and ii) enables precise, tactile, immersive and user-friendly object-level manipulations throught the multi-touch input of smartphone. In addition, Dual-MR allows multiple users to join the same MR coordinate system to facilite the collaborate in the same physical space, which further broadens its usability. A preliminary user study shows that our system easily overwhelms the conventional interface, which combines in-air hand gesture and gaze, in the completion time for a series of 3D object manipulation tasks in MR.",3D manipulation interface; human computer interaction; mixed reality; multi-touch input; smartphone,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Effect of accompanying onomatopoeia to interaction sound for altering user perception in virtual reality,VRST - Virtual Reality Software and Technology,A,"Onomatopoeia refers to a word that phonetically imitates, resembles the sound, or depict an event at hand. In languages like Korean and Japanese, it is used in everyday conversation to emphasize certain situation and enrich the prose. In this poster, we explore if the use of onomatopoeia, visualized and added to the usual sound feedback, could be taken advantage to increase or alter the perceived realism of the sound feedback itself, and furthermore of the situation at hand in virtual reality. A pilot experiment was run to compare the user's subjective perceived realism and experience under four test conditions of presenting a simple physical interaction, accompanying it with: (1) just the ""as-is"" sound (baseline), (2) ""as-is"" sound and onomatopoeia, (3) a representative sound sample (e.g. one for all different collision conditions), and (4) a representative sound sample and onomatopoeia. Our pilot study has found that the use of onomatopoeia can alter and add on to the perceived realism/naturalness of the virtual situation such that the experiences of the single representative sound added with the onomatopoeia and ""as-is"" sound were deemed similar.",onomatopoeia; realism; sound feedback; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Effect of accompanying onomatopoeia with sound feedback toward presence and user experience in virtual reality,VRST - Virtual Reality Software and Technology,A,"Onomatopoeia refers to a word that phonetically imitates the sound. It is often used, in comics or video, in caption as a way to dramatize, emphasize, exaggerate and draw attention the situation. In this paper we explore if the use of onomatopoeia could also bring about similar effects and improve the user experience in virtual reality. We present an experiment comparing the user's subjective experiences and attentive performance in two virtual worlds, each configured in two test conditions: (1) sound feedback with no onomatopoeia and (2) sound feedback with it. Our experiment has found that the moderate and strategic use of onomatopoeia can indeed help direct user attention, offer object affordance and thereby enhance user experience and even the sense of presence and immersion.",onomatopoeia; sounds visualization; user experience; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Effects of head-display lag on presence in the oculus rift,VRST - Virtual Reality Software and Technology,A,"We measured presence and perceived scene stability in a virtual environment viewed with different head-to-display lag (i.e., system lag) on the Oculus Rift (CV1). System lag was added on top of the measured benchmark system latency (22.3 ms) for our visual scene rendered in OpenGL Shading Language (GLSL). Participants made active head oscillations in pitch at 1.0Hz while viewing displays. We found that perceived scene instability increased and presence decreased when increasing system lag, which we attribute to the effect of multisensory visual-vestibular interactions on the interpretation of the visual information presented.",oculus rift; perception; presence; virtual reality; VR,Keywords,TRUE,
Scopus,conferencePaper,2018,Evaluating ray casting and two gaze-based pointing techniques for object selection in virtual reality,VRST - Virtual Reality Software and Technology,A,"Selecting an object is a basic interaction task in virtual reality (VR) environments. Interaction techniques with gaze pointing have potential for this elementary task. There appears to be little empirical evidence concerning the benefits and drawbacks of these methods in VR. We ran an experiment studying three interaction techniques: ray casting, dwell time and gaze trigger, where gaze trigger was a combination of gaze pointing and controller selection. We studied user experience and interaction speed in a simple object selection task. The results indicated that ray casting outperforms both gaze-based methods while gaze trigger performs better than dwell time.",controller pointing; gaze pointing; object selection in virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,EXController: enhancing interaction capability for VR handheld controllers using real-time vision sensing,VRST - Virtual Reality Software and Technology,A,"This paper presents EXController, a new controller-mounted finger posture recognition device specially designed for VR handheld controllers. We seek to provide additional input through real-time vision sensing by attaching a near infrared (NIR) camera onto the controller. We designed and implemented an exploratory prototype with a HTC Vive controller. The NIR camera is modified from a traditional webcam and applied with a data-driven Convolutional Neural Network (CNN) classifier. We designed 12 different finger gestures and trained the CNN classifier with a dataset from 20 subjects, achieving an average accuracy of 86.17% across - subjects, and, approximately more than 92% on three of the finger postures, and more than 89% on the top-4 accuracy postures. We also developed a Unity demo that shows matched finger animations, running at approximately 27 fps in real-time.",gesture recognition; handheld controller; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2018,Experience the dougong construction in virtual reality,VRST - Virtual Reality Software and Technology,A,"Dougong is a unique culture in Chinese traditional architecture. In University, the Architectural students usually use video, pictures, and even handmade craft to learn the knowledge and culture about Dougong. However, making these complicated Dougong components by hands requires a lot of facilities. To solve these problems, this paper builds a learning application using Virtual Reality (VR) technology, where students can master how to construct Dougong by interacting with the virtual models. In addition to learning module, the application creates a simulated scene showing students the great charm and design ideas of ancient Chinese buildings. The comparison experiments indicate that the students learning via VR-based application identify more Dougong components and their placement than those learning via conventional teaching.",architecture education; experiential teaching; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Extending recreational environments with a landscape-superimposed display using mixed reality,VRST - Virtual Reality Software and Technology,A,"Herein, we describe a system that extends recreational experiences by overlaying a virtual landscape of a remote place over the currently experienced real landscape using mixed reality (MR) technology and displaying avatars of other users. There are many recreational activities that can be performed outdoors. However, such activities usually involve some traveling costs, preparation time, and require schedule adjustments. To reduce the impact of these factors, we implemented a system that extends recreational environments, thereby allowing free movement through the manipulation of the visual information using MR.",mixed reality; panorama; recreation; virtual human,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Gigapixel virtual reality employing live superzoom cameras,VRST - Virtual Reality Software and Technology,A,"We present a live gigapixel virtual reality system employing a 360° camera, a superzoom camera with a pan-tilt robotic head, and a head-mounted display (HMD). The system is capable of showing on-demand gigapixel-level subregions of 360° videos. Similar systems could be used to have live feed for foveated rendering HMDs.",360° video; foveated rendering; gigapixel; head-mounted display; superzoom; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Hamlet: directing virtual actors in computational live theater,VRST - Virtual Reality Software and Technology,A,"We present ""Hamlet"", a prototype implementation of a virtual reality experience in which a player takes on a role of the theater director. The objective of the experience is to direct Adam, a virtual actor, to deliver the best possible performance of Hamlet's famous ""To be, or not to be"" soliloquy. The player interacts with Adam using voice commands, gestures, and body motion. Adam responds to acting directions, offers his own interpretations of the soliloquy, acquires the choreography from the player's body motion, and learns the scene blocking by following the player's pointing gestures.",immersion; virtual performance; virtual reality; virtual theater,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Hands-free vibrotactile feedback for object selection tasks in virtual reality,VRST - Virtual Reality Software and Technology,A,"Interactions between humans and virtual environments rely on timely and consistent sensory feedback, including haptic feedback. However, many questions remain open concerning the spatial location of haptics on the user's body in VR. We studied how simple vibrotactile collision feedback on two less studied locations, the temples, and the wrist, affects an object picking task in a VR environment. We compared visual feedback to three visual-haptic conditions, providing haptic feedback on the participants' (N=16) wrists, temples or simultaneously on both locations. The results indicate that for continuous, hand-based object selection, the wrist is a more promising feedback location than the temples. Further, even a suboptimal feedback location may be better than no haptic collision feedback at all.",collision detection; haptic feedback; object selection in virtual reality; visual feedback,Title_Keywords,TRUE,
Scopus,conferencePaper,2018,Illumination for 360 degree cameras,VRST - Virtual Reality Software and Technology,A,"Additional illumination improves the capture of omnidirectional 360° video and images, especially for dark or high-contrast environments. There is no ""behind"" for 360° cameras, so the placement of lights is a problem. We explore ways to position lights on some 360° cameras, and propose two good locations.",360° cameras; cinematic virtual reality; head-mounted display; illumination; surround video; visualization,Keywords,TRUE,
Scopus,conferencePaper,2018,Image compensation and stabilization for immersive 360-degree videos from capsule endoscopy,VRST - Virtual Reality Software and Technology,A,"This paper describes image processing that can be used to develop immersive 360-degree videos using capsule endoscopy procedures. When viewed through a head-mounted display (HMD), doctors are able to inspect the human gastrointestinal tract as if they were inside the patient's body. Although the endoscopy capsule has two tiny fisheye cameras, the images captured by these cameras cannot be converted to equirectangular images which is the basic format used to produce 360-degree videos. This study proposes a method to generate a pseudo-omnidirectional video from the original images and stabilizes the video to prevent virtual reality (VR) sickness.",capsule endoscopy; omnidirectional video; video stabilization,Abstract,TRUE,
Scopus,conferencePaper,2018,Interactive virtual exhibition: creating custom virtual art galleries using web technologies,VRST - Virtual Reality Software and Technology,A,"This paper presents an immersive 3D virtual reality application accessed through the web that allows users to create their own custom virtual art galleries. The application allows users to select paintings based on a time range or country and then it dynamically generates the 3D virtual exhibit. Various features about the exhibit can be customized, such as the floor texture and wall color. Users can also save their exhibit, so it can be shared with others.",art gallery; immersive world; user interface; virtual reality,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2018,Low-cost VR collaborative system equipped with haptic feedback,VRST - Virtual Reality Software and Technology,A,"In this paper, we present a low-cost virtual reality (VR) collaborative system equipped with a haptic feedback sensation system. This system is composed of a Kinect sensor for bodies and gestures detection, a microcontroller and vibrators to simulate outside interactions, and smartphone powered cardboard, all of this are put into a network implemented with Unity 3D game engine.",collaborative virtual reality; haptic feedback system,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Measuring physical exertion in virtual reality exercise games,VRST - Virtual Reality Software and Technology,A,"We demonstrate a novel method of applying the capabilities of mobile virtual reality technology to the health sciences by measuring physical exertion in a VR exercise game. By measuring changes in heart rate in a thirteen person user study, we find evidence to suggest that virtual reality exercise is able to induce a moderate to high level of physical exertion and produce an immersive an intriguing experience.",exercise games; human-computer interaction; virtual reality; walking-in-place,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Multi-view augmented reality with a drone,VRST - Virtual Reality Software and Technology,A,"This paper presents some early results from an exploration into Augmented Reality (AR) applications where users have access to controllable alternative viewing positions based on a camera mounted unmanned aerial vehicle (UAV). These results include a system specification that defines and identifies the requirements of multi-view AR; and a demo application where the user can switch between the traditional first person and third person view. While being an initial step in the investigation, the results do illustrate practical applications for multi-view AR functionality. The paper concludes with a discussion on the next steps for the investigation.",augmented reality; camera views; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Perceptual model optimized efficient foveated rendering,VRST - Virtual Reality Software and Technology,A,"Higher resolution, wider FOV and increasing frame rate of HMD are demanding more VR computing resources. Foveated rendering is a key solution to these challenges. This paper introduces a perceptual model optimized foveated rendering. Tessellation levels and culling areas are adaptively adjusted based on visual sensitivity. We improve rendering performance while satisfying visual perception.",computer graphics; perceptual model; rendering; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2018,PeriTextAR: utilizing peripheral vision for reading text on augmented reality smart glasses,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) provides real-time information by superimposing virtual information onto users' view of the real world. Our work is the first to explore how peripheral vision, instead of central vision, can be used to read text on AR and smart glasses. We present PeriTextAR, a multiword reading interface using rapid serial visual presentation (RSVP)[5]. This enables users to observe the real world using central vision, while using peripheral vision to read virtual information. We first conducted a lab-based study to determine the effect of different text transformation by comparing reading efficiency among 3 capitalization schemes, 2 font faces, 2 text animation methods, and 3 different numbers of words for RSVP paradigm. Another lab-based study followed, investigating the performance of the PeriTextAR against control text, and the results showed significant better performance.",augmented reality; mobile; multiword; peripheral vision; rapid serial visual presentation; reading interface,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,plARy: sound augmented reality system using video game background music,VRST - Virtual Reality Software and Technology,A,"The authors of this paper explored the possibility of enhancing reality interpretation by synchronizing real-life situation with videogame soundtrack. ""plARy"" is a music based augmented reality application that immerses users in a world of video games with playing soundtracks, enhancing user's interpretation of the real world. By playing known game music according to the locations of individual users, they will recall the scenes and emotions experienced while playing the game based on users' previous learning. The authors of this paper implemented a system that uses Apple iBeacon for proximity detection and evaluated it through experiment. From participants reviews, many people answered that they felt they had imagined a world of the game, and felt that the background music became associated with locations.",augmented reality; location-based; music; video game,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Realistic simulation of progressive vision diseases in virtual reality,VRST - Virtual Reality Software and Technology,A,"People with a visual impairment perceive their surroundings differently than those with healthy vision. It can be difficult to understand how affected perceive their surroundings, even for themselves. We introduce a virtual reality (VR) platform capable of simulating the effects of common visual impairments. With this system we are able to create a realistic VR representation of actual visual fields obtained from a medical perimeter.",glaucoma; medical devices; ophthalmology; perimetry; virtual reality; vision diseases; vision simulation; visual field,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Real-time human motion forecasting using a RGB camera,VRST - Virtual Reality Software and Technology,A,"We propose a real-time human motion forecasting system which visualize the future pose in virtual reality using a RGB camera. Our system consists of three parts: 2D pose estimation from RGB frames using a residual neural network, 2D pose forecasting using a recurrent neural network, and 3D recovery from the predicted 2D pose using a residual linear network. To improve the prediction learning quantity of temporal feature, we propose a special method using lattice optical flow for the joints movement estimation. After fitting the skeleton, a predicted 3d model of target human will be built 0.5s in advance in a 30-fps video.",deep neural network; motion forecasting; real-time pose prediction,Abstract,TRUE,
Scopus,conferencePaper,2018,Resolving occlusion for 3D object manipulation with hands in mixed reality,VRST - Virtual Reality Software and Technology,A,"Due to the need to interact with virtual objects, the hand-object interaction has become an important element in mixed reality (MR) applications. In this paper, we propose a novel approach to handle the occlusion of augmented 3D object manipulation with hands by exploiting the nature of hand poses combined with tracking-based and model-based methods, to achieve a complete mixed reality experience without necessities of heavy computations, complex manual segmentation processes or wearing special gloves. The experimental results show a frame rate faster than real-time and a great accuracy of rendered virtual appearances, and a user study verifies a more immersive experience compared to past approaches. We believe that the proposed method can improve a wide range of mixed reality applications that involve hand-object interactions.",hand tracking; mixed reality; occlusion,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,System of delivering virtual object to user in remote place by handing gestures,VRST - Virtual Reality Software and Technology,A,"In order to communicate with a person in a remote place, there are many means such as sending sentences, making a phone call, chatting by video. A contact system with a distant person becomes a communication tool through an avatar by a virtual reality system, and we feel that there is a barrier to reality. So, we build a system to deliver virtual objects to a user in remote place by behaving as if handing the objects. Remote and present space views are projected on a wall using video chat, and each virtual object is handed over by using an Augmented Reality (AR) marker. The system promotes communication by feeling the connection of the space in a remote place.",augmented reality; gesture; video chat,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Texture synthesis for stable planar tracking,VRST - Virtual Reality Software and Technology,A,"We propose a texture synthesis method to enhance the trackability of a target planar object by embedding natural features into the object in the object design process. To transform an input object into an easy-to-track object in the design process, we extend an inpainting method for naturally embedding the features into the texture. First, a feature-less region in an input object is extracted based on feature distribution based segmentation. Then, the region is filled by using an inpainting method with a feature-rich region searched in an object database. By using context based region search, the inpainted region can be consistent in terms of the object context while improving the feature distribution.",augmented reality; keypoint matching; planar tracking,Keywords,TRUE,
Scopus,conferencePaper,2018,The impact of camera height in cinematic virtual reality,VRST - Virtual Reality Software and Technology,A,"Watching a 360° movie with Head Mounted Displays (HMDs) the viewer feels to be inside the movie and can experience it in an immersive way. The head of the viewer is exactly in the same place as the camera was when the scene was recorded. Viewing a movie by HMDs from the perspective of the camera can raise some challenges, e.g. heights of well-known objects can irritate the viewer in the case the camera height does not correspond to the physical eye height. The aim of this work is to study how the position of the camera influences presence, sickness and the user experience of the viewer. For that we considered several watching postures as well as various camera heights. The results of our experiments suggest that differences between camera and eye heights are more accepted, if the camera position is lower than the viewer's own eye height. Additionally, sitting postures are preferred and can be adapted easier than standing postures. These results can be applied to improve guidelines for 360° filmmakers.",360° movie; camera height; cinematic virtual reality; eye height,Title_Keywords,TRUE,
Scopus,conferencePaper,2018,Towards unobtrusive obstacle detection and notification for VR,VRST - Virtual Reality Software and Technology,A,We present results of a preliminary study on our planned system for the detection of obstacles in the physical environment by means of an RGB-D sensor and their unobtrusive signalling using metaphors within the virtual environment (VE).,3D interaction; collision avoidance; interaction metaphor; notifications; range imaging; RGB-D; virtual reality; walking workspace,Keywords,TRUE,
Scopus,conferencePaper,2018,User-centric classification of virtual reality locomotion,VRST - Virtual Reality Software and Technology,A,"Traveling in a virtual world, while confined in the real world requires a virtual reality locomotion (VRL) method. VRL remains an issue because of three fundamental challenges, sickness, presence, and fatigue. We propose a User-Centric Classification (UCC) of VRL methods based on a method's ability to address these challenges. UCC provides a framework to discuss and compare different VRL methods and to examine performance trade-offs. We designed and implemented a testbed to study several VRL methods, and initial results demonstrated the effectiveness of the UCC framework [1].",fatigue; locomotion; presence; sickness; virtual reality; VR,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,"""Virtual ability simulation"" to boost rehabilitation exercise performance and confidence for people with disability",VRST - Virtual Reality Software and Technology,A,"The purpose of this paper is to investigate a concept called virtual ability simulation (VAS) for people with disability in a virtual reality (VR) environment. In a VAS people with disabilities perform tasks that are made easier in the virtual environment (VE) compared to the real world. We hypothesized that putting people with disabilities in a VAS will increase confidence and enable more efficient task completion than without a VAS. To investigate this hypothesis, we conducted a within-subjects experiment in which participants performed a virtual task called ""kick the ball"" in two different conditions: a no gain condition (i.e., same difficulty as in the real world) and a rotational gain condition (i.e., physically easier than the real world but visually the same). The results from our study suggest that VAS increased participants' confidence which in turn enables them to perceive the difficulty of the same task easier.",ability simulation; cybersickness; head-mounted display; HMD; immersion; presence; virtual ability simulation; virtual reality; VR,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Virtual gaze: exploring use of gaze as rich interaction method with virtual agent in interactive virtual reality content,VRST - Virtual Reality Software and Technology,A,"Nonverbal cues, especially eye gaze, plays an important role in our daily communication, not just as an indicator of interest, but also as a method to convey information to another party. In this work, we propose a simulation of human eye gaze in Virtual Reality content to improve immersion of interaction between user and virtual agent. We developed an eye-tracking integrated interactive narrative content with a focus on player's interaction with gaze aware virtual agent, which is capable of reacting towards the player's gaze to simulate real human-to-human communication in VR environment and conducted an initial study to measure user's reaction.",eye tracking; game; human computer interaction; virtual agent; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Virtual reality interactivity in a museum environment,VRST - Virtual Reality Software and Technology,A,"We present research based off of the needs of museum interaction between users and artwork. Through 360 degree footage, we were able to explore the possibilities of an augmented and virtual environment. Adding user interaction features to enhance the surroundings helped us to achieve the type of immersion that a museum could elicit. Museums are struggling to connect with the younger generation these days, therefore, incorporating virtual reality technology not only adds an exciting element for regular visitors, but entices new visitors as well. Our goal was to find ways to use virtual reality to do exactly this, enhancing and expanding the impact of these provoking spaces.",360 video; art museum; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,Visualization of neural networks in virtual reality using Unreal Engine,VRST - Virtual Reality Software and Technology,A,Many applications today use deep learning to provide intelligent behavior. To understand and explain how deep learning models come to certain decisions can be hard or completely in-transparent. We propose a visualization of convolutional neural networks in Virtual Reality (VR). The interactive application shows the internal processes and allows to inspect the results. Large networks can be visualized in real-time with special rendering techniques.,deep learning; explainable ai; neural networks; visualization,Title_Abstract,TRUE,
Scopus,conferencePaper,2018,Visualization of software components and dependency graphs in virtual reality,VRST - Virtual Reality Software and Technology,A,We present the visualization of component-based software architectures in Virtual Reality (VR) to understand complex software systems. We describe how to get all relevant data for the visualization by data mining on the whole source tree and on source code level. The data is stored in a graph database for further analysis and visualization. The software visualization uses an island metaphor. Storing the data in a graph database allows to easily query for different aspects of the software architecture.,3D visualization; graph database; OSGi; real-world metaphor; software architecture; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2018,VR sickness measurement with EEG using DNN algorithm,VRST - Virtual Reality Software and Technology,A,"Recently, VR technology is rapidly developing and attracting public attention. However, VR Sickness is a problem that is still not solved in the VR experience. The VR sickness is presumed to be caused by crosstalk between sensory and cognitive systems [1]. However, since there is no objective way to measure sensory and cognitive systems, it is difficult to measure VR sickness. In this paper, we collect EEG data while participants experience VR videos. We propose a Deep Neural Network (DNN) deep learning algorithm by measuring VR sickness through electroencephalogram (EEG) data. Experiments have been conducted to search for an appropriate EEG data preprocessing method and DNN structure suitable for the deep learning, and the accuracy of 99.12% is obtained in our study.",deep learning; EEG; virtual reality; VR sickness,Keywords,TRUE,
Scopus,conferencePaper,2019,Out-of-body Locomotion: Vectionless Navigation with a Continuous Avatar Representation,VRST - Virtual Reality Software and Technology,A,"Teleportation is a popular and low risk means of navigating in VR. Because teleportation discontinuously translates the user’s viewpoint, no optical flow is generated that could lead to vection-induced VR sickness. However, instant viewpoint translations and resulting discontinuous avatar representation is not only detrimental to presence and spatial awareness but also presents a challenge for gameplay design–particularly for multiplayer games. We compare out-of-body locomotion, a hybrid viewpoint technique that lets users seamlessly switch between a first-person and third-person avatar view, to traditional pointer-based teleportation. While in third-person, if the user doesn’t move, the camera remains stationary to avoid any optical flow generation. Third-person also lets users precisely and continuously navigate their avatar without risk of getting VR sick. The viewpoint automatically switches back to first-person as soon the users breaks line of sight with their avatar or the user requests to rejoin the avatar with a button press. A user study compares out-of-body locomotion to teleportation with participants (n=22) traversing an obstacle course. Results show that out-of-body locomotion requires significantly fewer (67%) viewpoint transitions than teleportation while there was no significant difference in performance. In addition to being able to offer a continuous avatar representation, participants also deemed out-of-body locomotion to be faster.",Locomotion; Teleportation.; Virtual Reality; VR sickness,Keywords,TRUE,
Scopus,conferencePaper,2019,Obstacle Detection and Alert System for Smartphone AR Users,VRST - Virtual Reality Software and Technology,A,"This paper presents an obstacle detection and alert system for the pedestrians who use smartphone AR applications. The system analyzes the input camera image to extract feature points and determines whether the feature points come from obstacles ahead in the path. With the obstacle detector, two experiments were made. The first investigated the obstacle alert interfaces, and the second investigated the orientation guide interfaces that instruct users to hold their smartphones with some angles/orientations appropriate to capture the environment. Then, the best interfaces identified from the experiments were integrated and tested to examine their usability and user experiences.",alert interface; augmented reality; pedestrian safety,Keywords,TRUE,
Scopus,conferencePaper,2019,Sensitivity to Rate of Change in Gains Applied by Redirected Walking,VRST - Virtual Reality Software and Technology,A,"Redirected walking allows for natural locomotion in virtual environments that are larger than a user’s physical environment. The mapping between real and virtual motion is modified by scaling some aspect of motion. As a user traverses the virtual environment these modifications (or gains) must be dynamically adjusted to prevent collision with physical obstacles. A significant body of work has established perceptual thresholds on rates of absolute gain, but the effect of changing gain is little understood. We present the results of a user study on the effects of rate of gain change. A psychophysical experiment was conducted with 21 participants. Each participant completed a series of two-alternative forced choice tasks in which they determined whether their virtual motion differed from their physical motion while experiencing one of three different methods of gain change: sudden gain change, slow gain change and constant gain. Gain thresholds were determined by 3 interleaved 2-up 1-down staircases, one per condition. Our results indicate that slow gain change is significantly harder to detect than sudden gain change.",head-mounted display; redirected walking; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2019,Lower body control of a semi-autonomous avatar in Virtual Reality: Balance and Locomotion of a 3D Bipedal Model,VRST - Virtual Reality Software and Technology,A,"Animated virtual humans may rely on full-body tracking system to reproduce user motions. In this paper, we reduce tracking to the upper-body and reconstruct the lower body to follow autonomously its upper counterpart. Doing so reduces the number of sensors required, making the application of virtual humans simpler and cheaper. It also enable deployment in cluttered scenes where the lower body is often hidden. The contribution here is the inversion of the well-known capture problem for bipedal walking. It determines footsteps rather than center-of-mass motions and yet can be solved with an off-the-shelf capture problem solver. The quality of our method is assessed in real-time tracking experiments on a wide variety of movements.",Humanoid Locomotion; Motion Capture; Virtual Reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2019,Technologies for Social Augmentations in User-Embodied Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Technologies for Virtual, Mixed, and Augmented Reality (VR, MR, and AR) allow to artificially augment social interactions and thus to go beyond what is possible in real life. Motivations for the use of social augmentations are manifold, for example, to synthesize behavior when sensory input is missing, to provide additional affordances in shared environments, or to support inclusion and training of individuals with social communication disorders. We review and categorize augmentation approaches and propose a software architecture based on four data layers. Three components further handle the status analysis, the modification, and the blending of behaviors. We present a prototype (injectX) that supports behavior tracking (body motion, eye gaze, and facial expressions from the lower face), status analysis, decision-making, augmentation, and behavior blending in immersive interactions. Along with a critical reflection, we consider further technical and ethical aspects.",artificial intelligence; augmented social interaction; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Avatar Type Affects Performance of Cognitive Tasks in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Current consumer virtual reality applications typically represent the user by an avatar comprising a simple head/torso and decoupled hands. In the prior work of Steed et al. it was shown that the presence or absence of an avatar could have a significant impact on the cognitive load of the user. We extend that work in two ways. First they only used a full-body avatar with articulated arms, so we add a condition with hands-only representation similar to the majority of current consumer applications. Second we provide a real-world benchmark so as to start to get at the impact of using any immersive system. We validate the prior results: real and full body avatar performance on a memory task is significantly better than no avatar. However the hands only condition is not significantly different than either these two extremes. We discuss why this might be, in particular we discuss the potential for a individual variation in response to the embodiment level.",Avatar; Cognitive Tasks; Embodiment; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Being More Focused and Engaged in Firefighting Training: Applying User-Centered Design to VR System Development,VRST - Virtual Reality Software and Technology,A,"Although virtual reality (VR) programs to provide firefighting training continue to be developed and adopted, our investigation with 15 firefighters indicates that a current VR training system tends to convey behavioral tips and does not sufficiently reflect actual firefighters’ needs and realities in the field. It often provides somewhat simplified fire simulations and actually lowers the effectiveness of the training. In this paper, we employ Human-Computer Interaction (HCI) methods to examine and identify core elements in firefighting scenarios and develop a VR system that incorporates such elements. We evaluate our system with respect to presence and three design components of the VR simulation (i.e., reality, meaning, play) through a user study with 22 participants. Our study results demonstrate greater user experience and perception toward the four elements in firefighting training with our VR system compared to the existing one. We discuss design implications (e.g., move control, degree of freedom, sight hindrance by smoke, unexpected events) of our study that are expected to help implement and provide an effective VR training system for firefighters.",Firefighting training system; Presence; Triadic game design; User study; User-centered design; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Fearing Disengagement from the Real World,VRST - Virtual Reality Software and Technology,A,"With the adoption of mobile head mounted displays (HMDs) amongst non-experts outside of lab settings, it becomes increasingly important to understand what factors influence a holistic mobile virtual reality (MVR) user experience. We present the results of a field study (N=34), in which we used three methods - a drawing task, a storytelling exercise, and the technology acceptance questionnaire (TAM) - to explore factors, beyond technical capability, that influence the user experience of HMDs. Our analysis (1) highlights factors that designers and researchers can adopt to create and evaluate socially acceptable MVR systems for non-expert users outside a lab context, and (2) puts these factors in context with existing research from industry and academia.",Field study; Mobile Virtual Reality; Qualitative study,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Immersive Insights: A Hybrid Analytics System forCollaborative Exploratory Data Analysis,VRST - Virtual Reality Software and Technology,A,"In the past few years, augmented reality (AR) and virtual reality (VR) technologies have experienced terrific improvements in both accessibility and hardware capabilities, encouraging the application of these devices across various domains. While researchers have demonstrated the possible advantages of AR and VR for certain data science tasks, it is still unclear how these technologies would perform in the context of exploratory data analysis (EDA) at large. In particular, we believe it is important to better understand which level of immersion EDA would concretely benefit from, and to quantify the contribution of AR and VR with respect to standard analysis workflows. In this work, we leverage a Dataspace reconfigurable hybrid reality environment to study how data scientists might perform EDA in a co-located, collaborative context. Specifically, we propose the design and implementation of Immersive Insights, a hybrid analytics system combining high-resolution displays, table projections, and augmented reality (AR) visualizations of the data. We conducted a two-part user study with twelve data scientists, in which we evaluated how different levels of data immersion affect the EDA process and compared the performance of Immersive Insights with a state-of-the-art, non-immersive data analysis system.",Augmented Reality; Clustering; Data Visualization; Dataspace; Exploratory Data Analysis; Hybrid Reality; Virtuality Continuum,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Adventures in Hologram Space: Exploring the Design Space of Eye-to-eye Volumetric Telepresence,VRST - Virtual Reality Software and Technology,A,"Modern volumetric projection-based telepresence approaches are capable of providing realistic full-size virtual representations of remote people. Interacting with full-size people may not be desirable due to the spatial constraints of the physical environment, application context, or display technology. However, the miniaturization of remote people is known to create an eye gaze matching problem. Eye-contact is essential to communication as it allows for people to use natural nonverbal cues and improves the sense of “being there”. In this paper we discuss the design space for interacting with volumetric representations of people and present an approach for dynamically manipulating scale, orientation and the position of holograms which guarantees eye-contact. We created a working augmented reality-based prototype and validated it with 14 participants.",Augmented Reality; Eye-to-eye; Holograms; Volumetric Projection,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Exploring the Use of a Robust Depth-sensor-based Avatar Control System and its Effects on Communication Behaviors,VRST - Virtual Reality Software and Technology,A,"To interact as fully-tracked avatars with rich hand gestures in Virtual Reality (VR), we often need to wear a tracking suit or attach extra sensors on our bodies. User experience and performance may be impacted by the cumbersome devices and low fidelity behavior representations, especially in social scenarios where good communication is required. In this paper, we use multiple depth sensors and focus on increasing the behavioral fidelity of a participant’s virtual body representation. To investigate the impact of the depth-sensor-based avatar system (full-body tracking with hand gestures), we compared it against a controller-based avatar system (partial-body tracking with limited hand gestures). We designed a VR interview simulation for a single user to measure the effects on presence, virtual body ownership, workload, usability, and perceived self-performance. Specifically, the interview process was recorded in VR, together with all the verbal and non-verbal cues. Subjects then took a third-person view to evaluate their previous performance. Our results show that the depth-sensor-based avatar control system increased virtual body ownership and also improved the user experience. In addition, users rated their non-verbal behavior performance higher in the full-body depth-sensor-based avatar system.",avatar control; communication behavior; depth sensor; motion capture; tracking; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Investigating the Detection of Bimanual Haptic Retargeting in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Haptic retargeting is a virtual reality (VR) interaction technique enabling virtual objects to be ”remapped” to different haptic proxies by offsetting the user’s virtual hand from their physical hand. While researchers have investigated single-hand retargeting, the effects of bimanual interaction in the context of haptic retargeting have been less explored. In this study, we present an evaluation of perceptual detection rates for bimanual haptic retargeting in VR. We tested 64 combinations of simultaneous left- and right-hand retargeting ranging from − 24° to + 24° offsets and found that bimanual retargeting can be more noticeable to users when the hands are redirected in different directions as opposed to the same direction.",bimanual; Haptic retargeting; virtual reality.; visuo-haptic illusion,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,ElectroCutscenes: Realistic Haptic Feedback in Cutscenes of Virtual Reality Games Using Electric Muscle Stimulation,VRST - Virtual Reality Software and Technology,A,"Cutscenes in Virtual Reality (VR) games enhance story telling by delivering output in the form of visual, auditory, or haptic feedback (e.g., using vibrating handheld controllers). Since they lack interaction in the form of user input, cutscenes would significantly benefit from improved feedback. We introduce the concept and implementation of ElectroCutscenes, where Electric Muscle Stimulation (EMS) is leveraged to elicit physical user movements to different body parts to correspond to those of personal avatars in cutscenes of VR games while the user stays passive. Through a user study (N=22) in which users passively received kinesthetic feedback resulting in involuntarily movements, we show that ElectroCutscenes significantly increases perceived presence and realism compared to controller-based vibrotactile and no haptic feedback. Furthermore, we found preliminary evidence that combining visual and EMS feedback can evoke movements that are not actuated by either of them alone. We discuss how to enhance realism and presence of cutscenes in VR games even when EMS can partially rather than completely actuate the desired body movements.",EMS; Haptic Feedback; Haptics; Head-mounted Displays,Title_Abstract,TRUE,
Scopus,conferencePaper,2019,Virtual Reality Forge: Pattern-Oriented Authoring of Virtual Reality Nuggets,VRST - Virtual Reality Software and Technology,A,"A current educational trend is to divide learning content in relatively small and independent learning units, referred to as learning nuggets. These “bite-sized” nuggets often rely on patterns in order to reuse these patterns within highly diverse curricular structures like lessons, presentations or demos. In this paper, we explore how virtual reality (VR) can be utilized as a medium for learning purposes similar to learning nuggets. We present a nugget-inspired VR system design and dovetail the pattern-oriented nugget concept in relatively small VR systems. We call this authoring approach with VR nuggets forging. Furthermore, we propose a VR authoring system for these VR nuggets – the VR forge. The system design for realizing VR nuggets and the authoring system are presented and implemented in Unity. For an example we utilize a set of basic patterns from the educational domain. In an expert user study, we use the resulting bite-sized VR applications to evaluate four critical aspects concerning VR and nugget-like usage and show that the educational experts accepted the VR nuggets. Within an additional study, we indicate that our authoring system which reflects the simplistic pattern-oriented content creation paradigm of learning nuggets has potential for general laymen authoring of VR application.",Laymen Authoring; Microlearning; Pattern-Based Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,POL360: A Universal Mobile VR Motion Controller using Polarized Light,VRST - Virtual Reality Software and Technology,A,"We introduce POL360: the first universal VR motion controller that leverages the principle of light polarization. POL360 enables a user who holds it and wears a VR headset to see their hand motion in a virtual world via its accurate 6-DOF position tracking. Compared to other techniques for VR positioning, POL360 has several advantages as follows. (1) Mobile compatibility: Neither additional computing resource like a PC/console nor any complicated pre-installation is required in the environment. Only necessary device is a VR headset with an IR LED module as a light source to which a thin-film linear polarizer is attached. (2) On-device computing: Our POL360’s computation for positioning is completed on the microprocessor in the device. Thus, it does not require additional computing resource of a VR headset. (3) Competitive accuracy and update rate: In spite of POL360’s superior mobile compatibility and affordability, POL360 attains competitive performance of accuracy and fast update rates. That is, it achieves the subcentimeter accuracy of positioning and the tracking rate higher than 60 Hz. In this paper, we derive the mathematical formulation of 6-DOF positioning using light polarization for the first time and implement a POL360 prototype that can directly operate with any commercial VR headset systems. In order to demonstrate POL360’s performance and usability, we carry out thorough quantitative evaluation and a user study and develop three game demos as use cases.",light polarization; spatial interaction; Virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2019,HawKEY: Efficient and Versatile Text Entry for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Text entry is still a challenging task in modern Virtual Reality (VR) systems. The lack of efficient text entry methods limits the applications that can be used productively in VR. Previous work has addressed this issue through virtual keyboards or showing the physical keyboard in VR. While physical keyboards afford faster text entry, they usually require a seated user and an instrumented environment. We introduce a new keyboard, worn on a hawker’s tray in front of the user, which affords a compact, simple, flexible, and efficient text entry solution for VR, without restricting physical movement. In our new video condition, we also show the keyboard only when the user is looking down at it. To evaluate our novel solution and to identify good keyboard visualizations, we ran a user study where we asked participants to enter both lowercase sentences as well as complex text while standing. The results show that text entry rates are affected negatively by simplistic keyboard visualization conditions and that our solution affords desktop text entry rates, even when standing.",Text Entry; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,DexController : Designing a VR Controller with Grasp-Recognition for Enriching Natural Game Experience,VRST - Virtual Reality Software and Technology,A,"We present DexController, which is a hand-held controller leveraging grasp as an additional modality for virtual reality (VR) game. The pressure-sensitive surface of DexController was designed to recognize two different grasp-poses (i.e. precision grip and power grip) and detect grasp-force. Based on the results of two feasibility tests, a VR defense game was designed in which players could attack each enemy using the proper weapon with a proper level of force. A within-subject comparative study is conducted with a button-based controller which has the same physical form of DexController. The results indicated that DexController enhanced the perceived naturalness of the controller and game enjoyment, with having acceptable physical demand. This study clarifies the empirical effect of utilizing grasp-recognition on VR game controller to enhance interactivity. Also, we provide insight for the integration of VR game elements with the grasping modality of a controller.",controller; game experience; gaming; natural interaction; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,A Technique for Mixed Reality Remote Collaboration using 360 Panoramas in 3D Reconstructed Scenes,VRST - Virtual Reality Software and Technology,A,"Mixed Reality (MR) remote collaboration provides an enhanced immersive experience where a remote user can provide verbal and nonverbal assistance to a local user to increase the efficiency and performance of the collaboration. This is usually achieved by sharing the local user's environment through live 360 video or a 3D scene, and using visual cues to gesture or point at real objects allowing for better understanding and collaborative task performance. While most of prior work used one of the methods to capture the surrounding environment, there may be situations where users have to choose between using 360 panoramas or 3D scene reconstruction to collaborate, as each have unique benefits and limitations. In this paper we designed a prototype system that combines 360 panoramas into a 3D scene to introduce a novel way for users to interact and collaborate with each other. We evaluated the prototype through a user study which compared the usability and performance of our proposed approach to live 360 video collaborative system, and we found that participants enjoyed using different ways to access the local user's environment although it took them longer time to learn to use our system. We also collected subjective feedback for future improvements and provide directions for future research.",360 Panorama; 3D Scene Reconstruction; Interaction Methods; Mixed Reality; Remote Collaboration; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Depth Perception in Projective Augmented Reality: An Evaluation of Advanced Visualization Techniques,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) is a promising tool to convey useful information at the place where it is needed. However, perceptual issues with augmented reality visualizations affect the estimation of distances and depth and thus can lead to critically wrong assumptions. These issues have been successfully investigated for video see-through modalities. Moreover, advanced visualization methods encoding depth information by displaying additional depth cues were developed. In this work, state-of-the-art visualization concepts were adopted for a projective AR setup. We conducted a user study to assess the concepts’ suitability to convey depth information. Participants were asked to sort virtual cubes by using the provided depth cues. The investigated visualization concepts consisted of conventional Phong shading, a virtual mirror, depth-encoding silhouettes, pseudo-chromadepth rendering and an illustrative visualization using supporting line depth cues. Besides different concepts, we altered between a monoscopic and a stereoscopic display mode to examine the effects of stereopsis. Consistent results across variables show a clear ranking of examined concepts. The supporting lines approach and the pseudo-chromadepth rendering performed best. Stereopsis was shown to provide significant advantages for depth perception, while the current visualization technique had only little effect on investigated measures in this condition. However, similar results were achieved using the supporting lines and the pseudo-chromadepth concepts in a monoscopic setup. Our study showed the suitability of advanced visualization concepts for the rendering of virtual content in projective AR. Specific depth estimation results contribute to the future design and development of applications for these systems.",Depth Perception; Distance Estimation; Projective Augmented Reality; Visualization,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Portable Mid-air Imaging Optical System on Glossy Surface,VRST - Virtual Reality Software and Technology,A,"We propose a portable optical system, PortOn, that displays an upright mid-air image when simply placed on a flat and glossy surface such as a desk or floor. Mid-air imaging is promising for glasses-free mixed reality because the user can see images without wearing a special device. However, there is a limitation in terms of where the conventional mid-air imaging optical systems can be installed. Therefore, we propose a mid-air optical system that solves this limitation. Our contribution is a practical optical design that enables the system to be easily installed. The advantage of our method is that it erases unnecessary light that is produced when mid-air images are displayed and shows beautiful mid-air images clearly when view-angle control and polarization are added to the system. We evaluate whether undesired light is erased by measuring luminance. As a result, the luminance of the undesired light is much lower than that of mid-air images.",glass-free mixed reality; Muller matrix; polarizer,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,The Stroop Room: A Virtual Reality-Enhanced Stroop Test,VRST - Virtual Reality Software and Technology,A,"The Stroop Test is a well known and regularly employed stressor in laboratory research. In contrast to other methods, it is not based on fear of physical harm or social shame. Consequently, it is more likely accepted by a wide population. In our always-on, technology-driven, social-media centered world, large-scale in-field stress research will need adequate experimental tools to explore the increasing prevalence of stress-related diseases without bringing subjects into laboratories. This is why we designed the Stroop Room: A virtual reality-based adaptation of the Stroop Test using elements of the virtual world to extend the demands of the original test and at the same time make it easily accessible. It is open source and can be used and improved by anyone as an in-the-wild, repeatable, laboratory-quality stressor. In this work, the method is presented and an evaluation study described, to demonstrate its effectiveness in provoking cognitive stress. 16 male and 16 female subjects were tested in the Stroop Room while recording the electrocardiogram, electrodermal activity, saliva based cortisol and alpha-amylase, performance metrics and an array of questionnaire-based assessments regarding psychological confounders, stress state and likability of the simulation. Our results show that the Stroop Room increases heart rate on average by 19%, other heart rate variability time-domain parameters (RMSSD, pNN50) decrease by 24%-47%, and its most stress-correlated frequency-parameter (LF/HF) increases by 107%. Skin conductance (SC) level increases by 63% and non-specific SC responses by 135% on average. Salivary cortisol and alpha-amylase concentrations increase significantly in some specific conditions. Compared to related work using the Stroop Test, this is an improvement for some metrics by around 30%-40%. Questionnaire evaluation show a strong engagement of users with the simulation and some aspects of a flow-induction. These findings support the effectiveness of a Stroop Test involving 3-dimensional interactivity and thus the Stroop Room demonstrates how this can be applied in a playful interaction that could be used pervasively.",amylase; cortisol; EDA; heart rate; HRV; psychological stress; stroop test; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Underwater Virtual Reality System for Neutral Buoyancy Training: Development and Evaluation,VRST - Virtual Reality Software and Technology,A,"During terrestrial activities, sensation of pressure on the skin and tension in muscles and joints provides information about how the body is oriented relative to gravity and how the body is moving relative to the surrounding environment. In contrast, in aquatic environments when suspended in a state of neutral buoyancy, the weight of the body and limbs is offloaded, rendering these cues uninformative. It is not yet known how this altered sensory environment impacts virtual reality experiences. To investigate this question, we converted a full-face SCUBA mask into an underwater head-mounted display and developed software to simulate jetpack locomotion outside the International Space Station. Our goal was to emulate conditions experienced by astronauts during training at NASA's Neutral Buoyancy Lab. A user study was conducted to evaluate both sickness and presence when using virtual reality in this altered sensory environment. We observed an increase in nausea related symptoms underwater, but we cannot conclude that this is due to VR use. Other measures of sickness and presence underwater were comparable to measures taken above water. We conclude with suggestions for improved underwater VR systems and improved methods for evaluation of these systems based on our experience.",Head-mounted Display; Presence; Sickness; Simulation; Space; Spacewalk; Training; Underwater; Virtual Reality; Waterproof,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Hitting the Wall: Mid-Air Interaction for Eye-Hand Coordination,VRST - Virtual Reality Software and Technology,A,"Reaction time training systems are used to improve user performance. Until now, such setups use physical 2D flat surfaces, e.g., a 2D touch screen or buttons mounted on a wall. We designed and investigated a mid-air reaction time training system with an immersive virtual reality (VR) headset. 12 participants performed an eye-hand coordination reaction test in three conditions: both in mid-air with or without VR controller as well as with passive haptic feedback through hitting a soft-surface wall. We also altered target and cursor sizes and used a Fitts’ law task to analyze user performance. According to the results, subjects were slower and their throughput was lower when they hit a solid surface to interact with virtual targets. Our results show that Fitts’s model can be applied to these systems to measure and assess participant training.",Fitts’ task; mid-air interaction; performance assessment; reaction test; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Juggling in VR: Advantages of Immersive Virtual Reality in Juggling Learning,VRST - Virtual Reality Software and Technology,A,"In this paper, we follow up on research dealing with motion learning in Virtual Reality (VR). We investigate the impact of VR motion learning on motion performance, motivation for motion learning and willingness to continue with the motion learning. In our research, we used three ball juggling as a subject of learning. We performed a user study with 30 participants. A VR application was used in our study which allows setting up lower gravity and thus slowing down the motion for learning purposes. The results were statistically evaluated and we comment on the positive influence of virtual reality on motivation and possibilities of using VR in the motion learning process.",juggling; motion learning; motor learning; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Investigating a Physical Dial as a Measurement Tool for Cybersickness in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This study explores ways to increase comfort in Virtual Reality by minimizing cybersickness. Cybersickness is related to classical motion sickness and causes unwanted symptoms when using immersive technologies. We developed a dial interface to accurately capture momentary user cybersickness and feed this information back to the user. Using a seated VR roller coaster environment, we found that the dial is significantly positively correlated with post-immersion questionnaires and is a valid tool compared to verbal rating approaches.",cybersickness; human-computer interaction; physical dial; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,"In AI We Trust: Investigating the Relationship between Biosignals, Trust and Cognitive Load in VR",VRST - Virtual Reality Software and Technology,A,"Human trust is a psycho-physiological state that is difficult to measure, yet is becoming increasingly important for the design of human-computer interactions. This paper explores if human trust can be measured using physiological measures when interacting with a computer interface, and how it correlates with cognitive load. In this work, we present a pilot study in Virtual Reality (VR) that uses a multi-sensory approach of Electroencephalography (EEG), galvanic skin response (GSR), and Heart Rate Variability (HRV) to measure trust with a virtual agent and explore the correlation between trust and cognitive load. The goal of this study is twofold; 1) to determine the relationship between biosignals, or physiological signals with trust and cognitive load, and 2) to introduce a pilot study in VR based on cognitive load level to evaluate trust. Even though we could not report any significant main effect or interaction of cognitive load and trust from the physiological signal, we found that in low cognitive load tasks, EEG alpha band power reflects trustworthiness on the agent. Moreover, cognitive load of the user decreases when the agent is accurate regardless of task’s cognitive load. This could be possible because of small sample size, tasks not stressful enough to induce high cognitive load due to lab study and comfortable environment or timestamp synchronisation error due to fusing data from various physiological sensors with different sample rate.",Cognitive Load; Physiological signals; Trust; Virtual Assistant; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Is the Pen Mightier than the Controller? A Comparison of Input Devices for Selection in Virtual and Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Controllers are currently the typical input device for commercial Virtual Reality (VR) systems. Yet, such controllers are not as efficient as other devices, including the mouse. This motivates us to investigate devices that substantially exceed the controller’s performance, for both VR and Augmented Reality (AR) systems. We performed a user study to compare several input devices, including a mouse, controller, and a 3D pen-like device on a VR and AR pointing task. Our results show that the 3D pen significantly outperforms modern VR controllers in all evaluated measures and that it is comparable to the mouse. Participants also liked the 3D pen more than the controller. Finally, we show how 3D pen devices could be integrated into today’s VR and AR systems.",3D pointing; input devices; Virtual and Augmented Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,"Augmented Reality for Children in a Confirmation Task: Time, Fatigue, and Usability",VRST - Virtual Reality Software and Technology,A,"The objective of this paper is to explore three different interaction methods in a confirmation task on a head-mounted Augmented Reality (AR) device with a population of children aged 9-11 years. The three interaction methods we look at are voice recognition, gesture recognition, and controller. We conducted a within-subjects study using a Fitts’ Law confirmation task performed by children with a Microsoft HoloLens. We measured elapsed time during the completion of the tasks. Also, we collected usability and fatigue measures using the System Usability Scale and the OMNI RPE (Ratings of Perceived Exertion) scale. We found significant differences between voice and controller for time, fatigue and usability. We also found significant differences between gesture and controller for time, fatigue and usability. We hope to apply the results of this study to improve augmented reality educational tools for children in the future.",Augmented Reality; Children; Fitts’ Law; Usability Studies,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Smart3DGuides: Making Unconstrained Immersive 3D Drawing More Accurate,VRST - Virtual Reality Software and Technology,A,"Most current commercial Virtual Reality (VR) drawing applications for creativity rely on freehand 3D drawing as their main interaction paradigm. However, the presence of the additional third dimension makes accurate freehand drawing challenging. Some systems address this problem by constraining or beautifying user strokes, which can be intrusive and can limit the expressivity of freehand drawing. In this paper, we evaluate the effectiveness of relying solely on visual guidance to increase overall drawing shape-likeness. We identified a set of common mistakes that users make while creating freehand strokes in VR and then designed a set of visual guides, the Smart3DGuides, which help users avoid these mistakes. We evaluated Smart3DGuides in two user studies, and our results show that non-constraining visual guides help users draw more accurately.",3D User Interfaces; Drawing; Virtual Reality Drawing,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Extended Sliding in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Although precise 3D positioning is not always necessary in virtual environments, it is still an important task for current and future applications of Virtual Reality (VR), including 3D modelling, engineering, and scientific applications. We focus on 3D positioning techniques in immersive environments that use a 6DOF controller as input device and present a new technique that improves 3D positioning performance in VR, in both speed and accuracy. Towards this goal, we adapted an extended sliding technique to VR systems with a controller as input device and compared it with previously presented 3DOF positioning techniques. The results showed that our new Extended VR Sliding technique significantly improved the accuracy for 3D positioning tasks, especially for targets in contact with the scene.",3D positioning; object sliding,Title_Abstract,TRUE,
Scopus,conferencePaper,2019,SlingDrone: Mixed Reality System for Pointing and Interaction Using a Single Drone,VRST - Virtual Reality Software and Technology,A,"We propose SlingDrone, a novel Mixed Reality interaction paradigm that utilizes a micro-quadrotor as both pointing controller and interactive robot with a slingshot motion type. The drone attempts to hover at a given position while the human pulls it in desired direction using a hand grip and a leash. Based on the displacement, a virtual trajectory is defined. To allow for intuitive and simple control, we use virtual reality (VR) technology to trace the path of the drone based on the displacement input. The user receives force feedback propagated through the leash. Force feedback from SlingDrone coupled with visualized trajectory in VR creates an intuitive and user friendly pointing device. When the drone is released, it follows the trajectory that was shown in VR. Onboard payload (e.g. magnetic gripper) can perform various scenarios for real interaction with the surroundings, e.g. manipulation or sensing. Unlike HTC Vive controller, SlingDrone does not require handheld devices, thus it can be used as a standalone pointing technology in VR.",haptics; human-robot interaction; mixed reality; quadrotor,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,2D/3D Mixed Interface for Furniture Placement in Smartphone-based Mobile Augmented Reality,VRST - Virtual Reality Software and Technology,A,"In this work, we propose to use an approximate 2D map of the environment generated from the latest environment modeling technology and enhance the object manipulation performance for the touch based mobile augmented reality. We validated the advantage of the proposed interface through a pilot experiment and confirmed that the use of the 2D map helps reduce the task completion time almost 2 times and improve the usability as well.",3D interaction technique; Augmented reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,A Comparative Study of Planar Surface and Spherical Surface for 3D Pointing Using Direct Touch,VRST - Virtual Reality Software and Technology,A,"We investigated the performance of 3D pointing using direct touch in a planar surface condition (PC) and a spherical surface condition (SC). In addition, we examined the performance in terms of Fitts’ law. Although the results showed that the performance in SC was slightly worse than PC, SC was higher conformed to Fitts’ law than PC without the conditions involving head rotation (PC’s and SC’s R2 is 0.945 and 0.971, respectively).",Curved surface; Fitts’ law; selection performance; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2019,A Content-Aware Approach for Analysing Eye Movement Patterns in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Observing eye movement is a direct way to analyse human’s attention. Eye movement patterns in normal environment have been widely investigated. In virtual reality (VR) environment, previous studies of eye movement patterns are mainly based on content-unrelated influential factors. Considering this issue, in this paper, a novel content-related factor is studied. One crucial kind of region of interest (ROI), namely vision-penetrable entrance, is chosen to analyse eye movement pattern differences. The results suggest that users show more interest in vision-penetrable entrances than in other regions. Furthermore, this kind of difference is identified as higher average density of fixation. As far as we know, this paper is the first attempt to study specific types of ROI in virtual reality environments. The method utilised in this paper can be applied in other ROI analysis.",Omnidirectional panoramas; Virtual reality; Visual saliency,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,A Framework for Virtual Reality Training to Improve Public Speaking,VRST - Virtual Reality Software and Technology,A,"This paper presents the logic and construction of a prototype virtual reality (VR) tool for public speech training. It reflects upon previous endeavours in this area, using them to make informed design decisions. A dictation recognizer is implemented to perform speech to text conversions. With this training simulator, users are be able to step into a virtual environment resembling a podium in an auditorium, with their speech appearing on a virtual cue card. Also, users are presented with a performance metric at the end of their speech to grade their overall performance. We suggest that the VR immersive prototype using speech-to-text recognition has a potential to be engaging and to serve as a tool for public speaking training.",public speaking; usability test; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,A Mobile Augmented Reality Interface for Teaching Folk Dances,VRST - Virtual Reality Software and Technology,A,"This paper presents a prototype mobile augmented reality interface for assisting the process of learning folk dances. As a case study, a folk dance was digitized based on recordings from professional dancers. To assess the effectiveness of the technology, it was comparatively evaluated with a large back-projection system in laboratory conditions. Sixteen participants took part in the study, and their movements were captured using motion capture system and then compared with the recordings from the professional dancers. Experimental results indicate that augmented reality has the potential to be used for learning folk dances.",augmented reality; motion capturing; motion tracking; user studies,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,A Scalability Benchmark for a Virtual Audience Perception Model in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we describe the implementation and performance of a Virtual Audience perception model for Virtual Reality (VR). The model is a VR adaptation of an existing desktop model. The system allows a user in VR to easily build and experience a wide variety of atmospheres with small or large groups of virtual agents.The paper describes results of early evaluations for this model in VR. Our first scalability benchmark results demonstrated the ability to simultaneously handle one hundred virtual agents without significantly affecting there commended frame rate for VR applications.This research is conducted in the context of a classroom simulation software for teachers’ training.",Education; Perception model; Virtual Agent; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,A Virtual Reality Simulator for Training Gaze Control of Wheeled Tele-Robots,VRST - Virtual Reality Software and Technology,A,"People who cannot use their hands may use eye-gaze to interact with robots. Emerging virtual reality head-mounted displays (HMD) have built-in eye-tracking sensors. Previous studies suggest that users need substantial practice for gaze steering of wheeled robots with an HMD. In this paper, we propose to apply a VR-based simulator for training of gaze-controlled robot steering. The simulator and preliminary test results are presented.",eye tracking; gaze interaction; head-mounted display; human-robot interaction; simulator; tele-robots; training; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,A Web-based Augmented Reality Plat-form using Pictorial QR Code for Educational Purposes and Beyond,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) provides the capability to overlay virtual 3D information onto a 2D printed flat surface; for example, displaying a 3D model on a single flat card that accompanies with the diagram shown in a learning text-book. The student can zoom in and out, rotate, and perceive the animation of the figure in real-time. This will make the educational theory more attractive; hence, motivates students to learn. AR is a great tool; however, the setup and display are not straight-forward (there are many different AR markers with different encryption, decryption methods, and displaying flat-forms). In this paper, we proposed a portable browser-based platform which uses the advantages of AR along with scan-able QR Code on mobile phones to enhance instant 3D visualisation. The user only needs a smart-phone (Apple iPhone or Android) with Internet-enabled; no specific Apps are needed to install. The user scans the QR Code embedded in a colour image, the code will link to a public website, and the website will produce AR Experience right on top of the browser. As a result, it provides a stress-free, low-cost, portable, and promising solution for not only educational purposes but also many other fields such as gaming, property selling, e-commerce, reporting. The set up is convenient: the user uploads a picture (e.g. a racing car), and what actions to be related to it (a 3D model to display, or a movie to play). The system will add on the picture one small colour QR code (to redirect to an online URL) and a thin black border. The user also uploads the 3D model (GLTF files) that he wants to display on top of the card to finish the set-up. At the display, the user can print the AR card, point their smart-phone towards the card, and pre-setup AR models or actions will appear on it. To students, these 3D graphics or animations will allow them to learn and understand the lessons in a much more intuitive way.",Augmented Reality; Education; QR Code,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,An Evaluation of Head-Mounted Virtual Reality for Special Education from the Teachers’ Perspective,VRST - Virtual Reality Software and Technology,A,"In this research, we explore the use of head-mounted virtual reality for special education from the teachers’ perspective. We asked a group of special educators to assess the use of VR headset while students with mental disabilities played a VR game. The teachers concluded that head-mounted VR can be used for teaching students to follow instruction and training for work.",head-mounted display; mental disabilities; special education; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Action Units: Directing User Attention in 360-degree Video based VR,VRST - Virtual Reality Software and Technology,A,"A key challenge to effective storytelling using Virtual Reality (VR), such as with 360-degree videos, is how to direct user attention to important content without taking away user agency for free exploration. In this paper, we introduce the notion of an Action Unit system, composed of social cues such as head and arm movements, as a way of directing users to focus on content important for the given narrative. We applied this idea to a 360-degree VR tour, and evaluated its effects on memory, engagement, enjoyment, and cyber-sickness. The results indicate that the levels of engagement and enjoyment increased when these Action Units were applied. Users also preferred the Action Units for their diegetic aspects.",360-degree video; social cues; storytelling; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,AHMED: Toolset for Ad-Hoc Mixed-reality Exhibition Design,VRST - Virtual Reality Software and Technology,A,"We present “AHMED”, a mixed-reality toolset that allows visitors to experience mixed-reality museum or art exhibitions created ad-hoc at locations such as event venues, private parties,or a living room. The system democratizes access to exhibitions for populations that cannot visit these exhibitions in person for reasons of disability, time-constraints, travel restrictions, or socio-economic status.",augmented reality; heritage; mixed reality; museum; photogrammetry; volumetric capture,Keywords,TRUE,
Scopus,conferencePaper,2019,Analysis of VR Sickness and Gait Parameters During Non-Isometric Virtual Walking with Large Translational Gain,VRST - Virtual Reality Software and Technology,A,,Cybersickness; Locomotion; Navigation; Redirected Walking; Virtual Reality; Walking,Keywords,TRUE,
Scopus,conferencePaper,2019,Augmented Reality Approach For Position-based Service using Handheld Smartphone,VRST - Virtual Reality Software and Technology,A,"In this work, we present an augmented reality (AR) approach for position based service using a smartphone in an indoor environment. The AR method, combined with position estimation, provides a user with a smartphone with a service that is specific to a particular position without using a marker or any other hardware device. The position in an indoor environment is estimated using an IMU sensor only in the smartphone. The accuracy of the position and heading direction of the user is improved by integrating the values from the accelerometer and the gyro using Principal Component Analysis(PCA) and Extended Kalman Filter(EKF). Then, a drift noise of the estimated position is reduced by a registration step performed at a specific position. The estimated position is given to the position based service, which is provided to the user on the smartphone screen through AR. The concept of the proposed method is demonstrated with some examples.",Augmented reality; indoor position estimation; position-based service,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Augmented Reality Visualisation Concepts to Support Intraoperative Distance Estimation,VRST - Virtual Reality Software and Technology,A,"The estimation of distances and spatial relations between surgical instruments and surrounding anatomical structures is a challenging task for clinicians in image-guided surgery. Using augmented reality (AR), navigation aids can be displayed directly at the intervention site to support the assessment of distances and reduce the risk of damage to healthy tissue. To this end, four distance-encoding visualisation concepts were developed using a head-mounted optical see-through AR setup and evaluated by conducting a comparison study. Results suggest the general advantage of the proposed methods compared to a blank visualisation providing no additional information. Using a Distance Sensor concept signalising the proximity of nearby structures resulted in the least time the instrument was located below 5mm to surrounding risk structures and yielded the least amount of collisions with them.",Distance Estimation; Medical Augmented Reality; Visualisation,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Can We Predict Susceptibility to Cybersickness?,VRST - Virtual Reality Software and Technology,A,This study investigated whether individual differences in postural stability/activity can be used to predict who will become sick when exposed to head-mounted display (HMD) based virtual reality (VR). We found that participants who reported feeling sick after at least one exposure to VR displayed different postural activity than those who remained well. Importantly these differences were present in their sway data before they even donned the HMD. These results are inline with the postural instability theory of motion sickness and suggest that we can identify individuals who are more susceptible HMD-based cybersickness based on their spontaneous postural sway.,Cybersickness; Head-Mounted Display; Motion sickness; Spontaneous Postural Sway; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Cinévoqué: Development of a Passively Responsive Framework for Seamless Evolution of Experiences in Immersive Live-Action Movies,VRST - Virtual Reality Software and Technology,A,"Cinematic Virtual Reality’s (CVR) inherent feature of allowing the user to choose their Point of View (POV) within a 360° space brings forth new challenges to storytelling. The approaches used in traditional films do not translate directly to this medium, as it is uncertain if the user would follow all the Points of Interest (POIs) consistently. Our framework, Cinévoqué, aims to address this issue by using the real-time data generated during a VR film to passively alter the narrative and parts of the experience to suit the user’s viewing behavior. In this poster, we discuss the technical approaches used to implement this framework and create responsive live-action CVR.",Presence; Responsive Narrative; Storytelling; Virtual Reality; VR Cinema,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Comfortable Locomotion in VR: Teleportation is Not a Complete Solution,VRST - Virtual Reality Software and Technology,A,"We compared two common techniques of controller-based locomotion (teleportation and steering locomotion) in virtual reality (VR) in terms of the cybersickness they produce. Participants had to continuously navigate a commercial VR application for 16 minutes using each technique, while standing and seated. While teleportation produced less cybersickness than steering locomotion on average, a number of participants reported teleportation to be more sickening. These ‘telesick’ participants were found to have greater medio/lateral positional variability in their spontaneous postural sway than ‘steersick’ participants prior to VR exposure. We conclude that different individuals may require unique techniques to comfortably locomote in VR.",Cybersickness; Head-Mounted Display; Locomotion; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Design and Realization of Sustainable Rural Housing Using Immersive Virtual Reality Platform,VRST - Virtual Reality Software and Technology,A,"Rapid urbanization in developing countries has paved way to spontaneous settlements, which are overcrowded. The aim of this work is to assess the impact of Virtual Reality (VR) on different types of sustainable construction techniques that are proposed for rural slum communities. The work mainly focuses on a walkthrough and interactions on a prototype of a sustainable housing unit in a rural slum community built with eco-friendly building materials, natural light source and ventilation.",Controller based interaction; Prototype; Rural slums; Sustainable design; Virtual reality; Walkthrough,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Design of Portable Thermal Sensor Device for VR,VRST - Virtual Reality Software and Technology,A,"This study proposes a thermal sensibility haptic system that can be used in the VR environment to stimulate multiple sensory receptors. In addition, the object can be distinguished through the touch if it is reproduced by adjusting the intensity of the stimulus based on the intrinsic thermal energy and surface curvature of the object.",Contact Temperature; Haptic Feedback; Thermal Cues; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2019,Drone-Steering: A Novel VR Traveling Technique,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel technique of navigation in Virtual Reality (VR) called Drone-Steering. This technique has been designed to facilitate path learning and traveling in VR by reducing both cybersickness and disorientation. We compared this technique to traditional Hand-Steering in a landmark-free environment. Our first experiment confirmed a significantly lower level of cybersickness during traveling and significantly better path learning. We believe that our technique constitutes a promising alternative to current VR navigation techniques, and will especially interest researchers and developers targeting large VR environments.",Path Learning; Sickness; Travel; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Effects of Age and Motivation for Visiting on AR Museum Experiences,VRST - Virtual Reality Software and Technology,A,"Augmented reality(AR) provides a unique viewing experience at museums where people understand abstract history through physical artifacts. Although AR usage in museum settings has been increasing, it is not well understood how AR viewing experience differs in different groups of visitors, which can be problematic considering that museums are places visited by diverse groups of people. In this study, we evaluate the differences in AR experiences according to the characteristics of the visitors. The results show the effect of AR usage in museum settings with visitors’ different age groups and motivations for visiting.",Augmented reality; Museums; User characteristics,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Effects of Tactile Perception on Emotion and Immersion to Film Viewing in a Virtual Environment,VRST - Virtual Reality Software and Technology,A,"In this research, we compared three tactile conditions (No vs. Soft vs. Spiky) in both positive and negative scenes to explore whether tactile perception could influence emotional responses and immersive experience in a virtual environment (VE). The results showed that, when viewing positive scenes with soft stimuli, participants experienced an increase in both positive emotions and their level of immersion compared to those in the No and Spiky tactile conditions. We also found that participants in the No and Spiky tactile conditions reported no significant differences in either emotion or immersion when viewing positive scenes. During the viewing of negative scenes, spiky stimuli did not intensify negative feelings, while soft stimuli decreased negative emotions. In terms of immersion, there was no meaningful difference between the three tactile conditions for negative scenes. Overall, this study has demonstrated the important association between tactile perception, emotion, and immersion in a VE.",Emotion; Immersion; Tactile Perception; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2019,Enhancement of Pointing Towards Non-Haptic Augmented Reality Interfaces by Increasing the Arm Position Sense,VRST - Virtual Reality Software and Technology,A,"Interactive user interfaces in head-mounted Augmented Reality environments are not always projected onto a physical surface. However, operating such free-floating interfaces by touch gestures is challenging because they do not provide haptic feedback. Considering a pointing gesture, in this work we present a user study evaluating the benefits of increasing the arm position sense for operating non-haptic interface. Our findings confirm that haptic feedback is required and show that an increased arm sense compensates for the lack of haptic feedback. The results suggest that applying 0.3 times of the pointing arm’s weight significantly speeds up direct object selection for free-floating interfaces. We also show that the correction phase of the underlying pointing movement is affected by boosting the arm sense.",Augmented Reality; haptic feedback; interaction; selection,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Evaluation of Navigation Operations in Immersive Microscopic Visualization,VRST - Virtual Reality Software and Technology,A,"In this study, we evaluated the quantitative effectiveness of navigation operation in a virtual reality (VR) volumetric viewer, in order to confirm the effectiveness of VR in life sciences. The analytical work for biological data is a promising application of VR because users can manipulate 3D data intuitively in VR. However, few studies have focused on the quantitative evaluations of such applications. Therefore, we conducted an experiment to evaluate the speedup of navigation operation (sequences of translation, rotation, and scaling) in VR applications for 3D microscopy. We compared the task completion time between a non-VR visualization tool and a VR visualization tool. The speedup by the VR immersive visualizer was found to be 203% in the most effective case. The result showed that the VR immersive visualizer enables more efficient navigation than the conventional volumetric viewer.",immersive visualization; life science; microscope; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Experiencing Waiting Time in Virtual Reality,VRST - Virtual Reality Software and Technology,A,This article investigates the impact of waiting in Virtual Reality (VR) on the perception of time. We manipulated the visual quality of a virtual room replicating a real one (360-picture vs. 3D-model) with and without avatar embodiment (no-avatar vs. avatar). We only observed a significant difference in the estimated time duration between the real and the virtual worlds when using no avatar within a 3D model of the room. Our early results suggest that a VR environment with an avatar and a simple 3D model or 360 picture room is not significantly perturbing time perception and thus could be used for diagnosis and therapy of psychiatric conditions related to altered time perception.,Avatar Embodiment; Time Perception; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Exploring Experiences of Virtual Reality among Young and Older Adults in a Subway Fire Scenario: a Pilot Study,VRST - Virtual Reality Software and Technology,A,"We report a pilot study investigating experiences of virtual reality (VR) among young and older adults in a subway fire scenario. We created VR environments in subway fire scenarios and ran an experiment by asking 5 young and 5 older adults to explore VR environments. After the experiment, participants were asked to fill out a survey questionnaire to report their feelings. Additionally, we conducted semi-structured interviews with participants to understand challenges they faced while exploring VR environments. We found that compared with young adults, older adults tended to be different during the process of evacuating a subway station in virtual reality. We suggest design opportunities for creating VR environments for more effective training of older adults.",fire evacuation; older adults; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Exploring Immersive Technologies to Simulate Fear of Crime,VRST - Virtual Reality Software and Technology,A,"The implementation of Virtual Reality (VR) tools in criminological research is very scarce, and almost non-existent in the fear of crime (FoC) field. Our objective is to assess the feasibility of Immersive Technologies for research on FoC. To do so, a simulation (360° video) grounded on the manipulation of environmental variables (street lighting) was conducted. Our preliminary results suggest that: (a) virtual simulation of absence of urban lighting elicits experiences of FoC, and (b) that simulation of experiences of FoC in virtual reality is an adequate strategy for analysis of this phenomenon.",360° video; Criminology; environmental variables.; fear of crime; immersive technologies; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Impact of Gamified Interaction with Virtual Nature on Sustained Attention and Self-Reported Restoration &nbsp;—&nbsp; A Pilot Study,VRST - Virtual Reality Software and Technology,A,"Interaction with nature in virtual reality has been shown to induce similar restorative benefits as interaction with real-life nature. Drawing from Attention restoration theory, restorative benefits from being in virtual nature are likely to be improved through greater active engagement techniques with specific virtual natural features. Gamification is the process of adding game design elements in non-game scenarios in order to improve engagement and motivation. In the present pilot study, six participants completed either a gamified interaction with virtual nature, one where game design elements had been added in order to improve engagement with specific virtual nature features and thus possibly further facilitating sustained attention and self-reported restoration, having them pick plants and gain rewards such as a higher level in return, or a non-gamified task, one where they explored the virtual nature environment and looked at plants at their own pace without any game design elements. Gamified interaction improved sustained attention restoration more than non-gamified interaction. Additionally, gamified interaction was also shown to have reduced negative effect in self-reported restoration more than non-gamified interaction. While there are still several limitations, gamified interaction with virtual nature seems to offer vast potential as an engagement technique in improving sustained attention and self-reported restoration.",engagement; gamification; sustained attention; virtual nature; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Indian Virtual reality affective database with self-report measures and EDA,VRST - Virtual Reality Software and Technology,A,"The current work assesses the physiological and psychological responses to the 360° emotional videos selected from Stanford virtual reality (VR) affective database [Li et al., 2017], presented using VR head-mounted display (HMD). Participants were asked to report valence and arousal level after watching each video. The electro-dermal activity (EDA) was recorded while watching the videos. The current pilot study shows no significant difference in skin-conductance response (SCR) between the high and low arousal experience. Similar trends were observed during high and low valence. The self-report pilot data on valence and arousal shows no statistically significant difference between Stanford VR affective responses and the corresponding Indian population psychological responses. Despite positive result of no-significant difference in self-report across cultures, we are limited to generalize the result because of small sample size.",360° videos; Arousal; EDA; Valence; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Interactive Virtual-Reality Fire Extinguisher with Haptic Feedback,VRST - Virtual Reality Software and Technology,A,"We present an interactive virtual-reality (VR) fire extinguisher that provides both realistic viewing using a head-mounted display (HMD) and kinesthetic experiences using a pneumatic muscle and vibrotactile transducer. The VR fire extinguisher is designed to train people to use a fire extinguisher skillfully in real fire situations. We seamlessly integrate three technologies: VR, object motion tracking, and haptic feedback. A fire scene is immersed in the HMD, and a motion tracker is used to replicate a real designed object into the virtual environment to realize augmented reality. In addition, when the handle of the fire extinguisher is squeezed to release the extinguishing agent, the haptic device generates both vibrotactile and air flow tactile feedback signals, providing the same experience as that obtained while using a real fire extinguisher.",Firefighting; Haptic feedback; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Interactive Visualization of Painting Data with Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Exploration of Augmented Reality technologies has increased substantially and the increase in both popularity and technological maturity has also led to several applications being developed for educational and museum environments. Specifically, a greater focus has been placed upon creating memorable experiences that both attract and educate museum patrons. Attempts to do this involve creating both Virtual Reality and Augmented Reality experiences, such as having users enter into immersive worlds that demonstrate the history of a certain time period, or applications that overlay life-like models of those animals in the very room the user is standing in. Many of these experiences are quite exceptional but begin to lack in variety when moving towards the art gallery, and mainly focus on making painting information more accessible. In an attempt to address this, this project outlines the design and evaluation of a proof-of-concept meant to study if adding interaction through Augmented Reality to paintings themselves would be both technologically feasible and desirable.",art; augmented reality; interactive; painting; visualization,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,IRIS: Inter-Reality Interactive Surface,VRST - Virtual Reality Software and Technology,A,"While many metaphors were developed for interactions from a specific point at the reality-virtuality continuum, much less attention has been paid to designing metaphors that allow the users to cross the boundaries between the virtual, the augmented, and the real. We propose a use of an Inter-Reality Interactive Surface (IRIS) that enables users to collaborate across the reality-virtuality continuum within the same application. While we examine IRIS in the context of an immersive educational platform, UniVResity, the metaphor can be generalized to many other application domains.",augmented reality; collaboration; immersive learning; interaction metaphor; reality-virtuality continuum; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2019,Learning-based Estimation of 6-DoF Camera Poses from Partial Observation of Large Objects for Mobile AR*,VRST - Virtual Reality Software and Technology,A,"We propose a method that estimates 6-DoF camera pose from a partially visible large object, by exploiting information of its subparts that are detected using a state-of-the-art convolutional neural network (CNN). The trained CNN outputs two-dimensional bounding boxes around subparts and associated classes. Information from detection is then fed to a deep neural network that regresses to camera's 6-DoF poses. Experimental results show that the proposed method is more robust to occlusions than conventional learning-based methods.",deep learning; large object; Mobile augmented reality; partial observation; pose estimation,Keywords,TRUE,
Scopus,conferencePaper,2019,Mixed Reality Speaker Identification as an Accessibility Tool for Deaf and Hard of Hearing Users,VRST - Virtual Reality Software and Technology,A,"People who are Deaf or Hard of Hearing (DHH) benefit from text captioning to understand audio, yet captions alone are often insufficient for the complex environment of a panel presentation, with rapid and unpredictable turn-taking among multiple speakers. It is challenging and tiring for DHH individuals to view captioned panel presentations, leading to feelings of misunderstanding and exclusion. In this work, we investigate the potential of Mixed Reality (MR) head-mounted displays for providing captioning with visual cues to indicate which person on the panel is speaking. For consistency in our experimental study, we simulate a panel presentation in virtual reality (VR) with various types of MR visual cues; in a study with 18 DHH participants, visual cues made it easier to identify speakers.",Deaf and Hard of Hearing; Mixed Reality; Speaker Identification,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Monocular Viewing Protects Against Cybersickness Produced by Head Movements in the Oculus Rift,VRST - Virtual Reality Software and Technology,A,We compared the cybersickness produced when a virtual environment (VE) was viewed binocularly and monocularly through an Oculus Rift CV1 head-mounted display (HMD). During each exposure to the VE participants made continuous yaw head movements in time with a computer-generated metronome. Across trials we also varied their head movement frequency (0.5 or 1.0 Hz) and motion-to-photon delays (from ∼5 - ∼212 ms). We found that: 1) cybersickness severity increased with added display lag; and 2) monocular viewing appeared to protect against these increases in cybersickness. We conclude that active binocular viewing with this HMD introduced artifacts that increased the likelihood of more severe sickness.,Cybersickness; Oculus Rift; Perception; Virtual Reality; VR,Keywords,TRUE,
Scopus,conferencePaper,2019,Optical-Reflection Type 3D Augmented Reality Mirrors,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) mirrors can show virtual objects overlaid onto the physical world reflected in the mirror. Optical-reflection type AR mirror displays use half-silvered mirrors attached in front of a digital display. However, prior work suffered from visual depth mismatch between the optical reflection of the 3D physical space and 2D images displayed on the surface of the mirror. In this research, we use 3D visualisation to overcome this problem and improve the user experience by providing better depth perception for watching and interacting with the content displayed on an AR mirror. As a proof of concept, we developed two prototype optical-reflection type 3D AR mirror displays, one using glasses-free multi-view 3D display and another using a head tracked 3D stereoscopic display that supports hand gesture interaction.",3D visualization; Augmented mirror; depth mismatch,Title_Abstract,TRUE,
Scopus,conferencePaper,2019,Predicting the Torso Direction from HMD Movements for Walk-in-Place Navigation through Deep Learning,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose to use the deep learning technique to estimate and predict the torso direction from the head movements alone. The prediction allows to implement the walk-in-place navigation interface without additional sensing of the torso direction, and thereby improves the convenience and usability. We created a small dataset and tested our idea by training an LSTM model and obtained a 3-class prediction rate of about 90%, a figure higher than using other conventional machine learning techniques. While preliminary, the results show the possible inter-dependence between the viewing and torso directions, and with richer dataset and more parameters, a more accurate level of prediction seems possible.",deep learning; locomotion; Virtual reality; walking in place,Keywords,TRUE,
Scopus,conferencePaper,2019,Preliminary Evaluation of the Usability of a Virtual Reality Game for Mudslide Education for Children,VRST - Virtual Reality Software and Technology,A,"Mudslide education is important for children. In this study, a design-based research approach was used to develop an educational VR mudslide game for children. Eleven children participated in the usability evaluations. The results indicated the importance of intuitive, easy-to-learn controls. Six major refinements of the VR mudslide game were made to increase usabilities. Feedback from the participants will guide future game refinements to increase users’ engagement and interaction.",,Title,TRUE,
Scopus,conferencePaper,2019,Proposing a Hand-Tracking Device using a Tangential Force Mechanical Sensor,VRST - Virtual Reality Software and Technology,A,"Conventional hand-tracking devices are constructed with inertial measurement units, bending sensors, and optical technologies. However, these are limited by their high-cost and environmental factors. In this research, a hand-tracking device using a tangential force mechanical sensor for use in Immersive Virtual Environments is proposed.",cyberglove; hand-tracking; sensor; virtual reality; wearable,Keywords,TRUE,
Scopus,conferencePaper,2019,Real-time Monitoring Method for Cybersickness using Physiological Signals,VRST - Virtual Reality Software and Technology,A,"The potential for cybersickness remains a critical problem when engaged in Virtual Reality experiences. Cybersickness is difficult to resolve because, although there are commonly accepted symptoms and theories, there is still no consensus on how to overcome the problem. In this study, a method of real-time monitoring of physiological signals is proposed as an approach to measure the potential onset of cybersickness. An application called Cybatica which displays physiological data and a unique metric termed Onset of Cybersickness (OCS) has been developed.",Cybersickness; Physiological Data; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Scalebridge VR: Immersive Proportional Reasoning Game for Children with Brain-Computer Interface for Difficulty Scaling,VRST - Virtual Reality Software and Technology,A,"We present the design and evaluation of Scalebridge VR, an immersive educational game that teaches children the mathematical skill of proportional reasoning. The game uses brain-computer-interface-based adaptive level difficulty to modulate difficulty of the game based on the player’s attention and meditation state. The game is an adaptation of previously introduced Scalebridge game that did not use virtual reality, but was shown to be an effective tool for learning proportional reasoning.",freehand interaction; immersive learning; mathematical skills; proportional reasoning; STEM; virtual reality; VR game,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,SolarVR for inter-cognitive and intra-cognitive communication,VRST - Virtual Reality Software and Technology,A,"The aim of this research is to design and implement a Solar Virtual Reality environment (SolarVR) for inter-cognitive and intra-cognitive communication by connecting users and sensors to a real-world solar panel plant for remote monitoring, maintenance and collaboration. The paper outlines the development of a VR solution which can be utilized for remote monitoring and communication, skills training and science education.",communication; design; solar; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,The Impact of Stereo Rendering on the Perception of Normal Mapped Geometry in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This paper investigates the effects of normal mapping on the perception of geometric depth between stereoscopic and non-stereoscopic views. Results show, that in a head-tracked environment, the addition of binocular disparity has no impact on the error rate in the detection of normal-mapped geometry. It does however significantly shorten the detection time.",Binocular Disparity; Motion Parallax; Normal Maps; Virtual Reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2019,Toward Effective Virtual Reality Intervention Development Planning for People with Persistent Postural-Perceptual Dizziness,VRST - Virtual Reality Software and Technology,A,"Persistent Postural-Perceptual Dizziness (PPPD) is defined by World Health organization as ”Persistent non-vertiginous dizziness, unsteadiness, or both lasting three months or more”. With the most common provocations are situations like up-right position, self-motions, looking at fast moving objects or disruptions in a crowded environment. Besides conventional treatments, scientists are looking at the possibility of using creative technology including virtual reality (VR) to assist improving symptoms. Here, we have proposed a strategy that would strengthen the initial phase of discussion between VR technologists and PPPD experts on developing an effective VR based intervention tool.",intervention; persistent postural-perceptual dizziness; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Towards Dynamic Positioning of Text Content on a Windshield Display for Automated Driving,VRST - Virtual Reality Software and Technology,A,"Windshield displays (WSDs) are a promising new technology to augment the entire windscreen with additional information about vehicle state, highlight critical objects in the surrounding, or serve as replacement for conventional displays. Typically, augmentation is provided in a screen-fixed manner as overlay on the windscreen. However, it is unclear to date if this is optimal in terms of usability/UX. In this work, we propose ”StickyWSD” – a world-fixed positioning strategy – and evaluate its impact on quantitative measures compared to screen-fixed positioning. Results from a user study conducted in a virtual reality driving simulator (N = 23) suggest that the dynamic world-fixed positioning technique shows increased performance and lowered error rates as well as take-over times. We conclude that the ”StickyWSD” approach offers lot of potential for WSDs that should be researched further.",augmented reality; automated driving; focus distance; user study; virtual reality; windshield display,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,TTT: Time Synchronization Method by Time Distortion for VR Training including Rapidly Moving Objects,VRST - Virtual Reality Software and Technology,A,"Providing an experience that includes high-speed objects, such as tennis balls, with a virtual reality (VR) training environment might provide efficient training for trainers but is challenging to achieve. Because of the drawing performance of the display, high-speed objects are perceived as poor visual information more than in reality, such as images in a stroboscope. The faster the object, the more noticeable it becomes, and the harder it is to perceive it correctly. Therefore, if the training is performed at the actual speed, the perception becomes more difficult than real space training due to the low reproduction accuracy. To solve this problem, we propose the computational time-space that controls high-speed objects in VR space, based on the user’s body movement. The method facilitates the perception of fast-moving objects by synchronizing the time of the ball with the movement of the body.",Assisting Training; Immersive Virtual Environments; Time Perception,Abstract,TRUE,
Scopus,conferencePaper,2019,Understanding Enjoyment in VR Games with GameFlow,VRST - Virtual Reality Software and Technology,A,"In this paper, we report on a work in progress project that aims to understand affordances and inhibiters of enjoyment in virtual reality (VR) video games. We apply the GameFlow model to review and analyse VR and non-VR versions of the same games to identify differences in enjoyment. Our approach includes conducting expert reviews using the GameFlow model, as well as conducting qualitative analysis on video game reviews, using GameFlow as a conceptual foundation. In this paper, we report our initial findings for the game Superhot. Our ongoing work evaluates a selection of games to map opportunities and pitfalls when designing games for VR.",enjoyment; player experience; videogames; Virtual reality; VR,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,UniVResity: Face-to-Face Class Participation for Remote Students using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We describe a prototype of the virtual reality remote classroom participation system called UniVResity. UniVResity mirrors in virtual reality the ongoing face-to-face classroom activities, taking into account potentially low bandwidth data connection and lack of VR equipment in class. Our system attempts to combine the benefits of online education and face-to-face education, and makes face-to-face learning more accessible.",distance learning; education; remote learning; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Vertical Locomotion in VR Using Full Body Gestures,VRST - Virtual Reality Software and Technology,A,"Virtual Reality experiences today are majorly based on horizontal locomotion. In these experiences, movement in the virtual space is accomplished using teleportation, gaze input or tracking in physical space which is limited to a certain extent. Our work focuses on intuitive interactions for vertical locomotion involving both hands and feet. Such an instance of vertical locomotion is - ladder climbing. In this paper, we present an interaction technique for climbing a ladder in Virtual Reality (VR). This technique is derived from the natural motions of the limbs while climbing a ladder in reality, adhering to safe climbing practices. The developed interaction can be used in training experiences as well as gaming experiences. Preliminary evaluation of our interaction technique showed positive results across dimensions like - learnability, natural mapping, and intuitiveness.",Gestures; Interactions; Ladder Climbing; Locomotion; Teleportation; Tracking; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Virtual Immersive Educational Systems:The case of 360° video and co-learning design.,VRST - Virtual Reality Software and Technology,A,"The focus of this research is to depict the design process of a cost-effective, robust but user-friendly Virtual Immersive Educational (VIE) system. Thus, assist researchers, instructors and designers in identifying an effective method to design VIE systems. In this report, we describe our initial steps to design such a system in order to educate engineering students on the basic health and safety guidelines of safe interaction with a robotic arm. To do so, a set of 360° videos have been designed, developed and tested.",360° videos; Immersive Education Systems; Immersive technologies; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2019,Visualizing Convolutional Neural Networks with Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Software systems and components are increasingly based on machine learning methods, such as Convolutional Neural Networks (CNNs). Thus, there is a growing need for common programmers and machine learning newcomers to understand the general functioning of these algorithms. However, as neural networks are complex in nature, novel presentation means are required to enable rapid access to the functionality. For that purpose, this paper examines how CNNs can be visualized in Virtual Reality. A first exploratory study has confirmed that our visualization approach is both intuitive to use and conductive to learning.",knowledge learning; neural networks; virtual reality; visualization,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Where to Place the Camera,VRST - Virtual Reality Software and Technology,A,"This paper describes aspects which are important for camera positioning in cinematic virtual reality. For our findings, we took a closer look at proxemics, the study on how humans behave in regard to space and distances. We explored well-known shot sizes used in traditional filmmaking and put them in relation to proxemics distances. The results were adapted to camera distances in cinematic virtual reality.",360° movies; camera distances; camera positions; character distances; Cinematic Virtual Reality; proxemics; shot sizes,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,WiredSwarm: High Resolution Haptic Feedback Provided by a Swarm of Drones to the User’s Fingers for VR interaction,VRST - Virtual Reality Software and Technology,A,"We propose a concept of a novel interaction strategy for providing rich haptic feedback in Virtual Reality (VR), when each user’s finger is connected to micro-quadrotor with a wire. Described technology represents the first flying wearable haptic interface. The solution potentially is able to deliver high resolution force feedback to each finger during fine motor interaction in VR. The tips of tethers are connected to the centers of quadcopters under their bottom. Therefore, flight stability is increasing and the interaction forces are becoming stronger which allows to use smaller drones.",drone; force feedback; haptics; human-swarm interaction; multi-agent system; quadrotor; swarm; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,ALS-SimVR: Advanced Life Support Virtual Reality Training Application,VRST - Virtual Reality Software and Technology,A,The delivery of ongoing training and support to Advanced Life Support (ALS) teams poses significant resourcing and logistical challenges. A reduced exposure to cardiac arrests and mandated re-accreditation pose further challenges for educators to overcome. This work presents the ALS-SimVR (Advanced Life Support Simulation in VR) application. The application is intended for use as a supplementary training and refresher asset for ALS team leaders. The purpose of the application is to allow critical care clinicians to rehearse the role of ALS Team leader in their own time and location of choice. The application was developed for the Oculus-Go and ported to the Oculus-Quest. The application is also supported for a desktop and server based streaming release.,Clinical; Simulation; Virtual Reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2019,AssessAR: An Augmented Reality Based Environmental Impact Assessment Framework,VRST - Virtual Reality Software and Technology,A,"Human activities can have a lasting impact on the environment and society. Environmental impact assessment (EIA) which focusses on evaluating the impact of proposed developmental projects on the environment, helps in transparent decision-making and involves multiple stakeholders. However, EIA is data and effort-intensive and often becomes complex and long-drawn. Moreover, EIA is currently performed using primarily two-dimensional traditional mediums which could be vastly restrictive and difficult to navigate and comprehend. Here, we present an immersive approach which can create 3D interactive elements, modelling the real-world using augmented/mixed reality. Because of the inherent benefits of using three-dimensional representations and associated real-world interactions, we posit that our approach will facilitate better and faster, collaboration-enabled analysis of a developmental project proposal, thereon reducing processing time and promoting high fidelity.",Augmented Reality; Environmental Impact Assessment,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Development of MirrorShape: High Fidelity Large-Scale Shape Rendering Framework for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Today there is a high variety of haptic devices capable of providing tactile feedback. Although most of existing designs are aimed at realistic simulation of the surface properties, their capabilities are limited in attempts of displaying shape and position of virtual objects. This paper suggests a new concept of distributed haptic display for realistic interaction with virtual object of complex shape by a collaborative robot with shape display end-effector. MirrorShape renders the 3D object in virtual reality (VR) system by contacting the user hands with the robot end-effector at the calculated point in real-time. Our proposed system makes it possible to synchronously merge the position of contact point in VR and end-effector in real world. This feature provides presentation of different shapes, and at the same time expands the working area comparing to desktop solutions. The preliminary user study revealed that MirrorShape was effective at reducing positional error in VR interactions. Potentially this approach can be used in the virtual systems for rendering versatile VR objects with wide range of sizes with high fidelity large-scale shape experience.",3D interaction; collaborative technologies; haptics; interaction technologies; robotics; shape-changing interfaces; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,DexController : Hand-Held Controller Recognizing Grasp-Pose and Grasp-Force in Virtual Reality Defense Game,VRST - Virtual Reality Software and Technology,A,"We developed a hand-held controller named DexController, leveraging grasp as an additional input modality for virtual reality(VR) game. The pressure-sensitive surface of DexController could recognize two different grasp-poses (i.e. precision grip and power grip) and detect grasp-force. For demonstration, we designed a VR defense game in which players should attack different virtual enemies using the proper weapon with a proper level of force. User study confirmed that utilizing meaningful information of grasping facilitates natural mapping with game contents, which led VR game users to experience enhanced presence and enjoyment.",controller; game experience; natural interaction; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,"Dynamic Virtual Proteins: Visualization, Interaction and Collaboration in Virtual Reality",VRST - Virtual Reality Software and Technology,A,,Amino-acid; Animation; Augmented Reality; Bio-chemistry; Biology; Education; Interaction Design; Mixed Reality; Physics; Protein Structure; Science; Usability; User Experience; User Interaction; Virtual Protein; Virtual Reality; Visualization,Title_Keywords,TRUE,
Scopus,conferencePaper,2019,Emotion Evoking Art Exhibition in VR,VRST - Virtual Reality Software and Technology,A,"Many museums today lack an aspect of technology that will attract younger visitors to visit the art. By implementing Virtual Reality into art museum solves this problem. Virtual Reality is a popular phenomenon that attracts many viewers and is growing every day. Art museums want to express emotion through their art and Virtual Reality can evoke that emotion more. By creating a virtual museum that not only has all the art on display but also is set an outdoor environment such as a garden or a dark forest will further enhance the emotion. If a piece of art is supposed to show warmth or positive feelings, why not place it in a garden? If the art is supposed to show darkness or cold why not place it in a dead forest? Using Virtual Reality allows us to place art in these environments so further museum goal of expressing emotion.",art; interactive; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Extended Reality for Chronic Pain Relief,VRST - Virtual Reality Software and Technology,A,"Chronic pain is ongoing pain lasting for long periods of time after the initial injury or disease has healed. Chronic pain is difficult to treat and can affect the daily lives of patients. Distraction therapy is a proven way of relieving pain by redirecting the focus of patients’ attention. Virtual reality is an effective platform for distraction therapy as it immerses the user visually, aurally, and even somewhat physically in a virtual world detached from reality. There is little research done on the effects that physical interactions have on pain management. This project aims to evaluate different types of extended reality (XR) interactions, including full body movement, for chronic pain patients to determine which is the best for pain relief. We are building a prototype for participants to interact both mentally and physically and measuring the reduction in subjective pain ratings at various points of the XR experience.",augmented reality; chronic pain; mixed reality; pain management; pain relief; user study; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Interactive Visualization of Painting Data with Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Exploration of Augmented Reality technologies has increased substantially and the increase in both popularity and technological maturity has also led to several applications being developed for educational and museum environments. Specifically, a greater focus has been placed upon creating memorable experiences that both attract and educate museum patrons. Attempts to do this involve creating both Virtual Reality and Augmented Reality experiences, such as having users enter into immersive worlds that demonstrate the history of a certain time period, or applications that overlay life-like models of those animals in the very room the user is standing in. Many of these experiences are quite exceptional but begin to lack in variety when moving towards the art gallery, and mainly focus on making painting information more accessible. In an attempt to address this, this project outlines the design and evaluation of a proof-of-concept meant to study if adding interaction through Augmented Reality to paintings themselves would be both technologically feasible and desirable.",art; augmented reality; interactive; painting; visualization,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2019,Layered Horizons: a Geospatial Humanities Research Platform,VRST - Virtual Reality Software and Technology,A,"In this demo we showcase Layered Horizons, a Virtual Reality (VR) experience we have developed for use in an ARC-funded research project, Waves of Words: Mapping and Modelling Australia’s Pacific Past. This platform allows users to connect different geospatial datasets (for our purposes, from the humanities and social sciences) into layers that can then be explored by the use of natural gesture and body movement. This kind of interaction design in VR takes full advantage of the media’s affordances, without relying on metaphors from other interactive media, yet being familiar enough as to engender intuitive and meaningful use. We demonstrate how the platform is currently being used to connect linguistic data (word lists) with archaeological data (e.g. on the spread of bananas through the Asia-Pacific region, or canoe styles found in different locations) and anthropological data (e.g. shared cultural features like chieftainship systems or kinship systems). Taking into account what we also know about Pacific navigation and simulated canoe travel, we can therefore build a complex layered map of the region over time that allows us to better discover probable human migration and contact patterns.",data visualisation; gesture; interfaces; languages; leap motion; research platform; research through design; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,OORT: An Air-flow based Cooling System for Long-term Virtual Reality Sessions,VRST - Virtual Reality Software and Technology,A,"In this paper, we present OORT, a cooling system for head-mounted displays (HMDs) that improves wearing comfort by decreasing skin temperatures of the facial areas covered by the headset. The integrated cooling system consists of an electronically controlled fan blower. The fan compartment is integrated into an hmd padding element with custom-designed air flow channels that provide cool air circulation around the covered facial regions. We report on the design and implementation of OORT as a viable way to provide thermal comfort during long-term virtual reality experiences.",Cooling; HMDs; Oort; Thermal Comfort; Virtual Reality; VR,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,PillowVR: Virtual Reality in Bed,VRST - Virtual Reality Software and Technology,A,"We demonstrate PillowVR, virtual reality framework that integrates the smartphone, magnifier and sensors into a pillow/cushion for immersive VR experience in bed. PillowVR is applied for presenting immersive bed time stories to children to help them go to sleep and therefore its interaction was designed to minimize excessive bodily movements – only simple back-of-the head pressure events are used to browse the content. PillowVR illustrates shows how VR can be more woven into our daily lives inexpensively and naturally by customizing the set up and interaction for the specific task and experience.The actual demonstration of PillowVR would be very simple (as intended). In this paper, we can watch 360° video because viewpoint can be switching in “Non Ready” state. Our team will place an exercise pad or long picnic chair (instead of an actual bed) in the demo area. The user will enact the whole process as if being at home from the very start – sit/lie on the chair, insert the smartphone, wear the PillowVR, browse the content, pretend as if fallen to sleep, and assess the experience from the beginning to the very end (when one wakes up in the morning).",Interaction; Virtual Reality; VR Device,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Spatially Accurate Generative Music with AR Drawing,VRST - Virtual Reality Software and Technology,A,"Recent experiments in semi-automatically generating ambient music have yielded emotionally affecting results, leading scientists and musicians alike to develop and experiment with computational systems for creating audible art with varying degrees of success. Most of these systems are based either in analogue technology such as classic tape-reel recording systems or digital systems like virtual synthesizers triggered by a combination of developer-defined values and random number generation. In this paper, I outline the conceptual reasoning behind and development of one such generative music system which uses a simple but versatile virtual synthesizer to generate sound and sequences of repeating randomly generated notes drawn by the user in augmented reality to formulate the patterns and spatial origin of each sound contributing to the entire generative piece.",art; augmented reality; education; interactive; museum; music; visualization,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2019,Text Entry Method for Immersive Virtual Environments Using Curved Keyboard,VRST - Virtual Reality Software and Technology,A,"In this paper, we introduce a curved QWERTY keyboard, bent spherically in front of the user, to facilitate 3D word-gesture text entry in immersive virtual environments. Using the curved keyboard, the number of candidate words in the 3D word-gesture text entry is reduced compared with that using a planar keyboard. In the pilot study, the text entry performance of the first author was 21.0&nbsp;WPM (SD = 5.06), with a total error rate of 26.0% (SD = 15.2).",Curved surface; spherical surface; text entry; virtual reality; WPM,Keywords,TRUE,
Scopus,conferencePaper,2019,Virtual environment for processing medial axis representations of 3D nanoscale reconstructions of brain cellular structures,VRST - Virtual Reality Software and Technology,A,"We present a novel immersive environment for the interactive analysis of nanoscale cellular reconstructions of rodent brain samples acquired through electron microscopy. The system is focused on medial axis representations (skeletons) of branched and tubular structures of brain cells, and it is specifically designed for: i) effective semi-automatic creation of skeletons from surface-based representations of cells and structures ii) fast proofreading, i.e., correcting and editing of semi-automatically constructed skeleton representations, and iii) useful exploration, i.e., measuring, comparing, and analyzing geometric features related to cellular structures based on medial axis representations. The application runs in a standard PC-tethered virtual reality (VR) setup with a head mounted display (HMD), controllers, and tracking sensors. The system is currently used by neuroscientists for performing morphology studies on sparse reconstructions of glial cells and neurons extracted from a sample of the somatosensory cortex of a juvenile rat.",immersive neuroscience; nanoscale brain reconstruction,Abstract,TRUE,
Scopus,conferencePaper,2019,VR Minecraft for Art,VRST - Virtual Reality Software and Technology,A,"Art museums are becoming very boring to many people especially to the younger generation. The purpose of this project is to try to make a new type of art museum, one that is engaging and interactive. This project aims to answer the research question: Can a VR Minecraft museum enhance the user experience by giving them something that a typical art museum can’t? To answer this question, we’ve create a VR art museum within Minecraft and added features to make it more interactive and interesting. A more engaging environment is a great atmosphere to want to learn more. A scavenger hunt was added to the art museum to give the player a reason to walk around the entire museum. The player can build a sculpture after he/she completes the scavenger hunt which allows the player to be creative and imaginative. The user is then provided with different colored blocks to create a painting of his/her own. In order to answer the research question, we demoed the museum to a few people and then interviewed them. Their answers were very positive towards the VR Minecraft museum which leads me to believe that a VR Minecraft museum can indeed enhance the user experience.",art; interactive; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2020,DualVib: Simulating Haptic Sensation of Dynamic Mass by Combining Pseudo-Force and Texture Feedback,VRST - Virtual Reality Software and Technology,A,"We present DualVib, a compact handheld device that simulates the haptic sensation of manipulating dynamic mass; mass that causes haptic feedback as the user’s hand moves (e.g., shaking a jar and feeling coins rattling inside). Unlike other devices that require actual displacement of weight, DualVib dispenses with heavy and bulky mechanical structures and, instead, uses four vibration actuators. DualVib simulates a dynamic mass by simultaneously delivering two types of haptic feedback to the user’s hand: (1) pseudo-force feedback created by asymmetric vibrations that render the kinesthetic force arising from the moving mass; and (2) texture feedback through acoustic vibrations that render the object’s surface vibrations correlated with mass material properties. By means of our user study, we found out that DualVib allowed users to more effectively distinguish dynamic masses when compared to using either pseudo-force or texture feedback alone. We also report qualitative feedback from users who experienced five virtual reality applications with our device.",Haptics; Mass Perception; Vibration; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Docking Haptics: Extending the Reach of Haptics by Dynamic Combinations of Grounded and Worn Devices,VRST - Virtual Reality Software and Technology,A,"Grounded haptic devices can provide a variety of forces but have limited working volumes. Wearable haptic devices operate over a large volume but are relatively restricted in the types of stimuli they can generate. We propose the concept of docking haptics, in which different types of haptic devices are dynamically docked at run time. This creates a hybrid system, where the potential feedback depends on the user’s location. We show a prototype docking haptic workspace, combining a grounded six degree-of-freedom force feedback arm with a hand exoskeleton. We are able to create the sensation of weight on the hand when it is within reach of the grounded device, but away from the grounded device, hand-referenced force feedback is still available. A user study demonstrates that users can successfully discriminate weight when using docking haptics, but not with the exoskeleton alone. Such hybrid systems would be able to change configuration further, for example docking two grounded devices to a hand in order to deliver twice the force, or extend the working volume. We suggest that the docking haptics concept can thus extend the practical utility of haptics in user interfaces.",force feedback; haptics; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2020,VRSketchPen: Unconstrained Haptic Assistance for Sketching in Virtual 3D Environments,VRST - Virtual Reality Software and Technology,A,"Accurate sketching in virtual 3D environments is challenging due to aspects like limited depth perception or the absence of physical support. To address this issue, we propose VRSketchPen – a pen that uses two haptic modalities to support virtual sketching without constraining user actions: (1)&nbsp;pneumatic force feedback to simulate the contact pressure of the pen against virtual surfaces and (2)&nbsp;vibrotactile feedback to mimic textures while moving the pen over virtual surfaces. To evaluate VRSketchPen, we conducted a lab experiment with 20 participants to compare (1)&nbsp;pneumatic, (2)&nbsp;vibrotactile and (3)&nbsp;a combination of both with (4)&nbsp;snapping and no assistance for flat and curved surfaces in a 3D virtual environment. Our findings show that usage of pneumatic, vibrotactile and their combination significantly improves 2D shape accuracy and leads to diminished depth errors for flat and curved surfaces. Qualitative results indicate that users find the addition of unconstraining haptic feedback to significantly improve convenience, confidence and user experience.",3D User Interfaces; Haptics; Pneumatic Actuation; Sketching; Vibrotactile Actuation; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2020,The Impact of Missing Fingers in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Avatars in virtual reality (VR) can have body structures that differ from the physical self. Game designers, for example, often stylize virtual characters by reducing the number of fingers. Previous work found that the sensation of presence in VR depends on avatar realism and the number of limbs. However, it is currently unknown how the removal of individual fingers affects the VR experience, body perception, and how fingers are used instead. In a study with 24 participants, we investigate the effects of missing fingers and avatar realism on presence, phantom pain perception, and finger usage. Our results show that particularly missing index fingers decrease presence, show the highest phantom pain ratings, and significantly change hand interaction behavior. We found that relative usage of thumb and index fingers in contrast to middle, ring, and little finger usage was higher with abstract hands than with realistic ones – even when the fingers were missing. We assume that dominant fingers are firstly integrated into the own body schema when an avatar does not resemble one’s own appearance. We discuss cognitive mechanisms in experiencing virtual limb loss.",avatars; missing fingers; phantom pain; presence; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Moving Virtual Reality out of its Comfort Zone and Into the African Kalahari Desert Field: Experiences From Technological Co-Exploration With an Indigenous San Community in Namibia,VRST - Virtual Reality Software and Technology,A,"Indigenous people (IP) living in remote areas, at the margins of mainstream society, are often the last ones to experience emerging technologies and even less to shape those experiences. It could be argued technology exposure and experience is necessary for IP to gain agency in making informed decisions on the rejection or appropriation of novel technologies. In this paper, VR is introduced to a remote San community within a broader community-based research collaboration considering political and ethical perspectives of technology inclusion. The intent was to familiarise the community with the technology through the development and playthrough of a game, to explore future opportunities for joint co-designs of VR applications, meanwhile gauging the barriers for how VR operates outside of its intended setting. The community members expressed their excitement about the experience and the desire to re-create traditional San games in VR. The paper reflects on the community experiences, the setup and use of VR in remote settings, and the choices made to facilitate the familiarization of emerging technology.",Cultural Heritage; Indigenous Knowledge; Indigenous People; Namibia; San People; User Experiences; Virtual Reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2020,Investigating Immersive Virtual Reality as an Educational Tool for Quantum Computing,VRST - Virtual Reality Software and Technology,A,"Quantum computing (QC) is an intrinsically complex yet exciting discipline with increasing practical relevance. A deep understanding of QC requires the integration of knowledge across numerous technical fields, such as physics, computing and mathematics. This work aims to investigate how immersive Virtual Reality (VR) compares to a desktop environment (‘web-applet’) as an educational tool to help teach individuals QC fundamentals. We developed two interactive learning tutorials, one utilising the ‘Bloch sphere’ visualisation to represent a single-qubit system, and the other exploring multi-qubit systems through the lens of ‘quantum entanglement’. We evaluate the effectiveness of each medium to teach QC fundamentals in a user study with 24 participants. We find that the Bloch sphere visualisation was well-suited to VR over a desktop environment. Our results also indicate that mathematics literacy is an important factor in facilitating greater learning with this effect being notably more pronounced when using VR. However, VR did not significantly improve learning in a multi-qubit context. Our work provides valuable insights which contribute to the emerging field of Quantum HCI (QHCI) and VR for education.",Education; Learning; Quantum Computing; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Virtual Reality in Computer Science Education: A Systematic Review,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) technologies have become more affordable and accessible in recent years. This is opening up new methods and opportunities in the field of digital learning. VR can offer new forms of interactive learning and working, especially for subjects from the STEM (Science, technology, engineering, and mathematics) area. In this context we investigate the potential and application of VR for computer science education with a systematic review in this paper. We present a formal literature review on the use of VR technologies in computer science education. We focus on the identification of factors such as learning objectives, technologies used, interaction characteristics, and challenges and advantages of using fully immersive VR for computer science education.",Computer Science Education; Literature Review; Virtual Reality; VR,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Virtual Navigation considering User Workspace: Automatic and Manual Positioning before Teleportation,VRST - Virtual Reality Software and Technology,A,"Teleportation is a navigation technique widely used in virtual reality applications using head-mounted displays. Basic teleportation usually moves a user’s viewpoint to a new destination of the virtual environment without taking into account the physical space surrounding them. However, considering the user’s real workspace is crucial for preventing them from reaching its limits and thus managing direct access to multiple virtual objects. In this paper, we propose to display a virtual representation of the user’s real workspace before the teleportation, and compare manual and automatic techniques for positioning such a virtual workspace. For manual positioning, the user adjusts the position and orientation of their future virtual workspace. A first controlled experiment compared exocentric and egocentric manipulation techniques with different virtual workspace representations, including or not an avatar at the user’s future destination. Although exocentric and egocentric techniques result in a similar level of performance, representations with an avatar help the user to understand better how they will land after teleportation. For automatic positioning, the user selects their future virtual workspace among relevant options generated at runtime. A second controlled experiment shows that the manual technique selected from the first experiment and the automatic technique are more efficient than the basic teleportation. Besides, the manual technique seems to be more suitable for crowded scenes than the automatic one.",Locomotion; real workspace; spatial awareness.; teleportation; virtual object access; virtual workspace,Abstract,TRUE,
Scopus,conferencePaper,2020,Towards Physically Interactive Virtual Environments: Reactive Alignment with Redirected Walking,VRST - Virtual Reality Software and Technology,A,"Interactions with the physical environment, such as passive haptic feedback, have been previously shown to provide richer and more immersive virtual reality experiences. A strict correspondence between the virtual and real world coordinate systems is a staple requirement for physical interaction. However, many of the commonly employed VR locomotion techniques allow for, or even require, this relationship to change as the experience progresses. The outcome is that experience designers frequently have to choose between flexible locomotion or physical interactivity, as the two are often mutually exclusive. To address this limitation, this paper introduces reactive environmental alignment, a novel framework that leverages redirected walking techniques to achieve a desired configuration of the virtual and real world coordinate systems. This approach can transition the system from a misaligned state to an aligned state, thereby enabling the user to interact with physical proxy objects or passive haptic surfaces. Simulation-based experiments demonstrate the effectiveness of reactive alignment and provide insight into the mechanics and potential applications of the proposed algorithm. In the future, reactive environmental alignment can enhance the interactivity of virtual reality systems and inform new research vectors that combine redirected walking and passive haptics.",alignment; locomotion; redirected walking; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Evaluating Automatic Parameter Control Methods for Locomotion in Multiscale Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Virtual environments with a wide range of scales are becoming commonplace in Virtual Reality applications. Methods to control locomotion parameters can help users explore such environments more easily. For multi-scale virtual environments, point-and-teleport locomotion with a well-designed distance control method can enable mid-air teleportation, which makes it competitive to flying interfaces. Yet, automatic distance control for point-and-teleport has not been studied in such environments. We present a new method to automatically control the distance for point-and-teleport. In our first user study, we used a solar system environment to compare three methods: automatic distance control for point-and-teleport, manual distance control for point-and-teleport, and automatic speed control for flying. Results showed that automatic control significantly reduces overshoot compared with manual control for point-and-teleport, but the discontinuous nature of teleportation made users prefer flying with automatic speed control. We conducted a second study to compare automatic-speed-controlled flying and two versions of our teleportation method with automatic distance control, one incorporating optical flow cues. We found that point-and-teleport with optical flow cues and automatic distance control was more accurate than flying with automatic speed control, and both were equally preferred to point-and-teleport without the cues.",Automatic control; multiscale virtual environments; Point-and-teleport; VR navigation,Abstract,TRUE,
Scopus,conferencePaper,2020,Virtual Projection Planes for the Visual Comparison of Photogrammetric 3D Reconstructions with Photo Footage,VRST - Virtual Reality Software and Technology,A,"Image-based 3D reconstructions and their visualization in virtual reality promise novel opportunities to explore and analyze 3D reconstructions of real objects, buildings and places. However, the faithfulness of the presented data is not always obvious and, in most cases, a 3D reconstruction cannot be compared directly to its corresponding real world instance. However, in case of reconstruction methods based on structure from motion (SFM), a large number of raw photos is available. This motivated us to develop a novel interaction technique for the visual comparison of details of 3D models with projections of the corresponding image sections, e.g. in order to rapidly verify the authenticity of perceived features. The results of a formal user study (n=18) demonstrate the general usability of such visual provenance information as well as benefits of the comparison in vicinity of the features in question over a separate image gallery. Further observations informed our iterative design process and led to the development of an improved interactive visualization. Our final implementation provides a spatial and content-related overview while retaining the efficiency of the original approach.",image browsing; interaction design; magic lenses; spatially registered images; virtual reality; visual comparison,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Body LayARs: A Toolkit for Body-Based Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Technological advances are enabling a new class of augmented reality (AR) applications that use bodies as substrates for input and output. In contrast to sensing and augmenting objects, body-based AR applications track people around the user and layer information on them. However, prototyping such applications is complex, time-consuming, and cumbersome, due to a lack of easily accessible tooling and infrastructure. We present Body LayARs, a toolkit for fast development of body-based AR prototypes. Instead of directly programming for a device, Body LayARs provides an extensible graphical programming environment with a device-independent runtime abstraction. We focus on face-based experiences for headset AR, and show how Body LayARs makes a range of body-based AR applications fast and easy to prototype.",Augmented reality; body-based augmentation; toolkit,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,CorFix: Virtual Reality Cardiac Surgical Planning System for Designing Patient Specific Vascular Grafts,VRST - Virtual Reality Software and Technology,A,"Patients with single ventricle heart defect undergo Fontan surgery to reroute the blood flow from the lower body to the lung by connecting the inferior vena cava to the pulmonary artery using a vascular graft. Since each patient has an unique anatomical structure and blood flow dynamics, the graft design is a critical factor for maximizing the long-term survival rate of Fontan patients. Currently, designing and evaluating grafts involve computer aided design (CAD) and computational fluid dynamics (CFD) skills. CAD incorporates numerous tools for design but lacks depth perception, surgical features, and design parameters for creating vascular grafts while visualizing and modifying patient anatomies. These limitations may lead to long lead times, inconsistent workflow, and surgically infeasible graft designs. In this paper, we introduce a novel virtual reality vascular graft modeling software - CorFix, that provides solutions to these challenges. CorFix includes several visualization features for performing diagnostics and surgical features with design guidelines for creating patient specific tube-shaped grafts in 3D. The designed vascular graft can be exported into a 3D model, which can be utilized for performing computational fluid dynamic analysis and 3D printing. The patient specific vascular graft designs in CorFix were compared to an engineering CAD software, SolidWorks (Dassault Systèmes, Vélizy-Villacoublay, France), by 8 participants. Through all participants had only received one time 10-minute tutorial on CorFix, CorFix had a higher success rate and 3.4 times faster performance in designing surgically feasible grafts than CAD. CorFix also scored higher in usability and lower in perceived workload than CAD. CorFix may be the tool that can enable medical doctors without 3D modeling background to design patient specific grafts.",3D Modeling; Applications; Prototyping/Implementation; Usability Study; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,"A Call for Auditable Virtual, Augmented and Mixed Reality",VRST - Virtual Reality Software and Technology,A,"XR (Virtual, Augmented and Mixed Reality) technologies are growing in prominence. However, they are increasingly being used in sectors and in situations that can result in harms. As such, this paper argues the need for auditability to become a key consideration of XR systems. Auditability entails capturing information of a system’s operation to enable oversight, inspection or investigation. Things can and will go wrong, and information that helps unpack situations of failure or harm, and that enables accountability and recourse, will be crucial to XR’s adoption and acceptance. In drawing attention to the urgent need for auditability, we illustrate some risks associated with XR technology and their audit implications, and present some initial findings from a survey with developers indicating the current ‘haphazard’ approach towards such concerns. We also highlight some challenges and considerations of XR audit in practice, as well as areas of future work for taking this important area of research forward.",accountability; audit; responsibility; reviewability; transparency,Title_Abstract,TRUE,
Scopus,conferencePaper,2020,The Effects of Visual Realism on Spatial Memory and Exploration Patterns in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Understanding the effects of environmental features such as visual realism on spatial memory can inform a human-centered design of virtual environments. This paper investigates the effects of visual realism on object location memory in virtual reality, taking account of individual differences, gaze, and locomotion. Participants freely explored two environments which varied in visual realism, and then recalled the locations of objects by returning the misplaced objects back to original locations. Overall, we did not find a significant relationship between visual realism and object location memory. We found, however, that individual differences such as spatial ability and gender accounted for more variance than visual realism. Gaze and locomotion analysis suggest that participants exhibited longer gaze duration and more clustered movement patterns in the low realism condition. Preliminary inspection further found that locomotion hotspots coincided with objects that showed a significant gaze time difference between high and low visual realism levels. These results suggest that high visual realism still provides positive spatial learning affordances but the effects are more intricate.",Spatial ability; Spatial memory; Virtual reality; Visual fidelity; Visual realism,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Eye-Hand Coordination Training for Sports with Mid-air VR,VRST - Virtual Reality Software and Technology,A,"A relatively recent application area for Virtual Reality (VR) systems is sports training and user performance assessment. One of these applications is eye-hand coordination training systems (EHCTSs). Previous research identified that VR-based training systems have great potential for EHCTSs. While previous work investigated 3D targets on a 2D plane, here we aim to study full 3D movements and extend the application of throughput analysis to EHCTSs. We conducted two user studies to investigate how user performance is affected by different target arrangements, feedback conditions, and handedness in VR-based EHCTSs. In the first study, we explored handedness as well as vertical and horizontal target arrangements, and showed that user performance increases with the dominant hand and a vertical target plane. In the second study, we investigated different combinations of visual and haptic feedback and how they affect user performance with different target and cursor sizes. Results illustrate that haptic feedback did not increase user performance when it is added to visual feedback. Our results inform the creation of better EHCTSs with mid-air VR systems.",Fitts’ task; haptic feedback; mid-air interaction; performance assessment; reaction test; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Empirical Evaluation of Gaze-enhanced Menus in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Many user interfaces involve attention shifts between primary and secondary tasks, e.g., when changing a mode in a menu, which detracts the user from their main task. In this work, we investigate how eye gaze input affords exploiting the attention shifts to enhance the interaction with handheld menus. We assess three techniques for menu selection: dwell time, gaze button, and cursor. Each represents a different multimodal balance between gaze and manual input. We present a user study that compares the techniques against two manual baselines (dunk brush, pointer) in a compound colour selection and line drawing task. We show that user performance with the gaze techniques is comparable to pointer-based menu selection, with less physical effort. Furthermore, we provide an analysis of the trade-off as each technique strives for a unique balance between temporal, manual, and visual interaction properties. Our research points to new opportunities for integrating multimodal gaze in menus and bimanual interfaces in 3D environments.",Design; Gaze; Manual input; Menu; Pointing; Virtual Reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2020,Temporal Consistent Motion Parallax for Omnidirectional Stereo Panorama Video,VRST - Virtual Reality Software and Technology,A,"We present a new pipeline to enable head-motion parallax in omnidirectional stereo (ODS) panorama video rendering using a neural depth decoder. While recent ODS panorama cameras record short-baseline horizontal stereo parallax to offer the impression of binocular depth, they do not support the necessary translational degrees-of-freedom (DoF) to also provide for head-motion parallax in virtual reality (VR) applications. To overcome this limitation, we propose a pipeline that enhances the classical ODS panorama format with 6 DoF free-viewpoint rendering by decomposing the scene into a multi-layer mesh representation. Given a spherical stereo panorama video, we use the horizontal disparity to store explicit depth information for both eyes in a simple neural decoder architecture. While this approach produces reasonable results for individual frames, video rendering usually suffers from temporal depth inconsistencies. Thus, we perform successive optimization to improve temporal consistency by fine-tuning our depth decoder for both temporal and spatial smoothness. Using a consumer-grade ODS camera, we evaluate our approach on a number of real-world scene recordings and demonstrate the versatility and robustness of the proposed pipeline.",neural network; rendering; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Capture to Rendering Pipeline for Generating Dynamically Relightable Virtual Objects with Handheld RGB-D Cameras,VRST - Virtual Reality Software and Technology,A,"We present a complete end-to-end pipeline for generating dynamically relightable virtual objects captured using a single handheld consumer-grade RGB-D camera. The proposed system plausibly replicates the geometry, texture, illumination, and surface reflectance properties of non-Lambertian objects, making them suitable for integration within virtual reality scenes that contain arbitrary illumination. First, the geometry of the target object is reconstructed from depth images captured using a handheld camera. To get nearly drift-free texture maps of the virtual object, a set of selected images from the original color stream is used for camera pose optimization. Our approach further separates these images into diffuse (view-independent) and specular (view-dependent) components using low-rank decomposition. The lighting conditions during capture and reflectance properties of the virtual object are subsequently estimated from the computed specular maps. By combining these parameters with the diffuse texture, the reconstructed model can then be rendered in real-time virtual reality scenes that plausibly replicate real world illumination at the point of capture. Furthermore, these objects can interact with arbitrary virtual lights that vary in direction, intensity, and color.",content creation; reconstruction; scanning; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Investigating Effects and User Preferences of Extra- and Intradiegetic Virtual Reality Questionnaires,VRST - Virtual Reality Software and Technology,A,"Virtual realities (VR) are becoming an integral part of product development across many industries, for example to assess aesthetics and usability of new features in the automotive industry. The recording of the evaluation is typically conducted by filling out questionnaires after the study participants left the virtual environment. In this paper, we investigate how questionnaires can be best embedded within the virtual environment and compare how VR-questionnaires differ from classical post-test evaluations regarding preference, presence, and questionnaire completion time. In the first study (N = 11), experts rated four design concepts of questionnaires embedded in VR, of which two were designed as extradiegetic and two as intradiegetic user interfaces. We show that intradiegetic UIs have a significantly higher perceived user experience and presence while the usability remains similar. Intradiegetic UIs are preferred by the majority. Based on these findings, we compared intradiegetic VR-questionnaires with paper-based evaluations in a follow up study (N = 24). 67% of the participants preferred the evaluation in VR, even though it takes significantly longer. We found no effect on presence.",Evaluation; INVR- questionnaires; Presence; User Interface (UI); Virtual Reality (VR),Title_Keywords,TRUE,
Scopus,conferencePaper,2020,Effects of Immersive Virtual Reality Content Type to Mindfulness and Physiological Parameters,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) has been applied as a complimentary way to conventional treatment for mental disorders successfully. On the other hand, it has not been clearly shown what type of immersive media such as VR can directly affect one’s physiological parameters, associated with the state of mindfulness. We sought to assess how being subjected to differently designed VR contents can affect and modulate one’s anxiety both psychologically and more importantly physiologically. We empirically tested the comparative effects of two polarizing VR content types to this effect: (1) “calm/soothing” content and (2) “disturbing”. Twenty-five adults participated and their mental state, anxiety level and physiological signals were measured before and after experiencing the respective VR content type. The experiment found a statistically significant effect of the content type to the changes in these measures and confirmed that the “calm” content was helpful for one to self-regulate to lower heart rate and blood pressure, stable GSR, and the “disturbing” content in the opposite way. We applied this result to calm down and stabilize vital signs of patients during actual coronary angiography and catheterization operations. We were able to observe the same effect with positive comments from the patients and operating team.",Blood Pressure; Galvanic Skin Response (Skin Conductance); Haptic feedback; Heart Rate; Nervous System; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Anonymity vs. Familiarity: Self-Disclosure and Privacy in Social Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Understanding how and why users reveal information about their self in online social spaces and what they perceive as privacy online is a central research agenda in HCI. Drawing on 30 in-depth interviews, in this paper we focus on what type of information users disclose, to whom they reveal information, and concerns they had regarding self-disclosure in social Virtual Reality (VR) - where multiple users can interact with one another through VR head-mounted displays in 3D virtual spaces. Our findings show that overall, users felt comfortable to disclose their emotions, personal experience, and personal information in social VR. However, they also acknowledged that disclosing personal information in social VR was an inevitable trade-off: giving up bio-metric information in order to better use the system. We contribute to existing literature on self-disclosure and privacy online by focusing on social VR as an emerging novel online social space. We also explicate implications for designing and developing future social VR applications.",digital privacy; online social interaction; self-disclosure; social virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,A Quest for Co-Located Mixed Reality: Aligning and Assessing SLAM Tracking for Same-Space Multi-User Experiences,VRST - Virtual Reality Software and Technology,A,"Current solutions for creating co-located Mixed Reality (MR) experiences typically rely on platform-specific synchronisation of spatial anchors or Simultaneous Localisation and Mapping (SLAM) data across clients, often coupled to cloud services. This introduces significant costs (in development and deployment), constraints (with interoperability across platforms often limited), and privacy concerns. For practitioners, support is needed for creating platform-agnostic co-located MR experiences. This paper explores the utility of aligned SLAM solutions by 1) surveying approaches toward aligning disparate device coordinate spaces, formalizing their theoretical accuracy and limitations; 2) providing skeleton implementations for audience-based, small-scale and large-scale co-location using said alignment approaches; and 3) detailing how we can assess the accuracy and safety of 6DoF/SLAM tracking solutions for any arbitrary device and dynamic environment without the need for an expensive ground truth optical tracking, by using trilateration and a $30 laser distance meter. Through this, we hope to further democratise the creation of cross-platform co-located MR experiences.",AR; Co-location; Mixed Reality; Multi-User; SLAM; VR,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,The Effects of Self- and External Perception of Avatars on Cognitive Task Performance in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) allows embodying any possible avatar. Known as the Proteus effect, avatars can change users’ behavior and attitudes. Previous work found that embodying Albert Einstein can increase cognitive task performance. The behavioral confirmation paradigm, however, predicts that our behavior is also affected by others’ perception of us. Therefore, we investigated the cognitive performance in collaborative VR when self-perception and external perception of the own avatar differ. 32 male participants performed a Tower of London task in pairs. One participant embodied Einstein or a young adult while the other perceived the participant as Einstein or a young adult. We show that the perception by others affects cognitive performance. The Einstein avatar also decreased the perceived workload. Results imply that avatars’ appearance to both, the user and the others must be considered when designing for cognitively demanding tasks.",avatar embodiment; body ownership; cognitive performance; Proteus effect; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,The Effects of Full-Body Avatar Movement Predictions in Virtual Reality using Neural Networks,VRST - Virtual Reality Software and Technology,A,"Motion tracking technologies and avatars in virtual reality (VR) showing the movements of the own body enable high levels of presence and a strong illusion of body ownership (IBO) – key features of immersive systems and gaming experiences in virtual environments. Previous work suggests using software-based algorithms that can not only compensate system latency but also predict future movements of the user to increase input performance. However, the effects of movement prediction in VR on input performance are largely unknown. In this paper, we investigate neural network-based predictions of full-body avatar movements in two scenarios: In the first study, we used a standardized 2D Fitts’ Law task to examine the information throughput in VR. In the second study, we utilized a full-body VR game to determine the users’ performance. We found that both performance and subjective measures in a standardized 2D Fitts’ law task could not benefit from the predicted avatar movements. In an immersive gaming scenario, however, the perceived accuracy of the own body location improved. Presence and body assessments remained more stable and were higher than during the Fitts’ task. We conclude that machine-learning-based predictions could be used to compensate system-related latency but participants only subjectively benefit under certain conditions.",avatars; movement prediction; neural networks.; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Realistic Virtual Humans from Smartphone Videos,VRST - Virtual Reality Software and Technology,A,"This paper introduces an automated 3D-reconstruction method for generating high-quality virtual humans from monocular smartphone cameras. The input of our approach are two video clips, one capturing the whole body and the other providing detailed close-ups of head and face. Optical flow analysis and sharpness estimation select individual frames, from which two dense point clouds for the body and head are computed using multi-view reconstruction. Automatically detected landmarks guide the fitting of a virtual human body template to these point clouds, thereby reconstructing the geometry. A graph-cut stitching approach reconstructs a detailed texture. Our results are compared to existing low-cost monocular approaches as well as to expensive multi-camera scan rigs. We achieve visually convincing reconstructions that are almost on par with complex camera rigs while surpassing similar low-cost approaches. The generated high-quality avatars are ready to be processed, animated, and rendered by standard XR simulation and game engines such as Unreal or Unity.",3D Reconstruction; Avatars; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2020,The Experience of Social Touch in Multi-User Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We present user study results on virtual body contact experience in a two-user VR scenario, in which participants performed different touches with a research assistant. The interaction evoked different emotional reactions in perceived relaxation, happiness, desire, anxiety, disgust, and fear. Congruent to physical social touch, the evaluation of virtual body contact was modulated by intimacy, touch direction, and sex. Further, individual comfort with interpersonal touch was positively associated with perceived relaxation and happiness. We discuss the results regarding implications for follow-up studies and infer implications for the use of social touch in social VR applications.",mediated social touch; multi-user VR; social touch; social VR; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2020,Virtual Humans in AR: Evaluation of Presentation Concepts in an Industrial Assistance Use Case,VRST - Virtual Reality Software and Technology,A,"Embedding virtual humans in educational settings enables the transfer of the approved concepts of learning by observation and imitation of experts to extended reality scenarios. Whilst various presentation concepts of virtual humans for learning have been investigated in sports and rehabilitation, little is known regarding industrial use cases. In prior work on manual assembly, Lampen et al.&nbsp;[21] show that three-dimensional (3D) registered virtual humans can provide assistance as effective as state-of-the-art HMD-based AR approaches. We extend this work by conducting a comparative user study (N=30) to verify implementation costs of assistive behavior features and 3D registration. The results reveal that the basic concept of a 3D registered virtual human is limited and comparable to a two-dimensional screen aligned presentation. However, by incorporating additional assistive behaviors, the 3D assistance concept is enhanced and shows significant advantages in terms of cognitive savings and reduced errors. Thus, it can be concluded, that this presentation concept is valuable in situations where time is less crucial, e.g. in learning scenarios or during complex tasks.",Augmented Reality; Expert-Based Learning; Virtual Human,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Don’t Bother Me: How to Handle Content-Irrelevant Objects in Handheld Augmented Reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we applied the concept of diminished reality to remove content-irrelevant pedestrian (i.e., real object) in the context of handheld augmented reality (AR). We prepared three view conditions: in Transparent (TP) condition, we removed the pedestrian entirely; in Semi-transparent (STP) condition, the pedestrian became semi-transparent; lastly, in Default (DF) condition, the pedestrian appeared as is. We conducted a user study to compare the effects of the three conditions on users’ engagement and perception of a virtual pet in the AR content. Our findings revealed that users felt less distracted to the AR content in TP and STP conditions, compared to the DF condition. Furthermore, users felt the virtual pet as more life-like, its behavior more plausible, and felt a higher spatial presence in the real environment, in the TP condition.",Diminished reality; Online user study; Perceptual issue,Title_Abstract,TRUE,
Scopus,conferencePaper,2020,Bacterial Load of Virtual Reality Headsets,VRST - Virtual Reality Software and Technology,A,"As commodity virtual reality (VR) systems become more common, they are rapidly gaining popularity for entertainment, education, and training purposes. VR utilizes headsets which come in contact with or close proximity to the user’s eyes, nose, and forehead. In this study, the potential for these headsets to become contaminated with bacteria was analyzed. To the best of our knowledge, this study is the first to address the potential for microorganisms to be transmitted via VR headsets. The data discussed herein were collected roughly one year prior to the outbreak of the COVID-19 pandemic in the United States. We feel it is important to be clear that this study focuses exclusively on bacteria, as opposed to viruses like those responsible for the present pandemic. The nosepieces and foreheads of two HTC Vive headsets were sampled over the course of a seven-week period in a VR software development course. Serial dilutions were performed, and samples were plated on various culture media. Following incubation, counts of bacteria were determined. DNA was extracted from bacterial colonies and the 16S rRNA gene was sequenced to identify bacterial contaminates present on the headsets. Chief among these contaminates was Staphylococcus aureus. The results of these tests indicated that the Staphylococcus aureus strains isolated from the headsets possessed high levels of antibiotic resistance. Other notable bacterial isolates included Moraxella osloensis, the bacteria responsible for foul odors in laundry and, Micrococcus luteus, a communalistic bacterial species capable of causing opportunistic infections. Other bacterial isolates were detected in variable amounts throughout the trial.",bacteria; hygine; pathogen; sanitation; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Dynamic Projection Mapping of Deformable Stretchable Materials,VRST - Virtual Reality Software and Technology,A,"We present a method for dynamic projection mapping on deformable, stretchable and elastic materials (e.g. cloth) using a time of flight (ToF) depth camera (e.g. Azure Kinect or Pico-Flexx) that come equipped with an IR camera. We use Bezier surfaces to model the projection surface without explicitly modeling the deformation. We devise an efficient tracking method that tracks the boundary of the surface material using the IR-Depth camera. This achieves realistic mapping even in the interior of the surface, with simple markers (e.g. black dots or squares) or without markers entirely, such that the projection appears to be printed on the material. The surface representation is updated in real-time using GPU based computations. Further, we also show that the speed of these updates is limited by the camera frame rate and therefore can be adopted for higher speed cameras as well. This technique can be used to project on several stretchable moving materials to change their appearance.",Appearance Editing; Deformable Materials; Dynamic Projection Mapping; Spatially Augmented Reality,Keywords,TRUE,
Scopus,conferencePaper,2020,AffectivelyVR: Towards VR Personalized Emotion Recognition,VRST - Virtual Reality Software and Technology,A,"We present AffectivelyVR, a personalized real-time emotion recognition system in Virtual Reality (VR) that enables an emotion-adaptive virtual environment. We used off-the-shelf Electroencephalogram (EEG) and Galvanic Skin Response (GSR) physiological sensors to train user-specific machine learning models while exposing users to affective 360° VR videos. Since emotions are largely dependent on interpersonal experiences and expressed in different ways for different people, we personalize the model instead of generalizing it. By doing this, we achieved an emotion recognition rate of 96.5% using the personalized KNN algorithm, and 83.7% using the generalized SVM algorithm.",EEG; Emotion Recognition; GSR; Machine Learning; Personalized; Physiology; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,An Examination of Effects of Fear Using VR Content on Time Estimation,VRST - Virtual Reality Software and Technology,A,"In recent years, the use of virtual reality (VR) attractions has been increasing in amusement facilities with the spread of head-mounted displays (HMDs) and the increase of VR content. VR attractions require less physical space than conventional attractions. However, because it takes a considerable time to adjust an HMD one by one, not many customers can play the attractions. In order to provide VR content with a long subjective time in a short time, this study presents VR content on an HMD that can evoke fear. Fear is one of the factors that influence psychological time. We investigate whether the VR content with fear influences time estimation.",fear; time estimation; VR,Abstract,TRUE,
Scopus,conferencePaper,2020,An Examination of Influence on Weight Perception by Visual Change of the Own Arm,VRST - Virtual Reality Software and Technology,A,"The purpose of this research is to make a change in weight perception by utilizing the potential impression that a person has acquired from visual information. Recently, the illusion caused by changes in visual information, which is represented by the rubber hand illusion and the Proteus effect, has been reported. In this research, we make one’s self-awareness change by the appearance of the own arm through AR technology. We conduct an experiment to verify that the change of the self-awareness influences the weight perception of grasping an actual object.",augmented reality; visual perception; VR psychology,Keywords,TRUE,
Scopus,conferencePaper,2020,An Immersive Decision Support System for Disaster Response,VRST - Virtual Reality Software and Technology,A,"This project introduces GeospatialVR, an open-source collaborative virtual reality framework to dynamically create 3D real-world environments that can be accessed via desktop and mobile devices as well as virtual and augmented reality headsets. The framework can generate realistic simulations of desired locations entailing the terrain, elevation model, infrastructures (e.g. buildings, roads, bridges), dynamic visualizations (e.g. water and fire simulation), and information layers (e.g. disaster damages and extent, sensor readings, surveillance data, occupancy, traffic, weather). The framework incorporates multiuser support to allow stakeholders to remotely work on the same VR environment, and thus, presenting the potential to be utilized as a virtual incident command platform or meeting room. To demonstrate the framework's usability and benefits, several case studies have been developed for flooding, wildfire, transportation, and active shooter response.",decision-support systems; environmental management; geospatial visualization; virtual reality; web-based interaction,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,An Open Framework for Infinite Walking With Saccadic Redirection,VRST - Virtual Reality Software and Technology,A,"In this project we created an expandable framework for allowing infinite walking in virtual reality in a closed play area. A saccade is a rapid eye movement with a unique property: the eye temporarily gathers reduced information – saccadic suppression. We leverage the suppression to redirect the user’s walking towards the center of the play area by rotating the virtual world around the camera’s location. With the VR environment and corresponding pre and post experience questions we could already show an improvement in understanding on a set of participants. Modern VR hardware such as the Vive Eye Pro allows a reasonable sample rate of eye movement measurements. A self-developed VR testing environment was used and with corresponding pre and post experience questions we tested a group of participants regarding general motion and VR- sickness parameters. We found a certain angle for the maximum saccade rotation which was base of further testing. We found, that our framework and the default settings successfully allow saccadic redirection with only marginal discomfort for the users.",eye tracking; mobility in virtual reality; virtual reality sickness,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Analyzing the Trade-off between Selection and Navigation in VR,VRST - Virtual Reality Software and Technology,A,"Navigation and selection are critical in very large virtual environments, such as a model of a whole city. In practice, many VR applications require both of these modalities to work together. We compare different combinations of two navigation and two selection methods in VR on selection tasks involving distant targets in a user study. The aim of our work is to discover the trade-off between navigation and selection techniques and to identify which combination leads to better interaction performance in large virtual environments. The results showed that users could complete the task faster with the fly/drive method and traveled less, compared to the teleportation method. Additionally, raycasting exhibited a better performance in terms of time and (less) distance traveled, however, it significantly increased the error rate for the selection of targets.",Interaction Design; Navigation; Selection; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2020,ARLS: An asymmetrical remote learning system for sharing anatomy between an HMD and a light field display,VRST - Virtual Reality Software and Technology,A,"VR remote learning is an environment-friendly approach for anatomy learning compared with the paper-based, physical model or prepared specimens. However, VR devices can potentially be costly pieces of equipment, and inexperienced users can experience unwanted symptoms like motion sickness. To solve these problems, an asymmetrical system has been developed to connect an experienced head-mounted-display user (lecturer) with light field display users (students) through the Internet. The scenes of lecturer’s end and students’ end are adjusted to match the corresponding displays technologies.",education; light field display; remote learning; scientific visualization; Virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2020,ARtist: Interactive Augmented Reality for Curating Children's Artworks,VRST - Virtual Reality Software and Technology,A,"ARtist is a mobile app that allows children to curate, display, and document their artworks through mobile augmented reality technology. This application aims to improve the traditional art display environment with augmented reality technology which enables users to utilize virtual space freely with interactive assets. It allows users to upload images of their artworks and decorate them with provided 3D frames and pedestals. Users can place their artworks and modify them in the augmented environment. ARtist is designed for children through a user-centered design process and developed using Unity and Google ARcore.",Art Education; Augmented Reality; Mobile Augmented Reality; Virtual Art Exhibition,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Assessing the Suitability and Effectiveness of Mixed Reality Interfaces for Accurate Robot Teleoperation,VRST - Virtual Reality Software and Technology,A,"In this work, a Mixed Reality (MR) system is evaluated to assess whether it can be efficiently used in teleoperation tasks that require an accurate control of the robot end-effector. The robot and its local environment are captured using multiple RGB-D cameras, and a remote user controls the robot arm motion through Virtual Reality (VR) controllers. The captured data is streamed through the network and reconstructed in 3D, allowing the remote user to monitor the state of execution in real time through a VR headset. We compared our method with two other interfaces: i) teleoperation in pure VR, with the robot model rendered with the real joint states, and ii) teleoperation in MR, with the rendered model of the robot superimposed on the actual point cloud data. Preliminary results indicate that the virtual robot visualization is better than the pure point cloud for accurate teleoperation of a robot arm.",Mixed Reality; Robot Teleoperation; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,"ATOM: HMD-VR Interface to Learn Atomic Structure, Bonding and Historical Research Experiments.",VRST - Virtual Reality Software and Technology,A,"Head-Mounted Displays (HMDs) based Virtual Reality (VR) has shown promising results in training and education. We present ATOM, an HMD-VR interface to educate students about atoms, atomic structures and historical research experiments conducted in understanding atomic structures. ATOM is designed to complement the classroom learning for grade 9 students through an interactive and practice-based learning experience. Preliminary evaluation with 10 students revealed higher interest, increase engagement and playfulness. The students also pointed out a few difficult user interactions.",Atomic structures; Chemistry education; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Augmented Reality World Editor,VRST - Virtual Reality Software and Technology,A,"Image inpainting allows for filling masked areas of an image with synthesized content that is indistinguishable from its environment. We present a video inpainting pipeline that enables users to “erase” physical objects in their environment using a mobile device. The pipeline includes an augmented reality application and an on-device conditional adversarial model for generating the inpainted textures. Users are able to interactively remove clutter in their physical space in realtime. The pipeline preserves frame to frame coherence, even with camera movements, using the Google ARCore SDK.",Augmented Reality; Conditional GANs; Video Inpainting,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,CasualVRVideos: VR videos from casual stationary videos,VRST - Virtual Reality Software and Technology,A,"Thanks to the ubiquity of devices capable of recording and playing back video, the amount of video files is growing at a rapid rate. Most of us have now video recordings of major events in our lives. However, until today, these videos are captured mainly in 2D and are mostly used for screen-based video replay. Currently there is no way for watching them in more immersive environments such as on a VR headset. They are simply not optimized for playback in stereoscopic displays or even tracked Virtual Reality devices. In this work, we present CasualVRVideos, a first approach that works towards solving these issues by extracting spatial information from video footage recorded in 2D, so that it can later be played back in VR displays to increase the immersion. We focus in particular on the challenging scenario when the camera itself is not moving.",content creation; single view geometry; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Computing Object Selection Difficulty in VR using Run-time Contextual Analysis,VRST - Virtual Reality Software and Technology,A,"This paper introduces a method for computing the difficulty of selection tasks in virtual environments using pointing metaphors by operationalizing an established human motor behavior model. In contrast to previous work, the difficulty is calculated automatically at run-time for arbitrary environments. We present and provide the implementation of our method within Unity 3D. The difficulty is computed based on a contextual analysis of spatial boundary conditions, i.e., target object size and shape, distance to the user, and occlusion. We believe our method will enable developers to build adaptive systems that automatically equip the user with the most appropriate selection technique according to the context. Further, it provides a standard metric to better evaluate and compare different selection techniques.",3DUI; Application; Performance Model; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2020,Creating a Smart Virtual-Reality based Contextual Diary for People with Persistent Postural-Perceptual Dizziness to Facilitate Habituation,VRST - Virtual Reality Software and Technology,A,"Postural-Perceptual Dizziness (PPPD) has variable levels of severity and triggers. Hence the use of an e-diary to capture triggers could be useful for both the patient and treating clinician. Virtual reality (VR) is not new to health sciences. This paper proposes a strategy that by using immersive VR environments at home, the technology could facilitate the user to identify baseline symptoms and record in an in-built virtual-reality based contextual diary (e-diary), plus an ability to alter the virtual environments to assess triggers, habituation of triggers and also treatment improvements. We discuss the type of VR designs that could be useful to incorporate a PPPD e-diary from the perspective of a VR designer. We also consider the development of the virtual reality environment that could be paired with e-diary responses.",artifical intelligence; habituation; persistent postural-perceptual dizziness; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Creating AR Applications for the IOT : a New Pipeline,VRST - Virtual Reality Software and Technology,A,"Prototyping Augmented Reality (AR) applications for smart environments is still a difficult task. Therefore, we propose a pipeline to help designers and developers to create AR applications for monitoring and controlling indoor environments equipped with connected objects. This pipeline starts with the capture (geometry and objects) of the real environment with an AR device. Then, it proposes a Virtual Reality (VR) tool to configure augmentations in this captured environment. This tool includes a feature to simulate AR devices to help anticipate the application’s rendering on real devices. The created application can then be seamlessly deployed on various AR devices including smartphones,tablets and glasses.",Augmented Reality; IoT; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Dual Phone AR: Exploring the use of Phones as Controllers for Mobile Augmented Reality,VRST - Virtual Reality Software and Technology,A,"The possible interactions with Mobile Augmented Reality applications today are largely limited to on-screen gestures and spatial movement. There is an opportunity to design new interaction methods that address common issues and go beyond the screen. Through this project, we explore the idea of using a second phone as a controller for mobile AR experiences. We develop prototypes that demonstrate the use of a second phone controller for tasks such as pointing, selecting, and drawing in 3D space. We use these prototypes and insights from initial remote evaluations to discuss the benefits and drawbacks of such an interaction method. We conclude by outlining opportunities for future research on Dual Phone AR for multiple usage configurations, and in collaborative settings.",Augmented Reality; Cross-Device Computing; Mobile Interaction,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Evaluation of Headset-based Viewing and Desktop-based Viewing of Remote Lectures in a Social VR Platform,VRST - Virtual Reality Software and Technology,A,"We study experiences of students attending classes remotely from home using a social VR platform, considering both desktop-based and headset-based viewing of remote lectures. Ratings varied widely. Headset viewing produced higher presence overall. Strong negative correlations between headset simulator sickness symptoms and overall experience ratings, and some other ratings, suggest that the headset experience was much better for comfortable users than for others. Reduced sickness symptoms, and no similar correlations, were found for desktop viewing. Desktop viewing appears to be a good alternative for students not comfortable with headsets. Future VR systems are expected to provide more stable and comfortable visuals, providing benefits to more users.",COVID-19; distance learning; educational VR; Mozilla Hubs; remote instruction; SARS-CoV-2; teleconferencing; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2020,HexTouch: Affective Robot Touch for Complementary Interactions to Companion Agents in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"There is a growing need for social interaction in Virtual Reality (VR). Current social VR applications enable human-agent or interpersonal communication, usually by means of visual and audio cues. Touch, which is also an essential method for affective communication, has not received as much attention. To address this, we introduce HexTouch, a forearm-mounted robot that performs touch behaviors in sync with the behaviors of a companion agent, to complement visual and auditory feedback in virtual reality. The robot consists of four robotic tactors driven by servo motors, which render specific tactile patterns to communicate primary emotions (fear, happiness, disgust, anger, and sympathy). We demonstrate HexTouch through a VR game with physical-virtual agent interactions that facilitate the player-companion relationship and increase the immersion of the VR experience. The player will receive affective haptic cues while collaborating with the agent to complete the mission in the game. The multisensory system for affective communication also has the potential to enhance sociality in the virtual world.",Emotion Communication; Expressive Robotics; Haptics; Physical Contact; Virtual Reality; Wearable,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,HMDPose: A large-scale trinocular IR Augmented Reality Glasses Pose Dataset,VRST - Virtual Reality Software and Technology,A,"Augmented Reality Glasses usually implement an Inside-Out tracking. In case of a driving scenario or glasses with less computation capabilities, an Outside-In tracking approach is required. However, to the best of our knowledge, no public datasets exist that collects images of users wearing AR glasses. To address this problem, we present HMDPose, an infrared trinocular dataset of four different AR Head-mounted displays captured in a car. It contains sequences of 14 subjects captured by three different cameras running at 60 FPS each, adding up to more than 3,000,000 labeled images in total. We provide a ground truth 6DoF-pose, captured by a submillimeter accurate marker-based tracker. We make HMDPose publicly available for non-profit, academic use and non-commercial benchmarking on ags.cs.uni-kl.de/datasets/hmdpose/.",AR Glasses; Dataset; Deep Learning; Object Pose Estimation; Tracking,Title_Abstract,TRUE,
Scopus,conferencePaper,2020,Human Following Behavior In Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Redirected Walking (RDW) allows users to perform real walking in virtual worlds that are larger than the available physical space. Many RDW algorithms rely on the prediction of users’ possible paths in the virtual environment (VE) to calculate where users should be redirected to. This prediction could be obtained from the structure of the VE, where users look, or from existing path models. In this work, we examine users’ walking behaviors in the presence of a virtual agent acting as a tour guide. Results showed that users changed their speed significantly to match the agent’s walking speed. Furthermore, users also tend to adapt their trajectories to match with the agent’s path.",following behaviour; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2020,Impact of Social Distancing to Mitigate the Spread of COVID-19 in a Virtual Environment,VRST - Virtual Reality Software and Technology,A,"A novel strand of Coronavirus has spread in the past months to the point of becoming a pandemic of massive proportions. In order to mitigate the spread of this disease, many different policies have been adopted, including a strict national lockdown in some countries or milder government policies: one common aspect is that they mostly rely around keeping distance between individuals. The aim of this work is to provide means of visualizing the impact of social distancing in an immersive environment by making use of the virtual reality technology. To this aim, we create a virtual environment which resembles a university setting (we based it on the University of Derby), and populate it with a number of AI agents. We assume that the minimum social distance is 2 meters. The main contribution of this work is twofold: the multi-disciplinary approach that results from visualizing the social distancing in an effort to mitigate the spread of the COVID-19, and the digital twin application in which the users can navigate the virtual environment whilst receiving visual feedback in the proximity of other agents. We named our application SoDAlVR, which stands for Social Distancing Algorithm in Virtual Reality.",Digital Twin; Prototyping/Implementation; Simulation.; Usability Study; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Inconsistencies of Presence Questionnaires in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Presence in virtual reality (VR) is typically assessed through questionnaires in the real world and after leaving an immersive experience. Previous research suggests that questionnaires in VR reduce biases caused by the real-world setup. However, it remains unclear whether presence questionnaires still provide valid results when subjects are being surveyed while the construct is perceived. In a user study with 36 participants, two standardized presence questionnaires (IPQ, SUSa) were either completed in the real lab, in a virtual lab scene, or in the actual scene after a virtual gaming experience. Our results show inconsistencies between the measurements and that main scores, as well as subscales of the presence measures are significantly affected by the subjects’ environment. As presence questionnaires have been designed to be answered after an immersive experience, we recommend revising those tools for measuring presence in VR.",break in presence.; presence; questionnaire; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,"Introduction to AR-Bot, an AR system for robot navigation",VRST - Virtual Reality Software and Technology,A,"We introduce a system to assign navigation tasks to a self-moving robot using an Augmented Reality (AR) application running on a smartphone. The system relies on a robot controller and a central server hosted on a PC. The user points at a target location in the phone camera view and the robot moves accordingly. The robot and the phone are independently located in the 3D space thanks to registration methods running on the server, hence they do not need to be spatially registered to each other nor in direct line of sight.",Augmented Reality; Registration; Relocalization; Robot navigation; User interface,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Life-size Sequential Photography in a Mixed Reality Environment,VRST - Virtual Reality Software and Technology,A,"We visualize life-size sequential photographs of sports activities in a mixed reality (MR) environment. Wearing a video-see-through head-mounted display, an observer records the motions of a player using a handheld camera. Our system then places billboards in the three-dimensional MR space, on which the sequential photographs of the player’s motion are presented at life-size. In a user study, we found that the observers perceived the size of the motions more accurately than when viewing sequential photographs on a monitor display.",Life-size Sequential Photography; Mixed Reality; sports motion,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Mediated-Timescale Learning: Manipulating Timescales in Virtual Reality to Improve Real-World Tennis Forehand Volley,VRST - Virtual Reality Software and Technology,A,"In tennis training, beginner players can fail to return the ball when the ball moves faster than what they can react to. In this paper, we propose a new training process of mediated-timescale learning (MTL) to manipulate the incoming ball’s motion. The ball first moves in slow motion, allowing more time for players to react and develop skills. The ball then moves in faster motion, challenging players with improved skills. To evaluate MTL, we implemented it in a virtual reality (VR)-oriented tennis training system. We piloted the MTL implementations (N = 12) to study players’ physical enjoyment. We then conducted an efficacy study (N = 8) to evaluate MTL’s training effects on player’s real-world performance. We found that in comparison to real-world training, five participants improved more in hitting the sweet spot after training with MTL.",Tennis training; timescales; training effects; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Metachron: A framework for time perception research in VR,VRST - Virtual Reality Software and Technology,A,"The perception of time is closely related to our well-being. Psycho-pathological conditions such as depression, schizophrenia and autism are often linked to a disturbed sense of time. In this paper we present a novel framework called Metachron, which is intended to support research in the field of time perception and manipulation in Virtual Reality (VR). Our system allows the systematic modification of events in real time along the three main event axes i) Velocity, ii) Syncronicity and iii) Density. Our future work will investigate the influence of each dimension on the passage of time (varying velocity of time flow) and the structure of time (varying synchronicity of events), which should provide insights for the design of VR diagnostic and therapeutic tools.",framework; therapy; time perception manipulation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Molecular MR Multiplayer: a cross-platform collaborative interactive game for scientists,VRST - Virtual Reality Software and Technology,A,"We present Molecular MR Multiplayer – an interactive, collaborative solution for material design on the edge devices on flat screens and in Mixed Reality (MR)&nbsp;[3]. The application provides a concept of emerging gaming-like remote collaboration experience for researchers, helping explore and design chemical compounds, Both single-user and multi-user modes are implemented. The multi-user mode allows mixed collaborative sessions between local and distant users. A concept of the digital overlays over physical world known as Metaverse is developed.",Interactive; Material Design; Mixed Reality; Multiplayer,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,On the Interplay of Foveated Rendering and Video Encoding,VRST - Virtual Reality Software and Technology,A,Humans have sharp central vision but low peripheral visual acuity. Prior work has taken advantage of this phenomenon in two ways: foveated rendering (FR) reduces the computational workload of rendering by producing lower visual quality for peripheral regions and foveated video encoding (FVE) reduces the bitrate of streamed video through heavier compression of peripheral regions. Remote rendering systems require both rendering and video encoding and the two techniques can be combined to reduce both computing and bandwidth consumption. We report early results from such a combination with remote VR rendering. The results highlight that FR causes large bitrate overhead when combined with normal video encoding but combining it with FVE can mitigate it.,cloud rendering; foveated rendering; video encoding; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2020,Physicalizing Virtual Objects with Affordances to Support Tangible Interactions in AR,VRST - Virtual Reality Software and Technology,A,"When interacting with virtual objects, we could discover some problems around us. Digital information has no physical properties, and sense organ(vision, tactile) does not match very well. These have a negative influence on the user’s interactive experience. In this paper, we propose to provide virtual objects with no physical properties with physical affordances to support tangible feedback via mechanical movement. In addition, we developed an interaction prototype system. The system could change the physical supports that are absorbed onto an electromagnet to adapt to the shape of different virtual objects and efficiently provide natural and consistent interactions when putting physical objects onto virtual objects in Augmented Reality (AR) scenarios.",affordance; Augmented Reality; physiaclizing virtual objects; tangible interaction,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Portals With a Twist: Cable Twist-Free Natural Walking in Room-Scaled Virtual Reality,VRST - Virtual Reality Software and Technology,A,"To provide naturally walking in small virtual reality (VR) tracking spaces while preventing cables of head-mounted displays (HMDs) getting twisted, we developed Portals With A Twist (PorTwist), a redirected walking method using a portal metaphor. We compared PorTwist with a teleportation method in a 2m × 2.7m tracking space in a within-design user study (N = 34). PorTwist resulted in significantly longer natural walking distances and could prevent HMD cables from getting twisted while providing comparable levels of perceived presence and simulator sickness as teleportation. We further identified potential to improve usability in the future.",navigation; redirected walking; room-scaled virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,RoadVR: Mitigating the Effect of Vection and Sickness by Distortion of Pathways for In-Car Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We explore a method to reduce motion sickness and allow people to use virtual reality while moving in vehicles. We put forth a usage scenario where the target VR content is based on constant road navigation so that the actual motion can enhance the VR experience. The method starts with a virtual scene and objects around an infinitely straight road. The motion of the vehicle is sensed by the GPS and IMU module. The sensed motion is reflected in a way that the virtual scene is navigated according to the vehicle motion, and its pathways distorted such that the virtual motion has a near-identical optical flow pattern to the actual. This would align the user’s visual and vestibular sense and reduce the effect of vection and motion sickness. We ran an pilot experiment to validate our approach, comparing the before and after sickness levels with the VR content (1) not aligned to the motion of the vehicle and (2) aligned by our method. Our preliminary results have shown the sickness was reduced significantly (but not eliminated to a negligible level yet) with our approach.",Distortion; Motion Sickness; Navigation; Simulator Sickness; Vection; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2020,Smart Thermo-Haptic Bracelet for VR Environment,VRST - Virtual Reality Software and Technology,A,"We propose a lightweight smart haptic bracelet-based stimulation system for VR applications. This wireless system equipped with vibrotactile tactors and peltier actuator, generates different haptic/thermal levels for different materials being touched in a VR environment.",embedded system; haptic and thermal feedback system; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2020,Teaching Scrum with a Virtual Sprint Simulation: Initial Design and Considerations,VRST - Virtual Reality Software and Technology,A,"Scrum is a well-developed and utilized agile project management framework, which requires extensive training and hands-on experience to master. The latter is not always possible, i.e. during the recent lockdown due to COVID-19. Thus, we propose the creation of a multi-user collaborative virtual simulation of a Scrum sprint that can provide an immersive training experience to remote trainees. Herein, we discuss the design considerations, elements of the virtual learning environment and the development process for the platform.",Education; Project Management; Software Engineering; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2020,Text Input Methods in Virtual Reality using Radial Layouts,VRST - Virtual Reality Software and Technology,A,"Currently, the most popular text input method in VR is Controller Pointing (CP). While this method is easy and intuitive to use, it requires users to have steady hands, and the overlaying of the keyboard onto the virtual scene occludes a part of the scene. In this work, we proposed two new text input methods: Sector Input and T9VR , that utilize a circular keyboard that is attached to the HTC Vive controller. A preliminary study with 24 subjects was conducted to explore the potential of the proposed methods, in comparison with CP. While CP performed significantly better than the proposed methods in terms of objective typing speed and error rate, T9VR was able to match with CP in a number of subjective measures.",,Title,TRUE,
Scopus,conferencePaper,2020,The Digital Docent: XR storytelling for a Living History Museum,VRST - Virtual Reality Software and Technology,A,"In this work, we describe the use of a digital docent: a 3D avatar, presented using virtual and augmented reality, as a means for providing interactive storytelling experiences at a living history museum. To allow flexibility depending on the user’s location and access to technology, the app is designed to provide a common experience supporting a variety of different delivery modalities including AR devices, mobile AR, and VR on the Web.",Arts and humanities; Augmented and virtual realities; Living Museums; Media Arts,Abstract,TRUE,
Scopus,conferencePaper,2020,Viewing Style of Augmented Reality/Virtual Reality Broadcast Contents while Sharing a Virtual Experience,VRST - Virtual Reality Software and Technology,A,"A conceptual space-sharing broadcasting service has been proposed, using augmented reality/virtual reality (AR/VR) with a head-mounted display. As proposed, virtual performers are displayed in their real-life sizes; the user experiences proximity to them. Family and friends living apart can also be displayed in this manner, and an individual can communicate with them in real time; this enables both the individual and their peer to enjoy the broadcast media together. In this study, we implemented this concept as a suitable style for daily use and confirmed the effect of the viewing experience. We developed a prototype of an environment for watching AR/VR mixed content along with a person in a distant place, which is expected to become a popular viewing style of future broadcast media. The individual is displayed as a live-action 3D point cloud image, and verbal and nonverbal communication with the individual are enabled. A demonstration showed that the system renders a sense of presence to a distant person and provides the feeling of sharing the same experience among all its users.",AR/VR; Broadcast; Telepresence; Virtual Space-sharing,Title_Abstract,TRUE,
Scopus,conferencePaper,2020,Volumetric capture for narrative films,VRST - Virtual Reality Software and Technology,A,"Volumetric capture is a technique that allows to create “holographic” recordings of actors, sets and props. The technique can be used to create immersive stories that sometimes reflect aspects of reality better than realistic 3D models. For example, volumetric captures of actors do not seem to cause uncanny valley effect. In this paper, we provide an overview of volumetric capture technology and its application to narrative filmmaking.",augmented reality; interactive narrative; mixed reality; photogrammetry; virtual reality; volumetric capture; volumetric film,Keywords,TRUE,
Scopus,conferencePaper,2020,VR Training for Warehouse Management,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) has evolved into a trending technology that has proven its worth in various application domains. VR is especially helpful for the training of complex tasks in special environments. In the area of logistics, warehouse management involves a complex workflow that consists of different order picking activities. Due to this complex workflow, stock discrepancies and misplaced wares are typical problems that often occur. To overcome this problem, we have developed a VR training application that integrates an existing warehouse management system and trains typical order picking processes. In our VR training demo, we simulate a real warehouse that is supplied with real stocks and real orders.",logistics; usability evaluation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,"Impostor-based Rendering Acceleration for Virtual, Augmented, and Mixed Reality",VRST - Virtual Reality Software and Technology,A,"This paper presents an image-based rendering approach to accelerate rendering time of virtual scenes containing a large number of complex high poly count objects. Our approach replaces complex objects by impostors, light-weight image-based representations leveraging geometry and shading related processing costs. In contrast to their classical implementation, our impostors are specifically designed to work in Virtual-, Augmented- and Mixed Reality scenarios (XR for short), as they support stereoscopic rendering to provide correct depth perception. Motion parallax of typical head movements is compensated by using a ray marched parallax correction step. Our approach provides a dynamic run-time recreation of impostors as necessary for larger changes in view position. The dynamic run-time recreation is decoupled from the actual rendering process. Hence, its associated processing cost is therefore distributed over multiple frames. This avoids any unwanted frame drops or latency spikes even for impostors of objects with complex geometry and many polygons. In addition to the significant performance benefit, our impostors compare favorably against the original mesh representation, as geometric and textural temporal aliasing artifacts are heavily suppressed.",image-based rendering; impostors; rendering acceleration,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,Inside-Out Instrument Tracking for Surgical Navigation in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Surgical navigation requires tracking of instruments with respect to the patient. Conventionally, tracking is done with stationary cameras, and the navigation information is displayed on a stationary display. In contrast, an augmented reality (AR) headset can superimpose surgical navigation information directly in the surgeon’s view. However, AR needs to track the headset, the instruments and the patient, often by relying on stationary infrastructure. We show that 6DOF tracking can be obtained without any stationary, external system by purely utilizing the on-board stereo cameras of a HoloLens 2 to track the same retro-reflective marker spheres used by current optical navigation systems. Our implementation is based on two tracking pipelines complementing each other, one using conventional stereo vision techniques, the other relying on a single-constraint-at-a-time extended Kalman filter. In a technical evaluation of our tracking approach, we show that clinically relevant accuracy of 1.70 mm/1.11°&nbsp;and real-time performance is achievable. We further describe an example application of our system for untethered end-to-end surgical navigation.",Augmented Reality; HoloLens 2; Surgical Navigation; Tracking,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Non-isomorphic Interaction Techniques for Controlling Avatar Facial Expressions in VR,VRST - Virtual Reality Software and Technology,A,"The control of an avatar’s facial expressions in virtual reality is mainly based on the automated recognition and transposition of the user’s facial expressions. These isomorphic techniques are limited to what users can convey with their own face and have recognition issues. To overcome these limitations, non-isomorphic techniques rely on interaction techniques using input devices to control the avatar’s facial expressions. Such techniques need to be designed to quickly and easily select and control an expression, and not disrupt a main task such as talking. We present the design of a set of new non-isomorphic interaction techniques for controlling an avatar facial expression in VR using a standard VR controller. These techniques have been evaluated through two controlled experiments to help designing an interaction technique combining the strengths of each approach. This technique was evaluated in a final ecological study showing it can be used in contexts such as social applications.",Avatar; Emoji; Emoticons; Emotion; Facial expression; VR,Abstract,TRUE,
Scopus,conferencePaper,2021,Ubiq: A System to Build Flexible Social Virtual Reality Experiences,VRST - Virtual Reality Software and Technology,A,"While they have long been a subject of academic study, social virtual reality (SVR) systems are now attracting increasingly large audiences on current consumer virtual reality systems. The design space of SVR systems is very large, and relatively little is known about how these systems should be constructed in order to be usable and efficient. In this paper we present Ubiq, a toolkit that focuses on facilitating the construction of SVR systems. We argue for the design strategy of Ubiq and its scope. Ubiq is built on the Unity platform. It provides core functionality of many SVR systems such as connection management, voice, avatars, etc. However, its design remains easy to extend. We demonstrate examples built on Ubiq and how it has been successfully used in classroom teaching. Ubiq is open source (Apache License) and thus enables several use cases that commercial systems cannot.",avatars; communication tools; networking; open source; social virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Investigating the Effect of Sensor Data Visualization Variances in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This paper investigates the effect of real-time sensor data variances on humans performing straightforward assembly tasks in a Virtual Reality-based (VR-based) training system. A VR-based training system has been developed to transfer color and depth images, and constructs colored point clouds data to represent objects in real-time. Various parameters that affect sensor data acquisition and visualization of remotely operated robots in the real-world are varied. Afterward, the associated task performance is observed. Experimental results from 12 participants performed a total of 95 VR-guided puzzle assembly tasks demonstrated that a combination of low resolution and uncolored points has the most significant effect on participants’ performance. Participants mentioned that they needed to rely upon tactile feedback when the perceptual feedback was minimal. The most insignificant parameter determined was the resolution of the data representations, which, when varied within the experimental bounds, only resulted in a 5% average change in completion time. Participants also indicated in surveys that they felt their performance had improved and frustration was reduced when provided with color information of the scene.",Rendering; Usability Study; Virtual Reality; Visual Perception,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Spatial Augmented Reality Visibility and Line-of-Sight Cues for Building Design,VRST - Virtual Reality Software and Technology,A,"Despite the technological advances in building design, visualizing 3D building layouts can be especially difficult for novice and expert users alike, who must take into account design constraints including line-of-sight and visibility. Using CADwalk, a commercial building design tool that utilizes floor-facing projectors to show 1:1 scale building plans, this work presents and evaluates two floor-based visual cues for assisting with evaluating line-of-sight and visibility. Additionally, we examine the impact of using virtual cameras looking from the inside-out (from user’s location to objects of interest) and outside-in (looking from an object of interest’s location back towards the user). Results show that floor-based cues led to participants more correctly rating visibility, despite taking longer to complete the task. This is an effective tradeoff, given the final outcome (the building design) where accuracy is paramount.",augmented reality; CAD; construction; design; spatial augmented reality; XR,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,ImNDT: Immersive Workspace for the Analysis of Multidimensional Material Data From Non-Destructive Testing,VRST - Virtual Reality Software and Technology,A,"An analysis of large multidimensional volumetric data as generated by non-destructive testing (NDT) techniques, e.g., X-ray computed tomography (XCT), can hardly be evaluated using standard 2D visualization techniques on desktop monitors. The analysis of fiber-reinforced polymers (FRPs) is currently a time-consuming and cognitively demanding task, as FRPs have a complex spatial structure, consisting of several hundred thousand fibers, each having more than twenty different extracted features. This paper presents ImNDT, a novel visualization system, which offers material experts an immersive exploration of multidimensional secondary data of FRPs. Our system is based on a virtual reality (VR) head-mounted device (HMD) to enable fluid and natural explorations through embodied navigation, the avoidance of menus, and manual mode switching. We developed immersive visualization and interaction methods tailored to the characterization of FRPs, such as a Model in Miniature, a similarity network, and a histo-book. An evaluation of our techniques with domain experts showed advantages in discovering structural patterns and similarities. Especially novices can strongly benefit from our intuitive representation and spatial rendering.",fibre-reinforced polymers; immersive analytics; interaction techniques; Multidimensional data visualization; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Effects of Image Realism on the Stress Response in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Safety critical situations, as they occur in professions such as medicine, nursing, and aviation, are often trained in simulators to prevent damages to personnel and material. These jobs often come with a high amount of stress, to which prolonged exposure can have devastating effects. Over the past years, stress inoculation training in conjunction with Virtual Reality has become focus of the research community and software companies. Especially the nursing profession can benefit from it, since stress-related illnesses are often the reason for an early exit from the workforce. However, since training facilities often need to compromise on their simulations due to monetary reasons, trade-offs must be made in the degree of detail of such simulations in order to keep development and acquisition costs low. One such possibility is in using low graphical fidelity. We present a psycho-physiological study on the influence of image realism of virtual environments on the stress response. In a within subject design study, we ask participants to complete nursing related, virtually recreated tasks in an artificial intensive care unit, whilst exposed to different stress factors. We provide our findings in the form of objective and subjective measures. Results show that one can elicit different stress responses by manipulating image realism in a sufficiently drastic manner. However, a life-like reaction does not seem to depend on a highly realistic environment.",image realism; nursing; stress; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,opticARe - Augmented Reality Mobile Patient Monitoring in Intensive Care Units,VRST - Virtual Reality Software and Technology,A,"German Intensive Care Units (ICUs) are in crisis, struggling with an increasing shortage of skilled workers, ultimately putting patients’ safety at risk. To counteract this process, researchers are increasingly concerned with finding digital solutions which aim to support healthcare professionals by enhancing the efficiency of reoccurring critical caring tasks and thus, improve working conditions. In this regard, this paper evaluates the application of Augmented Reality (AR) for patient monitoring for critical care nursing. Grounded on an observational study, semi-structured interviews, as well as a quantitative analysis, mobile patient monitoring scenarios, present particularly during patient transport, were identified as an innovative context of use of AR in the field. Additionally, user requirements such as high wearability, hands-free operability, and clear data representation could be derived from the obtained study results. For validation of these and identification of further requirements, three prototypes differing in their data illustration format were subsequently developed and quantitatively, as well as qualitatively evaluated by conducting an online survey. Thereby, it became evident that future implementations of a corresponding system for patient monitoring ought to integrate a context-dependent data presentation in particular, as this combines high navigability and availability of required data. Identifying patient monitoring during patient transport as a potential context of use, as well as distinguishing a context-dependent design approach as favorable constitute two key contributions of this work and provide a foundation on which future implementations of AR systems in the nursing domain and other related contexts can be established.",augmented reality; critical care; head-mounted display; patient monitoring; wearable,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,The Influence of in-VR Questionnaire Design on the User Experience,VRST - Virtual Reality Software and Technology,A,"Researchers study the user experience in Virtual Reality (VR) typically by collecting either sensory data or using questionnaires. While traditional questionnaire formats present it through web-based survey tools (out-VR), recent studies investigate the effects of presenting questionnaires directly in the virtual environment (in-VR). The in-VR questionnaire can be defined as an implemented user-interface object that allows interaction with questionnaires in VR that do not break the immersion. Integrating questionnaires directly into the virtual environment, however, also challenges design decisions. While most previous research presents in-VR questionnaires in the form of 2D panels in the virtual environment, we want to investigate the difference from such traditional formats to a presentation of a questionnaire format in the form of an interactive object as part of the environment. Accordingly, we evaluate and compare two different in-VR questionnaire designs and a traditional web-based form (out-VR) to assess user experience, the effect on presence, duration of completing the questionnaires, and users’ preferences. As the means for achieving this goal, we developed an immersive questionnaire toolkit that provides a general solution for implementing in-VR questionnaires and exchanging data with popular survey services. This toolkit enables us to run our study both on-site and remotely. As a first small study, 16 users, either on-site or remotely, attended by completing the System Usability Scale, NASA TLX, and the iGroup Presence Questionnaire after a playful activity. The first results indicate that there is no significant difference in the case of usability and presence between different design layouts. Furthermore, we could not find a significant difference also for the task load except between 2D and web-based layout for mental demand and frustration as well as the duration of completing the questionnaire. The results also indicate that users generally prefer in-VR questionnaire designs to the traditional ones. The study can be expanded to include more participants in user studies as a means of gaining more concrete results. Furthermore, additional questionnaire design alternatives can also help to provide us with a more usable and accurate questionnaire design in VR.",3D User Interface; Presence; Questionnaires; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Towards Context-aware Automatic Haptic Effect Generation for Home Theatre Environments,VRST - Virtual Reality Software and Technology,A,"The application of haptic technology in entertainment systems, such as Virtual Reality and 4D cinema, enables novel experiences for users and drives the demand for efficient haptic authoring systems. Here, we propose an automatic multimodal vibrotactile content creation pipeline that substantially improves the overall hapto-audiovisual (HAV) experience based on contextual audio and visual content from movies. Our algorithm is implemented on a low-cost system with nine actuators attached to a viewing chair and extracts significant features from video files to generate corresponding haptic stimuli. We implemented this pipeline and used the resulting system in a user study (n = 16), quantifying user experience according to the sense of immersion, preference, harmony, and discomfort. The results indicate that the haptic patterns generated by our algorithm complement the movie content and provide an immersive and enjoyable HAV user experience. This further suggests that the pipeline can facilitate the efficient creation of 4D effects and could therefore be applied to improve the viewing experience in home theatre environments.",4D effect generation; automatic haptic effect authoring; Haptics; home theatre; immersive experience,Abstract,TRUE,
Scopus,conferencePaper,2021,The Effect of Increased Body Motion in Virtual Reality on a Placement-Retrieval Task,VRST - Virtual Reality Software and Technology,A,"Previous work has shown that increased effort and use of one’s body can improve memory. When positioning windows inside a virtual reality, does the use of a larger volume, and using one’s legs to move around, improve ability to later find the windows? The results of our experiment indicate there can be a modest benefit for spatial memory and retrieval time, but at the cost of increased time spent initially positioning the windows.",controlled experiment; locomotion; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,VRGaitAnalytics: Visualizing Dual Task Cost for VR Gait Assessment,VRST - Virtual Reality Software and Technology,A,"Among its many promising applications, Virtual Reality (VR) can simulate diverse real-life scenarios and therefore help experimenters assess individuals’ gait performance (i.e., walking) under controlled functional contexts. VR-based gait assessment may provide low-risk, reproducible and controlled virtual environments, enabling experimenters to investigate underlying causes for imbalance by manipulating experimental conditions such as multi-sensory loads, mental processing loads (cognitive load), and/or motor tasks. We present a low-cost novel VR gait assessment system that simulates virtual obstacles, visual, auditory, and cognitive loads while using motion tracking to assess participants’ walking performance. The system utilizes in-situ spatial visualization for trial playback and instantaneous outcome measures which enable experimenters and participants to observe and interpret their performance. The trial playback can visualize any moment in the trial with embodied graphic segments including the head, waist, and feet. It can also replay two trials at the same time frame for trial-to-trial comparison, which helps visualize the impact of different experimental conditions. The outcome measures, i.e., the metrics related to walking performance, are calculated in real-time and displayed as data graphs in VR. The system can help experimenters get specific gait information on balance performance beyond a typical clinical gait test, making it clinically relevant and potentially applicable to gait rehabilitation. We conducted a feasibility study with physical therapy students, research graduate students, and licensed physical therapists. They evaluated the system and provided feedback on the outcome measures, the spatial visualizations, and the potential use of the system in the clinic. The study results indicate that the system was feasible for gait assessment, and the immediate spatial visualization features were seen as clinically relevant and useful. Limitations and considerations for future work are discussed.",gait balance; obstacle crossing; playback; Spatial visualization; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Catching Jellies in Immersive Virtual Reality: A Comparative Teleoperation Study of ROVs in Underwater Capture Tasks,VRST - Virtual Reality Software and Technology,A,"Remotely Operated Vehicles (ROVs) are essential to human-operated underwater expeditions in the deep sea. However, piloting an ROV to safely interact with live ecosystems is an expensive and cognitively demanding task, requiring extensive maneuvering and situational awareness. Immersive Virtual Reality (VR) Head-Mounted Displays (HMDs) could address some of these challenges. This paper investigates how VR HMDs influence operator performance through a novel telepresence system for piloting ROVs in real-time. We present an empirical user study [N=12] that examines common midwater creature capture tasks, comparing Stereoscopic-VR, Monoscopic-VR, and Desktop teleoperation conditions. Our findings indicate that Stereoscopic-VR can outperform Monoscopic-VR and Desktop ROV capture tasks, effectively doubling the efficacy of operators. We also found significant differences in presence, task load, usability, intrinsic motivation, and cybersickness. Our research points to new opportunities towards VR with ROVs.",Cybersickness; Head-Mounted Display; Human-Operated Vehicles; Immersive Applications; Immersive Virtual Reality; Remotely Operated Vehicle; Teleoperation; Telepresence; Usability,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,BreachMob: Detecting Vulnerabilities in Physical Environments Using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"BreachMob is a virtual reality (VR) tool that applies open design principles from information security to physical buildings and structures. BreachMob uses a detailed 3D digital model of a property owner's building. The model is then published as a virtual environment (VE), complete with all applicable security measures and released to the public to test the building's security and find any potential vulnerabilities by completing specified objectives. Our paper contributes a new method of applying VR to crowd source detection of physical environment vulnerabilities. We detail the technical realization of two BreachMob prototypes (a home and an airport) reflecting on static and dynamic vulnerabilities. Our design critique suggests that&nbsp;BreachMob&nbsp;promotes user immersion by allowing participants the freedom to behave in ways that align with the experience of breaching physical security protocols.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,EntangleVR: A Visual Programming Interface for Virtual Reality Interactive Scene Generation,VRST - Virtual Reality Software and Technology,A,"Entanglement is a unique phenomenon in quantum physics that describes a correlated relationship in the measurement of a group of spatially separated particles. In the fields of science fiction, game design, art and philosophy, it has inspired the creation of numerous innovative works. We present EntangleVR, a novel method to create entanglement-inspired virtual scenes with the goal to simplify representing this phenomenon in the design of interactive VR games and experiences. By providing a reactive visual programming interface, users can integrate entanglement into their design without requiring prior knowledge of quantum computing or quantum physics. Our system enables fast creation of complex scenes composed of virtual objects with manipulable correlated behaviors.",3D scene creation; art; creativity; entanglement; quantum computing; virtual reality; visual programming,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,Virtual Rotations for Maneuvering in Immersive Virtual Environments,VRST - Virtual Reality Software and Technology,A,"In virtual navigation, maneuvering around an object of interest is a common task which requires simultaneous changes in both rotation and translation. In this paper, we present Anchored Jumping, a teleportation technique for maneuvering that allows the explicit specification of a new viewing direction by selecting a point of interest as part of the target specification process. A first preliminary study showed that naïve Anchored Jumping can be improved by an automatic counter rotation that preserves the user’s relative orientation towards their point of interest. In our second, qualitative study, this extended technique was compared with two common approaches to specifying virtual rotations. Our results indicate that Anchored Jumping allows precise and comfortable maneuvering and is compatible with techniques that primarily support virtual exploration and search tasks. Equipped with a combination of such complementary techniques, seated users generally preferred virtual over physical rotations for indoor navigation.",3D navigation; Jumping; Maneuvering; Target-based travel; Teleportation; Virtual reality; Virtual rotation,Keywords,TRUE,
Scopus,conferencePaper,2021,Using Gaze Behavior and Head Orientation for Implicit Identification in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Identifying users of a Virtual Reality (VR) headset provides designers of VR content with the opportunity to adapt the user interface, set user-specific preferences, or adjust the level of difficulty either for games or training applications. While most identification methods currently rely on explicit input, implicit user identification is less disruptive and does not impact the immersion of the users. In this work, we introduce a biometric identification system that employs the user’s gaze behavior as a unique, individual characteristic. In particular, we focus on the user’s gaze behavior and head orientation while following a moving stimulus. We verify our approach in a user study. A hybrid post-hoc analysis results in an identification accuracy of up to 75&nbsp;% for an explainable machine learning algorithm and up to 100&nbsp;% for a deep learning approach. We conclude with discussing application scenarios in which our approach can be used to implicitly identify users.",eye tracking; gaze-based authentication; implicit identification; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,InteractML: Making machine learning accessible for creative practitioners working with movement interaction in immersive media,VRST - Virtual Reality Software and Technology,A,"Interactive Machine Learning offers a method for designing movement interaction that supports creators in implementing even complex movement designs in their immersive applications by simply performing them with their bodies. We introduce a new tool, InteractML, and an accompanying ideation method, which makes movement interaction design faster, adaptable and accessible to creators of varying experience and backgrounds, such as artists, dancers and independent game developers. The tool is specifically tailored to non-experts as creators configure and train machine learning models via a node-based graph and VR interface, requiring minimal programming. We aim to democratise machine learning for movement interaction to be used in the development of a range of creative and immersive applications.",artists; creative virtual reality; dancers; machine learning; movement interaction,Keywords,TRUE,
Scopus,conferencePaper,2021,Research and Practice Recommendations for Mixed Reality Design – Different Perspectives from the Community,VRST - Virtual Reality Software and Technology,A,"Over the last decades, different kinds of design guides have been created to maintain consistency and usability in interactive system development. However, in the case of spatial applications, practitioners from research and industry either have difficulty finding them or perceive such guides as lacking relevance, practicability, and applicability. This paper presents the current state of scientific research and industry practice by investigating currently used design recommendations for mixed reality (MR) system development. We analyzed and compared 875 design recommendations for MR applications elicited from 89 scientific papers and documentation from six industry practitioners in a literature review. In doing so, we identified differences regarding four key topics: Focus on unique MR design challenges, abstraction regarding devices and ecosystems, level of detail and abstraction of content, and covered topics. Based on that, we contribute to the MR design research by providing three factors for perceived irrelevance and six main implications for design recommendations that are applicable in scientific and industry practice.",Augmented Reality; Design Recommendations; Design Theory and Practice; Guidelines; Mixed Reality; User Interface Design,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Qualitative Dimensions of Technology-Mediated Reflective Learning: The Case of VR Experience of Psychosis,VRST - Virtual Reality Software and Technology,A,"Self-reflection is evaluation of one’s inferential processes often triggered by complex social and emotional experiences, characterized by their ambiguity and unpredictability, pushing one to re-interpret the experience, and update existing knowledge. Using immersive Virtual Reality (VR), we aimed to support social and emotional learning (SEL) through reflection in psychology education. We used the case of psychosis as it involves ambiguous perceptual experiences. With a codesign workshop, we designed a VR prototype that simulates the perceptual, cognitive, affective, and social elements of psychotic experiences, followed by a user-study with psychology students to evaluate the potential of this technology to support reflection. Our analyses suggested that technology-mediated reflection in SEL involves two dimensions: spontaneous perspective-taking and shared state of affect. By exploring the subjective qualities of reflection with the said dimensions, our work contributes to the literature on technology-supported learning and VR developers designing for reflection.",ambiguous design; experiential learning; reflection; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,RotoWrist: Continuous Infrared Wrist Angle Tracking using a Wristband,VRST - Virtual Reality Software and Technology,A,"We introduce RotoWrist, an infrared (IR) light based solution for continuously and reliably tracking 2-degree-of-freedom (DoF) relative angle of the wrist with respect to the forearm using a wristband. The tracking system consists of eight time-of-flight (ToF) IR light modules distributed around a wristband. We developed a computationally simple tracking approach to reconstruct the orientation of the wrist without any runtime training, ensuring user independence. An evaluation study demonstrated that RotoWrist achieves a cross-user median tracking error of 5.9° in flexion/extension and 6.8° in radial and ulnar deviation with no calibration required as measured with optical ground truth. We further demonstrate the performance of RotoWrist for a pointing task and compare it against ground truth tracking.",hand tracking; time-of-flight sensor; virtual and augmented reality; wearable device; wrist pose; wristband.,Keywords,TRUE,
Scopus,conferencePaper,2021,PAIR: Phone as an Augmented Immersive Reality Controller,VRST - Virtual Reality Software and Technology,A,"Immersive head-mounted augmented reality allows users to overlay 3D digital content on a user’s view of the world. Current-generation devices primarily support interaction modalities such as gesture, gaze and voice, which are readily available to most users yet lack precision and tactility, rendering them fatiguing for extended interactions. We propose using smartphones, which are also readily available, as companion devices complementing existing AR interaction modalities. We leverage user familiarity with smartphone interactions, coupled with their support for precise, tactile touch input, to unlock a broad range of interaction techniques and applications - for instance, turning the phone into an interior design palette, touch-enabled catapult or AR-rendered sword. We describe a prototype implementation of our interaction techniques using an off-the-shelf AR headset and smartphone, demonstrate applications, and report on the results of a positional accuracy study.",Augmented Reality; Input Techniques; Smartphone; Touch,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,TangibleData: Interactive Data Visualization with Mid-Air Haptics,VRST - Virtual Reality Software and Technology,A,"In this paper, we investigate the effects of mid-air haptics in interactive 3D data visualization. We build an interactive 3D data visualization tool that adapts hand gestures and mid-air haptics to provide tangible interaction in VR using ultrasound haptic feedback on 3D data visualization. We consider two types of 3D visualization datasets and provide different data encoding methods for haptic representations. Two user experiments are conducted to evaluate the effectiveness of our approach. The first experimental results show that adding a mid-air haptic modality can be beneficial regardless of noise conditions and useful for handling occlusion or discerning density and volume information. The second experiment results further show the strengths and weaknesses of direct touch and indirect touch modes. Our findings can shed light on designing and implementing a tangible interaction on 3D data visualization with mid-air haptic feedback.",Data visualization; haptics; immersive analytics; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,PneuMod: A Modular Haptic Device with Localized Pressure and Thermal Feedback,VRST - Virtual Reality Software and Technology,A,"Humans have tactile sensory organs distributed all over the body. However, haptic devices are often only created for one part (e.g., hands, wrist, or face). We propose PneuMod, a wearable modular haptic device that can simultaneously and independently present pressure and thermal (warm and cold) cues to different parts of the body. The module in PneuMod is a pneumatically-actuated silicone bubble with an integrated Peltier device that can render thermo-pneumatic feedback through shapes, locations, patterns, and motion effects. The modules can be arranged with varying resolutions on fabric to create sleeves, headbands, leg wraps, and other forms that can be worn on multiple parts of the body. In this paper, we describe the system design, the module implementation, and applications for social touch interactions and in-game thermal and pressure feedback.",fabrication; haptic communication; modular device; multimodal haptics; pneumatic feedback; thermal feedback; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,Ellipses Ring Marker for High-speed Finger Tracking,VRST - Virtual Reality Software and Technology,A,"High-speed finger tracking is necessary for augmented reality and operation in human-machine cooperation without latency discomfort, but conventional markerless finger tracking methods are not fast enough and the marker-based methods have low wearability. In this paper, we propose an ellipses ring marker (ERM), a finger-ring marker consisting of multiple ellipses and its high-speed image recognition algorithm. The finger-ring shape has highly wearing continuity, and the surface shape is suitable for various viewing angle observation. The invariance of the ellipse in the perspective projection enables accurate and low-latency posture estimation. We have experimentally investigated the advantage in normal distribution, validated the sufficient accuracy and computational cost in the marker tracking, and showed a demonstration of dynamic projection mapping on a palm.",dynamic projection mapping; hand tracking; high-speed image processing,Abstract,TRUE,
Scopus,conferencePaper,2021,Presenting Sense of Loud Vocalization Using Vibratory Stimuli to the Larynx and Auditory Stimuli,VRST - Virtual Reality Software and Technology,A,"In recent years, technologies related to virtual reality (VR) have continued to advance. As a method to enhance the VR experience, we focused on loud vocalization. This is because we believe that loud vocalization can enable us to engage with the VR environment in a more interactive way. Also, as loud vocalization is an action that is thought to be closely related to stress reduction and a sense of exhilaration, the stress reduction through VR with loud vocalization is also expected. But loud vocalization itself has disadvantages for physical, mental, and social reasons. Then, we hypothesized that loud vocalization itself is not necessary for such benefits; but the sense of loud vocalization plays an important role. Therefore, we focused on a method of substituting experience by presenting sensory stimuli. In this paper, we proposed a way to present the sense of loud vocalization through vibratory stimuli to the larynx and auditory stimuli to users who are actually vocalizing quietly with the expectation for the sense of loud vocalization. Our user study showed that the proposed method can extend the sense of vocalization and realize pseudo-loud vocalization. In addition, it was also shown that the proposed method can cause a sense of exhilaration. By contrast, excessively strong vibratory stimuli spoil the sense of loud vocalization, and thus the intensity of the vibration should be appropriately determined.",cross modal; loud vocalization; sense of vocalization; vibratory stimuli,Abstract,TRUE,
Scopus,conferencePaper,2021,Analysis of Detection Thresholds for Hand Redirection during Mid-Air Interactions in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Avatars in virtual reality (VR) with fully articulated hands enable users to naturally interact with the virtual environment (VE). Interactions are often performed in a one-to-one mapping between the movements of the user’s real body, for instance, the hands, and the displayed body of the avatar. However, VR also allows manipulating this mapping to introduce non-isomorphic techniques. In this context, research on manipulations of virtual hand movements typically focuses on increasing the user’s interaction space to improve the overall efficiency of hand-based interactions. In this paper, we investigate a hand retargeting method for decelerated hand movements. With this technique, users need to perform larger movements to reach for an object in the VE, which can be utilized, for example, in therapeutic applications. If these gain-based redirections of virtual hand movements are small enough, users become unable to reliably detect them due to the dominance of the visual sense. In a psychophysical experiment, we analyzed detection thresholds for six different motion paths in mid-air for both hands. We found significantly different detection thresholds between movement directions on each spatial axis. To verify our findings, we applied the identified gains in a playful application in a confirmatory study.",avatar; detection thresholds; hand redirection,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,Virtual Reality platform for functional magnetic resonance imaging in ecologically valid conditions,VRST - Virtual Reality Software and Technology,A,"Functional magnetic resonance Brain Imaging (fMRI) is a key non-invasive imaging technique for the study of human brain activity. Its millimetric spatial resolution is at the cost of several constraints: participants must remain static and experience artificial stimuli, making it difficult to generalize neuroscientific results to naturalistic and ecological conditions. Immersive Virtual Reality (VR) provides alternatives to such stimuli through simulation, but still requires an active first-person exploration of the environment to evoke a strong sense of presence in the virtual environment. Here, we report how to compensate for the inability to freely move in VR by leveraging on principles of embodiment for a virtual avatar, to eventually evoke a strong sense of presence with a minimal motion of the participant. We validated the functionality of the platform in a study where healthy participants performed several basic research tasks in an MR-specific immersive virtual environment. Our results show that our approach can lead to high sense of presence, strong body ownership, and sense of agency for a virtual avatar, with low movement-related MRI artifacts. Moreover, to exemplify the versatility of the platform, we reproduced several behavioral and fMRI results in the perceptual, motor, and cognitive domains. We discuss how to leverage such technology for neuroscience research and provide recommendations on efficient ways to implement and develop it successfully.",Ecological; fMRI; Immersive Virtual Reality; Presence,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Enhancing In-game Immersion Using BCI-controlled Mechanics,VRST - Virtual Reality Software and Technology,A,"Due to multimodal approach, the virtual reality experiences become increasingly more immersive and entertaining. New control modalities, such as brain-computer interfaces (BCIs), enable the players to engage in the game with both their bodies and minds. In our work, we investigate the influence of employing BCI-driven mechanics on player’s in-game immersion. We designed and implemented an escape room-themed game which employed player’s mental states of focus and relaxation as input for selected game mechanisms. Through a between-subject user study, we found that controlling the game with mental states enhances the in-game immersion and attracts the player’s engagement. At the same time, using BCIs did not impose additional cognitive workload. Our work contributes qualitative insights on psychocognitive effects of using BCIs in gaming and describing immersive gaming experiences.",brain-computer interface; virtual reality games,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Perceived Realism of Pedestrian Crowds Trajectories in VR,VRST - Virtual Reality Software and Technology,A,"Crowd simulation algorithms play an essential role in populating Virtual Reality (VR) environments with multiple autonomous humanoid agents. The generation of plausible trajectories can be a significant computational cost for real-time graphics engines, especially in untethered and mobile devices such as portable VR devices. Previous research explores the plausibility and realism of crowd simulations on desktop computers but fails to account the impact it has on immersion. This study explores how the realism of crowd trajectories affects the perceived immersion in VR. We do so by running a psychophysical experiment in which participants rate the realism of real/synthetic trajectories data, showing similar level of perceived realism.",crowd simulation; perception; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Pressing a Button You Cannot See: Evaluating Visual Designs to Assist Persons with Low Vision through Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Partial vision loss occurs in several medical conditions and affects persons of all ages. It compromises many daily activities, such as reading, cutting vegetables, or identifying and accurately pressing buttons, e.g., on ticket machines or ATMs. Touchscreen interfaces pose a particular challenge because they lack haptic feedback from interface elements and often require people with impaired vision to rely on others for help. We propose a smartglasses-based solution to utilize the user’s residual vision. Together with visually-impaired individuals, we designed assistive augmentations for touchscreen interfaces and evaluated their suitability to guide attention towards interface elements and to increase the accuracy of manual inputs. We show that augmentations improve interaction performance and decrease cognitive load, particularly for unfamiliar interface layouts.",Accessibility; Augmented Reality; Low Vision,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,Flyables: Haptic Input Devices for Virtual Reality using Quadcopters,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) has made its way into everyday life. While VR delivers an ever-increasing level of immersion, controls and their haptics are still limited. Current VR headsets come with dedicated controllers that are used to control every virtual interface element. However, the controller input mostly differs from the virtual interface. This reduces immersion. To provide a more realistic input, we present Flyables, a toolkit that provides matching haptics for virtual user interface elements using quadcopters. We took five common virtual UI elements and built their physical counterparts. We attached them to quadcopters to deliver on-demand haptic feedback. In a user study, we compared Flyables to controller-based VR input. While controllers still outperform Flyables in terms of precision and task completion time, we found that Flyables present a more natural and playful way to interact with VR environments. Based on the results from the study, we outline research challenges that could improve interaction with Flyables in the future.",Drones; Flyables; Haptics; Quadcopter; Toolkit.; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Object Manipulations in VR Show Task- and Object-Dependent Modulation of Motor Patterns,VRST - Virtual Reality Software and Technology,A,"Humans can perform object manipulations in VR in spite of missing haptic and acoustic information. Whether their movements under these artificial conditions do still rely on motor programs based on natural experience or are impoverished due to the restrictions imposed by VR is unclear. We investigated whether reach-to-place and reach-to-grasp movements in VR can still be adapted to the task and to the specific properties of the objects being handled, or whether they reflect a stereotypic, task- and object-independent motor program. We analyzed reach-to-grasp and reach-to-place movements from participants performing an unconstrained ”set-the-table” task involving a variety of different objects in virtual reality. These actions were compared based on their kinematic features. We encountered significant differences in peak speed and the duration of the deceleration phase which are modulated depending on the action and on the manipulated object. The flexibility of natural human sensorimotor control thus is at least partially transferred and exploited in impoverished VR conditions. We discuss possible explanations of this behavior and the implications for the design of object manipulations in VR.",kinematics; motor control; motor skill; object manipulation phases; reach and place movements; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Modeling Pointing for 3D Target Selection in VR,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) allows users to interact similarly to how they do in the physical world, such as touching, moving, and pointing at objects. To select objects at a distance, most VR techniques rely on casting a ray through one or two points located on the user’s body (e.g., on the head and a finger), and placing a cursor on that ray. However, previous studies show that such rays do not help users achieve optimal pointing accuracy nor correspond to how they would naturally point. We seek to find features, which would best describe natural pointing at distant targets. We collect motion data from seven locations on the hand, arm, and body, while participants point at 27 targets across a virtual room. We evaluate the features of pointing and analyse sets of those for predicting pointing targets. Our analysis shows an 87% classification accuracy between the 27 targets for the best feature set and a mean distance of 23.56&nbsp;cm in predicting pointing targets across the room. The feature sets can inform the design of more natural and effective VR pointing techniques for distant object selection.",pointing; target selection; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Virtual Object Categorisation Methods: Towards a Richer Understanding of Object Grasping for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Object categorisation methods have been historically used in literature for understanding and collecting real objects together into meaningful groups and can be used to define human interaction patterns (i. e grasping). When investigating grasping patterns for Virtual Reality (VR), researchers used Zingg’s methodology which categorises objects based on shape and form. However, this methodology is limited and does not take into consideration other object attributes that might influence grasping interaction in VR. To address this, our work presents a study into three categorisation methods for virtual objects. We employ Zingg’s object categorisation as a benchmark against existing real and virtual object interaction work and introduce two new categorisation methods that focus on virtual object equilibrium and virtual object component parts. We evaluate these categorisation methods using a dataset of 1872 grasps from a VR docking task on 16 virtual representations of real objects and report findings on grasp patterns. We report on findings for each virtual object categorisation method showing differences in terms of grasp classes, grasp type and aperture. We conclude by detailing recommendations and future ideas on how these categorisation methods can be taken forward to inform a richer understanding of grasping in VR.",Grasping; Interaction; Object Categorisation; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,"Actions, not gestures: contextualising embodied controller interactions in immersive virtual reality",VRST - Virtual Reality Software and Technology,A,"Modern immersive virtual reality (IVR) often uses embodied controllers for interacting with virtual objects. However, it is not clear how we should conceptualise these interactions. They could be considered either gestures, as there is no interaction with a physical object; or as actions, given that there is object manipulation, even if it is virtual. This distinction is important, as literature has shown that in the physical world, action-enabled and gesture-enabled learning produce distinct cognitive outcomes. This study attempts to understand whether sensorimotor-embodied interactions with objects in IVR can cognitively be considered as actions or gestures. It does this by comparing verb-learning outcomes between two conditions: (1) where participants move the controllers without touching virtual objects (gesture condition); and (2) where participants move the controllers and manipulate virtual objects (action condition). We found that (1) users can have cognitively distinct outcomes in IVR based on whether the interactions are actions or gestures, with actions providing stronger memorisation outcomes; and (2) embodied controller actions in IVR behave more similarly to physical world actions in terms of verb memorization benefits.",cognition; embodiment; HCI; immersive virtual reality; learning; sensorimotor; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,3D Printing an Accessory Dock for XR Controllers and its Exemplary Use as XR Stylus,VRST - Virtual Reality Software and Technology,A,"This article introduces the accessory dock, a 3D printed multi-purpose extension for consumer-grade XR controllers that enables flexible mounting of self-made and commercial accessories. The uniform design of our concept opens new opportunities for XR systems being used for more diverse purposes, e.g., researchers and practitioners could use and compare arbitrary XR controllers within their experiments while ensuring access to buttons and battery housing. As a first example, we present a stylus tip accessory to build an XR Stylus, which can be directly used with frameworks for handwriting, sketching, and UI interaction on physically aligned virtual surfaces. For new XR controllers, we provide instructions on how to adjust the accessory dock to the controller’s form factor. A video tutorial for the construction and the source files for 3D printing are publicly available for reuse, replication, and extension (https://go.uniwue.de/hci-otss-accessory-dock).",3D modeling; 3D printing; augmented reality; handwriting; passive haptic feedback; sketching; stylus; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,A Hat-shaped Pressure-Sensitive Multi-Touch Interface for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"We developed a hat-shaped touch interface for virtual reality viewpoint control. The hat is made of conductive fabric and thus is lightweight. The user can touch, drag, and push the surface, enabling three-dimensional viewpoint control.",multi-touch; touch interface; viewpoint control; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,A Perceptual Evaluation of the Ground Inclination with a Simple VR Walking Platform,VRST - Virtual Reality Software and Technology,A,"We evaluate how highly realistic the inclination of the ground can be perceived with our simple VR walking platform. Firstly we prepared seven maps with different ground inclinations of -30 to 30 degrees and every 10 degrees. Then we conducted a perception experiment of the inclination feeling with each of the treadmill and our proposed platform, and questionnaire evaluation about the presence, the fatigue, and the exhilaration. As a result, it was clarified that even if our proposed platform is used, not only the feeling of presence equivalent to that of the treadmill can be felt, but also the inclination of the ground up and down can be perceived.",locomotion interface; redirection; virtual reality; walking platform,Keywords,TRUE,
Scopus,conferencePaper,2021,A Pilot Study Examining the Unexpected Vection Hypothesis of Cybersickness.,VRST - Virtual Reality Software and Technology,A,"The relationship between vection (illusory self-motion) and cybersickness is complex. This pilot study examined whether only unexpected vection provokes sickness during head-mounted display (HMD) based virtual reality (VR). 20 participants ran through the tutorial of Mission: ISS (an HMD VR app) until they experienced notable sickness (maximum exposure was 15 minutes). We found that: 1) cybersickness was positively related to vection strength; and 2) cybersickness appeared to be more likely to occur during unexpected vection. Given the implications of these findings, future studies should attempt to replicate them and confirm the unexpected vection hypothesis with larger sample sizes and rigorous experimental designs.",Cybersickness; Oculus Rift; Perception; Vection; Virtual Reality; VR,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,A sharing system for the annoyance of menstrual symptoms using electrical muscle stimulation and thermal stimulations,VRST - Virtual Reality Software and Technology,A,,Electrical muscle stimulation (EMS); menstruation; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,A System for Practicing Ball/Strike Judgment in VR Environment,VRST - Virtual Reality Software and Technology,A,"The purpose of this study is to develop an easy-to-use ball/strike judgment practice system for inexperienced baseball umpires. The main idea is to provide a practice environment in a Virtual Reality (VR) space. With our system, users observe a pitched ball, perform ball/strike judgment, and review their judgment in a VR space. Since the whole process is completed in VR, users can practice the judgments without preparing a pitcher and catcher. A user investigation in which participants practiced with our system and judged balls thrown by a pitching machine was conducted. The participants responded positively when asked about the usefulness of our system.",,Abstract,TRUE,
Scopus,conferencePaper,2021,A Tangible Haptic Feedback Box for Mixed Reality Billiard in Tight Spaces,VRST - Virtual Reality Software and Technology,A,"This paper presents a system for a simulated billiard game with two players and an emphasis on haptic feedback. We devised a feedback box that is responsible for generating the inputs and providing immediate haptic feedback to the user. The simulation runs as an AR application and the player can use a real queue to hit the real ball. Although the haptic feedback is precise due to the usage of a real billiard ball and queue, the input accuracy of the angle and impulse measurement is limited.",Augmented Reality; Billiard; Haptic User Feedback; Mixed Reality; Tracking,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,"ALiSE: Non-wearable AR display through the looking glass, and what looks solid there",VRST - Virtual Reality Software and Technology,A,"With the Augmented Reality mirror display method using a half-mirror, there is a difference in the focal length between the mirror image and the AR image. Therefore, the observer perceives a mismatch in depth perception, which impairs usability. In this study, we developed an optical-reflection AR display, ALiSE (Augment Layer interweaved Semi-reflecting Existence), which enhances the depth perception experience of AR images by adding a gap zone with the same depth as the target depth between the display and the half-mirror. We conducted an experiment to view 3D objects and achieve virtual fitting using the existing AR with video synthesis and the proposed ALiSE method. As a result of the questionnaire survey, although the comfort of wearing virtual objects was below existing methods, we confirmed that the presence and solidity were superior with the proposed method to other approaches. This is an attempt to create a sense of the stereoscopic effect despite the 2D projection, as the object to be projected is simultaneously reflected in the mirror along with the observer themselves.",Augmented layer; Augmented mirror; Virtual fitting room,Abstract,TRUE,
Scopus,conferencePaper,2021,An Evaluation of Methods for Manipulating Virtual Objects at Different Scales,VRST - Virtual Reality Software and Technology,A,"Immersive Virtual Reality enables users to experience 3D models and other virtual content in ways that cannot be achieved on a flat screen, and several modern Virtual Reality applications now give users the ability to include or create their own content and objects. With user-generated content however, objects may come in all shapes and sizes. This necessitates the use of object manipulation methods that are effective regardless of object size. In this work we evaluate two methods for manipulating virtual objects of varying sizes. World Pull enables the user to directly manipulate and scale the virtual environment, while Pivot Manipulation enables the user to rotate objects around a set of predefined pivot points. The methods were compared to a traditional 6 degree of freedom manipulation method during a user study and the results showed that World Pull performed better in terms of precision for small and large objects, while Pivot Manipulation performed better for large objects.",large objects; precise object manipulation; small objects; virtual docking; virtual object manipulation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,An Infant-Like Device that Reproduces Hugging Sensation with Multi-Channel Haptic Feedback,VRST - Virtual Reality Software and Technology,A,"Proximity interaction, such as hugging, plays an essential role in building relationships between parents and children. However, parents and children cannot freely interact in the neonatal intensive care unit due to visiting restrictions imposed by COVID-19. In this study, we develop a system of pseudo-proximity interaction with a remote infant through a VR headset by using an infant-like device that reproduces the haptic feedback features of the hugging sensation, such as weight, body temperature, breathing, softness, and unstable neck.",Infant-Like Device; Multimodal; Proximity Interaction; Telepresence; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2021,An Interactive Flight Operation with 2-DOF Motion Platform,VRST - Virtual Reality Software and Technology,A,"We propose an interactive flight operation with 2-DOF motion platform that enables user to tilt greatly according to the user posture and VR environment. In order to realize a flight like a hang glider, this work interactively controls the motion platform according to the attitude of the user. By tilting the body back and forth and left and right while keeping the body horizontal based on a posture like the planche exercise, the virtual aircraft tilts in that direction and the motion platform also rolling movements. In addition, since our motion platform with the balance board swings by rolling motion, it is possible to realize a large swing at low-cost and safely.",balance board; hang glider; motion platform; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,Conference Talk Training With a Virtual Audience System,VRST - Virtual Reality Software and Technology,A,"This paper presents the first prototype of a virtual audience system (VAS) specifically designed as a training tool for conference talks. This system has been tailored for university seminars dedicated to the preparation and delivery of scientific talks. We describe the required features which have been identified during the development process. We also summarize the preliminary feedback received from lecturers and students during the first deployment of the system in seminars for bachelor and doctoral students. Finally, we discuss future work and research directions. We believe our system architecture and features are providing interesting insights on the development and integration of VR-based educational tools into university curriculum.",Education; Public Speaking; Training; Virtual Agent; Virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,Content-rich and Expansive Virtual Environments Using Passive Props As World Anchors,VRST - Virtual Reality Software and Technology,A,"In this paper, we present a system that allows developers to add passive haptic feedback into their virtual reality applications by making use of existing physical objects in the user’s real environment. Our approach has minimal dependence on procedural generation and does not limit the virtual space to the dimensions of the physical play-area.",,Abstract,TRUE,
Scopus,conferencePaper,2021,Dealing with a Panic Attack: a Virtual Reality Training Module for Postgraduate Psychology Students,VRST - Virtual Reality Software and Technology,A,"In this paper we present a virtual reality training simulator for postgraduate psychology students. This simulator features an interaction between a clinical psychologist (student) and a patient (virtual agent) suffering from Obsessive Compulsive Disorder (OCD). Our simulation focuses on the form of OCD treatment called “Exposure Therapy”. The traditional way of learning how to perform Exposure Therapy (ET) currently involves watching video recordings and discussing those in the class. In our simulation we conduct an immersive exposure therapy session in VR. This session involves a live interaction with a patient that at one stage triggers a panic attack. Our hypothesis is that the immersive nature of the training session will affect the decision making process of the students so that they are more likely to cease the exposure task than those student participating in a less immersive form of learning (watching a video recording). We also hypothesise that participating in an immersive VR training session is more effective than watching videos, as far as information retention goes.",Exposure Therapy; OCD; Panic Attack; Psychology training; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Double-Layered Cup-Shaped Device to Amplify Taste Sensation of Carbonation by the Electrical Stimulation on the Human Tongue,VRST - Virtual Reality Software and Technology,A,"We show that electrical stimulation on the human tongue amplifies the taste sensation of carbonated beverages. We have developed a novel electric taste system with two components: a cup-shaped device and its circuit for stimulation. The cup-shaped device has a double-layer structure. The circuit has a constant current control circuit and a signal generator, which allow adjustment of the electrical parameters. The device is hygiene when we demonstrate electric taste because the device has two-layered. Thus we can change the inner layer that touches the user’s mouth. The device is also inexpensive and easy to manufacture so that many people can experience them.",Carbonated Beverage; Electric Taste; Electrical Stimulation; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2021,Effects of User’s Gaze on the Unintended Positional Drift in Walk-in-Place,VRST - Virtual Reality Software and Technology,A,"Walk-In-Place (WIP) is a technique in which users perform walking or jogging-like movements in a stationary place to move around in virtual environments (VEs). However, unintended positional drift (UPD) while performing WIP often occurs, thus weakening its benefits of keeping users in a fixed position in a physical space. In this paper, we present our preliminary study exploring whether users’ gaze while WIP affects the direction of the UPD. Participants of the study jogged in a VE five times. Each time, we manipulated their gaze direction by displaying visual information in 5 different locations in their view. Although a correlation between the gaze and UPD direction was not found, we report the results from this study, including the amount of observed drift and preferred location of visual information, and discuss future research directions.",unintended positional drift; virtual reality; walk-in-place,Keywords,TRUE,
Scopus,conferencePaper,2021,Efficient Mapping Technique under Various Spatial Changes for SLAM-based AR Services,VRST - Virtual Reality Software and Technology,A,"Recently, many attempts have been made to apply real-time simultaneous localization and mapping (SLAM) technology to augmented reality (AR) applications. Such AR systems based on SLAM technology are generally implemented by augmenting virtual objects onto a diorama or three-dimensional sculpture. However, a new SLAM map needs to be generated if the space or lighting where the diorama is installed changes. This leads to the problem of updating the coordinate system each time a new SLAM map is generated. Updates to the coordinate system signify that the positions of the virtual objects placed in the AR space change as well. Therefore, we proposed a SLAM map regeneration technique in which the existing coordinate system is maintained even if a new map is generated.",Augmented Reality; Diorama; SLAM based AR,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Emotional Virtual Reality Stroop Task: Pilot Design,VRST - Virtual Reality Software and Technology,A,"Anxiety-inducing and assessment methods in Virtual Reality has been a topic of discussion in recent literature. The importance of the topic is related to the difficulty of getting accurate and timely measurements of anxiety without relying on self-report and breaking the immersion. To this end, the current study utilises the emotional version of a well-established cognitive task; the Stroop Color-Word Task and brings it to Virtual Reality. It consists of three levels; congruent which is used as control and corresponds with no anxiety, incongruent, which corresponds with mild anxiety and emotional, which corresponds with severe anxiety. This pilot serves two functions. The first is to validate the effects of the task using biosignal measurements. The second is to use the bio signal information and the labels to train a machine-learning algorithm. The information collected by the pilot will be used to decide what types of signals and devices to use in the final product, as well as what algorithm and time frame will be better suited for the purpose of accurately determining the user’s anxiety level within Virtual Reality without breaking the immersion.",anxiety; biosensors; biosignals; EEG; emotional stroop; GSR; PPG; VR,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,Evaluating the influence of interaction technology on procedural learning using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Within the context of industry 4.0, this paper studies the influence of interaction technology (Vive controller and Knuckles) on manufacturing assembly procedural training using Virtual Reality. To do so, an experiment with 24 volunteers have been conducted and these participants have been separated in two groups: one using Vive controller and the other using Knuckles. Our conclusions are based on two indicators: Time to realize all tasks and the number of manipulations. This study shows that, after get used to, volunteers using Knuckles are faster than the other group but for some very delicate tasks, they need more manipulations to succeed.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,Exploring Emotion Brushes for a Virtual Reality Painting Tool,VRST - Virtual Reality Software and Technology,A,"We present emoPaint, a virtual reality application that allows users to create paintings with expressive emotion-based brushes and shapes. While previous systems have introduced painting in 3D space, emoPaint focuses on supporting emotional characteristics by allowing users to use brushes corresponding to specific emotions or to create their own emotion brushes and paint with the corresponding visual elements. Our system provides a variety of line textures, shape representations and color palettes for each emotion to enable users to control expression of emotions in their paintings. In this work we describe our implementation and illustrate paintings created using emoPaint.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,Fishtank Sandbox: A Software Framework for Collaborative Usability Testing of Fish Tank Virtual Reality Interaction Techniques,VRST - Virtual Reality Software and Technology,A,"Human-computer interaction researchers have been studying how we can interact with virtual objects in a virtual environment efficiently. Many usability experiments do not have the same control parameters. The lack of consistency makes comparing different interaction techniques difficult. In this article, we present a software framework for usability study in FTVR interaction techniques. The software framework provides fixed control parameters (e.g., task, graphic settings, and measuring parameters), the ability for other researchers to incorporate their interaction techniques as an add-on, and enabling individuals to participate in the experiment over the internet. The article explores a new way for VR/AR researchers to approach usability experiments using the framework and discuss the challenges that it brings.",Fish Tank Virtual Reality; Framework; Usability Testing,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,Fluid3DGuides: A Technique for Structured 3D Drawing in VR,VRST - Virtual Reality Software and Technology,A,"We propose Fluid3DGuides, a drawing guide technique to help users draw structured sketches more accurately in VR. The prototype system continuously infers visual guide lines for the user based on the user’s instant stroke drawing intention and its potential constraint relationship with the existing strokes. We evaluated our prototype through a pilot user study with six participants by comparing the proposed guide technique against the non-guide drawing condition. Participants gave positive comments on ease of use and drawing accuracy. They found that the technique could reduce the time and effort required to find the corrected drawing perspective and obtain more accurate 3D structured sketches.",3D structured sketches; virtual reality; visual guidance,Keywords,TRUE,
Scopus,conferencePaper,2021,Force-Based Foot Gesture Navigation in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Navigation is a primary interaction in virtual reality. Previous research has explored different forms of artificial locomotion techniques for navigation, including hand gestures and body motions. However, few studies have investigated force-based foot gestures as a locomotion technique. We present three force-based foot gestures (Foot Fly, Foot Step and Foot Teleportation) for navigation in a virtual environment, relying on surface electromyography sensors readings from leg muscles. A pilot study comparing our techniques with controller-based techniques indicates that force-based foot gestures can provide a fun and engaging alternative. Of all six input techniques evaluated, Foot Fly was often most preferred despite requiring more exertion than the Controller Fly technique.",Locomotion; Navigation; Other Hardware; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Freehand Interaction in Virtual Reality: Bimanual Gestures for Cross-Workspace Interaction,VRST - Virtual Reality Software and Technology,A,"This work presents the design and evaluation of three bimanual interaction modalities for cross-workspace interaction in virtual reality (VR), in which the user can move items between a personal workspace and a shared workspace. We conducted an empirical study to understand three modalities and their suitability for cross-workspace interaction in VR.",bimanual gestures; cross-workspace interaction; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,GazeMOOC: A Gaze Data Driven Visual Analytics System for MOOC with XR Content,VRST - Virtual Reality Software and Technology,A,"MOOC is widely used and more popular after COVID-19.In order to improve the learning effect, MOOC is evolving with XR technologies such as avatars, virtual scenes and experiments. This paper proposes a novel visual analytics system GazeMOOC, that can evaluate learners’ learning engagement in MOOC with XR content. For same MOOC content, gaze data of all learners are recorded and clustered. By differentiating gaze data of distracted learners and active learners, GazeMOOC can help evaluate MOOC content and learners’ learning engagement.",Data visualization; Eye tracking; Learning Engagement; Mixed Reality,Keywords,TRUE,
Scopus,conferencePaper,2021,HapticPanel: An Open System to Render Haptic Interfaces in Virtual Reality for Manufacturing Industry,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) allows simulation of machine control panels without physical access to the machine, enabling easier and faster initial exploration, testing, and validation of machine panel designs. However, haptic feedback is indispensable if we want to interact with these simulated panels in a realistic manner. We present HapticPanel, an encountered-type haptic system that provides realistic haptic feedback for machine control panels in VR. To ensure a realistic manipulation of input elements, the user’s hand is continuously tracked during interaction with the virtual interface. Based on which virtual element the user intends to manipulate, a motorized panel with stepper motors moves a corresponding physical input element in front of the user’s hand, enabling realistic physical interaction.",Engineering Haptic Interactive Systems; Machine Interfaces; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,HoloKeys: Interactive Piano Education Using Augmented Reality and IoT,VRST - Virtual Reality Software and Technology,A,"The rise of online learning poses unique challenges in music education, where live demonstration and musical synchronization are critical for student success. We present HoloKeys, a music education interface which allows instructors to play remotely located pianos using an augmented reality headset and wifi-enabled microcontrollers. This approach allows students to receive distance education which is more direct, immersive, and comprehensive than conventional video conferencing allows for. HoloKeys enables remote students to observe live instructional demonstration on a physical keyboard in their immediate environment just as they would in traditional settings. HoloKeys consists of two separate components: an augmented reality user interface and a piano playing apparatus. Our system aims to extend online music education beyond desktop platforms into the physical world, thereby addressing crucial obstacles encountered by educators and students transitioning into online education.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,Immersive Visual Interaction with Autonomous Multi-Vehicle Systems,VRST - Virtual Reality Software and Technology,A,"With the emergence of multi-vehicular autonomous systems, such as AI controlled multiple fully autonomous vehicles, we need novel systems that provide tools for planning, executing, and reviewing of missions and keeping humans in the loop during all phases. We therefore present an immersive visualization system for interacting with these systems at a higher cognitive level than piloting of individual vehicles. Our system provides both desktop and VR modes for visual interaction with the robotic multi-vehicle AI system.",UAV; unmanned vehicle; USV; virtual reality; visualisation,Keywords,TRUE,
Scopus,conferencePaper,2021,Incorporating Human Behavior in VR Compartmental Simulation Models,VRST - Virtual Reality Software and Technology,A,"A novel strand of Coronavirus has affected a large number of individuals worldwide, putting a considerable stress to national health services and causing many deaths. Many control measures have been put in place across different countries with the aim to save lives at the cost of personal freedom. Computer simulations have played a role in providing policy makers with critical information about the virus. However, despite their importance in applied epidemiology, general simulation models, are difficult to validate because of how hard it is to predict and model human behavior. To this end, we propose a different approach by developing a virtual reality (VR) multi-agent virus propagation system where a group of agents interact with the user in a university setting. We created a VR digital twin replica of a building in the University of Derby campus, to enhance the user’s immersion in our study. Our work integrates human behavior seamlessly in a simulation model and we believe that this approach is crucial to have a deeper understanding on how to control the spread of a virus such as COVID-19.",,Abstract,TRUE,
Scopus,conferencePaper,2021,Managing a Crisis in Virtual Reality - Tackling a Wildfire,VRST - Virtual Reality Software and Technology,A,"In this paper we present a virtual reality application, where multiple users can observe and interact with a portion of geo-referenced terrain where a real wildfire took place. The application presents a layout with two maps, one is a three-dimensional view with terrain elevation and the other is a conventional two-dimensional view. The VR users can control different layers (roads, waterways, etc), control the wildfire’s playback, command vehicles to change positions and paint the terrain conveying information to one-another. This work explores how users interact with map visualizations and plan for a crisis management scenario within a virtual environment.",crisis management systems; geospatial visualization; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Miniature AR: Multi-view 6DOF Virtual Object Visualization for a Miniature Diorama,VRST - Virtual Reality Software and Technology,A,"We describe a miniature diorama AR system called ‘Miniature AR’ which can be applied to a mechanical diorama to extend the content’s feasibility by overlapping virtual objects on the complex diorama structure. The previous AR researches for the diorama are usually based on 2D planar recognition, and multiple user experience cannot be considered due to multi-devices synchronization. To overcome these constraints, in this paper, we show a new diorama AR system suitable for a tiny complex structure. The contributions of our work are i) design of the diorama AR system, ii) AR space generation and 6DOF view device tracking for diorama, iii) multiple view and event synchronization for multiple users. The utility of the approach has been demonstrated under a real diorama environment (miniature of a ski slope) using mobile devices.",6DOF view tracking; augmented reality; diorama; miniature AR,Keywords,TRUE,
Scopus,conferencePaper,2021,Multi-Componential Analysis of Emotions Using Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In this study, we propose our data-driven approach to investigate the emotional experience triggered using Virtual Reality (VR) games. We considered a full Component Process Model (CPM) which theorise emotional experience as a multi-process phenomenon. We validated the possibility of the proposed approach through a pilot experiment and confirmed that VR games can be used to trigger a diverse range of emotions. Using hierarchical clustering, we showed a clear distinction between positive and negative emotion in the CPM space.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2021,Multi-View AR Streams for Interactive 3D Remote Teaching,VRST - Virtual Reality Software and Technology,A,"In this work, we present a system that adds augmented reality interaction and 3D-space utilization to educational videoconferencing for a more engaging distance learning experience. We developed infrastructure and user interfaces that enable the use of an instructor’s physical 3D space as a teaching stage, promote student interaction, and take advantage of the flexibility of adding virtual content to the physical world. The system is implemented using hand-held mobile augmented reality to maximize device availability, scalability, and ready deployment, elevating traditional video lectures to immersive mixed reality experiences. We use multiple devices on the teacher’s end to provide different simultaneous views of a teaching space towards a better understanding of the 3D space.",AR; multi-view; remote teaching; telepresence; video conferencing,Abstract,TRUE,
Scopus,conferencePaper,2021,Natural walking speed prediction in Virtual Reality while using target selection-based locomotion,VRST - Virtual Reality Software and Technology,A,"Travelling speed plays an essential role in the overall user experience while navigating inside a virtual environment. Researchers have used various travelling speed that matches the user speed profile in order to give a natural walking experience. However, predicting a user’s instantaneous walking speed can be challenging when there is no continuous input from the user. Target selection-based techniques are those where the user selects the target to reach there automatically. These techniques also lack naturalness due to their low interaction fidelity. In this work, we have proposed a mathematical model that can dynamically compute the instantaneous natural walking speed while moving from one point to another in a virtual environment. We formulated our model with the help of user studies.",Natural walking speed prediction; Target selection-based locomotion speed; Teleportation speed; Travelling speed in VR; Virtual Reality Locomotion,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,Of Leaders and Directors: A visual model to describe and analyse persistent visual cues directing to single out-of view targets,VRST - Virtual Reality Software and Technology,A,Researchers have come up with many visual cues that can guide Virtual (VR) and Augmented Reality (AR) users to out of view objects. The paper provides a classification of cues and tasks and visual model to describe and analyse cues to support their design.,,Abstract,TRUE,
Scopus,conferencePaper,2021,ProMVR - Protein Multiplayer Virtual Reality Tool,VRST - Virtual Reality Software and Technology,A,"Due to the pandemic limitations caused by Covid-19, people need to work at home and carry on the meetings virtually. Virtual meeting tools start popularizing and thriving. Those tools allow users to see each other through screen and camera, chat through voice and text, and share content or ideas through screen share. However, screen sharing protein models through virtual meetings is not easy due to the difficulty of viewing protein 3D (Three Dimensional) structures from a 2D (Two Dimensional) screen. Moreover, interactions upon a protein are also limited.&nbsp;ProMVR is a tool the author developed to tackle the issue that protein designers may find limitations working in a traditional 2D or 3D environment and they may find it hard to communicate their ideas with other designers. Since ProMVR is a VR tool, it allows users to “jump into” a virtual environment, take a close look at protein models, and have intuitive interactions.",Multiplayer; Protein visualization; Virtual reality; Voice and text chat,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,Recreating a Medieval Mill as a Virtual Learning Environment,VRST - Virtual Reality Software and Technology,A,"Historic buildings shown in open-air museums often lack a good accessibility and visitors rarely can interact with them as well as displayed tools to learn about processes. Providing these buildings in Virtual Reality could be a great supplement for museums to provide accessible and interactive offers. To investigate the effectiveness of this approach and to derive design guidelines, we developed an interactive virtual replicate of a medieval mill. We present the design of the mill and the results of a preliminary usability evaluation.",immersive learning; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Remote Visual Line-of-Sight: A Remote Platform for the Visualisation and Control of an Indoor Drone using Virtual Reality,VRST - Virtual Reality Software and Technology,A,The COVID-19 pandemic has created the distinct challenge for the piloting of drones/other UAVs for researchers and educators who are restricted to working remotely. We propose a Remote Visual Line-of-Sight system that leverages the advantages of Virtual Reality (VR) and motion capture to allow users to fly a real-world drone from a remote location. The system was developed while our researcher (VR operator) was remotely working in Vietnam with the enclosed real-world environment located in Australia. Our paper will present the system design and the challenges found during the development of our system.,Drones Technology; Human-Robot Interaction; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Safety First: A Study of Users’ Perception of VR Adoption in Vehicles,VRST - Virtual Reality Software and Technology,A,"The increasing ubiquity and mobility of VR devices has introduced novel use cases, one of which is using VR while in dynamic, on-the-go environments. Hence, there is a need to examine the perceptual, cognitive, and behavioral aspects of both the driving experience and VR immersion, and how they influence each other. As an initial step towards this goal, we report on the results of an online survey that investigated users’ perceived safety of using VR in an AV. The results of the survey show a mix of expected and surprising attitudes towards VR-in-the-car.",automated vehicles; in-transit; perceived safety; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,Study of Heart Rate Visualizations on a Virtual Smartwatch,VRST - Virtual Reality Software and Technology,A,"In this paper, we present three visualizations showing heart rate&nbsp;(HR) data collected over time. Two visualizations present a summary chart (bar or radial chart), summarizing the amount of time spent per HR zone (i.e., low, moderate, high intensity). We conducted a pilot study with five participants to evaluate the efficiency of the visualizations when monitoring the intensity of an activity while playing a tennis-like Virtual Reality game. Preliminary results show that participants were performing (with respect to time and accuracy) better with and preferred the bar chart summary.",heart rate visualization; micro visualization; virtual smartwatch,Abstract,TRUE,
Scopus,conferencePaper,2021,Swaying Locomotion: A VR-based Locomotion System through Head Movements,VRST - Virtual Reality Software and Technology,A,"Locomotion systems used in virtual reality (VR) content have a significant impact on the content user experience. One of the most important factors of a walking system in VR is whether it can provide a plausible walking sensation because it is considered directly related to the user’s sense of presence. However, joystick-based and teleportation-based locomotion systems, which are commonly used today, can hardly provide an appropriate sense of presence to a user. To solve this problem, we present Swaying Locomotion, which is a novel VR-based locomotion system that uses head movements to support a user walking in a VR space while actually sitting in real space. Our user study suggests that Swaying Locomotion provides a better walking sensation than the traditional joystick-based approach.",locomotion; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Technical Factors Affecting Augmented Reality User Experiences in Sports Spectating,VRST - Virtual Reality Software and Technology,A,"The maturity of augmented reality (AR) technology and research now paves the way for dissemination of AR outside of the laboratory. However, it is still under-explored which factors are influencing the user experience of an AR application. In this poster, we describe some of the technical factors that could influence the user experience. We focus on a use-case in the field of on-site sports spectating with mobile AR. We present a study design which analyzes the influence of latency, registration accuracy, and jitter as factors on AR user experience.",augmented reality; situated visualization; user experience,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,The Application of Virtual Reality in Student Recruitment,VRST - Virtual Reality Software and Technology,A,"In this paper we present details of a virtual tour and game for VR headset that are designed to investigate an interactive and engaging approach of applying VR to student recruitment for an undergraduate course. The VR tour employs a floating menu to navigate through a set of 360° panoramic photographs of the teaching environment and uses hotspot interaction to display further information about the course. The VR game is a fast-paced shooting game. The course information is embedded on cubes that the player needs to focus on and destroy. The game experience is expected to generate an engaging way to promote the course. This work in progress outlines the concept and development of the prototype, and discusses the next stages of testing in order to evaluate the effectiveness of applying VR to undergraduate student recruitment.",Student Recruitment; Virtual Reality; VR Games,Title_Keywords,TRUE,
Scopus,conferencePaper,2021,The Effect of 2D Stylized Visualization of the Real World for Obstacle Avoidance and Safety in Virtual Reality System Usage,VRST - Virtual Reality Software and Technology,A,"Using virtual reality systems with the head-mounted display can incur interaction difficulties and safety problems because of the user’s view being isolated from the real world operating space. One possible solution is to super-impose the real world objects or environment information onto the virtual scene. A variety of such visualization methods have been proposed, all in hopes of minimizing the negative effects of introducing foreign elements to the original virtual scene. In this poster, we propose to apply the neural style transfer technique to blend in the real world operating environment in the style of the given virtual space to make the super-imposed resulting image as natural as possible, maintaining the sense of immersion with the least level of distraction. Our pilot experimental study has shown that the stylization obscured the clear presentation of the environment and worsened or did not improve the safe user performance, and was neither considered sufficiently natural.",Immersion; Neural style transfer; Obstacle avoidance; Presence; Safety; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,UGRA in VR: A Virtual Reality Simulation for Training Anaesthetists,VRST - Virtual Reality Software and Technology,A,"We present a virtual reality training simulator for medical interns practicing ultrasound-guided regional anaesthesia (UGRA). UGRA is a type of nerve block procedure performed commonly by critical care doctors such as anaesthetists, emergency medicine physicians, and paramedics. This procedure is complex and requires intense training. It is traditionally taught one-on-one by experts and is performed on simulated models long before attempting the procedure on live patients. We present our virtual reality application that allows for training this procedure in a simulated environment. The use of virtual reality makes training future doctors performing UGRA safer and more cost efficient than current approaches.",Ultrasound-guided regional anaesthesia; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,Using Hand Tracking and Voice Commands to Physically Align Virtual Surfaces in AR for Handwriting and Sketching with HoloLens 2,VRST - Virtual Reality Software and Technology,A,"In this paper, we adapt an existing VR framework for handwriting and sketching on physically aligned virtual surfaces to AR environments using the Microsoft HoloLens 2. We demonstrate a multimodal input metaphor to control the framework’s calibration features using hand tracking and voice commands. Our technical evaluation of fingertip/surface accuracy and precision on physical tables and walls is in line with existing measurements on comparable hardware, albeit considerably lower compared to previous work using controller-based VR devices. We discuss design considerations and the benefits of our unified input metaphor suitable for controller tracking and hand tracking systems. We encourage extensions and replication by providing a publicly available reference implementation (https://go.uniwue.de/hci-otss-hololens).",augmented reality; finger tracking; handwriting; holoLens; sketching; stylus; surface alignment; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2021,Visualisation methods for patient monitoring in anaesthetic procedures using augmented reality,VRST - Virtual Reality Software and Technology,A,"In health care, there are still many devices with poorly designed user interfaces that can lead to user errors. Especially in acute care, an error can lead to critical conditions in patients. Previous research has shown that the use of augmented reality can help to better monitor the condition of patients and better detect unforeseen events. The system created in this work is intended to aid in the detection of changes in patient and equipment-data in order to increase detection of critical conditions or errors.",augmented reality; health care; safety-critical-systems,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,VR Rehearse &amp; Perform - A platform for rehearsing in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose VR Rehearse &amp; Perform - a Virtual Reality application for enhancing the rehearsal efforts of performers by providing them access to accurate recreations - both visual and acoustical - of iconic concert venues.",Acoustic Environments; Musicians; Rehearsing; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,VRBT: A Non-pharmacological VR approach towards hypertension,VRST - Virtual Reality Software and Technology,A,"Hypertension is a prevalent disease that is known to affect the vascular system especially to the people with poor living habits and lifestyles. Virtual reality (VR) is effective to interact with people to release their pressure and cheer them up, which however is less conducted towards manipulating blood pressure and hypertension. In this paper, we consider how hypertension can be treated with VR devices and design virtual reality river bathing therapy (VRBT) with respect to a combination of traditional methods through sensory stimulation, audio interventions, and motor training.",Hypertension; non-pharmacological therapy; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2021,"XRSpectator: Immersive, Augmented Sports Spectating",VRST - Virtual Reality Software and Technology,A,"In-stadium sports spectating delivers a unique social experience in a variety of sports. However, in contrast to broadcast delivery, it lacks the provision of real-time information augmentation, like game statistics overlaid on screen. In an earlier iteration, we developed ARSpectator, a prototypical, mobile system which can be brought to the stadium to experience both, the live sport action and situated infographics spatially augmented into the scene. In some situations it is difficult or often impossible to go to the stadium though, for instance because of limited stadium access during pandemics or when wanting to conduct controlled user studies. We address this by turning our ARSpectator system into an indirect augmented reality experience deployed to an immersive, virtual reality head-mounted display: The live stadium experience is delivered by way of a surrounding 360 video recording while maintaining and extending the provision of interactive, situated infographics. With our XRSpectator demo prototype presented here, users can have an ARSpectator experience of a rugby game in our local stadium.",mixed reality; situated visualization; sports spectating,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Carousel: Improving the Accuracy of Virtual Reality Assessments for Inspection Training Tasks,VRST - Virtual Reality Software and Technology,A,"Training simulations in virtual reality (VR) have become a focal point of both research and development due to allowing users to familiarize themselves with procedures and tasks without needing physical objects to interact with or needing to be physically present. However, the increasing popularity of VR training paradigms raises the question: Are VR-based training assessments accurate? Many VR training programs, particularly those focused on inspection tasks, employ simple pass or fail assessments. However, these types of assessments do not necessarily reflect the user’s knowledge. In this paper, we present Carousel, a novel VR-based assessment method that requires users to actively employ their training knowledge by considering all relevant scenarios during assessments. We also present a within-subject user study that compares the accuracy of our new Carousel method to a conventional pass or fail method for a series of virtual object inspection tasks involving shapes and colors. The results of our study indicate that the Carousel method affords significantly more-accurate assessments of a user’s knowledge than the binary-choice method.",training assessments.; virtual inspections; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Leveraging VR Techniques for Efficient Exploration and Interaction in Large and Complex AR Space with Clipped and Small FOV AR Display,VRST - Virtual Reality Software and Technology,A,"In this paper, we propose to take advantage of the digital twinned environment to interact more efficiently in the large and complex AR space in spite of the limited sized and clipped FOV of the AR display. Using the digital twin of the target environment, “magical” VR interaction techniques can be applied, as visualized and overlaid through the small window, while still maintaining the spatial association to the augmented real world. First we consider the use of amplified movement within the corresponding VR twinned space to help the user search, plan, navigate and explore efficiently by providing an effectively larger view and thereby better spatial understanding of the same AR space with less amount of physical movements. Secondly, we also apply the amplified movement and in addition, the stretchable arm to interact with relatively large objects (or largely spaced objects) which cannot be seen in their entirety at a time with the small FOV glass. The results of the experiment with the proposed methods have showed advantages with regards to the interaction performance as the scene became more complex and task more difficult. The work illustrates the concept of and potential for XR based interaction where the user can leverage the advantages of both VR and AR mode operations.",augmented reality; digital twin; extended reality; interaction; mixed reality; navigation; object manipulation; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2022,VR Games for Chronic Pain Management,VRST - Virtual Reality Software and Technology,A,"Chronic pain is a continuous ailment lasting for long periods after the initial injury or disease has healed. Chronic pain is challenging to treat and affects the daily lives of patients. Distraction therapy is a proven method of relieving patients’ discomfort by taking their attention away from the pain. Virtual reality (VR) is a platform for distraction therapy by immersing the user in a virtual world detached from reality. However, there is little research on how physical interactions in VR affect pain management. We present a study to evaluate the effectiveness of physically active, mentally active, and passive interventions in VR using games with chronic pain patients. Our results indicate that physical and mental activities in VR are equally effective at reducing pain. Furthermore, These actively engage patients, while the effects of observing relaxing content persist outside VR. These findings can help inform the design of future VR games targeted at chronic pain management.",Chronic Pain Management; User Study; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,VISTA: User-centered VR Training System for Effectively Deriving Characteristics of People with Autism Spectrum Disorder,VRST - Virtual Reality Software and Technology,A,"Pervasive symptoms of people with autism spectrum disorder (ASD), such as a lack of social and communication skills, are major challenges to be embraced in the workplace. Although much research has proposed VR training programs, their effectiveness is somewhat unclear, since they provide limited, one-sided interactions through fixed scenarios or do not sufficiently reflect the characteristics of people with ASD (e.g., preference for predictable interfaces, sensory issues). In this paper, we present VISTA, a VR-based interactive social skill training system for people with ASD. We ran a user study with 10 people with ASD and 10 neurotypical people to evaluate user experience in VR training and to examine the characteristics of people with ASD based on their physical responses generated by sensor data. The results showed that ASD participants were highly engaged with VISTA and improved self-efficacy after experiencing VISTA. The two groups showed significant differences in sensor signals as the task complexity increased, which demonstrates the importance of considering task complexity in eliciting the characteristics of people with ASD in VR training. Our findings not only extend findings (e.g., low ROI ratio, EDA increase) in previous studies but also provide new insights (e.g., high utterance rate, large variation of pupil diameter), broadening our quantitative understanding of people with ASD.",Autism Spectrum Disorder (ASD); Social skills training system; User study; Virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2022,Exploring User Behaviour in Asymmetric Collaborative Mixed Reality,VRST - Virtual Reality Software and Technology,A,"A common issue for collaborative mixed reality is the asymmetry of interaction with the shared virtual environment. For example, an augmented reality (AR) user might use one type of head-mounted display (HMD) in a physical environment, while a virtual reality (VR) user might wear a different type of HMD and see a virtual model of that physical environment. To explore the effects of such asymmetric interfaces on collaboration we present a study that investigates the behaviour of dyads performing a word puzzle task where one uses AR and the other VR. We examined the collaborative process through questionnaires and behavioural measures based on positional and audio data. We identified relationships between presence and co-presence, accord and co-presence, leadership and talkativeness, head rotation velocity and leadership, and head rotation velocity and talkativeness. We did not find that AR or VR biased subjective responses, though there were interesting behavioural differences: AR users spoke more words, AR users had a higher median head rotation velocity, and VR users travelled further.",augmented reality; collaboration; mixed reality; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Marcus or Mira - Investigating the Perception of Virtual Agent Gender in Virtual Reality Role Play-Training,VRST - Virtual Reality Software and Technology,A,"Immersive virtual training environments are used in various domains. In this work we focus on role-play training in virtual reality. In virtual role-play training conversations and interactions with virtual agents are often fundamental to the training. Therefore, the appearance and behavior of the agents plays an important role when designing role-play training. We focus on the gender appearance of agents, as gender is an important aspect for differentiation between characters. We conducted a study with 40 participants in which we investigated how agents gender appearance influences the perception of the agents´ personality traits and the self-perception of a participants’ assumed role in a training for social skills. This work contributes towards understanding the design-space of virtual agent design, virtual agent gender identity, and the design and development of immersive virtual reality role-play training.",Gender; Training; Virtual Agents; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Evaluating the Effects of Virtual Human Animation on Students in an Immersive VR Classroom Using Eye Movements,VRST - Virtual Reality Software and Technology,A,"Virtual humans presented in VR learning environments have been suggested in previous research to increase immersion and further positively influence learning outcomes. However, how virtual human animations affect students’ real-time behavior during VR learning has not yet been investigated. This work examines the effects of social animations (i.e., hand raising of virtual peer learners) on students’ cognitive response and visual attention behavior during immersion in a VR classroom based on eye movement analysis. Our results show that animated peers that are designed to enhance immersion and provide companionship and social information elicit different responses in students (i.e., cognitive, visual attention, and visual search responses), as reflected in various eye movement metrics such as pupil diameter, fixations, saccades, and dwell times. Furthermore, our results show that the effects of animations on students differ significantly between conditions (20%, 35%, 65%, and 80% of virtual peer learners raising their hands). Our research provides a methodological foundation for investigating the effects of avatar animations on users, further suggesting that such effects should be considered by developers when implementing animated virtual humans in VR. Our findings have important implications for future works on the design of more effective, immersive, and authentic VR environments.",education; eye-tracking; immersive virtual reality; virtual human animation; visual attention,Keywords,TRUE,
Scopus,conferencePaper,2022,Effect of Stereo Deficiencies on Virtual Distal Pointing,VRST - Virtual Reality Software and Technology,A,"Previous work has shown that the mismatch between disparity and optical focus cues, i.e., the vergence and accommodation conflict (VAC), affects virtual hand selection in immersive systems. To investigate if the VAC also affects distal pointing with ray casting, we ran a user study with an ISO 9241:411 multidirectional selection task where participants selected 3D targets with three different VAC conditions, no VAC, i.e., targets placed roughly at 75 cm, which matches the focal plane of the VR headset, constant VAC, i.e., at 400 cm from the user, and varying VAC, where the depth distance of targets changed between 75 cm and 400 cm. According to our results, the varying VAC condition requires the most time and decreases the throughput performance of the participants. It also takes longer for users to select targets in the constant VAC condition than without the VAC. Our results show that in distal pointing placing objects at different depth planes has detrimental effect on the user performance.",Distal Pointing; Ray Casting; Selection; Stereo Deficiencies; Vergence-Accommodation Conflict; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2022,"Rich virtual feedback from sensorimotor interaction may harm, not help, learning in immersive virtual reality",VRST - Virtual Reality Software and Technology,A,"Sensorimotor interactions in the physical world and in immersive virtual reality (IVR) offer different feedback. Actions in the physical world almost always offer multi-modal feedback: pouring a jug of water offers tactile (weight-change), aural (the sound of running water) and visual (water moving out the jug) feedback. Feedback from pouring a virtual jug, however, depends on the IVR’s design. This study examines if the richness of feedback from IVR actions causes a detectable cognitive impact on users. To do this, we compared verb-learning outcomes between two conditions in which participants make actions with objects and (1) audiovisual feedback is presented; (2) audiovisual feedback is not presented. We found that participants (n = 74) had cognitively distinct outcomes based on the type of audiovisual feedback experienced, with a high feedback experience harming learning outcomes compared with a low feedback one. This result has implications for IVR system design and theories of cognition and memorisation.",embodiment; gestural input; interactive virtual environments; language; learning; motivation; presence; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Exploration of Form Factor and Bimanual 3D Manipulation Performance of Rollable In-hand VR Controller,VRST - Virtual Reality Software and Technology,A,"Virtual reality (VR) environments are expected to become future workspaces. An effective bimanual 3D manipulation technique would be essential to support this vision. A ball-shaped tangible input device that can be rolled in a hand is known to be useful for 3D object manipulation because such devices allow users to utilize their finger dexterity. In this study, we further explored the potential of a rollable in-hand controller. First, we evaluated the effects of its form factor on user behavior and performance. Although the size and shape of a rollable controller are expected to influence user behavior and performance, their effects have not been empirically explored in prior works. Next, we evaluated a rollable controller on bimanual 3D assembly tasks. A rollable controller may incur a high mental load as it requires users to use finger dexterity; therefore, the benefit of using such a device in each hand is not obvious. We found that a 5 cm-diameter ball-shaped controller was the most effective among the sizes and forms that we considered, and that a pair of in-hand rollable controllers showed significantly faster completion time than a pair of VR controllers for complex bimanual assembly tasks involving frequent rotations.",Bimanual Task; Finger Dexterity; Rollable In-hand Controller; Tangible Interface; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Eliciting Multimodal Gesture+Speech Interactions in a Multi-Object Augmented Reality Environment,VRST - Virtual Reality Software and Technology,A,"As augmented reality (AR) technology and hardware become more mature and affordable, researchers have been exploring more intuitive and discoverable interaction techniques for immersive environments. This paper investigates multimodal interaction for 3D object manipulation in a multi-object AR environment. To identify the user-defined gestures, we conducted an elicitation study involving 24 participants and 22 referents using an augmented reality headset. It yielded 528 proposals and generated a winning gesture set with 25 gestures after binning and ranking all gesture proposals. We found that for the same task, the same gesture was preferred for both one and two-object manipulation, although both hands were used in the two-object scenario. We present the gestures and speech results, and the differences compared to similar studies in a single object AR environment. The study also explored the association between speech expressions and gesture stroke during object manipulation, which could improve the recognizer efficiency in augmented reality headsets.",augmented reality; elicitation; gesture and speech interaction; multi-object AR environment; multimodal interaction,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Performance Analysis of Saccades for Primary and Confirmatory Target Selection,VRST - Virtual Reality Software and Technology,A,"In eye-gaze-based selection, dwell suffers from several issues, e.g., the Midas Touch problem. Here we investigate saccade-based selection techniques as an alternative to dwell. First, we designed a novel user interface (UI) for Actigaze and used it with (goal-crossing) saccades for confirming the selection of small targets (i.e., &lt; 1.5-2°). We compared it with three other variants of Actigaze (with button press, dwell, and target reverse crossing) and two variants of target magnification (with button press and dwell). Magnification-dwell exhibited the most promising performance. For Actigaze, goal-crossing was the fastest option but suffered the most errors. We then evaluated goal-crossing as a primary selection technique for normal-sized targets (≥ 2°) and implemented a novel UI for such interaction. Results revealed that dwell achieved the best performance. Yet, we identified goal-crossing as a good compromise between dwell and button press. Our findings thus identify novel options for gaze-only interaction.",Activation Methods; Eye-Gaze Tracking; Fitts’ Law; Saccade; Selection Techniques; Small Targets; Target Reverse Crossing; Throughput; Virtual Reality,Keywords,TRUE,
Scopus,conferencePaper,2022,Precueing Sequential Rotation Tasks in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Augmented reality has been used to improve sequential-task performance by cueing information about a current task step and precueing information about future steps. Existing work has shown the benefits of precueing movement (translation) information. However, rotation is also a major component in many real-life tasks, such as turning knobs to adjust parameters on a console. We developed an AR testbed to investigate whether and how much precued rotation information can improve user performance. We consider two unimanual tasks: one requires a user to make sequential rotations of a single object, and the other requires the user to move their hand between multiple objects to rotate them in sequence. We conducted a user study to explore these two tasks using circular arrows to communicate rotation. In the single-object task, we examined the impact of number of precues and visualization style on user performance. Results show that precues improved performance and that arrows with highlighted heads and tails, with each destination aligned with the next origin, yielded the shortest completion time on average. In the multiple-object task, we explored whether rotation precues can be helpful in conjunction with movement precues. Here, using a rotation cue without rotation precues in conjunction with a movement cue and movement precues performed the best, implying that rotation precues were not helpful when movement was also required.",cueing; object rotation; precueing,Title_Abstract,TRUE,
Scopus,conferencePaper,2022,Effects of Environmental Noise Levels on Patient Handoff Communication in a Mixed Reality Simulation,VRST - Virtual Reality Software and Technology,A,"When medical caregivers transfer patients to another person’s care (a patient handoff), it is essential they effectively communicate the patient’s condition to ensure the best possible health outcomes. Emergency situations caused by mass casualty events (e.g., natural disasters) introduce additional difficulties to handoff procedures such as environmental noise. We created a projected mixed reality simulation of a handoff scenario involving a medical evacuation by air and tested how low, medium, and high levels of helicopter noise affected participants’ handoff experience, handoff performance, and behaviors. Through a human-subjects experimental design study (N = 21), we found that the addition of noise increased participants’ subjective stress and task load, decreased their self-assessed and actual performance, and caused participants to speak louder. Participants also stood closer to the virtual human sending the handoff information when listening to the handoff than they stood to the receiver when relaying the handoff information. We discuss implications for the design of handoff training simulations and avenues for future handoff communication research.",environmental noise; human-subject research; Patient handoffs; virtual and mixed reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Timeline Design Space for Immersive Exploration of Time-Varying Spatial 3D Data,VRST - Virtual Reality Software and Technology,A,"Timelines are common visualizations to represent and manipulate temporal data. However, timeline visualizations rarely consider spatio-temporal 3D data (e.g. mesh or volumetric models) directly. In this paper, leveraging the increased workspace and 3D interaction capabilities of virtual reality (VR), we first propose a timeline design space for 3D temporal data extending the timeline design space proposed by Brehmer et al.&nbsp;[7]. The proposed design space adapts the scale, layout and representation dimensions to account for the depth dimension and how the 3D temporal data can be partitioned and structured. Moreover, an additional dimension is introduced, the support, which further characterizes the 3D dimension of the visualization. The design space is complemented by discussing the interaction methods required for the efficient visualization of 3D timelines in VR. Secondly, we evaluate the benefits of 3D timelines through a formal evaluation (n=21). Taken together, our results showed that time-related tasks can be achieved more comfortably using timelines, and more efficiently for specific tasks requiring the analysis of the surrounding temporal context. Finally, we illustrate the use of 3D timelines with a use-case on morphogenetic analysis in which domain experts in cell imaging were involved in the design and evaluation process.",3D temporal data; Multidimensional data; Timelines; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Virtual Air Conditioner’s Airflow Simulation and Visualization in AR,VRST - Virtual Reality Software and Technology,A,"This paper presents a mobile AR system for visualizing airflow and temperature change made by virtual air conditioners. Even though there have been efforts to integrate the results of airflow/temperature simulation into the real world via AR, they support neither interactive modeling of the environments nor real-time simulation. This paper presents an AR system, where 3D mapping and air conditioner installation are made interactively, and then airflow/temperature simulation and visualization are made at real time. The proposed system is designed in a client-server architecture, where the server is in charge of simulation and the rest is taken by the client.",Augmented reality; Flow visualization; HCI; Physics-based simulation,Keywords,TRUE,
Scopus,conferencePaper,2022,Nebula: An Affordable Open-Source and Autonomous Olfactory Display for VR Headsets,VRST - Virtual Reality Software and Technology,A,"The impact of olfactory cues on user experience in virtual reality is increasingly studied. However, results are still heterogeneous and existing studies difficult to replicate, mainly due to a lack of standardized olfactory displays. In that context, we present Nebula, a low-cost, open-source, olfactory display capable of diffusing scents at different diffusion rates using a nebulization process. Nebula can be used with PC VR or autonomous head-mounted displays, making it easily transportable without the need for an external computer. The device was calibrated to diffuse at three diffusion rates: no diffusion, low and high. For each level, the quantity of delivered odor was precisely characterized using a repeated weighting method. The corresponding perceived olfactory intensities were evaluated by a psychophysical experiment on sixteen participants. Results demonstrated the device capability to successfully create three significantly different perceived odor intensities (Friedman test p &lt; 10− 6, Wilcoxon tests padj &lt; 10− 3), without noticeable smell persistence and with limited noise and discomfort. For reproducibility and to stimulate further research in the area, 3D printing files, electronic hardware schemes, and firmware/software source-code are made publicly available.",Autonomous VR experiment; Wearable olfactory display,Abstract,TRUE,
Scopus,conferencePaper,2022,3D Reconstruction of Sculptures from Single Images via Unsupervised Domain Adaptation on Implicit Models,VRST - Virtual Reality Software and Technology,A,"Acquiring the virtual equivalent of exhibits, such as sculptures, in virtual reality (VR) museums, can be labour-intensive and sometimes infeasible. Deep learning based 3D reconstruction approaches allow us to recover 3D shapes from 2D observations, among which single-view-based approaches can reduce the need for human intervention and specialised equipment in acquiring 3D sculptures for VR museums. However, there exist two challenges when attempting to use the well-researched human reconstruction methods: limited data availability and domain shift. Considering sculptures are usually related to humans, we propose our unsupervised 3D domain adaptation method for adapting a single-view 3D implicit reconstruction model from the source (real-world humans) to the target (sculptures) domain. We have compared the generated shapes with other methods and conducted ablation studies as well as a user study to demonstrate the effectiveness of our adaptation method. We also deploy our results in a VR application.",3D Reconstruction; Domain Adaptation; Transfer Learning; Unsupervised Learning; VR,Abstract,TRUE,
Scopus,conferencePaper,2022,The Relative Importance of Depth Cues and Semantic Edges for Indoor Mobility Using Simulated Prosthetic Vision in Immersive Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Visual neuroprostheses (bionic eyes) have the potential to treat degenerative eye diseases that often result in low vision or complete blindness. These devices rely on an external camera to capture the visual scene, which is then translated frame-by-frame into an electrical stimulation pattern that is sent to the implant in the eye. To highlight more meaningful information in the scene, recent studies have tested the effectiveness of deep-learning based computer vision techniques, such as depth estimation to highlight nearby obstacles (DepthOnly mode) and semantic edge detection to outline important objects in the scene (EdgesOnly mode). However, nobody has yet attempted to combine the two, either by presenting them together (EdgesAndDepth) or by giving the user the ability to flexibly switch between them (EdgesOrDepth). Here, we used a neurobiologically inspired model of simulated prosthetic vision (SPV) in an immersive virtual reality (VR) environment to test the relative importance of semantic edges and relative depth cues to support the ability to avoid obstacles and identify objects. We found that participants were significantly better at avoiding obstacles using depth-based cues as opposed to relying on edge information alone, and that roughly half the participants preferred the flexibility to switch between modes (EdgesOrDepth). This study highlights the relative importance of depth cues for SPV mobility and is an important first step towards a visual neuroprosthesis that uses computer vision to improve a user’s scene understanding.",bionic vision; indoor mobility; scene simplification; simulated prosthetic vision; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Adaptive Field-of-view Restriction: Limiting Optical Flow to Mitigate Cybersickness in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Dynamic field-of-view (FOV) restriction is a widely used software technique to mitigate cybersickness in commercial virtual reality (VR) applications. The classical FOV restrictor is implemented using a symmetric mask that occludes the periphery in response to translational and/or angular velocity. In this paper, we introduce adaptive field-of-view restriction, a novel technique that responds dynamically based on real-time assessment of optical flow generated by movement through a virtual environment. The adaptive restrictor utilizes an asymmetric mask to obscure regions of the periphery with higher optical flow during virtual locomotion while leaving regions with lower optical flow visible. To evaluate the proposed technique, we conducted a gender-balanced user study (N = 38) in which participants completed in a navigation task in two different types of virtual scenes using controller-based locomotion. Participants were instructed to navigate through either close-quarter or open virtual environments using adaptive restriction, traditional symmetric restriction, or an unrestricted control condition in three VR sessions separated by at least 24 hours. The results showed that the adaptive restrictor was effective in mitigating cybersickness and reducing subjective discomfort, while simultaneously enabling participants to remain immersed for a longer amount of time compared to the control condition. Additionally, presence ratings were significantly higher when using the adaptive restrictor compared to symmetric restriction. In general, these results suggest that adaptive field-of-view restriction based on real-time measurement of optical flow is a promising approach for virtual reality applications that seek to provide a better cost-benefit tradeoff between comfort and a high-fidelity experience.",cybersickness; field-of-view; optical flow; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Sweating Avatars Decrease Perceived Exertion and Increase Perceived Endurance while Cycling in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Avatars are used to represent users in virtual reality (VR) and create embodied experiences. Previous work showed that avatars’ stereotypical appearance can affect users’ physical performance and perceived exertion while exercising in VR. Although sweating is a natural human response to physical effort, surprisingly little is known about the effects of sweating avatars on users. Therefore, we conducted a study with 24 participants to explore the effects of sweating avatars while cycling in VR. We found that visualizing sweat decreases the perceived exertion and increases perceived endurance. Thus, users feel less exerted while embodying sweating avatars. We conclude that sweating avatars contribute to more effective exergames and fitness applications.",avatars; body ownership; exergames; perception of effort; Proteus effect; sweating; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Standing Balance Improvement Using Vibrotactile Feedback in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) users often encounter postural instability, i.e., balance issues, which can be a significant impediment to universal usability and accessibility, particularly for those with balance impairments. Prior research has validated imbalance issues, but little effort has been made to mitigate them. We recruited 39 participants (with balance impairments: 18, without balance impairments: 21) to examine the effect of various vibrotactile feedback techniques on balance in virtual reality, specifically spatial vibrotactile, static vibrotactile, rhythmic vibrotactile, and vibrotactile feedback mapped to the center of pressure (CoP). Participants completed standing visual exploration and standing reach and grasp tasks. According to within-subject results, each vibrotactile feedback enhanced balance in VR significantly (p &lt;.001) for those with and without balance impairments. Spatial and CoP vibrotactile feedback enhanced balance significantly more (p &lt;.001) than other vibrotactile feedback. This study presents strategies that might be used in future virtual environments to enhance standing balance and bring VR closer to universal usage.",accessibiliity; standing balance; vibrotactile feedback; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,The Rubber Hand Illusion in Virtual Reality and the Real World - Comparable but Different,VRST - Virtual Reality Software and Technology,A,"Feeling ownership of a virtual body is crucial for immersive experiences in VR. Knowledge about body ownership is mainly based on rubber hand illusion (RHI) experiments in the real world. Watching a rubber hand being stroked while one’s own hidden hand is synchronously stroked, humans experience the rubber hand as their own hand and underestimate the distance between the rubber hand and the real hand (proprioceptive drift). There is also evidence for a decrease in hand temperature. Although the RHI has been induced in VR, it is unknown whether effects in VR and the real world differ. We conducted a RHI experiment with 24 participants in the real world and in VR and found comparable effects in both environments. However, irrespective of the RHI, proprioceptive drift and temperature differences varied between settings. Our findings validate the utilization of the RHI in VR to increase our understanding of embodying virtual avatars.",avatars; body ownership illusion; disownership; proprioceptive drift; rubber hand illusion; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2022,Walk This Beam: Impact of Different Balance Assistance Strategies and Height Exposure on Performance and Physiological Arousal in VR,VRST - Virtual Reality Software and Technology,A,"Dynamic balance is an essential skill for the human upright gait; therefore, regular balance training can improve postural control and reduce the risk of injury. Even slight variations in walking conditions like height or ground conditions can significantly impact walking performance. Virtual reality is used as a helpful tool to simulate such challenging situations. However, there is no agreement on design strategies for balance training in virtual reality under stressful environmental conditions such as height exposure. We investigate how two different training strategies, imitation learning, and gamified learning, can help dynamic balance control performance across different stress conditions. Moreover, we evaluate the stress response as indexed by peripheral physiological measures of stress, perceived workload, and user experience. Both approaches were tested against a baseline of no instructions and against each other. Thereby, we show that a learning-by-imitation approach immediately helps dynamic balance control, decreases stress, improves attention focus, and diminishes perceived workload. A gamified approach can lead to users being overwhelmed by the additional task. Finally, we discuss how our approaches could be adapted for balance training and applied to injury rehabilitation and prevention.",balance control; physiological arousal; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,“Kapow!”: Studying the Design of Visual Feedback for Representing Contacts in Extended Reality,VRST - Virtual Reality Software and Technology,A,"In absence of haptic feedback, the perception of contact with virtual objects can rapidly become a problem in extended reality (XR) applications. XR developers often rely on visual feedback to inform the user and display contact information. However, as for today, there is no clear path on how to design and assess such visual techniques. In this paper, we propose a design space for the creation of visual feedback techniques meant to represent contact with virtual surfaces in XR. Based on this design space, we conceived a set of various visual techniques, including novel approaches based on onomatopoeia and inspired by cartoons, or visual effects based on physical phenomena. Then, we conducted an online preliminary user study with 60 participants, consisting in assessing 6 visual feedback techniques in terms of user experience. We could notably assess, for the first time, the potential influence of the interaction context by comparing the participants’ answers in two different scenarios: industrial versus entertainment conditions. Taken together, our design space and initial results could inspire XR developers for a wide range of applications in which the augmentation of contact seems prominent, such as for vocational training, industrial assembly/maintenance, surgical simulation, videogames, etc.",Contact; Design Space; Extended Reality; User Experience; Visual Feedback,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Understanding Perspectives for Single- and Multi-Limb Movement Guidance in Virtual 3D Environments,VRST - Virtual Reality Software and Technology,A,"Movement guidance in virtual reality has many applications ranging from physical therapy, assistive systems to sport learning. These movements range from simple single-limb to complex multi-limb movements. While VR supports many perspectives – e.g., first person and third person – it remains unclear how accurate these perspectives communicate different movements. In a user study (N=18), we investigated the influence of perspective, feedback, and movement properties on the accuracy of movement guidance. Participants had on average an angle error of 6.2° for single arm movements, 7.4° for synchronous two arm movements, and 10.3° for synchronous two arm and leg movements. Furthermore, the results show that the two variants of third-person perspectives outperform a first-person perspective for movement guidance (19.9% and 24.3% reduction in angle errors). Qualitative feedback confirms the quantitative data and shows users have a clear preference for third-person perspectives. Through our findings we provide guidance for designers and developers of future VR movement guidance systems.",body visualization; Movement guidance; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Design and Evaluation of Electrotactile Rendering Effects for Finger-Based Interactions in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"The use of electrotactile feedback in Virtual Reality (VR) has shown promising results for providing tactile information and sensations. While progress has been made to provide custom electrotactile feedback for specific interaction tasks, it remains unclear which modulations and rendering algorithms are preferred in rich interaction scenarios. In this paper, we propose a unified tactile rendering architecture and explore the most promising modulations to render finger interactions in VR. Based on a literature review, we designed six electrotactile stimulation patterns/effects (EFXs) striving to render different tactile sensations. In a user study (N=18), we assessed the six EFXs in three diverse finger interactions: 1) tapping on a virtual object; 2) pressing down a virtual button; 3) sliding along a virtual surface. Results showed that the preference for certain EFXs depends on the task at hand. No significant preference was detected for tapping (short and quick contact); EFXs that render dynamic intensities or dynamic spatio-temporal patterns were preferred for pressing (continuous dynamic force); EFXs that render moving sensations were preferred for sliding (surface exploration). The results showed the importance of the coherence between the modulation an the interaction being performed and the study proved the versatility of electrotactile feedback and its efficiency in rendering different haptic information and sensations.",electrotactile feedback; human computer interaction; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,A Method of Estimating the Object of Interest from 3D Object and User’s Gesture in VR,VRST - Virtual Reality Software and Technology,A,"In VR, gaze information is useful for directly or indirectly analyzing a user’s interest. However, there are inconveniences in using the eye tracking in the VR device. To overcome the drawback, we propose a method of estimating an object of interest from user’s gesture instead of eye tracking. LightGBM model is trained by using distance and angle-based features that are extracted from 3d information of the object and the position and rotation of the VR device. We compared accuracy of each feature for VR device combinations and found out that it is more efficient to use all devices instead of individual devices and to use angle-based feature instead of distance-based feature with accuracy of 79.36%.",the object of interest; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2022,A Mixed Reality Platform for Collaborative Technical Assembly Training,VRST - Virtual Reality Software and Technology,A,"We have developed a mixed reality (MR)-based platform for basic mechanical engineering concepts as a learning environment for collaborative assembly tasks. In our platform, multiple co-located users interact with virtual objects simultaneously, and during that time, the platform collects data related to participants’ collaboration and team behavior. We implemented four main sections in the platform including setup, introduction, training, and assessment. The platform provides the opportunity for users to interact with virtual objects while also acquiring technical knowledge. Specifically, for the technical component of the platform, users are asked to assemble a hydraulic pump by manipulating and fitting various parts and pieces into a provided pre-assembled blueprint. We conducted a preliminary expert panel review composed of three experts and received positive feedback and suggestions for further development of the platform.",mixed reality; technical assembly; training,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,An Interactive Haptic Display System with Changeable Hardness Using Magneto-Rheological Fluid,VRST - Virtual Reality Software and Technology,A,"We present a haptic display system with changeable hardness using magneto-rheological (MR) fluid. The major component is the haptic device with layers of MR fluid, contact point and pressure sensors, and electromagnet. The system enables multi-modal interaction using this device with control circuits and a projector. We also developed two types of contents aiming for multi-modal virtual and mixed reality experiences.",display; haptics; hardness; Magneto-rheological fluid; surface,Abstract,TRUE,
Scopus,conferencePaper,2022,Augmented Reality Patient-Specific Registration for Medical Visualization,VRST - Virtual Reality Software and Technology,A,"In recent years, medical research has made extensive use of Augmented Reality (AR) for visualization. These visualizations provide improved 3D understanding and depth perception for surgeons and medical staff during surgical planning, medical training, and procedures. Often, AR in medicine involves impractical and extensive instrumentation in order to provide the precision needed for clinical use. We propose a mobile AR 3D model registration system for use in a practical, non-instrumented hospital setting. Our registration system takes as input a patient-specific model and overlays it on the patient using an accurate pose registration technique that requires a single marker as a point of reference to initialize a point cloud-based pose refinement technique. Our method is automatic, easy to use, and runs in real-time on a mobile phone. We conduct quantitative and qualitative analysis of the registration. The results confirm that our AR pose registration system produces an accurate and visually correct overlay of the medical data in real-time.",3D Pose Estimation; Depth Map; Point Cloud; Visualization,Title_Abstract,TRUE,
Scopus,conferencePaper,2022,Can Haptic Feedback on One Virtual Object Increase the Presence of Another Virtual Object?,VRST - Virtual Reality Software and Technology,A,"This paper investigated whether increased presence from experiencing haptic feedback on one virtual object can transfer to another virtual object. Two similar studies were run in different environments: an immersive virtual environment and a mixed environment. Results showed that participants reported a high presence of untouched virtual object after touching a virtual object in a virtual reality environment. On the other hand, it was difficult to confirm that such presence transfers occurred in an augmented reality environment.",Augmented Reality; Haptics; Presence; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Common Experience Sample 1.0: Developing a sample for comparing the characteristics of haptic displays,VRST - Virtual Reality Software and Technology,A,"Many haptic displays that provide haptic feedback to users have been proposed;however, differences in experimental environments make comparisons of displays difficult. Therefore, we categorized the characteristics of feedback based on existing research, and developed a common experience sample that includes virtual objects necessary for the expression of each characteristic. Additionally, we will study the methods of evaluating displays using the proposed sample, and aim at comparative evaluation of multiple displays.",crossmodal; evaluation; haptic; multimodal; sample; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2022,CourseExpo: An Immersive Collaborative Learning Ecosystem,VRST - Virtual Reality Software and Technology,A,"Inspired by the need for remote learning technologies due to the Covid-19 pandemic and the isolated sense of lonely learners, we reimagined a remote classroom that fosters collaboration, builds community and yet without the constraints of the physical world. This paper presents a collaborative learning ecosystem that resembles a traditional city square where avatars of learners and facilitators wander, commingle, discover, and learn together. Buildings in the city square are learning modules which include typical knowledge units, assessment booths, or custom collaborative sketching studios. Our attempted prototype at realizing this conceptualization demonstrated initial success and we offer recommendations for future work.",Classroom; Collaboration; Learning Ecosystem; Virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2022,Covid Reflections: AR in Public Health Communications,VRST - Virtual Reality Software and Technology,A,"Augmented reality in public health communications is an under-explored field. Researchers forward Covid Reflections, a public health communications installation which employs augmented reality enhanced with AI LiDAR body tracking to engage public audiences in short duration health-oriented experiences. Covid Reflections helps audiences to visualize potential health outcomes of Covid-19 through depicting the process of disease contraction, sickness, and potential hospitalization on a virtual avatar which mirrors the user’s physical body in real-time. The user is immersed in a “virtual first-hand experience” of Covid-19, and is thus supported in drawing concrete conclusions about the potential personal implications of contracting Covid-19.",,Abstract,TRUE,
Scopus,conferencePaper,2022,Dill Pickle: Interactive Theatre Play in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"“Dill Pickle”, is the first interactive immersive theatre experience in virtual reality that uses volumetric capture. In the play, a volumetrically captured actor plays the character of Robert. The user interacts with Robert through utterances that are memorized or prompted with text or audio. The set was recreated through a process of photogrammetry.",Immersive theatre; Virtual Reality; Volumetric Capture,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Dynamic X-Ray Vision in Mixed Reality,VRST - Virtual Reality Software and Technology,A,"X-ray vision, a technique that allows users to see through walls and other obstacles, is a popular technique for Augmented Reality (AR) and Mixed Reality (MR). In this paper, we demonstrate a dynamic X-ray vision window that is rendered in real-time based on the user’s current position and changes with movement in the physical environment. Moreover, the location and transparency of the window are also dynamically rendered based on the user’s eye gaze. We build this X-ray vision window for a current state-of-the-art MR Head-Mounted Device (HMD) – HoloLens 2 [5] by integrating several different features: scene understanding, eye tracking, and clipping primitive.",HoloLens; Mixed Reality; X-ray vision,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Evaluation of Pseudo-Haptics system feedbacking muscle activity,VRST - Virtual Reality Software and Technology,A,"Differences in perceptions between virtual reality (VR) and reality prevent immersion in VR. To improve immersion in VR, many methods have adopted haptic feedback in VR using pseudo-haptics. However, these methods have little evaluated the effect of force feedback on pseudo-haptics that reflect the user’s state. This paper proposes and evaluates the pseudo-haptics system that manipulates the control/display (C/D) ratio between reality and VR using muscle activity measured. We conducted a user study under three conditions: the C/D ratio is constant, large, or small, depending on the muscle activity. Our results indicated that pseudo-haptics were effective for small C/D ratio settings during low myoelectric intensity.",multisensory integration; muscle activity; pseudo-haptics; virtual weight,Abstract,TRUE,
Scopus,conferencePaper,2022,Exploration of inter-marker interactions in Tangible AR,VRST - Virtual Reality Software and Technology,A,"Inter-marker interactions in marker based Augmented Reality mobile applications are limited to movement and placement. In this paper we explore multiple inter-marker interactions in the tangible AR space along with their use cases. We developed prototypes that demonstrate primarily five inter-marker interactions; namely, proximity of two or more markers, placement of makers over each other, flipping of markers, marker as a toggle and marker as a controller. These interactions are designed such that they would correlate with multiple contexts of application. To demonstrate their usage we have chosen lattice structures in Chemistry as the context. Using our prototypes and the insights from initial evaluations, we discuss the benefits and drawbacks of such interaction methods. We further outline the opportunities of using these interactions and extending these concepts in several other contexts.",augumented reality; inter-marker interaction; tangible AR,Abstract,TRUE,
Scopus,conferencePaper,2022,Finger Kinesthetic Haptic Feedback Device Using Shape Memory Alloy-based High-Speed Actuation Technique,VRST - Virtual Reality Software and Technology,A,"Compared to the study on tactile feedback gloves, the kinesthetic feedback device, which has been studied for the past several decades, has difficulties in interacting with the user owing to various problems, such as large size, low portability, and high power consumption. Herein, we present a bidirectional finger kinesthetic feedback device that can provide an immersive virtual reality experience using a shape memory alloy (SMA). The proposed device provides kinesthetic feedback without heterogeneity by integrating efficient power control of the SMA actuator, fast cooling of the SMA within one second, and high-precision motion-tracking technology. The implemented device delivers a gripping and hand spreading force of up to 10N each to the index and middle fingers.",Kinesthetic haptic feedback; Shape memory alloy; Virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Haptic Interaction Module for VR Fishing Leisure Activity,VRST - Virtual Reality Software and Technology,A,This paper presents a tiny haptic interaction module which generates high resistive torque for VR fishing leisure activity. The presented haptic interaction module was developed by magnetic rheological fluids and optimizing its structure. The measured haptic torque was varied from 0.3 N·cm to 2.4 N·cm as the applied voltage increased from 0 V to 5 V. The performance of the proposed actuator was qualitatively evaluated by constructing virtual fishing environment where a user can feel not only the weight of a target object but also its motion.,Haptics; MR actuator; Virtual reality; VR leisure activity,Keywords,TRUE,
Scopus,conferencePaper,2022,Immersive Analytics for Spatio-Temporal Data on a Virtual Globe: Prototype and Emerging Research Challenges,VRST - Virtual Reality Software and Technology,A,"We present our approach for the immersive analysis of spatio-temporal data, using a three-dimensional virtual globe. We display quantitative data as country-shaped elevated polygons and animate elevation levels over time to represent the temporal dimension. This approach allows us to investigate global patterns of behaviour, like pandemic infection data. By using a virtual reality setting, we intend to increase our understanding of spatial data and potential global relationships. Based on the development of our prototype, we outline research challenges we see emerging in this context.",globe visualisation; pandemic data visualisation; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Improving Pedestrian Safety around Autonomous Delivery Robots in Real Environment with Augmented Reality,VRST - Virtual Reality Software and Technology,A,"In recent years, the use of autonomous vehicles and autonomous delivery robots (ADR) has increased. This paper explores how pedestrian safety around moving ADRs can be improved. To reduce pedestrian anxiety, we proposed the display of various real-time information from the ADR in Augmented Reality (AR). A preliminary experiment was conducted in an outdoor environment where an ADR was running, within a 5G network. We found AR has a positive effect in alleviating user anxiety around the ADR.",5G network; Augmented Reality; Autonomous Delivery Robot; VPS,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Investigation of User Performance in Virtual Reality-based Annotation-assisted Remote Robot Control,VRST - Virtual Reality Software and Technology,A,"This poster investigates the use of point cloud processing algorithms to provide annotations for robotic manipulation tasks completed remotely via Virtual Reality (VR). A VR-based system has been developed that receives and visualizes the processed data from real-time RGB-D camera feeds. A real-world robot model has also been developed to provide realistic reactions and control feedback. The targets and the robot model are reconstructed in a VR environment and presented to users in different modalities. The modalities and available information are varied between experimental settings, and the associated task performance is recorded and analyzed. The results accumulated from 192 experiments completed by 8 participants showed that point cloud data is sufficient for completing the task. Additional information, either image stream or preliminary processes presented as annotations, was found to not have a significant impact on the completion time. However, the combination of image stream and colored point cloud data visualization modalities was found to greatly enhance a user’s performance accuracy, with the number of target centers missed being reduced by 40%.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2022,Leveraging multimodal sensory information in cybersickness prediction,VRST - Virtual Reality Software and Technology,A,"Cybersickness is one of the problems that undermines user experience in virtual reality. While many studies are trying to find ways to alleviate cybersickness, only a few have considered cybersickness through multimodal perspectives. In this paper, we propose a multimodal, attention-based cybersickness prediction model. Our model was trained based on a total of 24,300 seconds of data from 27 participants and yielded the F1-score of 0.82. Our study results highlight the potential to model cybersickness from multimodal sensory information with a high level of performance and suggest that the model should be extended using additional, diverse samples.",,Abstract,TRUE,
Scopus,conferencePaper,2022,Mapping of Locomotion Paths between Remote Environments in Mixed Reality using Mesh Deformation,VRST - Virtual Reality Software and Technology,A,"Remote mixed reality (RMR) allows users to be present and interact in other users’ environments through their photorealistic avatars. Common interaction objects are placed on surfaces in each user's environments and interacting with these objects require users to walk towards them. However, since the user's and their avatar's room's spatial configuration are not exactly similar, for a particular user's walking path, an equivalent path must be found in the avatar's environment, according to its environment's spatial configuration. In this work, we use the concept of mesh deformation to obtain this path, where we deform the mesh associated with the user's environment to fit to the spatial configuration of the avatar's environment. This gives us the corresponding mapping of every point between the two environments from which the equivalent path can be generated.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2022,MetaTwin: Synchronizing Physical and Virtual Spaces for Seamless World,VRST - Virtual Reality Software and Technology,A,"This paper presents MetaTwin, a collaborative Metaverse platform that supports one-to-one spatiotemporal synchrony between physical and virtual spaces. The users can interact with other users and surrounding IoT devices without being tied to physical spaces. Resource sharing is implemented to allow users to share media, including presentation slides and music. We deploy MetaTwin in two different network environments (i.e., within the US, Korea-US international) and summarize users’ feedback about the experience.",Collaborative Platform; Digital Twin; IoT; Metaverse; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,PlayMeBack - Cognitive Load Measurement using Different Physiological Cues in a VR Game,VRST - Virtual Reality Software and Technology,A,"We present a Virtual Reality (VR) game, PlayMeBack, to investigate cognitive load measurement in interactive VR environments using pupil dilation, Galvanic Skin Response (GSR), Electroencephalogram (EEG) and Heart Rate (HR). The user is shown different patterns of tiles lighting up and is asked to replay the pattern back pressing the tiles in the same sequence they lit up. The task difficulty depends on the length of the observed pattern (3-6 keys). This task is designed to explore the effect of cognitive load on physiological cues, and if pupil dilation, EEG, GSR and HR can be used as measures of cognitive load.",Cognitive Load; Electroencephalogram; Galvanic Skin Response; Heart Rate.; Pupil Dilation; Virtual Reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Selection of Expanded Data Points in Immersive Analytics,VRST - Virtual Reality Software and Technology,A,"We propose a novel technique to facilitate the selection of data points, a type of data representation we often work with in immersive analytics. We designed and implemented this technique based on the expansion of data points following Fitt’s law. A user study was conducted in an headset-based augmented reality environment. The results significantly highlight the performance of our technique in helping the user select data points and their subjective appreciation in working with the expendable data points.",,Abstract,TRUE,
Scopus,conferencePaper,2022,"Sign Language in Immersive VR: Design, Development, and Evaluation of a Testbed Prototype",VRST - Virtual Reality Software and Technology,A,"Immersive Virtual Reality (IVR) systems support several modalities such as body, finger, eye, and facial expressions tracking, thus they can support sign-language-based communication. The combined utilization of tracking technologies requires careful evaluation to ensure high-fidelity transference of body posture, gestures, and facial expressions in real-time. This paper presents the design, development and evaluation of an IVR system utilizing state-of-the-art tracking options. The system is evaluated by certified sign language teachers to detect usability issues and examine appropriate methodology for large-scale follow-up evaluation by users fluent in sign language.",,Abstract,TRUE,
Scopus,conferencePaper,2022,Size Does Matter: An Experimental Study of Anxiety in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"The emotional response of users induced by VR scenarios has become a topic of interest, however, whether changing the size of objects in VR scenes induces different levels of anxiety remains a question to be studied. In this study, we conducted an experiment to initially reveal how the size of a large object in a VR environment affects changes in participants’ (N = 38) anxiety level and heart rate. To holistically quantify the size of large objects in the VR visual field, we used the omnidirectional field of view occupancy (OFVO) criterion for the first time to represent the dimension of the object in the participant’s entire field of view. The results showed that the participants’ heartbeat and anxiety while viewing the large objects were positively and significantly correlated to OFVO. These study reveals that the increase of object size in VR environments is accompanied by a higher degree of user’s anxiety.",anxiety; large object; user experience; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2022,The Community Game Development Toolkit,VRST - Virtual Reality Software and Technology,A,"The Community Game Development Toolkit is a set of tools that provide an accessible, intuitive work-flow within the Unity game engine for students, artists, researchers and community members to create their own visually rich, interactive 3D stories and immersive environments. The toolkit is designed to support diverse communities to represent their own traditions, rituals and heritages through interactive, visual storytelling, drawing on community members’ own visual assets such as photos, sketches and paintings, without requiring the use of coding or other specialized game-design skills. Projects can be built for desktop, mobile and VR applications. This paper describes the background, implementation and planned future developments of the toolkit, as well the contexts in which it has been used.",manipulation; prototyping/implementation; virtual reality,Keywords,TRUE,
Scopus,conferencePaper,2022,"The Effect of Training Communication Medium on the Social Constructs Co-Presence, Engagement, Rapport, and Trust: Explaining how training communication medium affects the social constructs co-presence, engagement, rapport, and trust",VRST - Virtual Reality Software and Technology,A,"Communication performance is highly context sensitive and difficult to quantify. In the SCOTTIE project (Systematic Communication Objectives and Telecommunications Technology Investigations and Evaluations), the goal is to investigate the impact of the communication medium on team performance and effectiveness. The human decision to travel or replace travel with telecommunications can be extracted from SCOTTIE, rather than relying on intuition and opinion. This poster analyzes four social communication constructs and compares them in Face-to-Face, Video Conferencing, and Virtual Reality training scenarios. Co-presence, engagement, rapport, and trust were the four constructs. Data from 105 participants across the three between-subject conditions showed that engagement was the only construct that had a statistically significant difference between the three training environments.",,Abstract,TRUE,
Scopus,conferencePaper,2022,Using Virtual Reality Food Environments to Study Individual Food Consumer Behavior in an Urban Food Environment,VRST - Virtual Reality Software and Technology,A,"The objective of this research was to explore whether virtual reality can be used to study individual food consumer decision-making and behavior through a public health lens by developing a simulation of an urban food environment that included a street-level scene and three prototypical stores. Twelve participants completed the simulation and a survey. Preliminary results showed that 72.7% of participants bought food from the green grocer, 18.2% from the fast food store, and 9.1% from the supermarket. The mean presence score was 38.9 out of 49 and the mean usability score was 85.9 out of 100. This experiment demonstrates that virtual reality should be further considered as a tool for studying food consumer behavior within a food environment.",food choice; food environment; nutrition disparities; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2022,Visual Considerations for Augmented Reality in Urban Planning,VRST - Virtual Reality Software and Technology,A,"The design process in architecture and urban planning has always been accompanied by a discourse on suitable visual representation. This resulted in both a wealth of visualisation styles and high sensitivity among planners regarding the visual communication of their work. Representation of projects throughout phases of the planning process often adheres to established visual standards, from concept sketch to high-end rendering. A look at contemporary Augmented Reality (AR) apps for urban planning indicates however that the quality and precision of representation seem to lag somewhat behind, entailing risks that projects are misinterpreted. This poster describes our design research on three urban planning apps developed with Swiss municipalities and outlines results that improve visual representation in AR throughout different planning phases.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2022,Visualizing Perceptions of Non-Player Characters in Interactive Virtual Reality Environments,VRST - Virtual Reality Software and Technology,A,"Visual effects and elements to visualize the perceptions of one’s own virtual character (also referred to as Visual Delegates) are often used in video games, e.g., status bars visualize the character’s sense of health, filters on the interface layer visualize the character’s state of mind. It is still largely unexplored whether Visual Delegates can also be used to transfer the perception of non-player characters comprehensibly. Therefore, we developed a medical virtual reality scenario using five different types of Visual Delegates to visualize three different perceptions of a virtual non-player patient. We tested for character assignment in a qualitative user study (N = 20). Our results can be used to decide more effectively what types of Visual Delegates can be used to convey perceptions of non-player characters.",non-player character; NPC; perception; serious games; user interface design; virtual character; virtual reality; visual delegate; VR,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Planning Locomotion Techniques for Virtual Reality Games,VRST - Virtual Reality Software and Technology,A,"Locomotion is a fundamental component in many virtual reality (VR) games. However, few techniques have been designed with game’s demands in mind. In this paper, we propose two locomotion techniques for fast-paced VR games: Repeated Short-Range Teleports and Continuous Movement Pads. We conducted a user study with 27 participants using these techniques against Smooth Locomotion and Teleport in a game-like scenario. We found that Movement Pads can be a suitable alternative for games, with competitive performance on various criteria such as time, damage taken, usability, workload, and user preference. On the other hand, Repeated Short-Range Teleport displayed lower usability and higher mental workload.",Games; Human-Computer Interaction; Locomotion; Movement; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Versatile Mixed-method Locomotion under Free-hand and Controller-based Virtual Reality Interfaces,VRST - Virtual Reality Software and Technology,A,"Locomotion systems that allow the user to interact with large virtual spaces require precise input, competing with the same inputs available for performing a task in the virtual world. Despite extensive research on hand tracking input modalities, there is a lack of a widely adopted mechanism that offers general-purpose, high-precision locomotion across various applications. This research aims to address this gap by proposing a design that combines teleportation with a grab-pull locomotion scheme to bridge the divide between long-distance and high-precision locomotion in both a tracked-controller and free-hand environment. The implementation details for both tracked controller and tracked hand environments are presented and evaluated through a user study. The study findings indicate that each locomotion mechanism holds value for different tasks, with grab-pull providing more benefit in scenarios where smaller, more precise positioning is required. As found in prior research, controller tracking was found to be faster than hand tracking, but all participants were able to successfully use the locomotion system with both interfaces.",controller; free-hand; hand tracking; interaction; locomotion; virtual reality,Title_Keywords,TRUE,
Scopus,conferencePaper,2023,Exploring User Engagement in Immersive Virtual Reality Games through Multimodal Body Movements,VRST - Virtual Reality Software and Technology,A,"User engagement in Virtual Reality (VR) games is crucial for creating immersive and captivating gaming experiences that meet the expectations of players. However, understanding and measuring these levels in VR games presents a challenge for game designers, as current methods, such as self-reports, may be limited in capturing the full extent of user engagement. Additionally, approaches based on biological signals to measure engagement in VR games present complications and challenges, including signal complexity, interpretation difficulties, and ethical concerns. This study explores body movements, as a novel approach to measure user engagement in VR gaming. We employ E4, emteqPRO, and off-the-shelf IMUs to measure the body movements from diverse participants engaged in multiple VR games. Further, we examine the simultaneous occurrence of player motivation and physiological responses to explore potential associations with body movements. Our findings suggest that body movements hold promise as a reliable and objective indicator of user engagement, offering game designers valuable insights on generating more engaging and immersive experiences.",Body Movements; Data-driven methods; Emotions; Engagement; Virtual Reality Games,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Cross-Reality Gaming: Comparing Competition and Collaboration in an Asymmetric Gaming Experience,VRST - Virtual Reality Software and Technology,A,"Due to the level of immersion and differences in the user interface, there can be a very large discrepancy in the user experience between users in immersive systems and non-immersive systems when playing games together. To investigate the impact of the cross-reality experience, which refers to the asymmetric use of eXtended Reality, we aim to understand the different affordances and experiences in an asymmetric setup, where one participant uses a desktop setup with a mouse and keyboard, and one uses a virtual reality (VR) headset and controller in two different task modes, Competition or Collaboration. In our research, a pair of participants played a game in real-time, using either the VR setup or the desktop setup. In Competition mode, the two participants were asked to defeat each other. In Collaboration mode, the pair of participants played as a team and were asked to defeat a pair of AI enemies. Our results show the VR group reported a better gaming experience and perceptual responses compared to the desktop group regardless of game mode, but that the desktop group showed superior gaming performance compared to the VR group in Competition mode.",asymmetric platform; collaboration; competition; cross-platform; cross-reality; gaming experience; shared experience; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Does One Keyboard Fit All? Comparison and Evaluation of Device-Free Augmented Reality Keyboard Designs,VRST - Virtual Reality Software and Technology,A,"Virtual keyboard designs are widely discussed with the increasing prevalence of head-mounted and lightweight Mixed Reality devices. However, isolated design suggestions with distinct implementations may lack comparability in terms of performance, learnability, and user preference. We compare three promising device-free text-entry solutions for Augmented Reality (AR) on the Microsoft HoloLens 2. The virtual keyboards comprise dwell-based eye-gaze input, eye-gaze with pinch-gesture-commit input, and mid-air tap typing on virtual QWERTY-keyboards. We conducted a controlled within-subjects lab experiment with 27 subjects measuring typing performance, task load, usability, and preference across the three keyboards. Users state distinct preferences for the respective keyboards and weight the advantages and disadvantages differently. Considering diverse usage scenarios, subjects would even prefer these input modes over speech or physical keyboard input. The results indicate that virtual keyboard design shall be tailored to individual user preferences. Therefore, this study provides essential insights into designing AR keyboards for heterogeneous user groups.",augmented reality; eye tracking; laboratory experiment; text entry,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Exploring Augmented Reality for Situated Analytics with Many Movable Physical Referents,VRST - Virtual Reality Software and Technology,A,"Situated analytics (SitA) uses visualization in the context of physical referents, typically by using augmented reality (AR). We want to pave the way toward studying SitA in more suitable and realistic settings. Toward this goal, we contribute a testbed to evaluate SitA based on a scenario in which participants play the role of a museum curator and need to organize an exhibition of music artifacts. We conducted two experiments: First, we evaluated an AR headset interface and the testbed itself in an exploratory manner. Second, we compared the AR headset to a tablet interface. We summarize the lessons learned as guidance for designing and evaluating SitA.",Augmented Reality; Immersive analytics; Situated analytics,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Exploring Users' Pointing Performance on Virtual and Physical Large Curved Displays,VRST - Virtual Reality Software and Technology,A,"Large curved displays have emerged as a powerful platform for collaboration, data visualization, and entertainment. These displays provide highly immersive experiences, a wider field of view, and higher satisfaction levels. Yet, large curved displays are not commonly available due to their high costs. With the recent advancement of Head Mounted Displays (HMDs), large curved displays can be simulated in Virtual Reality (VR) with minimal cost and space requirements. However, to consider the virtual display as an alternative to the physical display, it is necessary to uncover user performance differences (e.g., pointing speed and accuracy) between these two platforms. In this paper, we explored users’ pointing performance on both physical and virtual large curved displays. Specifically, with two studies, we investigate users’ performance between the two platforms for standard pointing factors such as target width, target amplitude as well as users’ position relative to the screen. Results from user studies reveal no significant difference in pointing performance between the two platforms when users are located at the same position relative to the screen. In addition, we observe users’ pointing performance improves when they are located at the center of a semi-circular display compared to off-centered positions. We conclude by outlining design implications for pointing on large curved virtual displays. These findings show that large curved virtual displays are a viable alternative to physical displays for pointing tasks.",Curved Display; Display Curvatures; Fitts Law; Large Physical Display; Large Virtual Display; Pointing Performance,Abstract,TRUE,
Scopus,conferencePaper,2023,Re-investigating the Effect of the Vergence-Accommodation Conflict on 3D Pointing,VRST - Virtual Reality Software and Technology,A,"The vergence-accommodation conflict (VAC) limits user performance in current Virtual Reality (VR) systems. In this paper, we investigate the effects of the VAC in a single-focal VR system using three experimental conditions: with no VAC, with a constant VAC, and with a varying VAC. Previous work in this area had yielded conflicting results, so we decided to re-investigate this issue. Eighteen participants performed an ISO 9241:411 task in a study that closely replicates previous work, except that the angle of the task space was rotated 20 degrees downward, to make the task less fatiguing to perform, which addresses a potential confound in previous work. We found that the varying VAC condition had worse performance than the other conditions, which indicates that the contrasting results in previous work were very likely due to biomechanical factors. We hope that our work contributes to the understanding of the influence of the VAC in VR systems and potential strategies for improving user experience and performance in immersive virtual environments.",3D pointing; Fitts’ Law; vergence-accommodation conflict; VR,Abstract,TRUE,
Scopus,conferencePaper,2023,Dialogues For One: Single-User Content Creation Using Immersive Record and Replay,VRST - Virtual Reality Software and Technology,A,"Non-player characters are an essential element of many 3D and virtual reality experiences. They can make the experiences feel more lively and populated. Animation for non-player characters is often motion-captured using expensive hardware and the post-processing steps are time-consuming, especially when capturing multiple people at once. Using record and replay techniques in virtual reality can offer cheaper and easier ways of motion capture since the user is already tracked. We use immersive record and replay to enable a single user to create stacked recordings of themselves. We provide tools to help the user interact with their previous recorded self and in doing so allow them to create believable interactive scenarios with multiple characters that can be used to populate virtual environments. We create a small dialogue dataset with two amateur actors who used our tool to record dialogues alone and together in virtual reality. To evaluate whether stacked recordings are qualitatively comparable to conventional multi-user recordings and whether people could tell the difference between the two, we conducted two user studies, one online and one in virtual reality with 89 participants in total. We found that participants could not tell the difference and even slightly preferred stacked recordings.",content creation; record and replay; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Dynascape : Immersive Authoring of Real-World Dynamic Scenes with Spatially Tracked RGB-D Videos,VRST - Virtual Reality Software and Technology,A,"In this paper, we present Dynascape, an immersive approach to the composition and playback of dynamic real-world scenes in mixed and virtual reality. We use spatially tracked RGB-D cameras to capture point cloud representations of arbitrary dynamic real-world scenes. Dynascape provides a suite of tools for spatial and temporal editing and composition of such scenes, as well as fine control over their visual appearance. We also explore strategies for spatiotemporal navigation and different tools for the in situ authoring and viewing of mixed and virtual reality scenes. Dynascape is intended as a research platform for exploring the creative potential of dynamic point clouds captured with mobile, tracked RGB-D cameras. We believe our work represents a first attempt to author and playback spatially tracked RGB-D video in an immersive environment, and opens up new possibilities for involving dynamic 3D scenes in virtual space.",Data Visualization; Human Computer Interaction; Immersive Authoring,Abstract,TRUE,
Scopus,conferencePaper,2023,Exploring Unimodal Notification Interaction and Display Methods in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"As we develop computing platforms for augmented reality (AR) head-mounted display (HMDs) technologies for social or workplace environments, understanding how users interact with notifications in immersive environments has become crucial. We researched effectiveness and user preferences of different interaction modalities for notifications, along with two types of notification display methods. In our study, participants were immersed in a simulated cooking environment using an AR-HMD, where they had to fulfill customer orders. During the cooking process, participants received notifications related to customer orders and ingredient updates. They were given three interaction modes for those notifications: voice commands, eye gaze and dwell, and hand gestures. To manage multiple notifications at once, we also researched two different notification list displays, one attached to the user’s hand and one in the world. Results indicate that participants preferred using their hands to interact with notifications and having the list of notifications attached to their hands. Voice and gaze interaction was perceived as having lower usability than touch.",augmented reality; display methods; eye gaze; interaction; notifications; voice commands,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Intuitive User Interfaces for Real-Time Magnification in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Various reasons exist why humans desire to magnify portions of our visually perceived surroundings, e.g., because they are too far away or too small to see with the naked eye. Different technologies are used to facilitate magnification, from telescopes to microscopes using monocular or binocular designs. In particular, modern digital cameras capable of optical and/or digital zoom are very flexible as their high-resolution imagery can be presented to users in real-time with displays and interfaces allowing control over the magnification. In this paper, we present a novel design space of intuitive augmented reality (AR) magnifications where an AR head-mounted display is used for the presentation of real-time magnified camera imagery. We present a user study evaluating and comparing different visual presentation methods and AR interaction techniques. Our results show different advantages for unimanual, bimanual, and situated AR magnification window interfaces, near versus far vergence distances for the image presentation, and five different user interfaces for specifying the scaling factor of the imagery.",3D User Interfaces; Augmented Reality; Magnification,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,When Filters Escape the Smartphone: Exploring Acceptance and Concerns Regarding Augmented Expression of Social Identity for Everyday AR,VRST - Virtual Reality Software and Technology,A,"Mass adoption of Everyday Augmented Reality (AR) glasses will enable pervasive augmentation of our expression of social identity through AR filters, transforming our perception of self and others. However, despite filters’ prominent and often problematic usage in social media, research has yet to reflect on the potential impact AR filters might have when brought into everyday life. Informed by our survey of 300 existing popular AR filters used on Snapchat, Instagram and Tiktok, we conducted an AR-in-VR user study where participants (N=24) were exposed to 18 filters across six categories. We evaluated the social acceptability of these augmentations around others and attitudes towards an individual’s augmented self.Our findings highlight 1) how users broadly respected another individual’s augmented self; 2) positive use cases, such as supporting the presentation of gender identity; and 3) tensions around applying AR filters to others (e.g. censorship, changing protected characteristics) and their impact on self-perception (e.g. perpetuating unrealistic beauty standards). We raise questions regarding the rights of individuals to augment and be augmented that provoke the need for further consideration of AR augmentations in society.",AR Filters; Augmented Identity; Augmented Reality; Identity; Mediated Perception; Self-Presentation; Social Identity,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,From Clocks to Pendulums: A Study on the Influence of External Moving Objects on Time Perception in Virtual Environments,VRST - Virtual Reality Software and Technology,A,"This paper investigates the relationship between perceived object motion and the experience of time in virtual environments. We developed an application to measure how the motion properties of virtual objects and the degree of immersion and embodiment may affect the time experience. A first study (n = 145) was conducted remotely using an online video survey, while a second study (n = 60) was conducted under laboratory conditions in virtual reality (VR). Participants in both studies experienced seven different virtual objects in a randomized order and then answered questions about time experience. The VR study added an ""embodiment"" condition in which participants were either represented by a virtual full body or lacked any form of virtual body representation. In both studies, time was judged to pass faster when viewing oscillating motion in immersive and non-immersive settings and independently of the presence or absence of a virtual body. This trend was strongest when virtual pendulums were displayed. Both studies also found a significant inverse correlation between the passage of time and boredom. Our results support the development of applications that manipulate the perception of time in virtual environments for therapeutic use, for instance, for disorders such as depression, autism, and schizophrenia. Disturbances in the perception of time are known to be associated with these disorders.",embodiment; extended reality; mixed reality; time perception; virtual reality; virtual time; virtual zeitgeber,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Music Therapy in Virtual Reality for Autistic Children with Severe Learning Disabilities,VRST - Virtual Reality Software and Technology,A,"Music Therapy (MT) has shown many benefits in helping autistic children, but some challenges remain due to children’s social anxiety and sensory issues. Yet, very few studies have investigated how Virtual Reality (VR) could help to increase the accessibility of MT approaches. This paper presents an exploratory study investigating the use of VR to perform MT sessions for autistic children with severe learning disabilities and complex needs. The study is performed in terms of acceptability, usability, and social communication. A collaborative MT approach was designed in close collaboration with music therapists from Denmark and psychologists from France, using head-mounted display-based VR. Testing were conducted with thirteen children with various neurodevelopmental conditions and intellectual disabilities at a children’s day hospital in Paris. The results indicate positive acceptability and usability for these children, and suggest a positive effect of MT in VR regarding communication.",autism; intellectual disabilities; music therapy; virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Gaze Assistance for Older Adults during Throwing in Virtual Reality and its Effects on Performance and Motivation,VRST - Virtual Reality Software and Technology,A,"Initial motivation when starting exergaming is a key factor towards enabling long-term engagement and adherence, especially among older adults. To increase, in particular, the initial motivation of older adults, we introduce the concept of diminishing gaze assistance (GA), assess its feasibility for virtual reality (VR) exergames, and investigate the effects on motor learning, performance, and motivation in older adult users. First, we conducted a focus group followed by a pre-study on the development of VR exergames for older adults and VR gaze assistance. The results informed the design and implementation of our gaze-assisted throwing exergame, which was then evaluated in a follow-up main study. Participants of the main study were randomly assigned to the GA and Motor (control) group, and had to complete a VR throwing task, in which participants had to aim and throw at three targets at varying angles. The GA group received declining gaze assistance, in which the ball trajectory was initially guided by their gaze (rather than their physical (motor) throwing) before guidance was gradually reduced until their physical (motor) throwing ability was solely responsible for hitting the target. Motivation and user experience were assessed using the Questionnaire on Current Motivation before and during, and the short scale of intrinsic motivation questionnaire after the task. The results show that the GA was generally perceived positively. In particular, the initial confidence of the GA group was rated higher, and we observed evidence suggesting increased confidence throughout the trial.",errorless learning; eyetracking; gaze assistance; motivation,Title_Abstract,TRUE,
Scopus,conferencePaper,2023,GazeRayCursor: Facilitating Virtual Reality Target Selection by Blending Gaze and Controller Raycasting,VRST - Virtual Reality Software and Technology,A,"Raycasting is a common method for target selection in virtual reality (VR). However, it results in selection ambiguity whenever a ray intersects multiple targets that are located at different depths. To resolve these ambiguities, we estimate object depth by projecting the closest intersection between the gaze and controller rays onto the controller ray. An evaluation of this method found that it significantly outperformed a previous eye convergence depth estimation technique. Based on these results, we developed GazeRayCursor, a novel selection technique that enhances Raycasting, by leveraging gaze for object depth estimation. In a second study, we compared two variations of GazeRayCursor with RayCursor, a recent technique developed for a similar purpose, in a dense target environment. The results indicated that GazeRayCursor decreased selection time by 45.0% and reduced manual depth adjustments by a factor of 10 in a dense target environment. Our findings showed that GazeRayCursor is an effective method for target disambiguation in VR selection without incurring extra effort.",controller; disambiguation; gaze; object selection; raycasting; VR,Title_Abstract,TRUE,
Scopus,conferencePaper,2023,Evaluating Augmented Reality Communication: How Can We Teach Procedural Skill in AR?,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) has great potential for use in healthcare applications, especially remote medical training and supervision. In this paper, we analyze the usage of an AR communication system to teach a medical procedure, the placement of a central venous catheter (CVC) under ultrasound guidance. We examine various AR communication and collaboration components, including gestural communication, volumetric information, annotations, augmented objects, and augmented screens. We compare how teaching in AR differs from teaching through videoconferencing-based communication. Our results include a detailed medical training steps analysis in which we compare how verbal and visual communication differs between video and AR training. We identify procedural steps in which medical experts give visual instructions utilizing AR components. We examine the change in AR usage and interaction over time and recognize patterns between users. Moreover, AR design recommendations are given based on post-training interviews.",Augmented Reality; Remote Collaboration; Telehealth; Volumetric Communication,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Hands-on DNA: Exploring the Impact of Virtual Reality on Teaching DNA Structure and Function,VRST - Virtual Reality Software and Technology,A,"Molecular biology is a demanding subject, requiring students to master abstract, three-dimensional (3D) concepts across a range of spatial scales. Virtual reality (VR) is a medium that excels at portraying scale and 3D concepts, and allows people to have tangible experiences of otherwise intangible subjects. This paper describes Hands-on DNA, a virtual reality learning experience for teaching undergraduate university students about the scale and structure of deoxyribose nucleic acid (DNA), a central molecule in molecular biology. The intention of Hands-on DNA is to leverage the advantages of virtual reality against specific challenges faced in teaching molecular biology. We derive design requirements motivated by pedagogy, provide guidelines, and discuss lessons learned during development. Our user study shows that students perceive Hands-on DNA as a fun, engaging, effective learning tool, and that it addresses some of the weaknesses in molecular biology education. Our results also suggest that new interaction techniques to support learning in VR need to be developed (e.g., for note taking) and that the increasing penetration of recreational VR increases students’ expectations and hence the risk of students being disappointed of VR learning tools.",constructivism; DNA; education; gamification; molecular biology; multimedia education; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Measuring and Comparing Collaborative Visualization Behaviors in Desktop and Augmented Reality Environments,VRST - Virtual Reality Software and Technology,A,"Augmented reality (AR) provides a significant opportunity to improve collaboration between co-located team members jointly analyzing data visualizations, but existing rigorous studies are lacking. We present a novel method for qualitatively encoding the positions of co-located users collaborating with head-mounted displays (HMDs) to assist in reliably analyzing collaboration styles and behaviors. We then perform a user study on the collaborative behaviors of multiple, co-located synchronously collaborating users in AR to demonstrate this method in practice and contribute to the shortfall of such studies in the existing literature. Pairs of users performed analysis tasks on several data visualizations using both AR and traditional desktop displays. To provide a robust evaluation, we collected several types of data, including software logging of participant positioning, qualitative analysis of video recordings of participant sessions, and pre- and post-study questionnaires including the NASA TLX survey. Our results suggest that the independent viewports of AR headsets reduce the need to verbally communicate about navigating around the visualization and encourage face-to-face and non-verbal communication. Our novel positional encoding method also revealed the overlap of task and communication spaces vary based on the needs of the collaborators.",Augmented reality; Co-located collaboration; Visualization,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Vicarious: Context-aware Viewpoints Selection for Mixed Reality Collaboration,VRST - Virtual Reality Software and Technology,A,"Mixed-perspective, combining egocentric (first-person) and exocentric (third-person) viewpoints, have been shown to improve the collaborative experience in remote settings. Such experiences allow remote users to switch between different viewpoints to gain alternative perspectives of the remote space. However, existing systems lack seamless selection and transition between multiple perspectives that better fit the task at hand. To address this, we present a new approach called Vicarious, which simplifies and automates the selection between egocentric and exocentric viewpoints. Vicarious employs a context-aware method for dynamically switching or highlighting the optimal viewpoint based on user actions and the current context. To evaluate the effectiveness of the viewpoint selection method, we conducted a user study (n = 27) using an asymmetric AR-VR setup where users performed remote collaboration tasks under four distinct conditions: No-view, Manual, Guided, and Automatic selection. The results showed that Guided and Automatic viewpoint selection improved users’ understanding of the task space and task performance, and reduced cognitive load compared to Manual or No-view selection. The results also suggest that the asymmetric setup had minimal impact on spatial and social presence, except for differences in task load and preference. Based on these findings, we provide design implications for future research in mixed reality collaboration.",360-degree Panoramic Video; Mixed Reality; Perspective Sharing.; Remote Collaboration; Telepresence; Viewpoint Sharing,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Ready Worker One? High-Res VR for the Home Office,VRST - Virtual Reality Software and Technology,A,"Many employees prefer to work from home, yet struggle to squeeze their office into an already fully-utilized space. Virtual Reality (VR) seemingly offered a solution with its ability to transform even modest physical spaces into spacious, productive virtual offices, but hardware challenges—such as low resolution—have prevented this from becoming a reality. Now that hardware issues are being overcome, we are able to investigate the suitability of VR for daily work. To do so, we (1) studied the physical space that users typically dedicate to home offices and (2) conducted an exploratory study of users working in VR for one week. For (1) we used digital ethnography to study 430 self-published images of software developer workstations in the home, confirming that developers faced myriad space challenges. We used speculative design to re-envision these as VR workstations, eliminating many challenges. For (2) we asked 10 developers to work in their own home using VR for about two hours each day for four workdays, and then interviewed them. We found that working in VR improved focus and made mundane tasks more enjoyable. While some subjects reported issues—annoyances with the fit, weight, and umbilical cord of the headset—the vast majority of these issues seem to be addressable. Together, these studies show VR technology has the potential to address many key problems with home workstations, and, with continued improvements, may become an integral part of creating an effective workstation in the home.",Field Study; Remote Work; Virtual Reality; Workstations,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,UniteXR: Joint Exploration of a Real-World Museum and its Digital Twin,VRST - Virtual Reality Software and Technology,A,"The combination of smartphone Augmented Reality (AR) and Virtual Reality (VR) makes it possible for on-site and remote users to simultaneously explore a physical space and its digital twin through an asymmetric Collaborative Virtual Environment (CVE). In this paper, we investigate two spatial awareness visualizations to enable joint exploration of a space for dyads consisting of a smartphone AR user and a head-mounted display VR user. Our study revealed that both, a mini-map-based method and an egocentric compass method with a path visualization, enabled the on-site visitors to locate and follow a virtual companion reliably and quickly. Furthermore, the embodiment of the AR user by an inverse kinematics avatar allowed the use of natural gestures such as pointing and waving which was preferred over text messages by the participants of our study. In an expert review in a museum and its digital twin we observed an overall high social presence for on-site AR and remote VR visitors and found that the visualizations and the avatar embodiment successfully facilitated their communication and collaboration.",asymmetric exploration; cross-device collaboration; digital twin; mixed reality; smartphone augmented reality; virtual reality,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Comparing Mixed Reality Agent Representations: Studies in the Lab and in the Wild,VRST - Virtual Reality Software and Technology,A,"Mixed-reality systems provide a number of different ways of representing users to each other in collaborative scenarios. There is an obvious tension between using media such as video for remote users compared to representations as avatars. This paper includes two experiments (total n = 80) on user trust when exposed to two of three different user representations in an immersive virtual reality environment that also acts as a simulation of typical augmented reality simulations: full body video, head and shoulder video and an animated 3D model. These representations acted as advisors in a trivia quiz. By evaluating trust through advisor selection and self-report, we found only minor differences between representations, but a strong effect of perceived advisor expertise. Unlike prior work, we did not find the 3D model scored poorly on trust, perhaps as a result of greater congruence within an immersive context.",avatars; collaboration; mixed reality; trust; Virtual reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,"Dynamic Theater: Location-Based Immersive Dance Theater, Investigating User Guidance and Experience",VRST - Virtual Reality Software and Technology,A,"Dynamic Theater explores the use of augmented reality (AR) in immersive theater as a platform for digital dance performances. The project presents a locomotion-based experience that allows for full spatial exploration. A large indoor AR theater space was designed to allow users to freely explore the augmented environment. The curated wide-area experience employs various guidance mechanisms to direct users to the main content zones. Results from our 20-person user study show how users experience the performance piece while using a guidance system. The importance of stage layout, guidance system, and dancer placement in immersive theater experiences are highlighted as they cater to user preferences while enhancing the overall reception of digital content in wide-area AR. Observations after working with dancers and choreographers, as well as their experience and feedback are also discussed.",Immersive Theater; Mobile Augmented Reality; User Study; Wide-Area,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Revisiting Consumed Endurance: A NICE Way to Quantify Shoulder Fatigue in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) is increasingly being adopted in fitness, gaming, and workplace productivity applications for its natural interaction with body movement. A widely accepted method for quantifying the physical fatigue caused by VR interactions is through metrics such as Consumed Endurance (CE). Proposed in 2014, CE calculates the shoulder torque to infer endurance time (ET)—i.e. the maximum amount of time a pose can be maintained—during mid-air interactions. This model remains widely cited but has not been closely examined beyond its initial evaluation, leaving untested assumptions about exertion from low-intensity interactions and its basis on torque. In this paper, we present two VR studies where we (1) collect a baseline dataset that replicates the foundation of CE and (2) extend the initial evaluation in a pointing task from a two-dimensional (2D) screen to a three-dimensional (3D) immersive environment. Our baseline dataset collected from a high-precision tracking system found that the CE model overestimates ET for low-exertion interactions. Further, our studies reveal that a biomechanical model based on only torque cannot account for additional exertion measured when the shoulder angle exceeds 90° elevation. Based on these findings, we propose a revised formulation of CE to highlight the need for a hybrid approach in future fatigue modelling.",Consumed Endurance; Endurance; Ergonomics; Interaction design; VR interactions,Title_Abstract,TRUE,
Scopus,conferencePaper,2023,Cognitive Load Measurement with Physiological Sensors in Virtual Reality during Physical Activity,VRST - Virtual Reality Software and Technology,A,"Many Virtual Reality (VR) experiences, such as learning tools, would benefit from utilising mental states such as cognitive load. Increases in cognitive load (CL) are often reflected in the alteration of physiological responses, such as pupil dilation (PD), electrodermal cctivity (EDA), heart rate (HR), and electroencephalography (EEG). However, the relationship between these physiological responses and cognitive load are usually measured while participants sit in front of a computer screen, whereas VR environments often require a high degree of physical movement. This physical activity can affect the measured signals, making it unclear how suitable these measures are for use in interactive Virtual Reality (VR). We investigate the suitability of four physiological measures as correlates of cognitive load in interactive VR. Suitable measures must be robust enough to allow the learner to move within VR and be temporally responsive enough to be a useful metric for adaptation. We recorded PD, EDA, HR, and EEG data from nineteen participants during a sequence memory task at varying levels of cognitive load using VR, while in the standing position and using their dominant arm to play a game. We observed significant linear relationships between cognitive load and PD, EDA, and EEG frequency band power, but not HR. PD showed the most reliable relationship but has a slower response rate than EEG. Our results suggest the potential for use of PD, EDA, and EEG in this type of interactive VR environment, but additional studies will be needed to assess feasibility under conditions of greater movement.",virtual reality; EEG; physical activity; pupil dilation; heart rate; cognitive load; galvanic skin response,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Exploring the Stability of Behavioral Biometrics in Virtual Reality in a Remote Field Study: Towards Implicit and Continuous User Identification through Body Movements,VRST - Virtual Reality Software and Technology,A,"Behavioral biometrics has recently become a viable alternative method for user identification in Virtual Reality (VR). Its ability to identify users based solely on their implicit interaction allows for high usability and removes the burden commonly associated with security mechanisms. However, little is known about the temporal stability of behavior (i.e., how behavior changes over time), as most previous works were evaluated in highly controlled lab environments over short periods. In this work, we present findings obtained from a remote field study (N = 15) that elicited data over a period of eight weeks from a popular VR game. We found that there are changes in people’s behavior over time, but that two-session identification still is possible with a mean F1-score of up to 71%, while an initial training yields 86%. However, we also see that performance can drop by up to over 50 percentage points when testing with later sessions, compared to the first session, particularly for smaller groups. Thus, our findings indicate that the use of behavioral biometrics in VR is convenient for the user and practical with regard to changing behavior and also reliable regarding behavioral variation.",Virtual Reality; Field Study; Continuous Identification.; Implicit User Identification,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Beyond Mirrors: Exploring Behavioral Changes through Comparative Avatar Design in VR Taiko Drumming,VRST - Virtual Reality Software and Technology,A,"Most studies on the Proteus Effect, which examines how avatars can influence users’ behavior through evoked stereotypes, have primarily manipulated only participants’ own avatars as the independent variable. However, in reality, there are numerous scenarios where individuals recognize their uniqueness by comparing themselves to others. Therefore, this study aimed to explore the impact of recognizing one’s distinctiveness by comparing one’s own avatar’s appearance with others on behavioral changes. In our experiment, participants and non-player characters engaged in playing the Japanese drum ‘Taiko’ together within a virtual environment. They utilized avatars dressed in suits or ‘Happi,’ which is a traditional Japanese festival costume. The results demonstrated that both the uniformity/distinctiveness and the type of avatar appearance played a joint role in influencing the speed and amplitude of arm swings during the taiko performance. This finding provides valuable insights into comprehending the mechanisms of behavior change in settings where multiple avatars interact, such as social virtual reality, and aids in designing virtual spaces that foster appropriate interactions among individuals.",virtual reality; avatar; identification; proteus effect; social comparison; behavioral changes; comparative avatar design,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Effect of Virtual Hand's Fingertip Deformation on the Stiffness Perceived Using Pseudo-Haptics,VRST - Virtual Reality Software and Technology,A,"In this study, using a novel method for haptic presentation based on pseudo-haptics, the perceived stiffness was visually altered by changing the fingertips shape of a virtual hand, as users engaged with objects in a VR environment. While past approaches have primarily focused on instigating such sensations through object deformation, we focused on how an individual’s fingertips deform upon making contact with an object. In this study, we investigated pseudo-haptics based on the deformation of the fingertips of a virtual hand. In Experiment 1, we determined how the shape deformation of a virtual hand’s fingertip affected the sense of body ownership. The experiment determined that the maximum change in the fingertip width should be 2.25 times. In Experiment 2, subjects touched a virtual object in the VR space and evaluated the perception of the stiffness of the virtual object. The results confirmed that when the deformation of the fingertip shape of the virtual hand was small, the object was perceived as hard, whereas when it was large, the object was perceived as soft. These results indicated that a haptic presentation is possible without using a haptic device that restricts user movement, which will users could broaden the range of natural interactions in VR spaces.",virtual reality; pseudo-haptics; haptics illusions,Keywords,TRUE,
Scopus,conferencePaper,2023,Exploring Real-time Precision Feedback for AR-assisted Manual Adjustment in Mechanical Assembly,VRST - Virtual Reality Software and Technology,A,"Augmented Reality (AR) based manual assembly nowadays enables to guide the process of physical tasks, providing intuitive instructions and detailed information in real-time. However, very limited studies have explored AR manual adjustment tasks with precision requirements. In this paper, we develop an AR-assisted guidance system for manual adjustments with relatively high-precision requirements. We first assessed the accuracy of the special-set OptiTrack system to determine the threshold of precision requirements for our user study. We further evaluated the performance of Number-based and Bar-based precision feedback by comparing orienting assembly errors and task completion time, as well as the usability in the user study. We found that the assembly errors of orientation in the Number-based and Bar-based interfaces were significantly lower than the baseline condition, while there was no significant difference between the Number-based and Bar-based interfaces. Furthermore, the Number-based showed faster task completion time, lower workload, and higher usability than the Bar-based condition.",Augmented Reality; manual adjustment; OptiTrack; precision feedback,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Exploring Visual Augmentations for Improving the Operation of a Hydraulic Excavator using Expert Operation Replay,VRST - Virtual Reality Software and Technology,A,"Hydraulic excavators are widely used in construction work owing to their versatility. However, the general operation of these excavators is complex and novice operators require extensive training to operate them. In this study, we propose a virtual reality (VR)-based training system with three types of visual augmentations using pre-recorded expert operations to support the skill acquirement for handling a hydraulic excavator. To evaluate the effectiveness of the proposed visual augmentations in terms of skill improvement, we compared the scores of the trainees before and after training including combinations of visual augmentations. The results indicated that the display of the lever movement significantly improved the trajectory of the bucket tip, while ghost animation and slow motion did not show significant effects. Furthermore, by showing the lever input and excavator movement of the expert in slow motion, the task completion time increased because of the aftereffect. Our findings not only provide a design guideline for VR-based excavator operation training but can also be applied to augmented reality (AR)/mixed reality (MR) support systems for supporting practical excavator operations.",Virtual Reality; Training; Visualization; Augmentations; Excavator,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Redirected Placement: Evaluating the Redirection of Passive Props during Reach-to-Place in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Hand redirection is an effective technique that can provide users with haptic feedback in virtual reality (VR) when a disparity exists between virtual objects and their physical counterparts. Psychophysiological research has revealed the distinct motion profiles of different kinematic phases when people operate hand-object interaction. In this paper, we proposed the Redirected Placement (RP), which determines the new placement of a physical prop using a constrained optimization problem. The visual illusion is used during the ""reach-to-place"" kinematic phase in the proposed RP method rather than the ""reach-to-grasp"" phase in the typical Redirected Reach (RR) method. We conducted two experiments based on the proposed RP method. Our first experiment showed that detection thresholds are generally higher with the proposed method compared to the RR method. The second experiment evaluated the embodiment experience with hand redirection using RR-only, RP-only, and RR&amp;RP methods. The results report an enhanced sense of embodiment with the combined use of both RR and RP techniques. Our study further indicates that a 1:1 combination ratio of RR&amp;RP resulted in the closest subjective experience to the baseline.",Virtual Reality; hand interaction; passive haptic feedback; hand redirection,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Instant Hand Redirection in Virtual Reality Through Electrical Muscle Stimulation-Triggered Eye Blinks,VRST - Virtual Reality Software and Technology,A,"In this paper we investigate the use of electrical muscle stimulation (EMS) to trigger eye blinks for instant hand redirection in virtual reality (VR). With the rapid development of VR technology and increasing user expectations for realistic experiences, maintaining a seamless match between real and virtual objects becomes crucial for immersive interactions. However, hand movements are fast and sometimes unpredictable, increasing the need for instantaneous redirection. We introduce EMS to the field of hand redirection in VR through precise stimulation of the eyelid muscles. By exploiting the phenomenon of change blindness through natural eye blinks, our novel stimulation model achieves instantaneous, imperceptible hand redirection without the need for eye tracking. We first empirically validate the efficiency of our EMS model in eliciting full eye closure. In a second experiment, we demonstrate the feasibility of using such a technique for seamless instantaneous displacement in VR and its particular impact for hand redirection. Among other factors, our analysis also delves into the under-explored domain of gender influence on hand redirection techniques, revealing significant gender-based performance disparities.",virtual reality; VR; eye blinks; EMS; redirection; Hand redirection,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Redirecting Rays: Evaluation of Assistive Raycasting Techniques in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Raycasting-based interaction techniques are widely used for object selection in immersive environments. Despite their intuitive use, they come with challenges due to small or far away objects, hand tremor, and tracking inaccuracies. Previous adaptations for raycasting, such as directly snapping the ray to the closest target, extruding the ray to a cone, or multi-step selection techniques, require additional time for users to become familiar with them. To address these issues, we propose three assistive techniques in which the visible selection ray is subtly redirected towards a target, with a proximity and gain based increase in the redirection amount. In a user study (N = 26), we compared these redirection techniques with a baseline condition based on a Fitts’ law task and collected performance measures as well as comprehensive subjective feedback. The results indicate that the three redirection techniques are significantly faster and have higher effective throughput than the baseline condition. Participants retained a high sense of agency with all redirection techniques and reported significantly lower total workload compared to the baseline. The majority of participants preferred selection with assistive ray redirection and perceived it as not distracting or intrusive. Our findings support that assistive redirected raycasting techniques can improve object selection performance and user experience in virtual environments.",virtual reality; selection; interaction techniques; raycast redirection,Title_Keywords,TRUE,
Scopus,conferencePaper,2023,Stay Vigilant: The Threat of a Replication Crisis in VR Locomotion Research,VRST - Virtual Reality Software and Technology,A,"The ability to reproduce previously published research findings is an important cornerstone of the scientific knowledge acquisition process. However, the exact details required to reproduce empirical experiments vary depending on the discipline. In this paper, we summarize key replication challenges as well as their specific consequences for VR locomotion research. We then present the results of a literature review on artificial locomotion techniques, in which we analyzed 61 papers published in the last five years with respect to their report of essential details required for reproduction. Our results indicate several issues in terms of the description of the experimental setup, the scientific rigor of the research process, and the generalizability of results, which altogether points towards a potential replication crisis in VR locomotion research. As a countermeasure, we provide guidelines to assist researchers with reporting future artificial locomotion experiments in a reproducible form.",Virtual Reality; Reproducibility; Teleportation; Locomotion; Replication Crisis; Steering,Keywords,TRUE,
Scopus,conferencePaper,2023,A Pilot Study on the Impact of Discomfort Relief Measures on Virtual Reality Sickness and Immersion,VRST - Virtual Reality Software and Technology,A,"While there are several theories of virtual reality (VR) sickness causes and pertinent methods suggested for mitigation, it remains an important problem. One possible solution might be to prescribe measures for just relieving the immediate symptoms (vs. addressing the very root causes). Understanding that the severity of the sickness may affect individuals differently, we examined three methods: (1) reducing the weight of the headset (using a suspension mechanism); (2) refreshing the user with a gentle breeze of wind (using a fan); (3) accompanying the VR viewing experience with mindful breathing. We assess the relative sickness reduction effect, if any, of these three measures through a comparative pilot experiment and individual case analysis. The preliminary results point to rather the importance of system usability and how it affects the relationship between the perceived immersion and the extent of sickness. The initial proposition to enhance the user’s physical condition as a way to better withstand VR sickness symptoms could not be established.",Virtual Reality; Multi-modal; Head-mounted Display; VR sickness,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,A Virtualized Augmented Reality Simulation for Exploring Perceptual Incongruencies,VRST - Virtual Reality Software and Technology,A,"When blending virtual and physical content, certain incongruencies emerge from hardware limitations, inaccurate tracking, or different appearances of virtual and physical content. They restrain us from perceiving virtual and physical content as one experience. Hence, it is crucial to investigate these issues to determine how they influence our experience. We present a virtualized augmented reality simulation that can systematically examine single incongruencies or different configurations.",Augmented Reality; Mixed Reality; Virtual Reality; Extended Reality; Visualization; Perception; Congruence,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Audio-based Vibrotactile Feedback in Multimodal VR Interactions,VRST - Virtual Reality Software and Technology,A,"While consumer-grade virtual reality (VR) hardware can deliver immersive audiovisual experiences, these systems often lack the ability to display realistic haptic feedback, or incorporate cost-efficient vibrotactile actuators with very limited abilities to provide tactile feedback. To overcome these limitations, we introduce an approach based on audio-based vibrotactile actuators. Due to their wide frequency response and multiple resonant frequencies, they can provide more tactile details. In our implementation, every VR interaction uses standard audio clips to provide simultaneous auditory and tactile feedback, as well as coupled realistic physics simulations for the visual feedback. We evaluate our approach to assess the benefits on the user’s experience regarding various interaction scenarios in VR, comparing our approach to a simulated fixed-frequency actuator as a baseline. The results confirmed the benefits of our approach in terms of user preference, perceived realism, comfort, sense of agency, and texture perception. Furthermore, multimodal feedback resulted in the best user experience.",virtual reality; haptics; multimodal; vibrotactile; audio-based,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Augmented Aroma: The Influence of Augmented Particles' Movement and Color on Emotion during Olfactory Perception,VRST - Virtual Reality Software and Technology,A,"This study investigates the impact of visual augmentation on the olfactory system by analyzing users’ emotional responses. Augmented particles were presented using HoloLens through five methods, involving adjustment in color and movement, alongside six odors. Through the experiments with 30 participants, we discovered that augmented particles could intensify or reduce emotional reactions based on their colors and movement directions.",Augmented reality; emotion analysis; olfactory perception; odor,Keywords,TRUE,
Scopus,conferencePaper,2023,Combining embodiment and 360 video for teaching protection of civilians to military officers,VRST - Virtual Reality Software and Technology,A,"This demo presents an innovative use of embodiment in combination with 360º video to support teaching the threat-based approach to protection of civilians at a military university. To create a realistic and emotionally appealing XR experience and at the same time save on developing time and costs, scanned 3D objects and avatar were integrated in 360º videos. The app also includes interactions with a virtual perpetrator and collaborative map exercise and received positive feedback from end users.",Extended Reality; Embodiment; 360º video; Interactive Learning Environment,Keywords,TRUE,
Scopus,conferencePaper,2023,Comparing Performance of Dry and Gel EEG Electrodes in VR using MI Paradigms,VRST - Virtual Reality Software and Technology,A,"Brain–computer interfaces (BCIs) are an emerging technology with numerous applications. Electroencephalogram (EEG) motor imagery (MI) is among the most common BCI paradigms and has been used extensively in healthcare applications such as post-stroke rehabilitation. Using a Virtual Reality (VR) game, Push Me, we conducted a pilot study to compare MI accuracy with Gel or active-dry EEG electrodes. The motivation was to (1) investigate the MI paradigm in a VR environment and (2) compare MI accuracy using active dry and gel electrodes with different Machine Learning (ML) classifications (SVM, KNN and RF). The results indicate that while gel-based electrodes, in combination with SVM, achieved the highest accuracy, dry electrode EEG caps achieved similar outcomes, especially with SVM and KNN models.",Virtual Reality; Machine Learning; Electroencephalogram; Brain Computer Interface; Motor Imagery,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Directional Multimodal Flow to Help Mitigate VR Sickness,VRST - Virtual Reality Software and Technology,A,"One way to alleviate VR sickness is to reduce the sensory mismatch between the visual and vestibular organ regarding the motion perception. Mixing in the motion trail in the reverse direction to the original has been suggested as one such method. However, as such visual feedback can be content intrusive, we consider supplementing it by the non-visual multimodal reverse flow. In particular, we have devised methods to supply sound effects as if heard from the reverse direction, and vibration and air flow likewise. Our validation experiment has shown that the multimodal feedback was effective in significantly reducing the sickness, but its direction (reverse or not) did not have an effect as hypothesized.",Virtual Reality; Multi-modal; Head-mounted Display; VR sickness,Keywords,TRUE,
Scopus,conferencePaper,2023,Diving Into The Twilight Zone VR for Marine Biology,VRST - Virtual Reality Software and Technology,A,Teaching students about underwater marine science is difficult due to the limitations required to access underwater environments. Marine science is typically not taught until tertiary education levels. We have developed a Virtual Reality experience for teaching marine science activities focusing on high school students. Our education programme and VR tool can help train the next generation of students into learning and being aware about marine science.,,Abstract,TRUE,
Scopus,conferencePaper,2023,Early User Feedback on a VR Interface Draft for Interaction with a Multi-Robot System in Ship Hull Inspection,VRST - Virtual Reality Software and Technology,A,"The use of multi-robot systems is a field that can benefit from VR by strengthening understanding of the situation and enabling seamless interaction with the actors involved. This work investigates how the usability of a design for interaction with a multi-robot system for ship maintenance is assessed. Furthermore, comments from the participants are consulted as impulses for improving the design.",virtual reality; human-robot interaction; multi-robot,Keywords,TRUE,
Scopus,conferencePaper,2023,Earnormous: An educational VR game about how humans hear,VRST - Virtual Reality Software and Technology,A,"We present a demo of Earnormous, a virtual reality game to teach about the human ear.",Virtual Reality; Hearing,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Effects of Vibrotactile Feedback on Aim-and-Throw Tasks in Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Vibrotactile feedback has been actively utilized in many virtual reality applications to provide the sense of touch. In this preliminary work, we investigated the effects of vibrotactile feedback in the dart throwing task in a virtual environment. The user study compared the task performance, as well as observed the participants’ behavior in throwing tasks with vibrotactile feedback or not. The results showed that the participants made larger body movements during the task when vibrotactile feedback was on, while the feedback did not affect the task performance.",Virtual Reality; Hand Tracking; Vibrotactile Feedback,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Enhancing VR Based Serious Games and Simulations Design: Bayesian Knowledge Tracing and Pattern-Based Approaches,VRST - Virtual Reality Software and Technology,A,"This paper explores how Bayesian Knowledge Tracing (BKT) can be integrated with a pattern-based approach to enhance the development of virtual reality (VR) based serious games and simulations. These technologies allow for the prediction of user progress and the utilization of Artificial Intelligence (AI) methods to tailor difficulty levels based on individual needs. By combining BKT, pattern-based mechanics, and affective feedback, comprehensive data on user interactions, skills, and emotional states can be collected. This data enables the estimation of learners’ knowledge levels and the prediction of their progress.",virtual reality; serious games; design patterns; simulations; Bayesian Knowledge Tracing,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Enhancing User Experience in VR using Wearable Olfactory System,VRST - Virtual Reality Software and Technology,A,"This paper introduces a mounted olfactory device prototype that instantly emits and quickly switches scents. Earlier approaches, such as fixed olfactory devices, have limitations in terms of the time it took for users to perceive a scent in Virtual Reality (VR) due to dissipating lingering scents in physical space, as well as the lack of instantaneous scent switching. To evaluate its effectiveness, we conducted a pilot user study comparing mounted and fixed olfactory devices.The results show that the mounted olfactory device provides a better VR experience than the fixed olfactory device.",,Abstract,TRUE,
Scopus,conferencePaper,2023,Estimating mechanical properties of soft objects using surface measurements from AR headsets,VRST - Virtual Reality Software and Technology,A,Physics-driven predictions of soft tissue mechanics are vital for various medical interventions. Insights on the mechanical properties of soft tissues are essential for obtaining personalised predictions from these models. This study aims to provide a workflow to identify the material parameters of soft homogeneous materials under gravity loading using 3D surface geometrical measurements acquired from a wearable augmented reality (AR) headset’s depth camera. Preliminary results show that the parameter estimation procedure can successfully recover the ground truth material parameter C1 of a cantilever beam using synthetic surface data. This workflow could be used for real-time navigational guidance during soft tissue treatment procedures.,augmented reality; Mechanical parameter estimation,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Exploratory Study on the Reinstatement Effect Under 360-Degree Video-Based Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Episodic memory incorporates environmental contexts, and memory retrieval is aided by matching the retrieval context to the encoding context. This study tested whether similar context-dependency of memory could be confirmed in virtual environments. Participants learned words in a 360-degree video-based virtual environment depicting either natural or urban landscapes. Immediately, they completed a test in the same virtual environment. After two days, half of the participants underwent a final test with the same context as that on the initial day, whereas the other half underwent it with a different context. Surprisingly, participants tested in a different context exhibited significantly lower forgetting than those tested in the same context, which contradicted our hypothesis.",virtual reality; 360-degree video; context-dependent memory,Keywords,TRUE,
Scopus,conferencePaper,2023,Fabric Electrodes for Physiological Sensing in a VR HMD,VRST - Virtual Reality Software and Technology,A,"This paper explores the development and testing of fabric electrodes to collect a range of physiological measures. The aim is to integrate these sensors into a Virtual Reality (VR) headset to collect physiological and muscular motion data that will help detect emotion, cognitive load and facial expressions. As part of an on-going project, we have already developed prototypes of the EMG and GSR sensors. A head phantom has been developed for the purpose of testing and validating electrode performance.",Virtual Reality; Empathic Computing; Physiological Sensors,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Immersive Climate Narratives: Using Extended Reality to Raise Climate Change Awareness,VRST - Virtual Reality Software and Technology,A,"Shadows of Tomorrow is an innovative public art installation utilizing LiDAR body tracking and augmented reality to promote climate change awareness. The installation projects depictions of real-world climate change scenarios from Australia, Kuwait, the United States, and Greenland on a large display. Shadows utilizes the Azure Kinect to capture and integrate audience member silhouettes as a simple user interface for the display. As audiences move in front of the display, their silhouette reveals the impact of climate change on the projected environment. By bringing global climate change stories to local audiences, we emphasize the universal, yet highly localized impacts of climate crisis. These interactive visualizations encourage audiences to engage with and understand the stark realities of climate change in regions far removed from their own. Created for display in high-transit public areas like museums, airports, and city centers, Shadows of Tomorrow aims to create a global dialogue around our shared responsibility for climate action.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2023,Immersive visualization for ecosystem services analysis,VRST - Virtual Reality Software and Technology,A,"Ecosystem services are benefits provided to humans by ecosystems through the natural processes and conditions which occur&nbsp;[7]. Interviews with land use scientists identified problems with currently available software applied to their ecosystem services analysis. A user centred design process is adopted and a visualization system, Immersive ESS Visualizer, is presented for visualizing data relating to ecosystem services analysis. Immersive ESS Visualizer is designed for both experts and non-experts and allows users to compare data visualized with multiple hand-manipulated maps. Users can glide over a landscape with data layers draped to analyse areas of interest. Immersive ESS Visualizer could augment a process for presenting ecosystem services analysis results.",Virtual Reality; Visualization; Ecosystems Services,Keywords,TRUE,
Scopus,conferencePaper,2023,Listen again: virtual reality based training for children with hearing impairments,VRST - Virtual Reality Software and Technology,A,"Although hearing loss is treated with hearing technology and rehabilitation, children with hearing loss still face challenges. Factors such as distance to the sound source and noise from the surroundings are the children’s biggest enemies. In the ""Listen Again"" project, a listening- and spatial awareness training application was co-designed together with deaf and hard-of-hearing children who use cochlear implants. This paper presents quantitative and qualitative results from a two-month evaluation where 22 children were asked to play with the VR solution for two months, three times a week.",spatial awareness; hearing aids; virtual reality.,Title_Keywords,TRUE,
Scopus,conferencePaper,2023,Navigating in VR using free-hand gestures and embodied controllers: A comparative evaluation,VRST - Virtual Reality Software and Technology,A,"While natural body-based movements are essential features for immersive VR (Virtual Reality) experiences, most of the available input techniques for navigation in VR involve the use of hand-held controllers. Alternatively, while body-based input for VR navigation has previously been explored in HCI using external tracking devices, there is little to no work that utilizes the in-built tracking functionalities of the predominant VR headsets (such as Meta Quest 2) for gesture-based navigation in VR. This paper addresses this research gap by proposing five free-hand gestures for 3-D navigation in VR using internal gesture-tracking functionality of Quest 2 headset. Additionally, a qualitative and quantitative comparison is presented between free-hand and controller-based navigation in VR using a custom designed task (with 10 users). Overall, the findings from the task-analysis indicate that while in-built tracking functionalities in VR headsets open doors for inexpensive gesture-based VR navigation, the mid-air hand-gestures result into greater fatigue as compared to using controllers for navigation in VR.",virtual reality; navigation; motion-capture; free-hand gestures; gesture-recognition,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Pain Distraction for Children Through VR- or Audio-haptic Soundscapes in Situ,VRST - Virtual Reality Software and Technology,A,"In this pilot study we compare two prototype applications developed in collaboration with Rigshospitalet, the main hospital in Denmark, aimed to evaluate the effectiveness of a virtual reality (VR)- versus an audio-haptic based solution, as a pain distraction tool for children aged 5 to 8 during needle-related medical procedures. Both prototypes were developed with a narrative where children help a farmer find hidden animals. The final prototype underwent testing in situ, at Rigshospitalet’s clinic for blood tests. Here, participants’ pain levels were assessed using the Wong-Baker FACES Scale [9] and the Visual Analogue Scale [5]. Both prototypes saw participants report reduced pain perception, skewing more in favor of the VR prototype. However, the audio-haptic prototype showed similar levels of reduction in pain perception when effective. The study concludes that both VR- and audio-haptic based distraction are viable methods, that each cover a group’s needs within medical procedures involving young children (those who need to not see the procedure, and those who do), and that these should be further developed and implemented in said medical procedures.",,Abstract,TRUE,
Scopus,conferencePaper,2023,Performing Tasks in Virtual Reality. Interplay between Realism and Visual Imagery,VRST - Virtual Reality Software and Technology,A,The main aims of the presented study are to verify whether the amount of textures in a virtual scene affects task performance and to test whether visual imagery changes the relationship between realism and task performance. An experimental study with three groups differed in visual realism was conducted (n=100). Participants were asked to perform a task: taking on the role of a marshaller and positioning the plane on the airport apron. Results indicate that texturing does not affect task performance. Visual imagery is a moderator of the relationship between perceived realism and task performance. A high level of imagery interferes with a high realism assessment decreasing task performance.,virtual reality; task performance; scene realism; visual imagery,Title_Keywords,TRUE,
Scopus,conferencePaper,2023,Pigments of Imagination: An Interactive Virtual Reality Composition,VRST - Virtual Reality Software and Technology,A,"Pigments of Imagination is an artistic interactive virtual reality experience based on an original fixed media composition. It is designed to reimagine the popular music video in a virtual space as a dynamic, emotionally engaging experience through exploration of novel approaches toward audiovisual reactivity and interactivity. In this piece the user can interact, directly affect, and build upon prior user interpolations of the environment’s sonic and visual qualities, allowing a narrative immersion that maintains a structured arc and conclusion but unique experience with each use.",virtual reality; haptics; audiovisual; spatial audio; interactivity; pentimento; VR music video,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Reducing Sensing Errors in a Mixed Reality Musical Instrument,VRST - Virtual Reality Software and Technology,A,"This paper describes the design and evaluation of Netz, a novel mixed reality musical instrument that leverages artificial intelligence for reducing errors in gesture interpretation by the system. We followed a participatory design approach over three months through regular sessions with a professional musician. We explain our design process and discuss technological sensing errors in mixed reality devices, which emerged during the design sessions. We investigate the use of interactive machine learning techniques to mitigate such errors. Results from statistical analyses indicate that a deep learning model based on interactive machine learning can significantly reduce the number of technological errors in a set of musical performance tasks with the mixed reality musical instrument. Based on our findings, we argue that the application of interactive machine learning techniques can be beneficial for embodied, hand-controlled musical instruments in the mixed reality domain.",participatory design; hand-pose estimation; mixed reality musical instruments,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,ShadowPlayVR: Understanding Traditional Shadow Puppetry Performance Techniques Through Non-Intuitive Embodied Interactions.,VRST - Virtual Reality Software and Technology,A,"""ShadowPlayVR"" is a virtual reality system designed to introduce the intricate art of Chinese shadow puppetry into Virtual Reality (VR), focusing on the non-intuitive embodied interactions that emulate puppetry performance. By incorporating an immersive, experiential learning approach, ShadowPlayVR offers users a hands-on understanding of this art form. Preliminary testing reveals the significant role of contextual information in facilitating understanding and mastering these complex interactions. The work also presented showcases how VR can serve as a powerful tool to preserve and engage with traditional cultural heritage in a contemporary digital context.",Interaction Design; Virtual Reality (VR); Embodied Interactions; Experiential Learning,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Sickness Reduction in FPV Drone Control: Improved Effects of Reverse Optical Flow with Static Landmarks Only,VRST - Virtual Reality Software and Technology,A,"Drones are controlled remotely through the on-board live camera often using a headset to immersively (without external distraction) situate the drone operator with a first-person view. The highly dynamic imagery of drone piloting can elicit significant VR sickness. In this poster, we demonstrate the application of mixing the reverse optical flow pattern into the drone piloting imagery for mitigating VR sickness, using only the features of the static landmarks in the scene. We compare it to the cases of applying no mitigation technique (baseline) and mixing in the optical flow pattern from “all” object features. The results show that the suggested method was significantly more effective in reducing sickness than when considering all object features, and had a higher preference for improved visibility and controllability.",Virtual Reality; Object Detection; Optical Flow; Drone; VR Sickness,Keywords,TRUE,
Scopus,conferencePaper,2023,Simple and Practical Dual Rendering for Reducing Eye Fatigue from Vergence-Accomodation Conflict in Stereoscopic Viewing,VRST - Virtual Reality Software and Technology,A,"Eye fatigue and unpleasant symptoms caused by the vergence and accommodation conflict with stereoscopic rendering pose a substantial usability issue in virtual reality. To address this problem, dual rendering of the stereoscopic imagery is proposed, aimed to alleviate such stress on the user. First, the scene is divided into two regions: the front proximal zone and the rest behind it. The back layer is rendered first for objects in the rest, with the conventional stereoscopic viewing parameter values. Then, remaining objects in the proximal zone are rendered using viewing parameters adjusted to reduce the VAC. The validation experiment confirmed that the proposed approach significantly reduced the overall eye fatigue without compromising the user’s depth perception ability to manipulate objects in the parameters-altered proximal zone.",Virtual Reality; Dual Rendering; Vergence Accommodation Conflict,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,Slingshot: A Novel Gesture Locomotion System for Fast-paced Gameplay in Virtual Reality,VRST - Virtual Reality Software and Technology,A,,,Title,TRUE,
Scopus,conferencePaper,2023,SpaceVR: Virtual Reality Space Science Outreach Experience,VRST - Virtual Reality Software and Technology,A,Teaching people about Space concepts is challenging with traditional text book and teaching methods. It is hard to encourage prospective students with these traditional methods as they lack engagement and interactivity. We have developed SpaceVR which is a VR application that provides high school students with an engaging outreach experience about Space Science. The application uses real images of the sun from NASA’s solar dynamics observatory satellite to create an interactive digital Sun for students to explore. The project investigates if adding gamification elements will increase student engagement with Space Science outreach efforts.,,Title,TRUE,
Scopus,conferencePaper,2023,Stress visualization in geometrically complex structures using Thermoelastic Stress Analysis and Augmented Reality,VRST - Virtual Reality Software and Technology,A,"We present a framework for the visualization of mechanical stress using augmented reality (AR) using Thermoelastic Stress Analysis (TSA). The 2D stress images generated by TSA are converted to a 3D stress map using computer vision technology and then superimposed on the real object using AR. Our framework enables in-situ visualization of stress in geometrically complex structural components, which can assist in the design, manufacture, test, and through-life sustainment of failure-critical engineering assets. We also discuss the challenges of such a TSA-AR combination and present a case study that demonstrates the performance and significance of our system.",,Title_Abstract,TRUE,
Scopus,conferencePaper,2023,Temporal Foveated Rendering for VR on Modern Mobile Architecture,VRST - Virtual Reality Software and Technology,A,"We introduce Temporal Foveated Rendering (TFR), a method of achieving GPU savings for VR content by reducing rendering frequency in the periphery of a fixed or eye tracked mobile VR headset utilizing tiled rendering. TFR saves GPU compute by rendering a peripheral “outset” at half rate, while maintaining full frame rate in a smaller ""inset"" centered at the gaze position. Judder is mitigated in the peripheral outset by applying asynchronous space warp, driven by System on Chip (SoC) derived motion vectors. This technique saves up to 17% more GPU compute on a mobile device, compared to a spatial foveated rendering technique called Fixed Foveated Rendering (FFR).",mobile devices; virtual reality; foveated rendering,Keywords,TRUE,
Scopus,conferencePaper,2023,The Detectability of Saccadic Hand Offset in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"On the way towards novel hand redirection (HR) techniques that make use of change blindness, the next step is to take advantage of saccades for hiding body warping. A prerequisite for saccadic HR algorithms, however, is to know how much the user’s virtual hand can unnoticeably be offset during saccadic suppression. We contribute this knowledge by conducting a psychophysical experiment, which lays the ground for upcoming HR techniques by exploring the perceptual detection thresholds (DTs) of hand offset injected during saccades. Our findings highlight the pivotal role of saccade direction for unnoticeable hand jumps, and reveal that most offset goes unnoticed when the saccade and hand move in opposite directions. Based on the gathered perceptual data, we derived a model that considers the angle between saccade and hand offset direction to predict the DTs of saccadic hand jumps.",virtual reality; saccades; detection thresholds; hand redirection,Title_Keywords,TRUE,
Scopus,conferencePaper,2023,The Effect of False but Stable Heart Rate Feedback via Sound and Vibration on VR User Experience,VRST - Virtual Reality Software and Technology,A,"Vital signals tend to become destabilized and generally increase when one’s physical condition is not well. For example, experiencing virtual reality (VR) sickness brings about a deteriorated physical condition, often accompanied by an increased heart rate. Several research have shown that providing feedback of false heart rate can induce various altered perceptions, such as increased effort and anxiety. In this poster, we propose to provide false but “stable” heart rate feedback through sound and vibration while navigating a sickness-inducing VR scene. We hypothesize that the false but stable heart rate feedback will have an induced effect of calming the user down (even stabilizing the heart rate itself) and reducing the unpleasant VR sickness symptoms. A pilot study was conducted to compare three conditions, namely viewing a sickness eliciting VR content, (1) as is, (2) with the false but stable heart rate feedback through sound, and (3) with the false but stable heart rate feedback through vibration. Results showed that the level of sickness was significantly reduced by sound and vibration feedback, respectively.",Virtual Reality; False Heart Rate; VR Sickness; Calming Effects,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,The Effect of Virtual Reality Level of Immersion on Spatial Learning Performance and Strategy Usage,VRST - Virtual Reality Software and Technology,A,"The utilization of the immersive and ecological nature of head-mounted displayed (HMD) virtual reality (VR) has been increasing in studies of human spatial learning, and various aspects of VR's impact on navigation have been examined. Nevertheless, the effect the VR level of immersion has on spatial learning strategy usage is yet to be determined. Here, we addressed this gap by comparing spatial learning properties and experience measures in three modality settings. We translated a classic spatial learning task from animals to humans, where three spatial learning strategies were observed (place/cue/response). We compared 3 conditions: wearing a VR headset while physically walking vs. using a controller, and a 2D screen display using a mouse and a keyboard for navigation. We examined various learning properties and used presence questionnaires to analyze experience measures. Our results show that learning measures including strategy usage were affected by the VR level of immersion, suggesting that modality characteristics should be considered during VR task design.",Navigation; Learning Performance; Level of Immersion; Spatial Learning Strategy,Title_Abstract,TRUE,
Scopus,conferencePaper,2023,The Effects of Customized Strategies for Reducing VR Sickness,VRST - Virtual Reality Software and Technology,A,"The extent of virtual reality (VR) sickness varies widely among users, as each user has different sensitivities to diverse causes of VR sickness. This can make prescribing a single particular reduction technique difficult and ineffective. In this poster, we present preliminary work examining the more effective and preferred sickness-reduction techniques for a given user under varied sickness-inducing motions. Based on the user-specific information collected using VR content, a customized strategy is developed for a given user and applied to the same VR content. We report the experimental results for testing the effectiveness of the customized reduction technique, comparing it to a single particular reduction method.",Virtual Reality; VR Sickness; Personalized Experiences.; Restricted Field-of-view; Reverse Optical Flow; Virtual Nose,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,The Staircase Procedure Toolkit: Psychophysical Detection Threshold Experiments Made Easy,VRST - Virtual Reality Software and Technology,A,"We propose a novel open-source software toolkit to support researchers in the domains of human-computer interaction (HCI) and virtual reality (VR) in conducting psychophysical experiments. Our toolkit is designed to work with the widely-used Unity engine and is implemented in C# and Python. With the toolkit, researchers can easily set up, run, and analyze experiments to find perceptual detection thresholds using the adaptive weighted up/down method, also known as the staircase procedure. Besides being straightforward to integrate in Unity projects, the toolkit automatically stores experiment results, features a live plotter that visualizes answers in real time, and offers scripts that help researchers analyze the gathered data using statistical tests.",Python; Unity; detection threshold; psychophysical experiments; staircase procedure; up/down method,Abstract,TRUE,
Scopus,conferencePaper,2023,Utilizing AR as a Tool for Assessing Accessibility in the Home,VRST - Virtual Reality Software and Technology,A,"Home modification interventions can remedy deficiencies in the home environments of the growing number of older adults that want to age in place. Unfortunately, performing an assessment of a home environment is a difficult process, often requiring numerous measurements from a skilled practitioner. To address these gaps in practice, we used an iterative co-design process to develop a first prototype of a novel augmented reality home assessment tool (ARHAT) that can be utilized more rapidly by both individuals in and outside of health care, as well as performed either on or off-site. The aim of this work is to create a tool for major stakeholders involved in supporting housing design and aging in place, thereby reaching and making a difference in the lives of more older adults.",Augmented Reality; Assessments; Measurements,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2023,VR Experiences of Pregnant Women During Antenatal Care,VRST - Virtual Reality Software and Technology,A,"Pregnant women use a range of non-pharmacological pain relief methods to help manage and reduce pain intensity and to induce relaxation. We conducted a study with 18 pregnant women to explore VR experiences as a non-pharmacological method of pain relief to determine the effect on pain intensity. The results of the study identified several themes: evoking emotion with sub-themes, memory, and imagination. The theme presence, with sub-themes of relatability, realism, immersion, interactivity, and narration. Finally, the escape and anchoring themes were descriptions of how women envisaged using VR antenatally. This study provides a novel contribution to the field of VR and antenatal and labour care which can help inform the design of VR experiences for pregnant women.",Virtual Reality; Analgesia; Antenatal; Birth; Labour Pain; Relaxation,Keywords,TRUE,
Scopus,conferencePaper,2023,Waddle: using virtual penguin embodiment as a vehicle for empathy and informal learning,VRST - Virtual Reality Software and Technology,A,"This paper presents, Waddle, a virtual experience to promote informal learning by embodying the user as an Adélie Penguin to partake in a narrative-based virtual reality application that shares the story of the lives of these unique animals. We test the effects of this experience on informal learning and empathy, an important component for fostering social engagement with ecology. The research demonstrates that the developed experience is able to support informal learning, virtual embodiment, and is able to create a positive change in empathy.",Informal Learning; Empathy; Virtual Embodiment,Abstract,TRUE,
Scopus,conferencePaper,2023,Walking-in-Flat-Place on Non-flat Virtual Environment can be Sickening!,VRST - Virtual Reality Software and Technology,A,"It is well-known that employing the Walking-in-Place (WIP) interface can significantly reduce VR sickness in addition to promoting the sense of presence, immersion, and natural interaction. In this poster, we re-examine the conditions for which WIP will effectively reduce VR sickness. In particular, we investigate and compare the cases of applying WIP to navigating on flat terrain vs. up-and-down ramps with respect to the sickness reduction effect. We point out that naively designed WIP/navigation content has the possibility of actually worsening the VR sickness due to the sensory and reality mismatch between the flat real operating environment and the inclined virtual terrain.",Virtual Reality; User Study; Cybersickness; Locomotion; Walking-in-Place (WIP),Keywords,TRUE,
Scopus,conferencePaper,2023,XR for Improving Cardiac Catheter Ablation Procedure,VRST - Virtual Reality Software and Technology,A,"Arrhythmia refers to abnormalities of the heart rhythm, and it is considered a life-threatening pathology. Catheter ablation is a minimally invasive procedure which provides the best therapeutic outcomes to cure the arrhythmia. The procedure consists of a series of intraoperative and training challenges that could potentially affect the procedure outcome. This study examines how Extended Reality (XR) technologies (AR/VR) can be used to improve the cardiac catheter ablation procedure for electrophysiologists.",,Abstract,TRUE,
Scopus,conferencePaper,2024,Context-Relevant Locations as an Alternative to the Place Illusion in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Presence is a powerful aspect of Virtual Reality (VR). However, there has been no consensus on how to achieve presence in Augmented Reality (AR) or whether it exists at all. The Place Illusion, a key component in presence as defined in VR, cannot be obtained in AR as there is no way to make the user feel as though they are transported somewhere else when they are limited to what they can physically see in front of them. However, recently it has been argued that coherence or congruence are important parts of the Place and Plausibility Illusions. The implication for AR is that the AR content might invoke a higher Plausibility Illusion if it is consistent with the physical place the content is situated in. In this study, we define the concept of a Context-Relevant Location (CRL), a physical place that is congruent with the experience. We present a study with a between-subjects design that allowed users to interact with AR objects in a CRL and in a generic environment. The results indicate that presence was higher in the CRL setting than the generic environment, contribute to the debate about providing a concrete description of presence-like phenomena in AR, and posit that CRLs play a similar role to the Place Illusion in an AR setting.",presence; Augmented reality; context-relevant location; plausibility,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,"Lifter for VR Headset: Enhancing Immersion, Presence, Flow, and Alleviating Mental and Physical Fatigue during Prolonged Use",VRST - Virtual Reality Software and Technology,A,"The virtual reality (VR) headset is still relatively heavy, causing a significant physical and mental burden and negatively affecting the VR user experience, particularly during extended periods of use. In this paper, we present a prototype design of the “Lifter,” which utilizes a counterbalanced wire-pulley mechanism to partially relieve the weight of the VR headset (between 50% and 85%). The human subject study has confirmed that the Lifter relieved not only physical fatigue but also significantly improved mental burden, sense of immersion, presence, and flow (perception of time passing) during prolonged usage (30 minutes or more).",Head-Mounted Display; Headset Weight; Weight Reduction,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,"MeetingBenji: Tackling Cynophobia with Virtual Reality, Gamification, and Biofeedback",VRST - Virtual Reality Software and Technology,A,"Phobias, particularly animal phobias like cynophobia (fear of dogs), disrupt the lives of those affected by, for instance, limiting outdoor activities. While virtual reality exposure therapy (VRET) has emerged as a potential treatment for this phobia, these efforts have been limited by high dropout rates and a lack of ability to handle stressful situations in people who suffer from cynophobia. Inspired by these challenges, we present MeetingBenji, a VRET system for cynophobia that uses (i) gamification to enhance motivation and engagement, and (ii) biofeedback to facilitate self-control and reduce physiological responses. In a study (N=10) that compared the effects of displaying dogs in 3D scenes and 360º videos using the Behavioral Approach Test (BAT) – in which participants are increasingly exposed to the source of phobia – participants reported a high level of immersion to the exposure sequence. Further, they reported feeling more anxiety with 3D content than 360º video (60%), lower heart rates in the presence of biofeedback (between 1.71% and 7.46%), and improved self-control across the three exposure levels. They appreciated our gamified elements – completing all exposure levels. This study suggests that VRET with gamification and biofeedback is an effective approach to stimulate the habituation of people with cynophobia.",biofeedback; gamification; cynophobia; VR exposure therapy,Title_Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,iStrayPaws: Immersing in a Stray Animal's World through First-Person VR to Bridge Human-Animal Empathy,VRST - Virtual Reality Software and Technology,A,"While Virtual Reality Perspective-Taking (VRPT) demonstrates its efficiency in inducing empathy, its application primarily focuses on vulnerable humans, not animals. Existing animal-related works mainly targets farm animals and wildlife. In this work, we focus on stray animals and introduce iStrayPaws, a VRPT system that simulates stray animals’ challenging lives. The system offers users an immersive first-person journey into the world of stray animals encountering different difficulties like inclement weather, hunger, and illnesses. Enriched with audio-visual and kinesthetic design, the system seeks to deepen users’ understanding of stray animals’ life and foster profound emotional connections. To evaluate the system, a user study was conducted, which showed that VRPT recipients exhibited significant improvement in both state and trait empathy compared to traditional method. Our research not only delivers a novel, accessible, and interactive animal empathy experience but also provides innovative solutions for addressing stray animal issues and advancing broader animal welfare work.",Virtual Reality; Empathy; Embodied Experience; Hand Mocap; Stray Animals,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Exploring Presence in Interactions with LLM-Driven NPCs: A Comparative Study of Speech Recognition and Dialogue Options,VRST - Virtual Reality Software and Technology,A,"Combining modern technologies like large-language models (LLMs), speech-to-text, and text-to-speech can enhance immersion in virtual reality (VR) environments. However, challenges exist in effectively implementing LLMs and educating users. This paper explores implementing LLM-powered virtual social actors and facilitating user communication. We developed a murder mystery game where users interact with LLM-based non-playable characters (NPCs) through interrogation, clue-gathering, and exploration. Two versions were tested: one using speech recognition and another with traditional dialog boxes. While both provided similar social presence, users felt more immersed with speech recognition but found it overwhelming, while the dialog version was more challenging. Slow NPC response times were a source of frustration, highlighting the need for faster generation or better masking for a seamless experience.",NPC; VR; Presence; Speech Recognition; Immersive systems; Large Language Models (LLM); Social Actors,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Effects of Different Tracker-driven Direction Sources on Continuous Artificial Locomotion in VR,VRST - Virtual Reality Software and Technology,A,"Continuous artificial locomotion in VR typically involves users selecting their direction using controller input, with the forward direction determined by the Head, Hands, or less commonly, the Hip. The effects of these different sources on user experience are under-explored, and Feet have not been used as a direction source. To address these gaps, we compared these direction sources, including a novel Feet-based technique. A user study with 22 participants assessed these methods in terms of performance, preference, motion sickness, and sense of presence. Our findings indicate high levels of presence and minimal motion sickness across all methods. Performance differences were noted in one task, where the Head outperformed the Hand. The Hand method was the least preferred, feeling less natural and realistic. The Feet method was found to be more natural than the Head and more realistic than the Hip. This study enhances understanding of direction sources in VR locomotion and introduces Feet-based direction as a viable alternative.",Virtual Reality; User Studies; Continuous Locomotion,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Influence of Rotation Gains on Unintended Positional Drift during Virtual Steering Navigation in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Unintended Positional Drift (UPD) is a phenomenon that occurs during navigation in Virtual Reality (VR). It is characterized by the user’s unconscious or unintentional physical movements in the workspace while using a locomotion technique (LT) that does not require physical displacement (e.g., steering, teleportation). Recent work showed that some factors, such as the LT used and the type of trajectory, can influence UPD. However, little is known about the influence of rotation gains (commonly used in redirection-based LTs) on UPD during navigation in VR. In this paper, we conducted two user studies to assess the influence of rotation gains on UPD. In the first study, participants had to perform consecutive turns in a corridor virtual environment. In the second study, participants had to explore a large office floor and collect spheres freely. We compared the conditions between rotation gains and without gains, and we also varied the turning angle to perform the turns while considering factors such as sensitivity to cybersickness and the learning effect. We found that rotation gains and lower turning angles decreased UPD during the first study, but the presence of rotation gains increased UPD in the second study. This work contributes to the understanding of UPD, which tends to be an overlooked topic and discusses the design implications of these results for improving navigation in VR.",Virtual Reality; Locomotion Techniques; Rotation Gains; Unintended Positional Drift,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Semi-Automated Guided Teleportation through Immersive Virtual Environments,VRST - Virtual Reality Software and Technology,A,"Immersive knowledge spaces like museums or cultural sites are often explored by traversing pre-defined paths that are curated to unfold a specific educational narrative. To support this type of guided exploration in VR, we present a semi-automated, hands-free path traversal technique based on teleportation that features a slow-paced interaction workflow targeted at fostering knowledge acquisition and maintaining spatial awareness. In an empirical user study with 34 participants, we evaluated two variations of our technique, differing in the presence or absence of intermediate teleportation points between the main points of interest along the route. While visiting additional intermediate points was objectively less efficient, our results indicate significant benefits of this approach regarding the user’s spatial awareness and perception of interface dependability. However, the user’s perception of flow, presence, attractiveness, perspicuity, and stimulation did not differ significantly. The overall positive reception of our approach encourages further research into semi-automated locomotion based on teleportation and provides initial insights into the design space of successful techniques in this domain.",Virtual Reality; 3D Navigation; Teleportation; 3D User Interfaces; Head-Mounted Display; Guided Navigation; Guided Tour,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,The Effects of Electrical Stimulation of Ankle Tendons on Redirected Walking with the Gradient Gain,VRST - Virtual Reality Software and Technology,A,"As a redirected walking technique, a method has been proposed to enable users to walk in an undulating virtual space even in a flat physical environment by setting the slope of the floor in the virtual environment to be different from that in the physical environment without causing discomfort. However, the slope range in which discrepancies between visual and proprioceptive sensations are not perceived is limited, restricting the slopes that can be presented. In this study, we proposed redirected walking using electrical stimulation of the Achilles and tibialis anterior muscle tendons, extending the applicable slope range of redirected walking without compromising the natural gait sensation. Electrical stimulation of the ankle tendons affects the proprioceptive sensation and gives the illusion of tilting in the standing posture, expanding the applicable slope range. Two experiments showed that the proposed method improved the experience of uphill and downhill walking in terms of the range of the virtual slope where a high naturalness of gait and a high congruency of visual and proprioceptive sensations are maintained. Notably, electrical stimulation of the Achilles tendons significantly improved the naturalness of the walking experience during virtual downhill walking, which has been considered more challenging in previous studies.",Virtual reality; Redirected walking; Electrical stimulation of ankle tendons; Locomotion technique; Transcutaneous electrical stimulation,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Neural Motion Tracking: Formative Evaluation of Zero Latency Rendering,VRST - Virtual Reality Software and Technology,A,"Low motion-to-photon latencies between physical movement and rendering updates are crucial for an immersive virtual reality (VR) experience and to avoidusers’ discomfort and sickness. Current methods aim to minimize the delay between the motion measurement and rendering at the cost of increasing technical complexity and possibly decreasing accuracy. By relying on capturing physical motion, these strategies will, by nature, not result in zero latency rendering or will be based on prediction and resulting uncertainty. This paper presents and evaluates a novel alternative and proof of principle for VR motion tracking that could enable motion-to-photon latencies of zero and below zero in time. We termed our concept Neural Motion Tracking, which we define as the sensing and assessment of motion through human neural activation of the somatic nervous system. In contrast to measuring physical activity, the key principle is that we aim to utilize the physiological timeframe between a user’s intention and the execution of motion. We aim to foresee upcoming motion ahead of the physical movement, by sampling preceding electromyographic signals before the muscle activation. The electromechanical delay (EMD) between potential change in the muscle activation and actual physical movement opens a gap in which measurement can be taken and evaluated before the physical motion. In a first proof of principle, we evaluated the concept with two activities, arm bending and head rotation, measured with a binary activation measure. Our results indicate that it is possible to predict movement and update a rendering up to 2&nbsp;ms before its physical execution, which is assessed by optical tracking after approximately 4&nbsp;ms. However, to make the best use of this advantage, electromyography (EMG) sensor data should be as high quality as possible (i.e., low noise and from muscle-near electrodes). Our results empirically quantify this characteristic for the first time when compared to state-of-the-art optical tracking systems for VR. We discuss our results and potential pathways to motivate further work toward marker- and latency-less motion tracking.",mixed reality; augmented reality; Virtual reality; latency; tracking; electromyography,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Investigation of Redirection Algorithms in Small Tracking Spaces,VRST - Virtual Reality Software and Technology,A,"In virtual reality, redirected walking lets users walk in larger virtual spaces than the physical tracking space set aside for their movements. This benefits immersion and spatial navigation compared to virtual locomotion techniques such as teleportation or joystick control. Different algorithms have tried to optimise redirected walking. These algorithms have been tested in simulation in large spaces and with small user studies. However, few studies have looked at the user experience of these algorithms in small tracking spaces. We conducted a user study to compare the performance of different redirected walking algorithms in a small tracking space of 3.5m x 3.5m. Three algorithms were chosen based on their approaches to redirection – Reset Only, Steer to Centre and Alignment Based Redirection Control. 36 people participated in the study. It was found users preferred Reset Only in the tracking space. Reset Only redirects users less and is easier to implement than Steer to Centre or Alignment Based Redirection Control. Additionally, Reset Only had similar performance to Steer to Centre and better task performance than Alignment Based Redirection Control despite resetting users more often. Based on these findings, we provide guidelines for developers working in small tracking spaces.",virtual reality; user study; user experience; locomotion; redirected walking,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Interactive Multi-GPU Light Field Path Tracing Using Multi-Source Spatial Reprojection,VRST - Virtual Reality Software and Technology,A,"Path tracing combined with multiview displays enables progress towards achieving ultrarealistic virtual reality. However, multiview displays based on light field technology impose a heavy workload for real-time graphics due to the large number of views to be rendered. In order to achieve low latency performance, computational effort can be reduced by path tracing only some views (source views), and synthesizing the remaining views (target views) through spatial reprojection, which reuses path traced pixels from source views to target views. Deciding the number of source views with respect to the computational resources is not trivial, since spatial reprojection introduces dependencies in the otherwise trivially parallel rendering pipeline and path tracing multiple source views increases the computation time. In this paper, we demonstrate how to reach near-perfect linear multi-GPU scalability through a coarse-grained distribution of the light field path tracing workload. Our multi-source method path traces a single source view per GPU, which helps decreasing the number of dependencies. Reducing dependencies reduces the overhead of image transfers and G-Buffers rasterization used for spatial reprojection. In a node of 4 × RTX A6000 GPUs, given 4 source views, we reach a light field rendering frequency of 3–19 Hz, which corresponds to interactive rate. On four test scenes, we outperform state-of-the-art multi-GPU light field path tracing pipelines, achieving a speedup of 1.65 × up to 4.63 × for 1D light fields of dimension 100 × 1, each view having a resolution of 768 × 432, and 1.51 × up to 3.39 × for 2D stereo near-eye light fields of size 12 × 6 (left eye: 6 × 6 views and right eye: 6 × 6 views), 1024 × 1024 per view.",Dependencies; Multiview; Parallel Rendering; View Synthesis,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Exploring Visual Conditions in Virtual Reality for the Teleoperation of Robots,VRST - Virtual Reality Software and Technology,A,"In the teleoperation of robots, the absence of proprioception means that visual information plays a crucial role. Previous research has investigated methods to offer optimal vantage points to operators during teleoperation, with virtual reality (VR) being proposed as a mechanism to give the operator intuitive control over the viewpoint for improved visibility and interaction. However, the most effective perspective for robot operation and the optimal portrayal of the robot within the virtual environment remain unclear. This paper examines the impact of various visual conditions on users’ efficiency and preference in controlling a simulated robot via VR. We present a user study that compares two operating perspectives and three robot appearances. The findings indicate mixed user preferences and highlight distinct advantages associated with each perspective and appearance combination. We conclude with recommendations on selecting the most beneficial perspective and appearance based on specific application requirements.",Virtual Reality; 3D User Interfaces; Teleoperation,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Choose Your Reference Frame Right: An Immersive Authoring Technique for Creating Reactive Behavior,VRST - Virtual Reality Software and Technology,A,"Immersive authoring enables content creation for virtual environments without a break of immersion. To enable immersive authoring of reactive behavior for a broad audience, we present modulation mapping, a simplified visual programming technique. To evaluate the applicability of our technique, we investigate the role of reference frames in which the programming elements are positioned, as this can affect the user experience. Thus, we developed two interface layouts: ""surround-referenced"" and ""object-referenced"". The former positions the programming elements relative to the physical tracking space, and the latter relative to the virtual scene objects. We compared the layouts in an empirical user study (n = 34) and found the surround-referenced layout faster, lower in task load, less cluttered, easier to learn and use, and preferred by users. Qualitative feedback, however, revealed the object-referenced layout as more intuitive, engaging, and valuable for visual debugging. Based on the results, we propose initial design implications for immersive authoring of reactive behavior by visual programming. Overall, modulation mapping was found to be an effective means for creating reactive behavior by the participants.",Virtual Reality; Immersive Authoring; Visual Programming; Empirical Evaluation; Spatial Reference Frames,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Motion Passwords,VRST - Virtual Reality Software and Technology,A,"This paper introduces “Motion Passwords”, a novel biometric authentication approach where virtual reality users verify their identity by physically writing a chosen word in the air with their hand controller. This method allows combining three layers of verification: knowledge-based password input, handwriting style analysis, and motion profile recognition. As a first step towards realizing this potential, we focus on verifying users based on their motion profiles. We conducted a data collection study with 48 participants, who performed over 3800 Motion Password signatures across two sessions. We assessed the effectiveness of feature-distance and similarity-learning methods for motion-based verification using the Motion Passwords as well as specific and uniform ball-throwing signatures used in previous works. In our results, the similarity-learning model was able to verify users with the same accuracy for both signature types. This demonstrates that Motion Passwords, even when applying only the motion-based verification layer, achieve reliability comparable to previous methods. This highlights the potential for Motion Passwords to become even more reliable with the addition of knowledge-based and handwriting style verification layers. Furthermore, we present a proof-of-concept Unity application demonstrating the registration and verification process with our pretrained similarity-learning model. We publish our code, the Motion Password dataset, the pretrained model, and our Unity prototype on https://github.com/cschell/MoPs",Biometrics; Extended Reality; Authentication; Verification,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Out-Of-Virtual-Body Experiences: Virtual Disembodiment Effects on Time Perception in VR,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel experiment investigating the relationship between virtual disembodiment and time perception in Virtual Reality (VR). Recent work demonstrated that the absence of a virtual body in a VR application changes the perception of time. However, the effects of simulating an out-of-body experience (OBE) in VR on time perception are still unclear. We designed an experiment with two types of virtual disembodiment techniques based on viewpoint gradual transition: a virtual body’s behind view and facing view transitions. We investigated their effects on forty-four participants in an interactive scenario where a lamp was repeatedly activated and time intervals were estimated. Our results show that, while both techniques elicited a significant virtual disembodiment perception, time duration estimations in the minute range were only shorter in the facing view compared to the eye view condition. We believe that reducing agency in the facing view is a key factor in the time perception alteration. This provides first steps towards a novel approach to manipulating time perception in VR, with potential applications for mental health treatments such as schizophrenia or depression and for improving our understanding of the relation between body, virtual body, and time.",Virtual Reality; Plausibility; Presence; Avatar; Embodiment; Time Perception; Disembodiment; Virtual Body,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Some Times Fly: The Effects of Engagement and Environmental Dynamics on Time Perception in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"An hour spent with friends seems shorter than an hour waiting for a medical appointment. Many physiological and psychological factors, such as body temperature and emotions, have been shown to correlate with our subjective perception of time. Experiencing virtual reality (VR) has been observed to make users significantly underestimate the duration. This paper explores the effect of virtual environment characteristics on time perception, focusing on two key parameters: user engagement and environmental dynamics. We found that increased presence and interaction with the environment significantly decreased the users’ estimation of the VR experience duration. Furthermore, while a dynamic environment lacks significance in shifting perception toward one specific direction, that is, underestimation or overestimation of the durations, it significantly distorts perceived temporal length. Exploiting these two factors’ influence smartly constitutes a powerful tool in designing intelligent and adaptive virtual environments that can reduce stress, alleviate boredom, and improve well-being by adjusting the pace at which we experience the passage of time.",Virtual Reality; User Engagement; Time Perception; Environmental Dynamics,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Enhancing VR Sketching with a Dynamic Shape Display,VRST - Virtual Reality Software and Technology,A,"Sketching on virtual objects in Virtual Reality (VR) can be challenging due to the lack of a physical surface that constrains the movement and provides haptic feedback for contact and movement. While using a flat physical drawing surface has been proposed, it creates a significant discrepancy between the physical and virtual surfaces when sketching on non-planar virtual objects. We propose using a dynamic shape display that physically mimics the shape of a virtual surface, allowing users to sketch on a virtual surface as if they are sketching on a physical object’s surface. We demonstrate this using VRScroll, a shape-changing device that features seven independently controlled flaps to imitate the shape of a virtual surface automatically. Our user study showed that participants exhibited higher precision when tracing simple shapes with the dynamic shape display and produced clearer sketches. We also provided several design implications for dynamic shape displays aimed at enabling precise sketching in VR.",virtual reality; dynamic shape display; on-surface interactions,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Simulating Object Weight in Virtual Reality: The Role of Absolute Mass and Weight Distributions,VRST - Virtual Reality Software and Technology,A,"Weight interfaces enable users of Virtual Reality (VR) to perceive the weight of virtual objects, significantly enhancing realism and enjoyment. While research on these systems primarily focused on their implementation, little attention has been given to determining the weight to be rendered by them: As the perceived weight of objects is influenced not only by their absolute mass, but also by their weight distribution and prior expectations, it is currently unknown which simulated mass provides the most realistic representation of a given object. We conducted a study, in which 30 participants chose the best fitting weight for a virtual object in 54 experimental trials. Across these trials, we systematically varied the virtual objects’ visual mass (three levels), their weight distribution (six levels), and the position of the physical mass on the grip (three levels). Our Bayesian analysis suggests that the visual weight distribution of objects does not affect which absolute physical mass best represents them, whereas the position of the provided physical mass does. Additionally, participants overweighted virtual objects with lower visual mass while underweighting objects with higher visual mass. We discuss how these findings can be leveraged by designers of weight interfaces and VR experiences to optimize realism.",virtual reality; weight perception; multisensory integration; weight simulation; weight interfaces,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Enriching Industrial Training Experience in Virtual Reality with Pseudo-Haptics and Vibrotactile Stimulation,VRST - Virtual Reality Software and Technology,A,"Virtual Reality (VR) technology facilitates effective, flexible, and safe industrial training for novice technicians when on-site training is not feasible. However, previous research has shown that training in VR may be less successful than traditional learning approaches in real-world settings, and haptic interaction may be the key to improving virtual training. In this study, we integrated pseudo-haptic feedback from motion delay with vibrotactile stimulation to enhance the sense of presence, enjoyment, and the perception of physical properties in VR, which may be crucial for achieving faithful simulations. The impact of combined haptic support was assessed in a complex industrial training procedure completing a variety of tasks such as component assembly and cleaning. The results indicate that vibrotactile cues are beneficial for presence and enjoyment, whereas pseudo-haptic illusions effectively enable kinesthetic sensations. Furthermore, multimodal haptic feedback that mixed the two yielded the most advantageous outcomes. Our findings highlight the potential of the pseudo-haptic and vibrotactile fusion in industrial training scenarios, presenting practical implications of the state-of-the-art haptic technologies for virtual learning.",Haptics; Virtual reality; User study; Multimodal interaction; Industrial training,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Investigating the Impact of Odors and Visual Congruence on Motion Sickness in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Motion sickness is a prevalent side effect of exposure to virtual reality (VR). Previous work found that pleasant odors can be effective in alleviating symptoms of motion sickness such as nausea. However, it is unknown whether pleasant odors that do not match the anticipated scent of the virtual environment are also effective as they could, in turn, amplify symptoms such as disorientation. Therefore, we conducted a study with 24 participants experiencing a pleasant odor (rose) and an unpleasant odor (garlic) while being immersed in a virtual environment involving either virtual roses or garlic. We found that participants had lower motion sickness when experiencing the rose odor, however, only in the rose environment. Accordingly, we also showed that the sense of disorientation was lower for the rose odor, however, only while being immersed in the rose environment. Results indicate that whether pleasant odors are effective in alleviating motion sickness symptoms depends on the visual appearance of the virtual environment. We discuss possible explanations for such effects to occur. Our work contributes to the goal of mitigating visually induced motion sickness in VR.",virtual reality; olfaction; motion sickness; odor; visually induced motion sickness,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Generative Terrain Authoring with Mid-air Hand Sketching in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Terrain generation and authoring in Virtual Reality (VR) offers unique benefits, including 360-degree views, improved spatial perception, immersive and intuitive design experience and natural input modalities. Yet even in VR it can be challenging to integrate natural input modalities, preserve artistic controls and lower the effort of landscape prototyping. To tackle these challenges, we present our VR-based terrain generation and authoring system, which utilizes hand tracking and a generative model to allow users to quickly prototype natural landscapes, such as mountains, mesas, canyons and volcanoes. Via positional hand tracking and hand gesture detection, users can use their hands to draw mid-air strokes to indicate desired shapes for the landscapes. A Conditional Generative Adversarial Network trained by using real-world terrains and their height maps then helps to generate a realistic landscape which combines features of training data and the mid-air strokes. In addition, users can use their hands to further manipulate their mid-air strokes to edit the landscapes. In this paper, we explore this design space and present various scenarios of terrain generation. Additionally, we evaluate our system across a diverse user base that varies in VR experience and professional background. The study results indicate that our system is feasible, user-friendly and capable of fast prototyping.",Virtual Reality; Generative Terrain Authoring; Hand Gesture Control; Hand Sketching,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,How Different Is the Perception of Vibrotactile Texture Roughness in Augmented versus Virtual Reality?,VRST - Virtual Reality Software and Technology,A,"Wearable haptic devices can modify the haptic perception of an object touched directly by the finger in a portable and unobtrusive way. In this paper, we investigate whether such wearable haptic augmentations are perceived differently in Augmented Reality (AR) vs. Virtual Reality (VR) and when touching with a virtual hand instead of one’s own hand. We first designed a system for real-time rendering of vibrotactile virtual textures without constraints on hand movements, integrated with an immersive visual AR/VR headset. We then conducted a psychophysical study with 20 participants to evaluate the haptic perception of virtual roughness textures on a real surface touched directly with the finger (1) without visual augmentation, (2) with a realistic virtual hand rendered in AR, and (3) with the same virtual hand in VR. On average, participants overestimated the roughness of haptic textures when touching with their real hand alone and underestimated it when touching with a virtual hand in AR, with VR in between. Exploration behaviour was also slower in VR than with real hand alone, although subjective evaluation of the texture was not affected. We discuss how the perceived visual delay of the virtual hand may produce this effect.",Augmented Reality; Virtual Reality; Haptic Perception; Psychophysical Study; Roughness Textures; Virtual Hands; Wearable Haptics,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,TeenWorlds: Supporting Emotional Expression for Teenagers with their Parents and Peers through a Collaborative VR Experience,VRST - Virtual Reality Software and Technology,A,"Adolescence is a period of growth and exploration, marked by influential relationships with peers and parents. These relationships are essential for teenagers’ well-being, highlighting the need to support their interpersonal interactions. Emotional expression is key in resolving conflicts that can frequently arise. This paper investigates the potential of TeenWorlds, a Virtual Reality (VR) application, to facilitate emotional expression and shared understanding among teenagers and their peers and parents. In our study, teenagers, accompanied by either a peer or a parent (total n=42), used TeenWorlds to visually represent their emotions during a shared conflict, discuss them, and collaborate on a joint VR drawing. Our findings indicate that TeenWorlds can foster communication, reflection, and strengthen interpersonal relationships. However, notable differences were observed in interactions with peers versus parents. We contribute insights into designing VR systems that support reflective experiences and meaningful family interactions, ultimately enhancing the well-being of adolescents, parents, and families.",Virtual Reality; reflection; collaboration; parents; adolescent; family; youth; emotional expression; teenager; CCI,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,HistoLab VR: A User Elicitation Study Exploring the Potential of Virtual Reality Game-based Learning for Hazard Awareness,VRST - Virtual Reality Software and Technology,A,"Occupational medicine is a vital field for workplace safety and health but often encounters challenges in engaging students and effectively communicating subtle yet critical workplace hazards. To tackle these issues, we developed HistoLab VR, a Virtual Reality (VR) game that immerses participants in a histology lab environment based on real-world practice. Our comprehensive user study with 17 students and experts assessed the game’s impact on hazard awareness, interest in occupational medicine, and user experience through quantitative and qualitative measures. Our findings show that HistoLab VR not just immersed participants in a relatable histology lab worker experience but that it effectively raised awareness about subtle hazards and conveyed the inherent stress of the job. We discuss our results and highlight the potential of VR as a valuable educational tool for occupational medicine training.",education; workplace; anxiety; ergonomics; hazard awareness; histology laboratory; occupational medicine; serious games.,Title_Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Game-Based Motivation: Enhancing Learning with Achievements in a Customizable Virtual Reality Environment,VRST - Virtual Reality Software and Technology,A,"Digital learning experiences that promote interactive learning and engagement are becoming increasingly relevant. Educational games can be used to create an engaging learning atmosphere that allows knowledge acquisition through hands-on activities. Combining it with virtual reality (VR) allows users to interact with virtual environments, leading to a highly immersive learning experience. In this study, we explore how game achievements impact motivation and learning in a customizable VR learning environment. Using an A/B test involving 50 students, we utilized an interactive wave simulation to assess motivation, engagement, and the overall learning experience. Data collection involved standardized questionnaires, along with tracking interaction time and interactions within the virtual environment. The findings revealed that users who earned game achievements to unlock customization features felt significantly more accomplished when they mastered challenges and obtained all achievements. However, it was observed that adding achievements could also create pressure on students, leading to feelings of embarrassment when facing task failures. While achievements have the potential to enhance engagement and motivation, their excessive use may lead to distractions, anxiety, and reduced overall engagement. It shows that is crucial to find a good balance in employing game achievements within educational environments to ensure they contribute positively to the learning experience without causing undue stress or deterring learners.",virtual reality; STEM education; interactive simulations; immersive learning; customizable learning,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Hands or Controllers? How Input Devices and Audio Impact Collaborative Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Advancing virtual reality technologies are enabling real-time virtual-face to virtual-face communication. Hand tracking systems that are integrated into Head-Mounted Displays (HMD) enable users to directly interact with their environments and with each other using their hands as opposed to using controllers. Due to the novelties of these technologies our understanding of how they impact our interactions is limited. In this paper, we investigate the consequences of using different interaction control systems, hand tracking or controllers, when interacting with others in a virtual environment. We design and implement NASA’s Survival on the Moon teamwork evaluation exercise in virtual reality (VR) and test for effects with and without allowing verbal communication. We evaluate social presence, perceived comprehension, team cohesion, group synergy, task workload, as well as task performance and duration. Our findings reveal that audio communication significantly enhances social presence, perceived comprehension, and team cohesion, but it also increases effort workload and negatively impacts group synergy. The choice of interaction control systems has limited impact on various aspects of virtual collaboration in this scenario, although participants using hand tracking reported lower effort workload, while participants using controllers reported lower mental workload in the absence of audio.",collaboration; Communication; avatars; gestures,Title_Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Exploring User Placement for VR Remote Collaboration in a Constrained Passenger Space,VRST - Virtual Reality Software and Technology,A,"Extended Reality (XR) offers the potential to transform the passenger experience by allowing users to inhabit varied virtual spaces for entertainment, work or social interaction, whilst escaping the constrained transit environment. XR allows remote collaborators to feel like they are together and enables them to perform complex 3D tasks. However, the social and physical constraints of the passenger space pose unique challenges to productive and socially acceptable collaboration. Using a collaborative VR puzzle task, we examined the effects of five different f-formations of collaborator placement and orientation in an interactive workspace on social presence, task workload, and implications for social acceptability. Our quantitative and qualitative results showed that face-to-face formations were preferred for tasks with a high need for verbal communication but may lead to social collisions, such as inadvertently staring at a neighbouring passenger, or physical intrusions, such as gesturing in another passenger’s personal space. More restrictive f-formations, however, were preferred for passenger use as they caused fewer intrusions on other passengers’ visual and physical space.",Mixed Reality; Virtual Reality; Collaboration; Social Acceptability; Constrained Spaces; Passenger Spaces,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Stand Alone or Stay Together: An In-situ Experiment of Mixed-Reality Applications in Embryonic Anatomy Education,VRST - Virtual Reality Software and Technology,A,"Where traditional media and methods reach their limits in anatomy education, mixed-reality (MR) environments can provide effective learning support because of their high interactivity and spatial visualization capabilities. However, the underlying design and pedagogical requirements are as diverse as the technologies themselves. This paper examines the effectiveness of individual- and collaborative learning environments for anatomy education, using embryonic heart development as an example. Both applications deliver the same content using identical visualizations and hardware but differ in interactivity and pedagogical approach. The environments were evaluated in a user study with medical students (n = 90) during their examination phase, assessing usability, user experience, social interaction/co-presence, cognitive load, and personal preference. Additionally, we conducted a knowledge test before and after an MR learning session to determine educational effects compared to a conventional anatomy seminar. Results indicate that the individual learning environment was generally preferred. However, no significant difference in learning effectiveness could be shown between the conventional approach and the MR applications. This suggests that both can effectively complement traditional seminars despite their different natures. Our study contributes to understanding how different MR settings could be tailored for anatomical education.",Mixed Reality; Collaborative Learning; Medical Education; Immersive Learning Environments; Individual Adaptive Learning,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Contextual Matching Between Learning and Testing Within VR Does Not Always Enhance Memory Retrieval,VRST - Virtual Reality Software and Technology,A,"Episodic memory is influenced by environmental contexts, such as location and auditory stimuli. The most well-known effect is the reinstatement effect, which refers to the phenomenon where contextual matching between learning and testing enhances memory retrieval. Previous studies have investigated whether the reinstatement effect can be observed within immersive virtual environments. However, only a limited number of studies have reported a significant reinstatement effect using virtual reality, while most have failed to detect it. In this study, we re-examined the reinstatement effect using 360-degree video-based virtual environments. Specifically, we carefully selected virtual environments to elicit different emotional responses, which has been suggested as a key factor in inducing a robust reinstatement effect in the physical world. Surprisingly, we found a significant reversed reinstatement effect with a large effect size. This counter-intuitive result suggests that contextual congruence does not necessarily enhance memory and may even interfere with it. This outcome may be explained by the retrieval-induced forgetting phenomenon, but further exploration is needed. This finding is particularly important for virtual reality-based, educational applications and highlights the need for a deeper understanding of the complex interactions between memory and contextual cues within virtual environments.",virtual reality; 360-degree video; environmental context-dependent memory; reinstatement; retrieval-induced forgetting,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Toward Facilitating Search in VR With the Assistance of Vision Large Language Models,VRST - Virtual Reality Software and Technology,A,"While search is a common need in Virtual Reality (VR) applications, current approaches are cumbersome, often requiring users to type on a mid-air keyboard using controllers in VR or remove VR equipment to search on a computer. We first conducted a literature review and a formative study, identifying six common search needs: knowing about one object, knowing about the object’s partial details, knowing objects with environmental context, knowing about interactions with objects, and finding objects within field of view (FOV) and out of FOV in the VR scene. Informed by these needs, we designed technology probes that leveraged recent advances in Vision Large Language Models and conducted a probe-based study with users to elicit feedback. Based on the findings, we derived design principles for VR designers and developers to consider when designing a user-friendly search interface in VR. While prior work about VR search tended to address specific aspects of search, our work contributes design considerations aimed at enhancing the ease of search in VR and potential future directions.",participatory design; Virtual reality; vision large language model; VR search,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Evaluating Gaze Interactions within AR for Nonspeaking Autistic Users,VRST - Virtual Reality Software and Technology,A,"Nonspeaking autistic individuals often face significant inclusion barriers in various aspects of life, mainly due to a lack of effective communication means. Specialized computer software, particularly delivered via Augmented Reality (AR), offers a promising and accessible way to improve their ability to engage with the world. While research has explored near-hand interactions within AR for this population, gaze-based interactions remain unexamined. Given the fine motor skill requirements and potential for fatigue associated with near-hand interactions, there is a pressing need to investigate the potential of gaze interactions as a more accessible option. This paper presents a study investigating the feasibility of eye gaze interactions within an AR environment for nonspeaking autistic individuals. We utilized the HoloLens 2 to create an eye gaze-based interactive system, enabling users to select targets either by fixating their gaze for a fixed period or by gazing at a target and triggering selection with a physical button (referred to as a ‘clicker’). We developed a system called HoloGaze that allows a caregiver to join an AR session to train an autistic individual in gaze-based interactions as appropriate. Using HoloGaze, we conducted a study involving 14 nonspeaking autistic participants. The study had several phases, including tolerance testing, calibration, gaze training, and interacting with a complex interface: a virtual letterboard. All but one participant were able to wear the device and complete the system’s default eye calibration; 10 participants completed all training phases that required them to select targets using gaze only or gaze-click. Interestingly, the 7 users who chose to continue to the testing phase with gaze-click were much more successful than those who chose to continue with gaze alone. We also report on challenges and improvements needed for future gaze-based interactive AR systems for this population. Our findings pave the way for new opportunities for specialized AR solutions tailored to the needs of this under-served and under-researched population.",augmented reality; assistive technology; eye tracking; nonspeaking autistic people,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Exploring Immersive Debriefing in Virtual Reality Training: A Comparative Study,VRST - Virtual Reality Software and Technology,A,"Simulation and debriefing are two essential and inseparable phases of virtual reality training. With the widespread adoption of these training tools, it is crucial to define the best pedagogical approaches for trainers and learners to maximize their effectiveness. However, despite their educational benefits, virtual reality-specific debriefing methods remain underexplored in research. This article proposes an architecture and interface for an all-in-one immersive debriefing module that is adaptable to different types of training, including a complete system for recording, replaying, and redoing actions. A study with 36 participants compared this immersive debriefing system with traditional discussion-based and video-supported debriefing. Participants were divided into three groups to evaluate the effectiveness of each method. The results showed no significant differences between these debriefing methods across several criteria, such as satisfaction, motivation, or information retention. Immersive debriefing is as usable and retentive as traditional or video debriefing in this context. The next step will be to evaluate the redo system in other training courses involving more dynamic scenarios.",Virtual Reality; Simulation; Debriefing; Immersive Learning; Trainer,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,"A Critical Review of Virtual and Extended Reality Immersive Police Training: Application Areas, Benefits &amp; Vulnerabilities",VRST - Virtual Reality Software and Technology,A,"Virtual and Extended Reality (VR/XR) headsets have promised to enhance police training through the delivery of immersive simulations able to be conducted anywhere, anytime. However, little consideration has been given to reviewing the evidenced benefits and potential issues posed by XR police training. In this paper, we summarise the evidenced usage and benefits of XR police training through a formative targeted literature review (n=41 publications). We then reflect on the prospective technical, security, social and legal issues posed by XR police training, identifying four areas where issues or vulnerabilities exist: training content, trainees and trainers, systems and devices, and state and institutional stakeholders. We highlight significant concerns around e.g. the validity of training; the psychological impact and risks of trauma; the safety and privacy risks posed to trainees and trainers; and the risks to policing institutions. We aim to encourage end-user communities (e.g. police forces) to more openly reflect on the risks of immersive training, so we can ultimately move towards transparent, validated, trusted training that is evidenced to improve policing outcomes.",Virtual Reality; Extended Reality; Police Training,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,The Impact of Task-Responsibility on User Experience and Behaviour under Asymmetric Knowledge Conditions,VRST - Virtual Reality Software and Technology,A,"Virtual Reality presents a promising tool for knowledge transfer, allowing users to learn in different environments and with the help of three-dimensional visualizations. At the same time, having to learn new ways of interacting with their environment can present a significant hurdle for novice users. When users enter a virtual space to receive knowledge from a more experienced person, the question arises as to whether they benefit from learning VR-specific interaction techniques instead of letting the expert take over some or all interactions. Based on related work about expert-novice interaction in virtual spaces, this paper presents a user study comparing three different distributions of interaction responsibilities between participants and an expert user. The Role-Based interaction mode gives the expert the full interaction responsibility. The Shared interaction mode gives both users the same interaction capabilities, allowing them to share the responsibility of interacting with the virtual space. Finally, the Parallel interaction mode gives participants full interaction responsibility, while the expert can provide guidance through oral communication and visual demonstration. Our results indicate that assuming interaction responsibility led to higher task loads but also increased the participant’s engagement and feeling of presence. For most participants, sharing interaction responsibilities with the expert represented the best trade-off between engagement and challenge. While we did not measure a significant increase in learning success, participant comments indicated that they also paid more attention to details when assuming more interaction responsibility.",Virtual Reality; Collaboration; 3D User Interfaces; Head-Mounted Display; Instruction; Knowledge-Transfer,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Evaluating the effects of Situated and Embedded Visualisation in Augmented Reality Guidance for Isolated Medical Assistance,VRST - Virtual Reality Software and Technology,A,"One huge advantage of Augmented Reality (AR) is its numerous possibilities of displaying information in the physical world, especially when applying Situated Analytics (SitA). AR devices and their respective interaction techniques allow for supplementary guidance to assist an operator carrying out complex procedures such as medical diagnosis and surgery, for instance. Their usage promotes user autonomy by presenting relevant information when the operator may not necessarily possess expert knowledge of every procedure and may also not have access to external help such as in a remote or isolated situation (e.g., International Space Station, middle of an ocean, desert). In this paper, we propose a comparison of two different forms of AR visualisation: An embedded visualisation and a situated projected visualisation, with the aim to assist operators with the most appropriate visualisation format when carrying out procedures (medical in our case). To evaluate these forms of visualisation, we carried out an experiment involving 23 participants possessing latent/novice medical knowledge. These participant profiles were representative of operators who are medically trained yet do not apply their knowledge every day (e.g., an astronaut in orbit or a sailor out at sea). We discuss our findings which include the advantages of embedded visualised information in terms of precision compared to situated projected information with the accompanying limitations in addition to future improvements to our proposition. We conclude with the prospects of our work, notably the continuation and possibility of evaluating our proposition in a less controlled and real context in collaboration with our national space agency.",Augmented Reality; Immersive Analytics; Situated Analytics; AR Guidance; Isolated Situation; Medical Assistance; Procedure Execution; Situated Visualisation,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,An Evaluation of Targeting Methods in Spatial Computing Interfaces with Visual Distractions,VRST - Virtual Reality Software and Technology,A,"In modern spatial computing devices, users are confronted with diverse methods for object selection, including eye gaze (cf. Apple Vision Pro), hand gestures (cf. Microsoft HoloLens 2), touch gestures (cf. Google Glass Enterprise Edition 2), and external controllers (cf. Magic Leap 2). Although there are a plethora of empirical studies on which selection techniques perform best, a common limiting factor stems from the partly artificial setups. These typically exclude practical influences such as visual distraction. In this paper, we present a user study comparing two hand-based and two gaze-based state-of-the-art selection methods, using the HoloLens 2. We extended a traditional Fitts’ law-inspired study design by incorporating a visual task that simulates changes in the user interface after a successful selection. Without a visual task, gaze-based techniques were on average faster than hand-based techniques. This performance gain was eliminated (for head gaze) or even reversed (for eye gaze) when the visual task was active. These findings underscore the value of continued practice-oriented research of targeting methods in virtual environments.",Augmented Reality; Interaction Techniques; Selection,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Evaluation of AR Pattern Guidance Methods for a Surface Cleaning Task,VRST - Virtual Reality Software and Technology,A,"Cleanroom cleaning is a surface coverage task where the pattern should be followed correctly, and the entire surface should be covered. We investigate the efficacy of augmented reality (AR) by implementing various pattern guidance designs to enhance a cleanroom cleaning task. We developed an AR guidance system for cleaning procedures and evaluated four distinct pattern guidance methods: (1) breadcrumbs, (2) examples, (3) middle lines, and (4) outlines. We vary the instructions on the entire surface or as a single step. To measure performance, accuracy, and user satisfaction associated with each guidance method, we conducted a large-scale (n=864) between-subjects study. Our findings indicate that single step instructions proved to be more intuitive and efficient than full instructions, especially for the breadcrumbs. We also discussed the implications of our results for the development of AR applications for surface coverage and pattern optimization.",Augmented Reality; Motion control.; Pattern guidance,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Editing Immersive Recordings: An Elicitation Study,VRST - Virtual Reality Software and Technology,A,"Immersive recordings capture virtual reality interactions and are used in various contexts such as education and entertainment. However, there has been only limited research on requirements and techniques for editing such recordings. We interviewed expert editors of video recordings to understand their workflows, familiarised them with immersive recordings, and asked them about what editing challenges and capabilities they can envision for immersive recordings. The experts identified several functionalities they considered relevant for editing, including viewer placement, control over the viewer’s size, support for live and asynchronous collaboration, and different transition types.",Virtual Reality; Immersive Editing; Immersive Recordings,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Towards an Avatar Customization System for Semi-realistic Ethnically-diverse Virtual Reality Avatars,VRST - Virtual Reality Software and Technology,A,"Due to the Proteus effect, in which people modify their behaviour based on their avatar, participant avatar representation is an important factor in virtual reality (VR) studies. We develop an open source prototype avatar customization system that enables quick customization of semi-realistic, ethnically-diverse avatars. The prototype provides options for customizing body and face shape, hairstyle, glasses, religious clothing, and skin, eye, and hair colour. The prototype generates avatar assets that are fully rigged and textured for incorporation into VR study code, and it serves as a step towards designing more inclusive VR research studies.",avatar customization; Proteus effect; virtual avatars,Title_Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,"Comparing Tracking Accuracy in Standalone MR-HMDs: Apple Vision Pro, Hololens 2, Meta Quest 3, and Pico 4 Pro",VRST - Virtual Reality Software and Technology,A,"Modern Mixed Reality Head-Mounted Displays (MR-HMDs) can track user movements across large spaces without external markers. This study evaluates the tracking accuracy and the loop closure capabilities of four commercially available MR-HMDs across four distinct scenarios. We found consistent tracking performance in well-lit and expansive environments for all devices. Tracking accuracy remained stable even in outdoor nighttime conditions. Furthermore, most HMDs demonstrated effective error correction during loop closure, with errors in non-loop scenarios consistently exceeding those in loop scenarios.",Mixed Reality; Tracking Accuracy; Visual Inertial Odometry,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Exploring Alternative Text Input Modalities in Virtual Reality: A Comparative Study,VRST - Virtual Reality Software and Technology,A,"Text input in Virtual Reality (VR) is crucial for communication, search, and productivity. We compared four keyboard designs for VR text entry, leveraging the flexibility and the tracking options of a 3D environment. We used the Dvorak layout to control for experience differences. The designs were: (a) a floating keyboard with touch input, (b) a keyboard attached on the back of the hand with touch input, (c) a floating keyboard with eye tracking and pinch input, and (d) a keyboard laid out over a rolling shape with touch input. Designs (b), (c), and (d) can move in 3D space, while design (a) is static. Design (d) had similar efficiency to design (a) but with better usability and lower Physical Demand. Design (b) led to higher Physical Demand, Effort, and Frustration. Design (c) had lower Physical Demand but higher Mental Demand, Effort, and error rates. Typing speeds averaged 6.51 WPM (1.24% error rate) for (a), 5.56 WPM (3.82% error rate) for (b), 5.33 WPM (1.43% error rate) for (c), and 6.70 WPM (1.64% error rate) for (d).",Virtual Reality; Text input; Interface design,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,A Study on the Effectiveness of Augmented Reality Signal-Integrated Camera Monitor Systems for Safe Lane Changing,VRST - Virtual Reality Software and Technology,A,"This study investigates the effectiveness of augmented reality (AR) signals in camera monitor systems (CMS) for enhancing safety during lane changes. Seventy participants used seven side mirror conditions, including traditional side mirrors and six CMS conditions with and without AR signals. Results showed that CMS with AR signals significantly reduced the number of collisions and reaction time compared to CMS without AR signals.",Augmented Reality; Virtual Reality; Camera Monitor Systems; Driving Safety; Driving Simulation,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Exploring Influencers' and Users' Experiences in Douyin's Virtual Reality Live-Streaming,VRST - Virtual Reality Software and Technology,A,"VR live-streaming has become an emerging part on Douyin. This study aims to explore the technical modes, content strategies, user experiences in Douyin‘s VR live-streaming. Through interviews and focus groups, we found that VR technology is recognized by influencers and has become an essential part of their creative practice. For some influencers, VR technology is a key factor in enhancing audience engagement and immersive experiences, although technical literacy barriers may arise when setting up VR scenes. We also provide dimensions for improving and developing user adoption and experience of VR technology in social media environments.",Virtual Reality; Douyin; Influencers; Live-Streaming; Users,Title_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Digital Eyes: Social Implications of XR EyeSight,VRST - Virtual Reality Software and Technology,A,"The EyeSight feature, introduced with the new Apple Vision Pro XR headset, promises to revolutionize user interaction by simulating real human eye expressions on a digital display. This feature could enhance XR devices’ social acceptability and social presence when communicating with others outside the XR experience. In this pilot study, we explore the implications of the EyeSight feature by examining social acceptability, social presence, emotional responses, and technology acceptance. Eight participants engaged in conversational tasks in three conditions to contrast experiencing the Apple Vision Pro with EyeSight, the Meta Quest 3 as a reference XR headset, and a face-to-face setting. Our preliminary findings indicate that while the EyeSight feature improves perceptions of social presence and acceptability compared to the reference headsets, it does not match the social connectivity of direct human interactions.",Extended Reality; Social Acceptability; Social Presence,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,SOLDAR: Supporting Low-Volume PCB Prototyping Using Collaborative Robots and Augmented Reality,VRST - Virtual Reality Software and Technology,A,"Printed circuit boards (PCBs) are fundamental to modern electronics and are present in almost every electronic device. However, despite their ubiquity, current PCB assembly methods can be time-consuming and lack flexibility for one-off designs. This poster investigates how low-volume PCB prototyping can be enhanced by integrating collaborative robots (cobots) and Augmented Reality (AR). Specifically, we introduce SOLDAR, a system that facilitates the soldering of electronic through-hole components on PCBs. By using a cobot for optimal PCB positioning and AR glasses for step-by-step guidance, SOLDAR aims to streamline the assembly process. The expected outcomes are increased efficiency, reduced assembly time, and greater flexibility for low-volume PCB prototyping designs. To validate these hypotheses, user experiments are necessary.",Augmented Reality; Prototyping; Collaborative Robots,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Earscape: A VR Auditory Educational Escape Room,VRST - Virtual Reality Software and Technology,A,"According to the World Health Organisation’s World Report on Hearing, there is a strong need to provide better education on hearing loss from a young age. This project aims to educate the Danish young population (13 to 17-year-olds) about the hearing sense through an educational multiplayer virtual reality-based escape room with the benefits of educational escape rooms. In collaboration with relevant audiologist stakeholders, this project follows an iterative process of design, implementation, and evaluation of the application. The developed solution will undergo several user studies in the following months.",Virtual Reality; Educational Escape Room; Hearing Loss; Multiplayer,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Haptic and Auditory Feedback on Immersive Media in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"In Virtual Reality (VR), visual and auditory sensations are effectively leveraged to create immersive experiences. However, touch is significantly underutilized in immersive media. We enhance the VR image viewing experience by integrating haptic and auditory feedback into 3D environments constructed from immersive media. We address the challenges of utilizing depth maps from various image formats to create intractable environments. The VR experience is enhanced using vibrohaptic feedback and audio cues triggered by controller collisions with haptic materials.",Virtual Reality; Haptic Feedback; Auditory Feedback; Immersive Media,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,From Ground to Sky: Flying-motion Generation via Motion Dataset Adaptation,VRST - Virtual Reality Software and Technology,A,"We conducted a study utilizing a lightweight generative network to create flying motions. The existing datasets used for training did not include any data on flying motions. Therefore, we selected certain classes from the existing motion datasets and transformed these motions to resemble flying actions. By training the existing generative network with the modified dataset, we were able to generate motions that closely resemble flying. The results of this study demonstrate the potential for generating flying motions. The generation of flying motions for human avatars is expected to be a critical technology not only in 3D animation or game industry but also in virtual environments, enabling users to experience various activities through their avatars.",Virtual Reality; dataset; Avatar motion,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Walking of uphill slopes in immersive virtual environments,VRST - Virtual Reality Software and Technology,A,"We explore three visual manipulation techniques aiming to create a realistic feeling of walking an uphill slope while in reality being on flat ground. The techniques are based on real physical visual perception and consist of modification of height and display of virtual shoes, modification of speed, and modification of view pitch. Quantitative and qualitative evaluation indicated that modification of speed, and pitch contributed to user discomfort, as well as a general increase in discomfort correlating with the slope’s increasing inclination. However, height manipulation was well received and can be used in future projects for more realistic landscape.",Virtual reality; walking; human factors,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Enhanced Wayfinding Insights Through VR and Eye-Tracking Analysis,VRST - Virtual Reality Software and Technology,A,"This paper presents a novel method for evaluating wayfinding within a public building to provide meaningful insights for stakeholders. Our approach features unique methods for both data collection and evaluation, with a holistic digital capture of the entire virtual environment experienced by participants, maintained in an interactive format for in-depth analysis. We also captured and output data in point cloud formats, raw data text files, and task-specific metrics, which support interactive replays of participants’ experiences. We developed algorithms to extract meaningful insights from the raw data based on assumptions about wayfinding characteristics. The contribution is a flexible framework that can be easily adapted for future projects with adjustable variables to suit specific applications.",virtual reality; point cloud; wayfinding; gaze tracking; signage; unreal engine,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Pipelining Processors for Decomposing Character Animation,VRST - Virtual Reality Software and Technology,A,"This paper presents an openly available implementation of a modular pipeline architecture for character animation. It effectively decomposes frequently necessary processing steps into dedicated character processors, such as copying data from various motion sources, applying inverse kinematics, or scaling the character. Processors can easily be parameterized, extended (e.g., with AI), and freely arranged or even duplicated in any order necessary, greatly reducing side effects and fostering fine-tuning, maintenance, and reusability of the complex interplay of real-time animation steps.",Virtual Reality; Avatars; Embodiment; Agents; Extended Reality.; Humanoid Characters; Open-Source; Virtual Humans,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Study of inpainting based on generative AI for noise-canceling HMDs,VRST - Virtual Reality Software and Technology,A,"Entering a small space such as an elevator or a crowded train with a stranger can cause discomfort and suffocation. This is because the stranger is invading the individual’s personal space. However, it is difficult to maintain an appropriate interpersonal distance from others at all times in various situations. Therefore, a noise-canceling HMD [2][3] that uses AR to change the size of the person in the field of vision has been proposed as a means of reducing noise such as discomfort caused by inappropriate interpersonal distance. In this paper, we propose an improvement method using generative AI for background completion in noise-canceling HMDs.",Augmented Reality; Noise-canceling HMD,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,A Comparison between Vibrotactile Error-correction Feedback on Upper and Lower Body in the VR Snowboard Balancing Task,VRST - Virtual Reality Software and Technology,A,"This study investigated the effect of vibrotactile stimulus location on the balancing task in virtual reality (VR). Using a virtual snowboarding system with wearable haptic devices, we conducted a between-subject user study comparing the effectiveness of two different body locations–upper body (UB; torso vibrations) and lower body (LB; ankle vibrations). The real-time vibrotactile balance-correction feedback was generated by the Center of Pressure (CoP) calculated from the sensor array on insoles. The initial results showed that UB feedback is better than LB to improve users’ balance ability.",virtual reality; balancing; center-of-pressure; Vibrotactile wearables,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,The MASTER XR Platform for Robotics Training in Manufacturing,VRST - Virtual Reality Software and Technology,A,"The MASTER project introduces an open Extended Reality (XR) platform designed to enhance human-robot collaboration and train workers in robotics within manufacturing settings. It includes modules for creating safe workspaces, intuitive robot programming, and user-friendly human-robot interactions (HRI), including eye-tracking technologies. The development of the platform is supported by two open calls targeting technical SMEs and educational institutes to enhance and test its functionalities. By employing the learning-by-doing methodology and integrating effective teaching principles, the MASTER platform aims to provide a comprehensive learning environment, preparing students and professionals for the complexities of flexible and collaborative manufacturing settings.",Robotics; Industry 4.0; Eye Tracking; Extended Reality (XR); Manufacturing; Human-Robot Collaboration; Worker Training,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,VR4UrbanDev: An Immersive Virtual Reality Experience for Energy Data Visualization,VRST - Virtual Reality Software and Technology,A,"In this demonstration paper, we present our interactive virtual reality (VR) experience, which has been designed to facilitate interaction with energy-related information. This experience consists of two main modes: the world in miniature for large-scale and first-person for real-world scale visualizations. Additionally, we presented our approach to potential target groups in interviews. The results of these interviews can help developers for future implementation considering the requirements of each group.",Virtual Reality; Data Visualization; Building Information Modeling,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,UXR-kit: An Ideation Kit and Method for Collaborative and User-Centered Design about Extended Reality systems.,VRST - Virtual Reality Software and Technology,A,"Emerging kits and methods about Extended Reality (XR) systems are mainly centered on the prototyping phase. The ideation phase, which comes before prototyping, is currently still under-explored. In this work, we propose UXR-kit: a toolkit and a method for the co-design of ideas for XR systems. UXR-kit is based on an approach inspired by design studios and generative techniques and highlights the specificities of XR systems. Results from an experimental study suggest that UXR-kit allows the emergence of ideas for XR designs through both World-In-Miniature representations and first-person representations at scale 1:1.",Mixed Reality; Extended Reality; Ideation; Design toolkit,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Cultural Windows: Towards Immersive Journeys into Global Living Spaces,VRST - Virtual Reality Software and Technology,A,"“Cultural Windows” is a research initiative aimed at enhancing cross-cultural understanding through immersive extended reality (XR) experiences. The project deploys AR and VR platforms to allow users to explore diverse living spaces, bridging the gap between preconceived notions and the actual appearance of these spaces. By using 3D scanning to create accurate models of culturally significant objects and integrating them into immersive systems, the project provides insights into the use of immersive technologies in cultural education, promoting engagement with global living designs.",Extended Reality (XR); Cross-Cultural Visualization; Cultural Awareness in Design,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,A Volumetric Video Application to Enhance Museum Experiences,VRST - Virtual Reality Software and Technology,A,"Volumetric video (VV) is an emerging 3D format that allows the integration of real people into XR (extended reality) applications. Recent cost-effective AI-based methods have enabled VV capture using single handheld cameras or mobile phones. This study addresses the quality, integration, and acceptance of AI-based VV content creation in an augmented reality (AR) application designed to enhance museum experiences. The main result reveals that, although the current VV quality is lower than professional standards, users still find significant added value and enjoy its immersive experience.",,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Effectiveness of Adaptive Difficulty Settings on Self-efficacy in VR Exercise,VRST - Virtual Reality Software and Technology,A,"The difficulty is a fundamental factor of the user’s motivation and engagement in some tasks. Dynamic difficulty adjustment (DDA) systems provide users with an optimal level of challenge. Previously, some studies developed a DDA system that can set the task’s difficulty to any level. However, these studies lack the investigation of the influence of the difficulty levels on the psychological aspect. For this purpose, we consider a difficulty setting that consists of stepwise difficulty levels (e.g., hard, normal, and easy) set to adapt to each user’s skill and evaluate it using self-efficacy. In the experiment, we employ a Kendama task in a VR space where the difficulty level can be easily adjusted. The result shows that the difficulty levels in our method can be set according to the user’s skill. Moreover, we experimentally clarify a strong correlation between successful experiences in imagination and the enhancement of self-efficacy in the difficulty setting, which means that adapting difficulty levels to the user’s skill has the potential to enhance self-efficacy effectively.",Virtual reality; User experience; Self-efficacy; Difficulty adjustment,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Investigation of Simulator Sickness in Walking with Multiple Locomotion Technologies in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"With the increasing development of Virtual Reality, locomotion has become an essential component of interaction in VR. Currently, various locomotion technologies have been developed to provide users with a natural walking experience in virtual environments. However, the multiple walking techniques impact users’ walking experience in different ways. Simulator sickness is a common issue in VR experiences. Since different walking methods may influence simulator sickness differently, we conducted a user study to evaluate simulator sickness in walking with three relevant walking methods: real walking, arm-swing, and omnidirectional treadmill, and the results indicated that these three walking methods caused different levels of simulator sickness, and people perceived stronger sickness when they walked on the omnidirectional treadmill.",Virtual Reality; Simulator Sickness; Locomotion Technologies; Natural Walking Techniques,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Exploring an XR Indoor Navigation System for Remote Collaboration,VRST - Virtual Reality Software and Technology,A,"While collaboration in shared extended reality spaces has been extensively explored, larger environments like entire floors or buildings have garnered less attention. To address this gap, spatial navigation and collaboration across realities must be made possible so that users can find each other and foster shared spatial understanding independent from reality. Current developments target either navigation or collaboration but lack the combination. In this poster, we present an extended reality remote collaboration system using an augmented reality (AR) based indoor navigation for on-site and a Building Information Model (BIM) of the physical environment Virtual Reality (VR) system for remote users. We conducted a user study with ten participants (five pairs) to gather initial insights into the system’s usability and preferences for collaborative tools. The results offer initial insights into creating shared spatial understanding across realities. Our work contributes to a collaborative XR navigation system for extensive shared spaces.",AR; VR; indoor navigation; remote; XR collaboration,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Wheel-Based Attachable Footwear for VR: Challenges and Opportunities in Seated Walking-in-Place Locomotion,VRST - Virtual Reality Software and Technology,A,"This poster explores the potential of Cybershoes, a foot-based consumer input device, used with a swivel chair to enable seated walking-in-place (WIP) locomotion in virtual reality (VR). Through a qualitative study with 12 participants, we investigated the effects of Cybershoes on user comfort, presence, motion sickness, and overall experience during various sightseeing tasks. Our findings reveal both opportunities and challenges for Cybershoes as a seated-WIP solution. Participants perceived Cybershoes as more natural for navigation compared to handheld controllers, with most reporting reduced motion sickness. However, challenges included perceived slower movement speed, ergonomic issues, and limited action detection. Our work also highlights Cybershoes’ potential beyond gaming, including applications in exercise, professional training, remote work, and accessibility.",VR; locomotion; shoes; input.; seated walking; virtual travel; wheel,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,White Lies in Virtual Reality: Impact on Enjoyment and Fatigue,VRST - Virtual Reality Software and Technology,A,"This study examined the impact of a ""white lie"" designed to boost motivation during virtual reality exercise on enjoyment and mental fatigue. Participants engaged in a ball-throwing or ball-targeting task and were randomly assigned to groups with or without the white lie. Results indicated that both groups experienced similar levels of enjoyment and fatigue, suggesting the white lie had minimal effect on these factors. All participants, regardless of group, reported high levels of enjoyment, with 17 out of 18 indicating they had fun, no significant differences in mental fatigue were found between groups while participants generally favored the white lie. However, the positive experience across all participants highlights the potential of Virtual Reality for promoting exercise engagement.",Virtual Reality; Fatigue; Enjoyment; White Lies,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Dynamic Difficulty Adjustment in Virtual Reality Exergaming to Regulate Exertion Levels via Heart Rate Monitoring,VRST - Virtual Reality Software and Technology,A,"By regulating exertion levels, Dynamic difficulty adjustment (DDA) has the potential to enhance user experience and optimize exercise in Virtual Reality (VR) exergames. This pilot study assesses the effectiveness of adjusting the difficulty of gameplay challenges based on heart rate (HR) data to control the intensity of physical activity in VR exergaming. Observational results from 13 participants indicate that the HR-based DDA more effectively maintained target heart rate zones compared to randomized adjustments. Improved perceived exertion, and increased enjoyment underlines the potential of this approach for VR-based exercise and rehabilitation programs.",heart rate; dynamic difficulty adjustment; virtual reality exergames,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Rendering diffraction Phenomena on rough surfaces in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Wave-optical phenomena, such as diffraction, significantly impact the visual appearance of surfaces. Despite their importance, wave-optical reflection models are rare and computationally expensive. Recently, we presented a real-time model that accounts for diffraction-induced color shifts and speckle. Given that diffraction phenomena are highly dependent on illumination and viewing directions, as well as stereoscopic vision, we developed a VR demo to evaluate the new model. This demo shows the substantial impact of diffraction on the appearance of rough surfaces, particularly in stereoscopic viewing.",Virtual Reality; Modeling; Diffraction; Predictive Rendering,Title_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Supporting Wildfire Evacuation Preparedness through a Virtual Reality Simulation,VRST - Virtual Reality Software and Technology,A,This demo presents a virtual reality simulation of a wildfire evacuation. Players are tasked with going through a home environment and collecting items they believe they would need and want to take if they were under an evacuation notice. The experience is playable on the Meta Quest 2 headset.,Virtual Reality; Training; Evacuation; Wildfires,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Row your boat in VR and solve thinking exercises on the way: The Brain-Row Challenge,VRST - Virtual Reality Software and Technology,A,"In this demo, we showcase Brain-Row Challenge. Brain-Row Challenge is a research prototype for dual-task training in Virtual Reality (VR). Dual-task training combines a mental and a physical task. This training is relevant in neurodegenerative diseases, especially in Parkinson’s disease. The user is rowing with a Concept 2 ergometer over a Nordic lake, must follow a marked route and answers multiple-choice questions by rowing through gates. Steering is done with an inertial measurement unit that is attached to the handlebar. The VR experience can also be compared to a less immersive representation of the rowing course on a TV screen.",Dual Tasking; Ergometers; Excer Game; Medical Application,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,ChronoShore: Diegetic Temporal Exploration in a Simulated Virtual Coast Environment,VRST - Virtual Reality Software and Technology,A,"This paper introduces ChronoShore, an immersive virtual reality (VR) experience designed to explore diegetic time manipulation mechanics within a semi-realistic coastal environment. Traditional 2D video scrubbing methods fall short in immersive settings, particularly for understanding time-bound processes such as simulations of geology or biology. ChronoShore addresses this by allowing users to interact with celestial bodies to dynamically control and experience the passage of time, currently showcasing different weather events and atmospheric phenomena.",virtual reality; simulation; Time manipulation,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Hands-On Plant Root System Reconstruction in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"VRoot is an immersive extended reality reconstruction tool for root system architectures from 3D volumetric scans of soil columns. We have conducted a laboratory user study to assess the performance of new users with our software in comparison to established software. We utilize a plant model to derive a synthetic root architecture, providing a baseline for reconstruction. This demo showcases the processes and techniques contributing to exact and efficient manual root architecture reconstruction in Virtual Reality. The extraction task typically is the sparse graph-structure extraction from a 3D magnetic-resonance imaging (MRI) data set. We visualize the RSA directly within the MRI and offer selection-set-based methods of adapting and augmenting the root architecture. This application is in productive use at our partner institute, where it is used to analyze complex root images.",virtual reality; 3D imaging; root reconstruction,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Off-The-Shelf: Exploring 3D Arrangements of See-Through Masks to Switch between Virtual Environments,VRST - Virtual Reality Software and Technology,A,"This demo explores prioritization techniques to arrange see-through masks in virtual reality (VR). The oval masks show live previews of different virtual environments (VEs) and allow for seamless teleportation into a corresponding VE by putting the mask on the face. Each environment includes a mini-game (e.g., basketball and archery) in which the user has to perform a small task. The arrangement of the masks changes depending on a calculated rating, which considers the time since the game was last played and the game score. We envision this system to help users to multitask in VR. For example, to control multiple characters in VR games, to experience multi-strand (nonlinear) narratives, and to supervise semi-autonomous agents in different VEs.",Virtual Reality; Multiverse; Transitions; Mask,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,EcoDive: Enhancing Presence and Ambient Environmental Awareness in a Virtual Reality Experience for Underwater Marine Debris Collection,VRST - Virtual Reality Software and Technology,A,"This paper presents a VR-based serious game. The game aims to raise awareness about ocean pollution by immersing players in a virtual underwater world where they collect trash to prevent coral bleaching and save marine life. Despite their efforts, players inevitably face game over, highlighting the futility of merely collecting trash and underscoring the need to prevent waste from entering oceans. The game uses various diegetic feedback mechanisms and enhanced user presence features to deepen emotional engagement and promote pro-environmental behavior.",Serious Game; Virtual Reality (VR); Coral Bleaching; Diegetic Feedback; Environmental Awareness; Environmental Education; Marine Conservation; Ocean Pollution; Pro-Environmental Behavior,Title_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,GazeLock: Gaze- and Lock Pattern-Based Authentication,VRST - Virtual Reality Software and Technology,A,"Password entry is common authentication approach in Extended Reality (XR) applications for its simplicity and familiarity, but it faces challenges in public and dynamic environments due to its cumbersome nature and susceptibility to observation attacks. Manual password input can be disruptive and prone to theft through shoulder surfing or surveillance. While alternative knowledge-based approaches exist, they often require complex physical gestures and are impractical for frequent public use. We present GazeLock, an eye-tracking and lock pattern-based authentication method. This method aims to provide an easy-to-learn and efficient alternative by leveraging familiar lock patterns operated through gaze. It ensures resilience to external observation, as physical interaction is unnecessary and eyes are obscured by the headset. Its hands-free, discreet nature makes it suitable for secure public use. We demonstrate this method by simulating the unlocking of a smart lock via an XR headset, showcasing its potential applications and benefits in real-world scenarios.",Eye Tracking; Extended Reality (XR); Authentication; Gaze-based Interaction,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Us Xtended - Tracking and Sensing through Embedded and Embodied Design in Virtual Reality,VRST - Virtual Reality Software and Technology,A,"This short paper presents an embodied and embedded design method via biometric data tracking on the example of the virtual reality prototype Us Xtended. Users are taken through different immersive worlds and their task is to manipulate the environments via a certain type of physiological interaction (i.e. heart rate, gaze, voice, cognitive load). By employing biofeedback, the system tailors the immersive environment via audiovisual and haptic stimuli to user’s psycho-physiological responses and reflects them on its scale which is part of the virtual environment. By recording their voice, users can self-assess their own affects. In the finale, users stand in a pastiche-like world filled with different artifacts of psycho-physiological evaluations they co-created with the biofeedback system throughout their journey.",virtual reality; biometrics; affect; embedded and embodied design; psycho-physiology; self-quantification,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Make America Great Again and Again: How to Adapt Interactive Installation Art for Virtual Reality,VRST - Virtual Reality Software and Technology,A,"Make America Great Again and Again features a large, fluttering American flag accompanied by the Star-Spangled Banner, with sixty small screens displaying one-minute video clips in sequence. This one-hour loop continues until participants upload their own videos, transforming the flag into a collage of visitor selfies. By providing a public sphere for local visitors, this interactive art project encourages them to share their opinions on this controversial issue. To capture global perspectives on the topic, the project was adapted into a virtual reality environment using the metaverse platform Styly. This paper outlines the process of converting the installation into virtual reality artwork.",Installation art; Interactive art; VR conversion,Title_Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Real-Time Scent Prediction and Release for Video Games,VRST - Virtual Reality Software and Technology,A,"This demo explores the use of computer vision technologies for the integration of scent in video games and interactive applications. We present an extendable system that is domain-independent and allows for customization and debugging based on the targeted game. Using Minecraft as a case study, we optimized the system configuration and evaluated its performance. Our aim is to advance the exploration of scent integration in gaming and inspire future designs for olfactory experiences.",Virtual Reality; Computer Vision; Scent Integration,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,ICELab Demo: an industrial digital-twin and simulator in VR,VRST - Virtual Reality Software and Technology,A,"In this demo we present an application featuring the integration of Virtual Reality (VR) technologies with the demonstration laboratory (ICELab) built around Industry 4.0/5.0 concepts. In particular, we showcase a digital twin of the real laboratory that allows the user to explore its environment in VR and interact with the different machinery to obtain several data and information.",Digital Twin; Computer Graphics; Cyber-Physical Factory,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,"Travel Speed, Spatial Awareness, And Implications for Egocentric Target-Selection-Based Teleportation - A Replication Design",VRST - Virtual Reality Software and Technology,A,"Virtual travel in Virtual Reality experiences is common, offering users the ability to explore expansive virtual spaces. Various interfaces exist for virtual travel, with speed playing a crucial role in user experience and spatial awareness. Teleportation-based interfaces provide instantaneous transitions, whereas continuous and semi-continuous methods vary in speed and control. Prior research by Bowman et al. highlighted the impact of travel speed on spatial awareness demonstrating that instantaneous travel can lead to user disorientation. However, additional cues, such as visual target selection, can aid in reorientation. This study replicates and extends Bowman’s experiment, investigating the influence of travel speed and visual target cues on spatial orientation.",,Abstract,TRUE,Duplicate
Scopus,conferencePaper,2024,Walking &gt; Walking-in-Place &gt; Flying/Steering &gt; Teleportation? Designing Locomotion Research for Replication and Extension,VRST - Virtual Reality Software and Technology,A,"In this abstract, we discuss the demand for replication and extension efforts related to two seminal studies focused on virtual reality (VR) locomotion interfaces, initially centered around a VR implementation of the Visual Cliff, often referred to as Virtual Pit. The original experiments by Slater et al. (1995) and Usoh et al. (1999) compared different locomotion methods, including Real Walking, Walking-in-Place, and Flying/Steering, with a focus on presence and ease of use. We discuss the importance of these studies for the field, motivate replication efforts focused on these studies, discuss potential confounding factors, and present considerations for a concerted effort to reproduce the findings with state-of-the-art VR systems and measures, extensions to locomotion methods like Teleportation, and means to support future replications and extensions.",presence; Virtual reality; user study; locomotion; walking; replication; teleportation; steering,Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Fade-to-Black Duration in Egocentric Target-Selection-Based Teleport - A Replication Design,VRST - Virtual Reality Software and Technology,A,"Fade-to-black animations are a commonly used technique to visualize transitions during teleportation. However, their duration varies across different implementations and has not been extensively researched. This abstract details a study design to understand how the level of environmental detail affects the preferred duration of fade-to-black animations. We propose a within-subject study, comparing participants’ preferred duration across three virtual environments with varying levels of detail. We discuss improvements to the task design of an existing study. Other than the level of environmental detail, we motivate research into the effects of different tasks (i.e. hurried or calm) on the preferred duration.",Virtual reality; user study; locomotion; transitions; replication; teleportation,Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Generative Multi-Modal Artificial Intelligence for Dynamic Real-Time Context-Aware Content Creation in Augmented Reality,VRST - Virtual Reality Software and Technology,A,"We introduce a framework that uses generative Artificial Intelligence (AI) for dynamic and context-aware content creation in Augmented Reality (AR). By integrating Vision Language Models (VLMs), our system detects and understands the physical space around the user, recommending contextually relevant objects. These objects are transformed into 3D models using a text-to-3D generative AI techniques, allowing for real-time content inclusion within the AR space. This approach enhances user experience by enabling intuitive customization through spoken commands, while reducing costs and improving accessibility to advanced AR interactions. The framework’s vision and language capabilities support the generation of comprehensive and context-specific 3D objects.",Augmented reality; generative AI; 3D object generation; vision language models,Title_Abstract_Keywords,TRUE,Duplicate
Scopus,conferencePaper,2024,Usable Authentication in Virtual Reality: Exploring the Usability of PINs and Gestures,ACNS - International Conference on Applied Cryptography and Network Security,B,"Virtual Reality (VR) is becoming increasingly popular with its ability to offer new forms of interaction, user interface, and immersion not only for recreation but also for work, therapy, arts, or education. These new spaces need to be safeguarded by authentication similar to conventional IT systems. However, porting conventional interfaces to VR has often been found to be less than optimal as it fails to fully embrace the technology’s potential and potentially disrupt the immersive experience. This paper evaluates and compares the usability of two major authentication methods for VR: 2D Personal Identification Number (PIN) and gesture-based authentication - with 40 participants. While prior research has shown promising results in authentication security, there is a lack of studies specifically on usability in VR. Our findings indicate that the type of authentication and the user’s experience level affect usability, with gesture-based authentication having a higher usability score than a PIN and having faster authentication times. Hereby, users with less VR experience profited the most from a natural interaction mode for VR. The results suggest that developers should rather choose a native interaction mode in VR than try to port a familiar conventional interaction such as number pads for PINs. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.",Authentication; Gestures; PINs; Usability; Virtual Reality,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2024,Leveraging Overshadowing for Time-Delay Attacks in 4G/5G Cellular Networks: An Empirical Assessment,"ARES - International Conference on Availability, Reliability and Security",B,"Ensuring both reliable and low-latency communications over 4G or 5G Radio Access Network (RAN) is a key feature for services such as smart power grids and the metaverse. However, the lack of appropriate security mechanisms at the lower-layer protocols of the RAN–a heritage from 4G networks–opens up vulnerabilities that can be exploited to conduct stealthy Reduction-of-Quality attacks against the latency guarantees. This paper presents an empirical assessment of a proposed time-delay attack that leverages overshadowing to exploit the reliability mechanisms of the Radio Link Control (RLC) in Acknowledged Mode. By injecting falsified RLC Negative Acknowledgements, an attacker can maliciously trigger retransmissions at the victim User Equipment (UE), degrading the uplink latency of application flows. Extensive experimental evaluations on open-source and commercial off-the-shelf UEs demonstrate the attack’s effectiveness in increasing latency, network load, and buffer occupancy. The attack impact is quantified by varying the bitrate representing different applications and the number of injected negative acknowledgments controlling the attack intensity. This work studies a realistic threat against the latency quality of service in 4G/5G RANs and highlights the urgent need to revisit protocol security at the lower-RAN layers for 5G (and beyond) networks.",Deny of Service; Latency; Man on the Side; Overshadowing; Radio Access Network; Reduction of Quality; Time-delay,Abstract,TRUE,
Scopus,conferencePaper,2024,A Domain-Specific Language for Augmented Reality Games,SAC - Selected Areas in Cryptography,B,"Augmented Reality (AR) applications have become popular over the last few years, with significant impact on video games. AR does not require advanced technology, but a mobile device with a camera is enough. However, building AR games is time-consuming and requires deep expertise in the tools, technologies and programming languages of the field, as well as on mathematical concepts related to the graphics and physics of the virtual objects. We attack this problem by means of a Domain-Specific Language (DSL) named argDSL, tailored to create AR games. It offers primitives to customise the domain and logic of the game, the physics of the virtual objects, and their graphical representation. We provide an Eclipse environment enabling the definition of AR games using the DSL, and an iOS client able to run the defined games.",augmented reality; domain-specific languages; games,Title_Abstract_Keywords,TRUE,
Scopus,conferencePaper,2024,High Energy Efficiency Mobile AR Applications under Adaptive Object Detection Engine and Self-learning Governor,SAC - Selected Areas in Cryptography,B,"Augmented reality (AR) applications aim to enhance user interactions by integrating virtual and real worlds, leveraging deep learning. However, implementing AR on mobile devices is challenging due to the high computational demands of deep learning, causing substantial energy drain on batteries. Server-assisted mobile augmented reality (MAR) systems address this by offloading computation, but network delays lead to worst object detection accuracy, degrading the user experience. We propose an adaptive object detection engine and self-learning governor framework. The engine maintains accuracy across network conditions and scenes, while the governor optimizes energy efficiency via self-learning models, dynamic voltage and frequency scaling (DVFS), and thread management. Our framework implemented on real platforms, significantly reduces energy consumption compared to state-of-the-art approaches.",energy efficiency; mobile augmented reality; mobile computing; object detection; scheduling,Abstract_Keywords,TRUE,
Scopus,conferencePaper,2024,An Adaptive Transmission Strategy for Tiled 360-Degree VR Videos in NOMA Systems,SAC - Selected Areas in Cryptography,B,"Recently, virtual reality (VR) has become popular in many fields. Users can view a 360-degree video in all directions and have an immersive experience with a VR device. However, the transmission of a 360-degree video, which is compared to a traditional video, is a significant burden on wireless networks. In other words, users may suffer poor quality of experience (QoE) under constrained power. In order to improve transmission efficiency and ensure the QoE of users, in this paper, we propose an adaptive transmission strategy (ATS) for resource allocation in 360-degree videos to multiple users using a non-orthogonal multiple access (NOMA) scheme. In ATS, the delivery mode (multicast or unicast) of tiles is based on the users' field of views (FoVs) and their channel conditions, and the quality of tiles is dynamically determined by their priorities. The simulation results show that ATS significantly improves the QoE of users under constrained power.",360-degree video; multicast; NOMA; unicast; virtual reality,Abstract_Keywords,TRUE,
