Database,Item Type,Publication Year,Title,Venue,Venue Rank,Abstract,Keywords,Found In,Duplicate
ACM DL,conferencePaper,2014,World-Driven Access Control for Continuous Sensing,CCS - ACM Conference on Computer and Communications Security,A*,"Modern applications increasingly rely on continuous monitoring of video, audio, or other sensor data to provide their functionality, particularly in platforms such as the Microsoft Kinect and Google Glass. Continuous sensing by untrusted applications poses significant privacy challenges for both device users and bystanders. Even honest users will struggle to manage application permissions using existing approaches.We propose a general, extensible framework for controlling access to sensor data on multi-application continuous sensing platforms. Our approach, world-driven access control, allows real-world objects to explicitly specify access policies. This approach relieves the user's permission management burden while mediating access at the granularity of objects rather than full sensor streams. A trusted policy module on the platform senses policies in the world and modifies applications' ""views"" accordingly. For example, world-driven access control allows the system to automatically stop recording in bathrooms or remove bystanders from video frames,without the user prompted to specify or activate such policies. To convey and authenticate policies, we introduce passports, a new kind of certificate that includes both a policy and optionally the code for recognizing a real-world object.We implement a prototype system and use it to study the feasibility of world-driven access control in practice. Our evaluation suggests that world-driven access control can effectively reduce the user's permission management burden in emerging continuous sensing systems. Our investigation also surfaces key challenges for future access control mechanisms for continuous sensing applications.",wearable; augmented reality; access control; permissions; continuous sensing,Keywords,
IEEE,conferencePaper,2015,SurroundWeb: Mitigating Privacy Concerns in a 3D Web Browser,SP - IEEE Symposium on Security and Privacy,A*,"Immersive experiences that mix digital and real-world objects are becoming reality, but they raise serious privacy concerns as they require real-time sensor input. These experiences are already present on smartphones and game consoles via Kinect, and will eventually emerge on the web platform. However, browsers do not expose the display interfaces needed to render immersive experiences. Previous security research focuses on controlling application access to sensor input alone, and do not deal with display interfaces. Recent research in human computer interactions has explored a variety of high-level rendering interfaces for immersive experiences, but these interfaces reveal sensitive data to the application. Bringing immersive experiences to the web requires a high-level interface that mitigates privacy concerns. This paper presents Surround Web, the first 3D web browser, which provides the novel functionality of rendering web content onto a room while tackling many of the inherent privacy challenges. Following the principle of least privilege, we propose three abstractions for immersive rendering: 1) the room skeleton lets applications place content in response to the physical dimensions and locations of render able surfaces in a room, 2) the detection sandbox lets applications declaratively place content near recognized objects in the room without revealing if the object is present, and 3) satellite screens let applications display content across devices registered with Surround Web. Through user surveys, we validate that these abstractions limit the amount of revealed information to an acceptable degree. In addition, we show that a wide range of immersive experiences can be implemented with acceptable performance.",augmented reality;JavaScript;web browser;projection mapping,Keywords,
IEEE,conferencePaper,2016,"Prepose: Privacy, Security, and Reliability for Gesture-Based Programming",SP - IEEE Symposium on Security and Privacy,A*,"With the rise of sensors such as the Microsoft Kinect, Leap Motion, and hand motion sensors in phones (i.e., Samsung Galaxy S6), gesture-based interfaces have become practical. Unfortunately, today, to recognize such gestures, applications must have access to depth and video of the user, exposing sensitive data about the user and her environment. Besides these privacy concerns, there are also security threats in sensor-based applications, such as multiple applications registering the same gesture, leading to a conflict (akin to Clickjacking on the web). We address these security and privacy threats with Prepose, a novel domain-specific language (DSL) for easily building gesture recognizers, combined with a system architecture that protects privacy, security, and reliability with untrusted applications. We run Prepose code in a trusted core, and only return specific gesture events to applications. Prepose is specifically designed to enable precise and sound static analysis using SMT solvers, allowing the system to check security and reliability properties before running a gesture recognizer. We demonstrate that Prepose is expressive by creating gestures in three representative domains: physical therapy, tai-chi, and ballet. We further show that runtime gesture matching in Prepose is fast, creating no noticeable lag, as measured on traces from Microsoft Kinect runs. To show that gesture checking at the time of submission to a gesture store is fast, we developed a total of four Z3-based static analyses to test for basic gesture safety and internal validity, to make sure the so-called protected gestures are not overridden, and to check inter-gesture conflicts. Our static analysis scales well in practice: safety checking is under 0.5 seconds per gesture, average validity checking time is only 188ms, lastly, for 97% of the cases, the conflict detection time is below 5 seconds, with only one query taking longer than 15 seconds.",augmented reality;domain-specific language;kinect;security;privacy,Keywords,
IEEE,conferencePaper,2017,Securing Augmented Reality Output,SP - IEEE Symposium on Security and Privacy,A*,"Augmented reality (AR) technologies, such as Microsoft's HoloLens head-mounted display and AR-enabled car windshields, are rapidly emerging. AR applications provide users with immersive virtual experiences by capturing input from a user's surroundings and overlaying virtual output on the user's perception of the real world. These applications enable users to interact with and perceive virtual content in fundamentally new ways. However, the immersive nature of AR applications raises serious security and privacy concerns. Prior work has focused primarily on input privacy risks stemming from applications with unrestricted access to sensor data. However, the risks associated with malicious or buggy AR output remain largely unexplored. For example, an AR windshield application could intentionally or accidentally obscure oncoming vehicles or safety-critical output of other AR applications. In this work, we address the fundamental challenge of securing AR output in the face of malicious or buggy applications. We design, prototype, and evaluate Arya, an AR platform that controls application output according to policies specified in a constrained yet expressive policy framework. In doing so, we identify and overcome numerous challenges in securing AR output.",security;augmented reality,Abstract_Keywords_Title,
IEEE,conferencePaper,2018,Towards Security and Privacy for Multi-user Augmented Reality: Foundations with End Users,SP - IEEE Symposium on Security and Privacy,A*,"Immersive augmented reality (AR) technologies are becoming a reality. Prior works have identified security and privacy risks raised by these technologies, primarily considering individual users or AR devices. However, we make two key observations: (1) users will not always use AR in isolation, but also in ecosystems of other users, and (2) since immersive AR devices have only recently become available, the risks of AR have been largely hypothetical to date. To provide a foundation for understanding and addressing the security and privacy challenges of emerging AR technologies, grounded in the experiences of real users, we conduct a qualitative lab study with an immersive AR headset, the Microsoft HoloLens. We conduct our study in pairs - 22 participants across 11 pairs - wherein participants engage in paired and individual (but physically co-located) HoloLens activities. Through semi-structured interviews, we explore participants' security, privacy, and other concerns, raising key findings. For example, we find that despite the HoloLens's limitations, participants were easily immersed, treating virtual objects as real (e.g., stepping around them for fear of tripping). We also uncover numerous security, privacy, and safety concerns unique to AR (e.g., deceptive virtual objects misleading users about the real world), and a need for access control among users to manage shared physical spaces and virtual content embedded in those spaces. Our findings give us the opportunity to identify broader lessons and key challenges to inform the design of emerging single-and multi-user AR technologies.",augmented reality;multi user interaction;privacy;security;user studies,Abstract_Keywords_Title,
IEEE,conferencePaper,2022,SoK: Authentication in Augmented and Virtual Reality,SP - IEEE Symposium on Security and Privacy,A*,"Augmented reality (AR) and virtual reality (VR) devices are emerging as prominent contenders to todayâ€™s personal computers. As personal devices, users will use AR and VR to store and access their sensitive data and thus will need secure and usable ways to authenticate. In this paper, we evaluate the state-of-the-art of authentication mechanisms for AR/VR devices by systematizing research efforts and practical deployments. By studying usersâ€™ experiences with authentication on AR and VR, we gain insight into the important properties needed for authentication on these devices. We then use these properties to perform a comprehensive evaluation of AR/VR authentication mechanisms both proposed in literature and used in practice. In all, we synthesize a coherent picture of the current state of authentication mechanisms for AR/VR devices. We draw on our findings to provide concrete research directions and advice on implementing and evaluating future authentication methods.",,Abstract_Title,
IEEE,conferencePaper,2023,Privacy Leakage via Unrestricted Motion-Position Sensors in the Age of Virtual Reality: A Study of Snooping Typed Input on Virtual Keyboards,SP - IEEE Symposium on Security and Privacy,A*,"Virtual Reality (VR) has gained popularity in numerous fields, including gaming, social interactions, shopping, and education. In this paper, we conduct a comprehensive study to assess the trustworthiness of the embedded sensors on VR, which embed various forms of sensitive data that may put usersâ€™ privacy at risk. We find that accessing most on-board sensors (e.g., motion, position, and button sensors) on VR SDKs/APIs, such as OpenVR, Oculus Platform, and WebXR, requires no security permission, exposing a huge attack surface for an adversary to steal the userâ€™s privacy. We validate this vulnerability through developing malware programs and malicious websites and specifically explore to what extent it exposes the userâ€™s information in the context of keystroke snooping. To examine its actual threat in practice, the adversary in the considered attack model doesnâ€™t possess any labeled data from the user nor knowledge about the userâ€™s VR settings. Extensive experiments, involving two mainstream VR systems and four keyboards with different typing mechanisms, demonstrate that our proof-of-concept attack can recognize the userâ€™s virtual typing with over 89.7% accuracy. The attack can recover the userâ€™s passwords with up to 84.9% recognition accuracy if three attempts are allowed and achieve an average of 87.1% word recognition rate for paragraph inference. We hope this study will help the community gain awareness of the vulnerability in the sensor management of current VR systems and provide insights to facilitate the future design of more comprehensive and restricted sensor access control mechanisms.",keystroke-inference;virtual-reality;cybersecurity,Abstract_Title,
IEEE,conferencePaper,2023,Low-effort VR Headset User Authentication Using Head-reverberated Sounds with Replay Resistance,SP - IEEE Symposium on Security and Privacy,A*,"While Virtual Reality (VR) applications are becoming increasingly common, efficiently verifying a VR device user before granting personal access is still a challenge. Existing VR authentication methods require users to enter PINs or draw graphical passwords using controllers. Though the entry is in the virtual space, it can be observed by others in proximity and is subject to critical security issues. Furthermore, the in-air hand movements or handheld controller-based authentications require active user participation and are not time-efficient. This work proposes a low-effort VR device authentication system based on the unique skull-reverberated sounds, which can be acquired when the user wears the VR device. Specifically, when the user puts on the VR device or is wearing it to log into an online account, the proposed system actively emits an ultrasonic signal to initiate the authentication session. The signal returning to the VR deviceâ€™s microphone has been reverberated by the userâ€™s head, which is unique in size, skull shape and mass. We thus extract head biometric information from the received signal for unobtrusive VR device authentication.Though active acoustic sensing has been broadly used on mobile devices, no prior work has ever successfully applied such techniques to commodity VR devices. Because VR devices are designed to provide users with virtual reality immersion, the echo sounds used for active sensing are unwanted and severely suppressed. The raw audio before this process is also not accessible without kernel/hardware modifications. Thus, our work further solves the challenge of active acoustic sensing under echo cancellation to enable deploying our system on off-the-shelf VR devices. Additionally, we show that the echo cancellation mechanism is naturally good to prevent acoustic replay attacks. The proposed system is developed based on an autoencoder and a convolutional neural network for biometric data extraction and recognition. Experiments with a standalone and a mobile phone VR headset show that our system efficiently verifies a user and is also replay-resistant.",Virtual Reality;Authentication;Biometric,Abstract_Keywords,
Web of Science,conferencePaper,2020,OcuLock: Exploring Human Visual System for Authentication in Virtual Reality Head-mounted Display,NDSS - Usenix Network and Distributed System Security Symposium,A*,"The increasing popularity of virtual reality (VR) in a wide spectrum of applications has generated sensitive personal data such as medical records and credit card information. While protecting such data from unauthorized access is critical, directly applying traditional authentication methods (e.g., PIN) through new VR input modalities such as remote controllers and head navigation would cause security issues. The authentication action can be purposefully observed by attackers to infer the authentication input. Unlike any other mobile devices, VR presents immersive experience via a head-mounted display (HMD) that fully covers users' eye area without public exposure. Leveraging this feature, we explore human visual system (HVS) as a novel biometric authentication tailored for VR platforms. While previous works used eye globe movement (gaze) to authenticate smartphones or PCs, they suffer from a high error rate and low stability since eye gaze is highly dependent on cognitive states. In this paper, we explore the HVS as a whole to consider not just the eye globe movement but also the eyelid, extraocular muscles, cells, and surrounding nerves in the HVS. Exploring HVS biostructure and unique HVS features triggered by immersive VR content can enhance authentication stability. To this end, we present OcuLock, an HVS-based system for reliable and unobservable VR HMD authentication. OcuLock is empowered by an electrooculography (EOG) based HVS sensing framework and a record-comparison driven authentication scheme. Experiments through 70 subjects show that OcuLock is resistant against common types of attacks such as impersonation attack and statistical attack with Equal Error Rates as low as 3.55% and 4.97% respectively. More importantly, OcuLock maintains a stable performance over a 2-month period and is preferred by users when compared to other potential approaches.",,Abstract_Title,
Web of Science,conferencePaper,2023,SoundLock: A Novel User Authentication Scheme for VR Devices Using Auditory-Pupillary Response,NDSS - Usenix Network and Distributed System Security Symposium,A*,"Virtual Reality (VR) has shown promising potential in many applications, such as e-business, healthcare, and social networking. Rich information regarding users’ activities and online accounts is stored in VR devices. If they are carelessly unattended, adversarial access will cause data breaches and other critical consequences. Practical user authentication schemes for VR devices are in dire need. Current solutions, including passwords, digital PINs, and pattern locks, mostly follow conventional approaches for general personal devices. They have been criticized for deficits in both security and usability. In this work, we propose SoundLock, a novel user authentication scheme for VR devices using auditory-pupillary response as biometrics. During authentication, auditory stimuli are presented to the user via the VR headset. The corresponding pupillary response is captured by the integrated eye tracker. User’s legitimacy is then determined by comparing the response with the template generated during the enrollment stage. To strike a balance between security and usability in the scheme design, an optimization problem is formulated. Due to its nonlinearity, a two-stage heuristic algorithm is proposed to solve it efficiently. The solution provides necessary guidance for selecting effective auditory stimuli and determining their corresponding lengths. We demonstrate through extensive in-field experiments that SoundLock outperforms state-of-the-art biometric solutions with FAR (FRR) as low as 0.76%(0.91%) and is well received among participants in the user study.",,Abstract,
Web of Science,conferencePaper,2022,Lumos: Identifying and Localizing Diverse Hidden IoT Devices in an Unfamiliar Environment,USS - Usenix Security Symposium,A*,"Hidden IoT devices are increasingly being used to snoop on users in hotel rooms or AirBnBs. We envision empowering users entering such unfamiliar environments to identify and locate (e.g., hidden camera behind plants) diverse hidden devices (e.g., cameras, microphones, speakers) using only their personal handhelds. What makes this challenging is the limited network visibility and physical access that a user has in such unfamiliar environments, coupled with the lack of specialized equipment. This paper presents Lumos, a system that runs on commodity user devices (e.g., phone, laptop) and enables users to identify and locate WiFi-connected hidden IoT devices and visualize their presence using an augmented reality interface. Lumos addresses key challenges in: (1) identifying diverse devices using only coarse-grained wireless layer features, without IP/DNS layer information and without knowledge of the WiFi channel assignments of the hidden devices; and (2) locating the identified IoT devices with respect to the user using only phone sensors and wireless signal strength measurements. We evaluated Lumos across 44 different IoT devices spanning various types, models, and brands across six different environments. Our results show that Lumos can identify hidden devices with 95% accuracy and locate them with a median error of 1.5m within 30 minutes in a two-bedroom, 1000 sq. ft. apartment.",,Abstract,
Web of Science,conferencePaper,2022,OVRSEEN: Auditing Network Traffic and Privacy Policies in Oculus VR,USS - Usenix Security Symposium,A*,"Virtual reality (VR) is an emerging technology that enables new applications but also introduces privacy risks. In this paper, we focus on Oculus VR (OVR), the leading platform in the VR space and we provide the first comprehensive analysis of personal data exposed by OVR apps and the platform itself, from a combined networking and privacy policy perspective. We experimented with the Quest 2 headset and tested the most popular VR apps available on the official Oculus and the SideQuest app stores. We developed OVRSEEN, a methodology and system for collecting, analyzing, and comparing network traffic and privacy policies on OVR. On the networking side, we captured and decrypted network traffic of VR apps, which was previously not possible on OVR, and we extracted data flows, defined as < app, data type, destination >. Compared to the mobile and other app ecosystems, we found OVR to be more centralized and driven by tracking and analytics, rather than by third-party advertising. We show that the data types exposed by VR apps include personally identifiable information (PII), device information that can be used for fingerprinting, and VR-specific data types. By comparing the data flows found in the network traffic with statements made in the apps' privacy policies, we found that approximately 70% of OVR data flows were not properly disclosed. Furthermore, we extracted additional context from the privacy policies, and we observed that 69% of the data flows were used for purposes unrelated to the core functionality of apps.",,Abstract,
Web of Science,conferencePaper,2021,Kal epsilon ido: Real-Time Privacy Control for Eye-Tracking Systems,USS - Usenix Security Symposium,A*,"Recent advances in sensing and computing technologies have led to the rise of eye-tracking platforms. Ranging from mobiles to high-end mixed reality headsets, a wide spectrum of interactive systems now employs eye-tracking. However, eye gaze data is a rich source of sensitive information that can reveal an individual's physiological and psychological traits. Prior approaches to protecting eye-tracking data suffer from two major drawbacks: they are either incompatible with the current eye-tracking ecosystem or provide no formal privacy guarantee. In this paper, we propose Kal epsilon ido, an eyetracking data processing system that (1) provides a formal privacy guarantee, (2) integrates seamlessly with existing eyetracking ecosystems, and (3) operates in real-time. Kal epsilon ido acts as an intermediary protection layer in the software stack of eye-tracking systems. We conduct a comprehensive user study and trace-based analysis to evaluate Kal epsilon ido. Our user study shows that the users enjoy a satisfactory level of utility from Kal epsilon ido. Additionally, we present empirical evidence of Kal epsilon ido's effectiveness in thwarting real-world attacks on eye-tracking data.",,Abstract,
Web of Science,conferencePaper,2021,AdCube: WebVR Ad Fraud and Practical Confinement of Third-Party Ads,USS - Usenix Security Symposium,A*,"Web technology has evolved to offer 360-degree immersive browsing experiences. This new technology, called WebVR, enables virtual reality by rendering a three-dimensional world on an HTML canvas. Unfortunately, there exists no browser-supported way of sharing this canvas between different parties. Assuming an abusive ad service provider who exploits this absence, we present four new ad fraud attack methods. Our user study demonstrates that the success rates of our attacks range from 88.23% to 100%, confirming their effectiveness. To mitigate the presented threats, we propose AdCube, which allows publishers to specify the behaviors of third-party ad code and enforce this specification. We show that AdCube is able to block the presented threats with a small page loading latency of 236 msec and a negligible frame-per-second (FPS) drop for nine WebVR official demo sites.",,Abstract,
Web of Science,conferencePaper,2019,Secure Multi-User Content Sharing for Augmented Reality Applications,USS - Usenix Security Symposium,A*,"Augmented reality (AR), which overlays virtual content on top of the user's perception of the real world, has now begun to enter the consumer market. Besides smartphone platforms, early-stage head-mounted displays such as the Microsoft HoloLens are under active development. Many compelling uses of these technologies are multi-user: e.g., inperson collaborative tools, multiplayer gaming, and telepresence. While prior work on AR security and privacy has studied potential risks from AR applications, new risks will also arise among multiple human users. In this work, we explore the challenges that arise in designing secure and private content sharing for multi-user AR. We analyze representative application case studies and systematize design goals for security and functionality that a multi-user AR platform should support. We design an AR content sharing control module that achieves these goals and build a prototype implementation (ShareAR) for the HoloLens. This work builds foundations for secure and private multi-user AR interactions.",,Abstract_Title,
Web of Science,conferencePaper,2016,Virtual U: Defeating Face Liveness Detection by Building Virtual Models From Your Public Photos,USS - Usenix Security Symposium,A*,"In this paper, we introduce a novel approach to bypass modern face authentication systems. More specifically, by leveraging a handful of pictures of the target user taken from social media, we show how to create realistic, textured, 3D facial models that undermine the security of widely used face authentication solutions. Our framework makes use of virtual reality (VR) systems, incorporating along the way the ability to perform animations (e.g., raising an eyebrow or smiling) of the facial model, in order to trick liveness detectors into believing that the 3D model is a real human face. The synthetic face of the user is displayed on the screen of the VR device, and as the device rotates and translates in the real world, the 3D face moves accordingly. To an observing face authentication system, the depth and motion cues of the display match what would be expected for a human face. We argue that such VR-based spoofing attacks constitute a fundamentally new class of attacks that point to a serious weaknesses in camera-based authentication systems: Unless they incorporate other sources of verifiable data, systems relying on color image data and camera motion are prone to attacks via virtual realism. To demonstrate the practical nature of this threat, we conduct thorough experiments using an end-to-end implementation of our approach and show how it undermines the security of several face authentication solutions that include both motion-based and liveness detectors.",,Abstract,
Web of Science,conferencePaper,2013,Enabling Fine-Grained Permissions for Augmented Reality Applications with Recognizers,USS - Usenix Security Symposium,A*,"Augmented reality (AR) applications sense the environment, then render virtual objects on human senses. Examples include smartphone applications that annotate storefronts with reviews and XBox Kinect games that show"" avatars"" mimicking human movements. No current OS has special support for such applications. As a result, permissions for AR applications are necessarily coarse-grained: applications must ask for access to raw sensor feeds, such as video and audio. These raw feeds expose significant additional information beyond what applications need, including sensitive information such as the user’s location, face, or surroundings.",,Abstract_Title,
Web of Science,conferencePaper,2023,Exploring User Reactions and Mental Models Towards Perceptual Manipulation Attacks in Mixed Reality,USS - Usenix Security Symposium,A*,"Perceptual Manipulation Attacks (PMA) involve manipulating users’ multi-sensory (e.g., visual, auditory, haptic) perceptions of the world through Mixed Reality (MR) content, in order to influence users’ judgments and following actions. For example, a MR driving application that is expected to show safety-critical output might also (maliciously or unintentionally) overlay the wrong signal on a traffic sign, misleading the user into slamming on the brake. While current MR technology is sufficient to create such attacks, little research has been done to understand how users perceive, react to, and defend against such potential manipulations. To provide a foundation for understanding and addressing PMA in MR, we conducted an in-person study with 21 participants. We developed three PMA in which we focused on attacking three different perceptions: visual, auditory, and situational awareness. Our study first investigates how user reactions are affected by evaluating their performance on “microbenchmark” tasks under benchmark and different attack conditions. We observe both primary and secondary impacts from attacks, later impacting participants’ performance even under non-attack conditions. We follow up with interviews, surfacing a range of user reactions and interpretations of PMA. Through qualitative data analysis of our observations and interviews, we identify various defensive strategies participants developed, and we observe how these strategies sometimes backfire. We derive recommendations for future investigation and defensive directions based on our findings.",,Abstract_Title,
Web of Science,conferencePaper,2023,LocIn: Inferring Semantic Location from Spatial Maps in Mixed Reality,USS - Usenix Security Symposium,A*,"Mixed reality (MR) devices capture 3D spatial maps of users' surroundings to integrate virtual content into their physical environment. Existing permission models implemented in popular MR platforms allow all MR apps to access these 3D spatial maps without explicit permission. Unmonitored access of MR apps to these 3D spatial maps poses serious privacy threats to users as these maps capture detailed geometric and semantic characteristics of users' environments. In this paper, we present LocIn, a new location inference attack that exploits these detailed characteristics embedded in 3D spatial maps to infer a user's indoor location type. LocIn develops a multi-task approach to train an end-to-end encoder-decoder network that extracts a spatial feature representation for capturing contextual patterns of the user's environment. LocIn leverages this representation to detect 3D objects and surfaces and integrates them into a classification network with a novel unified optimization function to predict the user's indoor location. We demonstrate LocIn attack on spatial maps collected from three popular MR devices. We show that LocIn infers a user's location type with an average 84.1% accuracy.",,Abstract_Title,
Web of Science,conferencePaper,2023,"Hidden Reality: Caution, Your Hand Gesture Inputs in the Immersive Virtual World are Visible to All!",USS - Usenix Security Symposium,A*,"Text entry is an inevitable task while using Virtual Reality (VR) devices in a wide range of applications such as remote learning, gaming, and virtual meeting. VR users enter passwords/pins to log in to their user accounts in various applications and type regular text to compose emails or browse the internet. The typing activity on VR devices is believed to be resistant to direct observation attacks as the virtual screen in an immersive environment is not directly visible to others present in physical proximity. This paper presents a video-based side-channel attack, Hidden Reality (HR), that shows – although the virtual screen in VR devices is not in direct sight of adversaries, the indirect observations might get exploited to steal the user’s private information. The Hidden Reality (HR) attack utilizes video clips of the user’s hand gestures while they type on the virtual screen to decipher the typed text in various key entry scenarios on VR devices including typed pins and passwords. Experimental analysis performed on a large corpus of 368 video clips show that the Hidden Reality model can successfully decipher an average of over 75% of the text inputs. The high success rate of our attack model led us to conduct a user study to understand the user’s behavior and perception of security in virtual reality. The analysis showed that over 95% of users were not aware of any security threats on VR devices and believed the immersive environments to be secure from digital attacks. Our attack model challenges users’ false sense of security in immersive environments and emphasizes the need for more stringent security solutions in VR space.",,Abstract,
Web of Science,conferencePaper,2023,Erebus: Access Control for Augmented Reality Systems,USS - Usenix Security Symposium,A*,"Augmented Reality (AR) is widely considered the next evolution in personal devices, enabling seamless integration of the digital world into our reality. Such integration, however, often requires unfettered access to sensor data, causing significant overprivilege for applications that run on these platforms. Through analysis of 17 AR systems and 45 popular AR applications, we explore existing mechanisms for access control in AR platforms, identify key trends in how AR applications use sensor data, and pinpoint unique threats users face in AR environments. Using these findings, we design and implement Erebus, an access control framework for AR platforms that enables fine-grained control over data used by AR applications. Erebus achieves the principle of least privileged through creation of a domain-specific language (DSL) for permission control in AR platforms, allowing applications to specify data needed for their functionality. Using this DSL, Erebus further enables users to customize app permissions to apply under specific user conditions. We implement Erebus on Google's ARCore SDK and port five existing AR applications to demonstrate Erebus capability to secure various classes of apps. Performance results using these applications and various microbenchmarks show that Erebus achieves its security goals while being practical, introducing negligible performance overhead to the AR system.",,Abstract_Title,
Web of Science,conferencePaper,2023,"Unique Identification of 50, 000+ Virtual Reality Users from Head & Hand Motion Data",USS - Usenix Security Symposium,A*,"With the recent explosive growth of interest and investment in virtual reality (VR) and the so-called ""metaverse,"" public attention has rightly shifted toward the unique security and privacy threats that these platforms may pose. While it has long been known that people reveal information about themselves via their motion, the extent to which this makes an individual globally identifiable within virtual reality has not yet been widely understood. In this study, we show that a large number of real VR users (N=55,541) can be uniquely and reliably identified across multiple sessions using just their head and hand motion relative to virtual objects. After training a classification model on 5 minutes of data per person, a user can be uniquely identified amongst the entire pool of 50,000+ with 94.33% accuracy from 100 seconds of motion, and with 73.20% accuracy from just 10 seconds of motion. This work is the first to truly demonstrate the extent to which biomechanics may serve as a unique identifier in VR, on par with widely used biometrics such as facial or fingerprint recognition.",,Abstract_Title,
Web of Science,conferencePaper,2023,Going through the motions: AR/VR keylogging from user head motions,USS - Usenix Security Symposium,A*,"Augmented Reality/Virtual Reality (AR/VR) are the next step in the evolution of ubiquitous computing after personal computers to mobile devices. Applications of AR/VR continue to grow, including education and virtual workspaces, increasing opportunities for users to enter private text, such as passwords or sensitive corporate information. In this work, we show that there is a serious security risk of typed text in the foreground being inferred by a background application, without requiring any special permissions. The key insight is that a user’s head moves in subtle ways as she types on a virtual keyboard, and these motion signals are sufficient for inferring the text that a user types. We develop a system, TyPose, that extracts these signals and automatically infers words or characters that a victim is typing. Once the sensor signals are collected, TyPose uses machine learning to segment the motion signals in time to determine word/character boundaries, and also perform inference on the words/characters themselves. Our experimental evaluation on commercial AR/VR headsets demonstrate the feasibility of this attack, both in situations where multiple users’ data is used for training (82% top-5 word classification accuracy) or when the attack is personalized to a particular victim (92% top-5 word classification accuracy). We also show that first-line defenses of reducing the sampling rate or precision of head tracking are ineffective, suggesting that more sophisticated mitigations are needed.",,Abstract,
Web of Science,conferencePaper,2023,Is Your Wallet Snitching On You? An Analysis on the Privacy Implications of Web3,USS - Usenix Security Symposium,A*,"With the recent hype around the Metaverse and NFTs, Web3 is getting more and more popular. The goal of Web3 is to decentralize the web via decentralized applications. Wallets play a crucial role as they act as an interface between these applications and the user. Wallets such as MetaMask are being used by millions of users nowadays. Unfortunately, Web3 is often advertised as more secure and private. However, decentralized applications as well as wallets are based on traditional technologies, which are not designed with privacy of users in mind. In this paper, we analyze the privacy implications that Web3 technologies such as decentralized applications and wallets have on users. To this end, we build a framework that measures exposure of wallet information. First, we study whether information about installed wallets is being used to track users online. We analyze the top 100K websites and find evidence of 1,325 websites running scripts that probe whether users have wallets installed in their browser. Second, we measure whether decentralized applications and wallets leak the user's unique wallet address to third-parties. We intercept the traffic of 616 decentralized applications and 100 wallets and find over 2000 leaks across 211 applications and more than 300 leaks across 13 wallets. Our study shows that Web3 poses a threat to users' privacy and requires new designs towards more privacy-aware wallet architectures.",,Abstract,
Web of Science,conferencePaper,2023,It's all in your head(set): Side-channel attacks on AR/VR systems,USS - Usenix Security Symposium,A*,"With the increasing adoption of Augmented Reality/Virtual Reality (AR/VR) systems, security and privacy concerns at tract attention from both academia and industry. This paper demonstrates that AR/VR systems are vulnerable to side channel attacks launched from software; a malicious appli cation without any special permissions can infer private in formation about user interactions, other concurrent applica tions, or even the surrounding world. We develop a number of side-channel attacks targeting different types of private information. Specifically, we demonstrate three attacks on the victim’s interactions, successfully recovering hand gestures, voice commands made by victims, and keystrokes on a virtual keyboard, with accuracy exceeding 90%. We also demon strate an application fingerprinting attack where the spy is able to identify an application being launched by the victim. The final attack demonstrates that the adversary can perceive a bystander in the real-world environment and estimate the bystander’s distance with Mean Absolute Error (MAE) of 10.3 cm. We believe the threats presented by our attacks are pressing; they expand our understanding of the threat model faced by these emerging systems and inform the development of new AR/VR systems that are resistant to these threats.",,Abstract,
Web of Science,conferencePaper,2023,A Peek into the Metaverse: Detecting 3D Model Clones in Mobile Games,USS - Usenix Security Symposium,A*,"3D models are indispensable assets in metaverse in general and mobile games in particular. Yet, these 3D models can be readily extracted, duplicated, or cloned, a reality that poses a considerable threat. Although instances of games duplicating 3D models from other games have been documented, the pervasiveness of this issue remains unexplored. In this paper, we undertake the first systematic investigation of 3D model cloning within mobile games. However, multiple challenges have to be addressed for clone detection, including scalability, precision, and robustness. Our solution is 3DSCAN, an open source 3D Scanning tool for Clone Assessment and Notification. We have evaluated 3DSCAN with about 12.2 million static 3D models and 2.5 million animated 3D models extracted from 176K mobile games. With these 3D models, 3DSCAN determined that 63.03% of the static models are likely cloned ones (derived from 1,046,632 distinct models), and 37.07% animated 3D models are likely cloned ones (derived from 180,174 distinctive models). With a heuristic-based clone detection algorithm, 3DSCAN finally detected 5,238 mobile games likely containing unauthorized 3D model clones.",,Abstract_Title,
Web of Science,conferencePaper,2017,HoloPair: Securing Shared Augmented Reality Using Microsoft HoloLens,ACSAC - Annual Computer Security Applications Conference,A,"Augmented Reality (AR) devices continuously scan their environment in order to naturally overlay virtual objects onto user's view of the physical world. In contrast to Virtual Reality, where one's environment is fully replaced with a virtual one, one of AR's killer features is co-located collaboration, in which multiple users interact with the same combination of virtual and real objects. Microsoft recently released HoloLens, the first consumer-ready augmented reality headset that needs no outside markers to achieve precise inside-out spatial mapping, which allows centimeter-scale hologram positioning. However, despite many applications published on the Windows Mixed Reality platform that rely on direct communication between AR devices, there currently exists no implementation or achievable proposal for secure direct pairing of two unassociated headsets. As augmented reality gets into mainstream, this omission exposes current and future users to a range of avoidable attacks. In order to close this real-world gap in both theory and engineering practice, in this paper we design and evaluate HoloPair, a system for secure and usable pairing of two AR headsets. We propose a pairing protocol and build a working prototype to experimentally evaluate its security guarantees, usability, and system performance. By running a user study with a total of 22 participants, we show that the system achieves high rates of attack detection, short pairing times, and a high average usability score. Moreover, in order to make an immediate impact on the wider developer community, we have published the full implementation and source code of our prototype, which is currently under consideration to be included in the official HoloLens development toolkit.",,Abstract_Title,
Web of Science,conferencePaper,2021,Global Feature Analysis and Comparative Evaluation of Freestyle In-Air-Handwriting Passcode for User Authentication,ACSAC - Annual Computer Security Applications Conference,A,"Freestyle in-air-handwriting passcode-based user authentication methods address the needs for Virtual Reality (VR)/Augmented Reality (AR) headsets, wearable devices, and game consoles where a physical keyboard cannot be provided for typing a password, but a gesture input interface is readily available. Such an authentication system can capture the hand movement of writing a passcode string in the air and verify the user identity using both the writing content (like a password) and the writing style (like a behavior biometric trait)Â â€¦",,Abstract,
Web of Science,conferencePaper,2019,A First Look into Privacy Leakage in 3D Mixed Reality Data,ESORICS - European Symposium on Research in Computer Security,A,"We have seen a rise in mixed (MR) and augmented reality (AR) applications and devices in recent years. Subsequently, we have become familiar with the sensing power of these applications and devices, and we are only starting to realize the nascent risks that these technology puts over our privacy and security. Current privacy protection measures are primarily aimed towards known and well-utilised data types (i.e. location, on-line activity, biometric, and so on) while a few works have focused on looking into the security and privacy risks of and providing protection on MR data, particularly on 3D MR data. In this work, we primarily reveal the privacy leakage from released 3D MR data and how the leakage persist even after implementing spatial generalizations and abstractions. Firstly, we formalize the spatial privacy problem in 3D mixed reality data as well as the adversary model. Then, we demonstrate through an inference model how adversaries can identify 3D spaces and, potentially, infer more spatial information. Moreover, we also demonstrate how compact 3D MR Data can be in terms of memory usage which allows adversaries to create lightweight 3D inference models of user spaces.",Mixed and augmented reality; 3D data; Point cloud data; Security and privacy,Abstract_Keywords_Title,
Web of Science,conferencePaper,2015,Visual Cryptography and Obfuscation: A Use-Case for Decrypting and Deobfuscating Information Using Augmented Reality,FC - Financial Cryptography and Data Security,A,"As new technologies emerge such as wearables, it opens up for new challenges, especially related to security and privacy. One such recent technology is smart glasses. The use of glasses introduces security and privacy concerns for the general public but also for the user itself. In this paper we present work which focus on privacy of the user during authentication. We propose and analyze two methods, visual cryptography and obfuscation for protecting the user against HUD and camera logging adversaries as well as shoulder-surfing.",Visual cryptography; Visual obfuscation; Augmented reality; Wearables,Keywords_Title,
Web of Science,conferencePaper,2015,Design and Analysis of Shoulder Surfing Resistant PIN Based Authentication Mechanisms on Google Glass,FC - Financial Cryptography and Data Security,A,"This paper explores options to the built-in authentication mechanism of the Google Glass which is vulnerable to shoulder surfing attacks. Two simple PIN-based authentication techniques are presented, both of which provide protection against shoulder surfing. The techniques employ two interfaces for entering the PIN, namely, voice (Voice-based PIN) and touchpad (Touch-based PIN). To enter the same PIN, user has the freedom to choose either technique and thereby interface, as per the environment in which authentication is being performed. A user study was conducted with 30 participants to compare the performance of the proposed methods with the built-in technique. The results show that the proposed mechanisms have a significantly better login success rate than the built-in technique. Interestingly, although the average authentication times of the proposed methods are higher than that of the built-in one, the users perceived them as being faster. The results also indicate that the proposed methods have better perceived security and usability than the built-in method. The study reveals that when it comes to authentication on augmented reality devices, there is a need for authentication mechanisms that complement each other as users tend to prefer a different interface in different contexts.",Google Glass; PIN; Authentication; Security; Usability,Abstract,
Web of Science,journalPaper,2019,GaitLock: Protect Virtual and Augmented Reality Headsets Using Gait,TODSC -Transactions on Dependable and Secure Computing,A,"With the fast penetration of commercial Virtual Reality (VR) and Augmented Reality (AR) systems into our daily life, the security issues of those devices have attracted significant interests from both academia and industry. Modern VR/AR systems typically use head-mounted devices (i.e., headsets) to interact with users, and often store private user data, e.g., social network accounts, online transactions or even payment information. This poses significant security threats, since in practice the headset can be potentially obtained and accessed by unauthenticated parties, e.g., identity thieves, and thus cause catastrophic breach. In this paper, we propose a novel GaitLock system, which can reliably authenticate users using their gait signatures. Our system doesn't require extra hardware, e.g., fingerprint sensors or retina scanners, but only uses the on-board inertial measurement units (IMUs) equipped in almost all mainstream VR/AR headsets to authenticate the legitimate users from intruders, by simply asking them to walk a few steps. To achieve that, we propose a new gait recognition model Dynamic-SRC, which combines the strength of Dynamic Time Warping (DTW) and Sparse Representation Classifier (SRC), to extract unique gait patterns from the inertial signals during walking. We implement GaitLock on Google Glass (a typical AR headset), and extensive experiments show that GaitLock outperforms the state-of-the-art systems significantly in recognition accuracy (> 98 percent success in 5 steps), and is able to run in-situ on the resource-constrained VR/AR headsets without incurring high energy cost.",Gait recognition; VR/AR; sparse representation classification; dynamic time warping,Abstract_Title,
Web of Science,journalPaper,2021,Immersive Virtual Reality Attacks and the Human Joystick,TODSC -Transactions on Dependable and Secure Computing,A,"This is one of the first accounts for the security analysis of consumer immersive Virtual Reality (VR) systems. This work breaks new ground, coins new terms, and constructs proof of concept implementations of attacks related to immersive VR. Our work used the two most widely adopted immersive VR systems, the HTC Vive, and the Oculus Rift. More specifically, we were able to create attacks that can potentially disorient users, turn their Head Mounted Display (HMD) camera on without their knowledge, overlay images in their field of vision, and modify VR environmental factors that force them into hitting physical objects and walls. Finally, we illustrate through a human participant deception study the success of being able to exploit VR systems to control immersed users and move them to a location in physical space without their knowledge. We term this the Human Joystick Attack. We conclude our work with future research directions and ways to enhance the security of these systems.",Security; Virtual reality; Software; Tracking; Cameras; Testing; Resists; Computer security; human computer interaction; privacy-invasive software; virtual reality,Abstract_Keywords_Title,
Web of Science,journalPaper,2022,Modeling and Defense of Social Virtual Reality Attacks Inducing Cybersickness,TODSC -Transactions on Dependable and Secure Computing,A,"Social Virtual Reality Learning Environments (VRLE) offer a new medium for flexible and immersive learning environments with geo-distributed users. Ensuring user safety in VRLE application domains such as education, flight simulations, military training is of utmost importance. Specifically, there is a need to study the impact of immersion attacks (e.g., chaperone attack, occlusion) and other types of attacks/faults (e.g., unauthorized access, network congestion) that may cause user safety issues (i.e., inducing of cybersickness). In this article, we present a novel framework to quantify the security, privacy issues triggered via immersion attacks and other types of attacks/faults. By using a real-world social VRLE viz., vSocial and creating a novel attack-fault tree model, we show that such attacks can induce undesirable levels of cybersickness. Next, we convert these attack-fault trees into stochastic timed automata (STA) representations to perform statistical model checking for a given attacker profile. Using this model checking approach, we determine the most vulnerable threat scenarios that can trigger high occurrence cases of cybersickness for VRLE users. Lastly, we show the effectiveness of our attack-fault tree modeling by incorporating suitable design principles such as hardening, diversity, redundancy and principle of least privilege to ensure user safety in a VRLE session.",Cybersickness; Security; Privacy; Safety; Solid modeling; Servers; Virtual environments; Security and privacy; user safety; cybersickness; virtual reality learning environments; attack-fault trees; statistical model checking; risk assessment; design principles,Abstract_Keywords_Title,
Web of Science,journalPaper,2015,3D-Model-Based Video Analysis for Computer Generated Faces Identification,TOIFS - Transactions on Information Forensics and Security,A,"Modern computer graphics technologies brought realism in computer-generated characters, making them achieve truly natural appearance. Besides traditional virtual reality applications such as avatars, games, or cinema, these synthetic characters may be used to generate realistic fakes, which may lead to improper use of the technology. This fact raises the demand for advanced tools able to discriminate real and artificial human faces in digital media. In this paper, we propose a method to distinguish between computer generated and natural faces by modeling and evaluating their dynamic behavior. Because of a 3D-model-based video analysis, the proposed technique allows identifying synthetic characters by detecting their more limited variability over time. Experimental results demonstrate the effectiveness of the proposed approach also on very challenging and realistic video sequences.",Computer generated versus natural; facial analysis; video forensics,Abstract,
Web of Science,journalPaper,2021,Designing Leakage-Resilient Password Entry on Head-Mounted Smart Wearable Glass Devices,TOIFS - Transactions on Information Forensics and Security,A,"With the boom of Augmented Reality (AR) and Virtual Reality (VR) applications, head-mounted smart wearable glass devices are becoming popular to help users access various services like E-mail freely. However, most existing password entry schemes on smart glasses rely on additional computers or mobile devices connected to smart glasses, which require users to switch between different systems and devices. This may greatly lower the practicability and usability of smart glasses. In this paper, we focus on this challenge and design three practical anti-eavesdropping password entry schemes on stand-alone smart glasses, named gTapper, gRotator and gTalker. The main idea is to break the correlation between the underlying password and the interaction observable to adversaries. In our IRB-approved user study, these schemes are found to be easy-to-use without additional hardware under various test conditions, where the participants can enter their passwords within moderate time, at high accuracy, and in various situations.",Password; Smart glasses; Eavesdropping; Usability; Glass; Authentication; Password entry; anti-eavesdropping; smart glasses; head-mounted device; usability and security,Abstract,
Web of Science,journalPaper,2023,A Secure Authentication Framework to Guarantee the Traceability of Avatars in Metaverse,TOIFS - Transactions on Information Forensics and Security,A,"Metaverse is a vast virtual environment parallel to the physical world in which users enjoy a variety of services acting as an avatar. To build a secure living habitat, it's vital to ensure the virtual-physical traceability that tracking a malicious player in the physical world via his avatars in virtual space. In this paper, we propose a two-factor authentication framework based on biometric-based authentication and chameleon signature. First, aiming at disguise in virtual space, we design an avatar's two-factor identity model to ensure the verifiability of avatar's virtual identity and physical identity. Second, facing at authentication efficiency and keys holding cost, we propose a chameleon collision signature algorithm to efficiently ensure that the avatar's virtual identity is associated with its physical identity. Finally, aiming at impersonation in the physical world, we design two decentralized authentication protocols based on the avatar's identity model and the chameleon collision signature to achieve real-time authentication on the avatar's identity. Security analysis indicates that the proposed authentication framework guarantees the consistency and traceability of the avatar's identity. Simulation experiments show that the framework not only completes the decentralized authentication between avatars but also achieves virtual-physical tracking.",Metaverse; avatar; authentication; traceability,Abstract_Keywords_Title,
Web of Science,journalPaper,2020,Mimicry Attacks on Smartphone Keystroke Authentication,TOPS - Transactions on Privacy and Security,A,"Keystroke behaviour-based authentication employs the unique typing behaviour of users to authenticate them. Recent such proposals for virtual keyboards on smartphones employ diverse temporal, contact, and spatial features to achieve over 95% accuracy. Consequently, they have been suggested as a second line of defense with text-based password authentication. We show that a state-of-the-art keystroke behaviour-based authentication scheme is highly vulnerable against mimicry attacks. While previous research used training interfaces to attack physical keyboards, we show that this approach has limited effectiveness against virtual keyboards. This is mainly due to the large number of diverse features that the attacker needs to mimic for virtual keyboards. We address this challenge by developing an augmented reality-based app that resides on the attacker's smartphone and leverages computer vision and keystroke data to provide real-time guidance during password entry on the victim's phone. In addition, we propose an audiovisual attack in which the attacker overlays transparent film printed with spatial pointers on the victim's device and uses audio cues to match the temporal behaviour of the victim. Both attacks require neither tampering or installing software on the victim's device nor specialized hardware. We conduct experiments with 30 users to mount over 400 mimicry attacks. We show that our methods enable an attacker to mimic keystroke behaviour on virtual keyboards with little effort. We also demonstrate the extensibility of our augmented reality-based technique by successfully mounting mimicry attacks on a swiping behaviour-based continuous authentication system.",Mimicry attacks; authentication; behavioural biometrics; spoofing attacks; augmented reality,Abstract_Keywords,
Web of Science,journalPaper,2022,Hidden in Plain Sight: Exploring Privacy Risks of Mobile Augmented Reality Applications,TOPS - Transactions on Privacy and Security,A,"Mobile augmented reality systems are becoming increasingly common and powerful, with applications in such domains as healthcare, manufacturing, education, and more. This rise in popularity is thanks in part to the functionalities offered by commercially available vision libraries such as ARCore, Vuforia, and Google's ML Kit; however, these libraries also give rise to the possibility of a hidden operations threat, that is, the ability of a malicious or incompetent application developer to conduct additional vision operations behind the scenes of an otherwise honest AR application without alerting the end-user. In this article, we present the privacy risks associated with the hidden operations threat and propose a framework for application development and runtime permissions targeted specifically at preventing the execution of hidden operations. We follow this with a set of experimental results, exploring the feasibility and utility of our system in differentiating between user-expectation-compliant and non-compliant AR applications during runtime testing, for which preliminary results demonstrate accuracy of up to 71%. We conclude with a discussion of open problems in the areas of software testing and privacy standards in mobile AR systems.",Augmented reality; mobile system security; user privacy,Abstract_Keywords_Title,
Scopus,conferencePaper,2023,SigA: RPPG-Based Authentication for Virtual Reality Head-Mounted Display,"RAID - The International Symposium on Research in Attacks, Intrusions and Defenses",A,"Consumer-grade virtual reality head-mounted displays (VR-HMD) are becoming increasingly popular. Despite VR’s convenience and booming applications, VR-based authentication schemes are underdeveloped. The recently proposed authentication methods (Electrooculogram based, Electrical Muscle Stimulation-based, and alike) require active user involvement, disturbing many scenarios like drone flight and telemedicine. This paper proposes an effective and efficient user authentication method in VR environments resilient to impersonation attacks using physiological signals — Photoplethysmogram (PPG), namely SigA. SigA exploits the advantage that PPG is a physiological signal invisible to the naked eye. Using VR-HMDs to cover the eye area completely, SigA reduces the risk of signal leakage during PPG acquisition. We conducted a comprehensive analysis of SigA’s feasibility on five publicly available datasets, nine different pre-trained models, three facial regions, various lengths of the video clips required for training, four different signal time intervals, and continuous authentication with different sliding window sizes. The results demonstrate that SigA achieves more than 95% of the average F1-score in a one-second signal to accommodate a complete cardiac cycle for most adults, implying its applicability in real-world scenarios. Furthermore, experiments have shown that SigA is resistant to zero-effort attacks, statistical attacks, impersonation attacks (with a detection accuracy of over 95%) and session hijacking attacks.",Biometric Authentication; Head-mounted Display; Photoplethysmography; Physiological Signal; Virtual Reality,Abstract_Keywords_Title,
Scopus,conferencePaper,2023,Understanding Person Identification Through Gait,PETS - International Symposium on Privacy Enhancing Technologies,A,"Gait recognition is the process of identifying humans from their bipedal locomotion such as walking or running. As such, gait data is privacy sensitive information and should be anonymized where possible. With the rise of higher quality gait recording techniques, such as depth cameras or motion capture suits, an increasing amount of detailed gait data is captured and processed. The introduction and rise of the Metaverse is an example of a potentially popular application scenario in which the gait of users is transferred onto digital avatars. As a first step towards developing effective anonymization techniques for high-quality gait data, we study different aspects of movement data to quantify their contribution to gait recognition. We first extract categories of features from the literature on human gait perception and then design experiments for each category to assess how much the information they contain contributes to recognition success. We evaluated the utility of gait perturbation by means of naturalness ratings in a user study. Our results show that gait anonymization will be challenging, as the data is highly redundant and inter-dependent.",,Abstract,
Scopus,conferencePaper,2023,Exploring the Privacy Risks of Adversarial VR Game Design,PETS - International Symposium on Privacy Enhancing Technologies,A,"Fifty study participants playtested an innocent-looking “escape room” game in virtual reality (VR). Within just a few minutes, an adversarial program had accurately inferred over 25 of their personal data attributes, from anthropometrics like height and wingspan to demographics like age and gender. As notoriously data-hungry companies become increasingly involved in VR development, this experimental scenario may soon represent a typical VR user experience. Since the Cambridge Analytica scandal of 2018, adversarially-designed gamified elements have been known to constitute a significant privacy threat in conventional social platforms. In this work, we present a case study of how metaverse environments can similarly be adversarially constructed to covertly infer dozens of personal data attributes from seemingly-anonymous users. While existing VR privacy research largely focuses on passive observation, we argue that because individuals subconsciously reveal personal information via their motion in response to specific stimuli, active attacks pose an outsized risk in VR environments.",,Abstract,
Scopus,conferencePaper,2023,Speculative Privacy Concerns About AR Glasses Data Collection,PETS - International Symposium on Privacy Enhancing Technologies,A,"As technology companies develop mass market augmented reality (AR) glasses that are increasingly sensor-laden and affordable, uses of such devices pose potential privacy and security problems. Though prior work has broadly addressed some of these problems, our work specifically addresses the potential data collection of 15 data types by AR glasses and five potential data uses. Via semi-structured interviews, we explored the attitudes and concerns of 21 current AR technology users regarding potential data collection and data use by hypothetical consumer-grade AR glasses. Participants expressed diverse concerns and suggested potential limits to AR data collection and use, evoking privacy concepts and informational norms. We discuss how participants’ attitudes and reservations about data collection and use, like definitions of privacy, are varying and context-dependent, and make recommendations for designers and policy makers, including customizable and multidimensional privacy solutions.",,Abstract,
Scopus,conferencePaper,2023,Securing Data Exchange in the Convergence of Metaverse and IoT Applications,"ARES - International Conference on Availability, Reliability and Security",B,"The convergence of Metaverse and Internet of Things (IoT) presents new opportunities for exchanging data, but it also introduces unprecedented security challenges. With the proliferation of IoT devices, the risk of unauthorized access and data breaches is on the rise, posing significant threats to data confidentiality and integrity. To address these challenges and protect user privacy, comprehensive security solutions are essential. We propose the SafeMetaNet approach, which combines proximity-based authentication, encryption, and blockchain technology to establish secure data exchange in the IoT-Metaverse convergence. SafeMetaNet ensures data confidentiality and integrity through encryption and establishes a tamper-proof record of data exchange using blockchain technology. We evaluated the approach's performance using various metrics, including latency, throughput, and two security metrics: data confidentiality and data integrity, and compared it with existing approaches. Our findings show that SafeMetaNet outperforms existing approaches, providing improved security. SafeMetaNet is a promising solution for secure data exchange in the IoT-Metaverse convergence. © 2023 ACM.",Blockchain; Data Exchange; Internet of Things; Metaverse; Security,Abstract_Keywords_Title,
Scopus,conferencePaper,2022,SoK: A Systematic Literature Review of Knowledge-Based Authentication on Augmented Reality Head-Mounted Displays,"ARES - International Conference on Availability, Reliability and Security",B,"The adoption of Augmented Reality (AR) technology has increased over the years. AR enhances various activities for consumers and businesses, particularly in industrial contexts. The three-dimensional virtual experience is realized by the usage of Head-Mounted Displays (HMD). These devices provide access to sensitive data and services. Thus, secure and usable authentication schemes are essential to control access to the HMD and the stored data as well as schemes to authenticate to the services one wants to use with the AR device. We conducted a systematic literature review on knowledge-based authentication schemes for AR HMD. 31 different schemes were identified. These schemes were assessed regarding various aspects including the type of AR HMD, the type of secret, how users input their secret, as well as usability and security aspects. We discuss gaps for future work.  © 2022 Owner/Author.",Augmented Reality; Authentication; Head-Mounted Display; Literature Review; Password Entry,Abstract_Keywords_Title,
Scopus,conferencePaper,2015,Usability of augmented reality for revealing secret messages to users but not their devices,SOUPS - Symposium On Usable Privacy and Security,B,"We evaluate the possibility of a human receiving a secret message while trusting no device with the contents of that message, by using visual cryptography (VC) implemented with augmented-reality displays (ARDs). In a pilot user study using Google Glass and an improved study using the Epson Moverio, users were successfully able to decode VC messages using ARDs. In particular, 26 out of 30 participants in the Epson Moverio study decoded numbers and letters with 100% accuracy. Our studies also tested assumptions made in previous VC research about users' abilities to detect active modification of a ciphertext. While a majority of the participants could identify that the images were modified, fewer participants could detect all of the modifications in the ciphertext or the decoded plaintext. © 2015 by The USENIX Association.",,Title,
Scopus,conferencePaper,2015,Ethics emerging: The story of privacy and security perceptions in virtual reality,SOUPS - Symposium On Usable Privacy and Security,B,"Virtual reality (VR) technology aims to transport the user to a virtual world, fully immersing them in an experience entirely separate from the real world. VR devices can use sensor data to draw deeply personal inferences (e.g., medical conditions, emotions) and can enable virtual crimes (e.g., theft, assault on virtual representations of the user) from which users have been shown to experience real, significant emotional pain. As such, VR may involve especially sensitive user data and interactions. To effectively mitigate such risks and design for safer experiences, we aim to understand end-user perceptions of VR risks and how, if at all, developers are considering and addressing those risks. In this paper, we present the first work on VR security and privacy perceptions: a mixed-methods study involving semi-structured interviews with 20 VR users and developers, a survey of VR privacy policies, and an ethics co-design study with VR developers. We establish a foundational understanding of perceived risks in VR; raise concerns about the state of VR privacy policies; and contribute a concrete VR developer “code of ethics”, created by developers, for developers. © 2018 by The USENIX Association All Rights Reserved.",,Abstract_Title,
Scopus,conferencePaper,2015,Evaluating and redefining smartphone permissions with contextualized justifications for mobile augmented reality apps,SOUPS - Symposium On Usable Privacy and Security,B,"Augmented reality (AR), and specifically mobile augmented reality (MAR) gained much public attention after the success of Pokémon Go in 2016, and since then has found application in online games, social media, entertainment, real estate, interior design, and other services. MAR apps are highly dependent on real time context-specific information provided by the different sensors and data processing capabilities of smartphones (e.g., LiDAR, gyroscope or object recognition). This dependency raises crucial privacy issues for end users. We evaluate whether the existing access permission systems, initially developed for non-AR apps, as well as proposed new permissions, relevant for MAR apps, provide sufficient and clear information to the users. We address this research goal in two online survey-based experiments with a total of 581 participants. Based on our results, we argue that it is necessary to increase transparency about MAR apps' data practices by requesting users' permissions to access certain novel and privacy invasive resources and functionalities commonly used in MAR apps, such as speech and face recognition. We also find that adding justifications, contextualized to the data collection practices of the app, improves transparency and can mitigate privacy concerns, at least in the context of data utilized to the users' benefit. Better understanding of the app's practices and lower concerns, in turn, increase the intentions to grant permissions. We provide recommendations for better transparency in MAR apps. © is held by the author/owner. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee. USENIX Symposium on Usable Privacy and Security (SOUPS) 2021.",,Abstract_Title,
Scopus,conferencePaper,2018,Ethics emerging: The story of privacy and security perceptions in virtual reality,SOUPS - Symposium On Usable Privacy and Security,B,"Virtual reality (VR) technology aims to transport the user to a virtual world, fully immersing them in an experience entirely separate from the real world. VR devices can use sensor data to draw deeply personal inferences (e.g., medical conditions, emotions) and can enable virtual crimes (e.g., theft, assault on virtual representations of the user) from which users have been shown to experience real, significant emotional pain. As such, VR may involve especially sensitive user data and interactions. To effectively mitigate such risks and design for safer experiences, we aim to understand end-user perceptions of VR risks and how, if at all, developers are considering and addressing those risks. In this paper, we present the first work on VR security and privacy perceptions: a mixed-methods study involving semi-structured interviews with 20 VR users and developers, a survey of VR privacy policies, and an ethics co-design study with VR developers. We establish a foundational understanding of perceived risks in VR; raise concerns about the state of VR privacy policies; and contribute a concrete VR developer “code of ethics”, created by developers, for developers. © 2018 by The USENIX Association All Rights Reserved.",,Abstract_Title,Duplicate
Scopus,conferencePaper,2021,Evaluating and redefining smartphone permissions with contextualized justifications for mobile augmented reality apps,SOUPS - Symposium On Usable Privacy and Security,B,"Augmented reality (AR), and specifically mobile augmented reality (MAR) gained much public attention after the success of Pokémon Go in 2016, and since then has found application in online games, social media, entertainment, real estate, interior design, and other services. MAR apps are highly dependent on real time context-specific information provided by the different sensors and data processing capabilities of smartphones (e.g., LiDAR, gyroscope or object recognition). This dependency raises crucial privacy issues for end users. We evaluate whether the existing access permission systems, initially developed for non-AR apps, as well as proposed new permissions, relevant for MAR apps, provide sufficient and clear information to the users. We address this research goal in two online survey-based experiments with a total of 581 participants. Based on our results, we argue that it is necessary to increase transparency about MAR apps' data practices by requesting users' permissions to access certain novel and privacy invasive resources and functionalities commonly used in MAR apps, such as speech and face recognition. We also find that adding justifications, contextualized to the data collection practices of the app, improves transparency and can mitigate privacy concerns, at least in the context of data utilized to the users' benefit. Better understanding of the app's practices and lower concerns, in turn, increase the intentions to grant permissions. We provide recommendations for better transparency in MAR apps. © is held by the author/owner. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee. USENIX Symposium on Usable Privacy and Security (SOUPS) 2021.",,Abstract_Title,Duplicate
Web of Science,conferencePaper,2023,Investigating Security Indicators for Hyperlinking Within the Metaverse,SOUPS - Symposium On Usable Privacy and Security,B,"Security indicators, such as the padlock icon indicating SSL encryption in browsers, are established mechanisms to convey secure connections. Currently, such indicators mainly exist for browsers and mobile environments. With the rise of the metaverse, we investigate how to mark secure transitions between applications in virtual reality to so-called sub-metaverses. For this, we first conducted in-depth interviews with domain experts (N=8) to understand the general design dimensions for security indicators in virtual reality (VR). Using these insights and considering additional design constraints, we implemented the five most promising indicators and evaluated them in a user study (N=25). While the visual blinking indicator placed in the periphery performed best regarding accuracy and task completion time, participants subjectively preferred the static visual indicator above the portal. Moreover, the latter received high scores regarding understandability while still being rated low regarding intrusiveness and disturbance. Our findings contribute to a more secure and enjoyable metaverse experience.",,Abstract_Title,
Web of Science,conferencePaper,2023,"An Investigation of Teenager Experiences in Social Virtual Reality from Teenagers', Parents', and Bystanders' Perspectives",SOUPS - Symposium On Usable Privacy and Security,B,"The recent rise of social virtual reality (VR) platforms has introduced new technology characteristics and user experiences, which may lead to new forms of online harassment, particularly among teenagers (aged 13-17). In this paper, we took a multi-stakeholder approach and investigate teenagers' experiences and safety threats in social VR from three perspectives (teenagers, parents, and bystanders) to cover complementary perspectives. Through an interview study with 24 participants (8 teenagers, 7 parents, and 9 bystanders), we found several safety threats that teenagers may face, such as virtual grooming, ability-based discrimination, unforeseeable threats in privacy rooms, etc. We highlight new forms of harassment in the social VR context, such as erotic role-play and abuse through phantom sense, as well as the discrepancies among teenagers, parents, and bystanders regarding their perceptions of such threats. We draw design implications to better support safer social VR environments for teenagers.",,Abstract_Title,
Scopus,conferencePaper,2021,Spectrum-flexible secure broadcast ranging,WiSec - Security and Privacy in Wireless and Mobile Networks,B,"Secure ranging is poised to play a critical role in several emerging applications such as self-driving cars, unmanned aerial systems, wireless IoT devices, and augmented reality. In this paper, we propose a design of a secure broadcast ranging system with unique features and techniques. Its spectral-flexibility, and low-power short ranging bursts enable co-existence with existing systems such as in the 2.4GHz ISM band. We exploit a set of RF techniques such as upsampling and successive interference cancellation to achieve high accuracy and scalability to tens of reflectors even when operating over narrow bands of spectrum. We demonstrate that it can be implemented on popular SDR platforms FPGA and/or hosts (with minimal FPGA modifications). The protocol design, and cryptographically generated/detected signals, and randomized timing of transmissions, provide stealth and security against denial of service, sniffing, and distance manipulation attacks. Through extensive experimental evaluations (and simulations for scalability to over 100 reflectors) we demonstrate an accuracy below 20cm on a wide range of SNR (as low as 0dB), spectrum 25MHz-100MHz, with bursts as short as 5us. © 2021 ACM.",privacy; protocols; software defined radio; wireless ranging,Abstract,
IEEE,conferencePaper,2017,Visualizing the New Zealand Cyber Security Challenge for Attack Behaviors,"TrustCom - International Conference on Trust, Security and Privacy in Computing and Communications",B,"Datasets are important for security analytics and mitigation processes in cyber security research and investigations. ""Cyber security challenge (CSC)"" events provide the means to collect datasets. The New Zealand National cyber security challenge event is designed to promote cyber security education, awareness and equally as important, collect datasets for research purposes. In this paper, we present the: (1) Importance of cyber security challenge events, (2) Highlight the importance of collecting datasets, and (3) present a user-centric security visualization model of attack behaviors. User-centric features with the theoretical concept of Data Provenance as a Security Visualization Service (DPaaSVS) reused to display attacks commencing at the reconnaissance stage through to compromising a defending team machine and exploiting the systems. DPaaSVS creates the ability for users to interact and observe correlations between cyber-attacks. Finally we provide future work on Security Visualization with Augmented Reality capabilities to enhance and improve user interactions with the security visualization platform.",Security Visualization;Cyber-attacks;Usercentricity;Data Provenance;Datasets,Abstract,
Web of Science,journalPaper,2023,Virtually secure: A taxonomic assessment of cybersecurity challenges in virtual reality environments,CS - Computers and Security,B,"Although Virtual Reality (VR) is certainly not a new technology, its recent adoption across several sectors beyond entertainment has led the information security research community to take note of the new cy-ber threats that come with it. The variety of system components presents an extensive attack surface that can be exploited. At the same time, VR's emphasis on immersion, interaction and presence means that the user can be targeted directly, yet the use of head-mounted displays may prevent them from observing a cyber attack's impact in their immediate physical environment. This paper presents the first taxonomic representation of VR security challenges. By systemically classifying existing VR cyber threats against ex-isting defences in a single comparative matrix, we aim to help researchers from different backgrounds to identify key focus areas where further research would be most beneficial.(c) 2022 Elsevier Ltd. All rights reserved.",Virtual Reality; Cyber-physical attacks; Cybersecurity; Privacy; Taxonomy,Abstract_Keywords_Title,
Web of Science,journalPaper,2023,A systematic threat analysis and defense strategies for the metaverse and extended reality systems,CS - Computers and Security,B,"With the rapid development and evolution of immersive technologies there are growing concerns of security and privacy threats to the metaverse and extended reality (XR) systems. Immersive reality solutions are a combination of multiple vulnerable technologies allowing attackers to easily undermine security. Furthermore the deployment of appropriate security controls and defensive mechanisms for resource constrained proprietary XR products has been limited. In this paper, we provide a comprehensive overview of extended reality systems and the metaverse with emphasis on technology weaknesses, cyber security challenges and users' safety concerns. Five major taxonomies have been presented in this research with an aim of identifying privacy inference vectors and potential cyber threats; determining the impact on human health and the extent to which cyberstalking, and digital currency scam activities proliferate when using XR. This research also proposes strategies for primary lines of defense and provides recommendations on the adoption of safety measures.",Extended reality (XR); Metaverse; Cyber defense; Privacy; Cyber threats; Cyberstalking; Physical safety; XR commerce; Virtual reality; Augmented reality; Mixed reality; Blockchain; Cybersickness; Currency scams,Abstract_Keywords_Title,
Web of Science,journalPaper,2023,Rise of the Metaverse?s Immersive Virtual Reality Malware and the Man-in-the-Room Attack & Defenses,CS - Computers and Security,B,"The allure of the metaverse along with Virtual Reality (VR) technologies and speed at which they are deployed may shift focus away from security and privacy fundamentals. In this work we employ classic exploitation techniques against cutting edge devices to obtain equally novel results. The unique features of the Virtual Reality landscape set the stage for our primary account of a new attack, the Man-in-the-Room (MitR). This attack, realized from a vulnerable social networking application led to both worming and botnet capabilities being adapted for VR with potential critical impacts affecting millions of users. Our work improves the state-of-the-art in Virtual Reality (VR) security and socio-technical research in VR. It shares several analytical and attacking tools, example exploits, evaluation dataset, and vulnerability signatures with the scientific and professional communities to ensure secure VR software development. The presented results demonstrate the detection and prevention of VR vulnerabilities, and raise questions in the law and policy domains pertaining to VR security and privacy. Published by Elsevier Ltd.",Emerging technologies; Network-level security and protection; Network communications; Network Protocols; Protection mechanisms; Quality analysis and evaluation; System issues; Security and Privacy Protection; Authentication; Communications Applications; Virtual reality; Security and Protection; Artificial augmented and virtual realities; Invasive software (viruses worms Trojan; horses); Unauthorized access (hacking phreaking),Abstract_Keywords_Title,
Web of Science,journalPaper,2023,Virtual reality for improving cyber situational awareness in security operations centers,CS - Computers and Security,B,"Security operations centers (SOCs) are the 911 centers of many organizational networks, except they not only respond, but also monitor. SOC operators are charged with detection, response, and mitigation. This is a tall task when one considers the volume, velocity, and variety of both internal and external organizational network and system data. SOC operations are truly a big data problem. Security orchestration, incident event management, data fusion, and anomaly detection systems help, but more is needed. This study examines the impact virtual reality (VR) can have on SOC operator performance and perceived task load. We developed a VR based network monitoring tool and assigned human subjects to one of three conditions - VR only, traditional tool only, or both. Our results, though small in scale, provide very promising indication that VR based technology may be beneficial for improving cyber situational awareness (SA), particularly with overall data perception involving novice SOC operators. The results are promising, but the sample size is small, so future research should validate this pilot study. Given the workforce challenges in the cybersecurity space, and the need to perceive large quantities of data, VR may be a very good addition to SOCs.",Cyber situational awareness; Virtual reality; Security operations centers; Network monitoring,Abstract_Keywords_Title,
IEEE,journalPaper,2018,Arya: Operating System Support for Securely Augmenting Reality,SPM - Security & Privacy Magazine,B,"Augmented reality (AR) applications capture sensor input from a userâ€™s surroundings and overlay virtual output on their perception of the world, enabling new, immersive experiences. However, this technology raises serious security and privacy risks such as malicious or buggy AR output.",,Abstract,
IEEE,journalPaper,2023,In Your Eyes,SPM - Security & Privacy Magazine,B,"In mixed reality, will virtual content change our perception of the physical world?",,Abstract,
IEEE,conferencePaper,2023,VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage,ASE - Automated Software Engineering,A*,"Virtual Reality (VR) is an emerging technique that has been applied to more and more areas such as gaming, remote conference, and education. Since VR user interface has very different characteristics compared with traditional graphic user interface (GUI), VR applications also require new testing techniques for quality assurance. Recently, some frameworks (e.g., VRTest) have been proposed to automate VR user interface testing by automatically controlling the player camera. However, their testing strategies are not able to address VR-specific testing challenges such as object occlusion and movement. In this paper, we propose a novel testing technique called VRGuide to explore VR scenes more efficiently. In particular, VRGuide adapts a computer geometry technique called Cut Extension to optimize the camera routes for covering all interact-able objects. We compared the testing strategy with VRTest on eight top VR software projects with scenes. The results show that VRGuide is able to achieve higher test coverage upon testing timeout in two of the projects, and achieve saturation coverage with averagely 31% less testing time than VRTest on the remaining six projects. Furthermore, VRGuide detected and reported four unknown bugs confirmed by developers, only one of which is also detected by VRTest.",Software Testing;Virtual Reality;Scene Exploration,Abstract_Keywords_Title,
IEEE,conferencePaper,2023,PhyFu: Fuzzing Modern Physics Simulation Engines,ASE - Automated Software Engineering,A*,"A physical simulation engine (PSE) is a software system that simulates physical environments and objects. Modern PSEs feature both forward and backward simulations, where the forward phase predicts the behavior of a simulated system, and the backward phase provides gradients (guidance) for learning-based control tasks, such as a robot arm learning to fetch items. This way, modern PSEs show promising support for learning-based control methods. To date, PSEs have been largely used in various high-profitable, commercial applications, such as games, movies, virtual reality (VR), and robotics. Despite the prosperous development and usage of PSEs by academia and industrial manufacturers such as Google and NVIDIA, PSEs may produce incorrect simulations, which may lead to negative results, from poor user experience in entertainment to accidents in robotics-involved manufacturing and surgical operations. This paper introduces PhyFu, a fuzzing framework designed specifically for PSEs to uncover errors in both forward and backward simulation phases. PHyFu mutates initial states and asserts if the PSE under test behaves consistently with respect to basic Physics Laws (PLs). We further use feedback-driven test input scheduling to guide and accelerate the search for errors. Our study of four PSEs covers mainstream industrial vendors (Google and NVIDIA) as well as academic products. We successfully uncover over 5K error-triggering inputs that generate incorrect simulation results spanning across the whole software stack of PSEs.",Software testing;Physics simulator;Robotics;Fuzzing,Abstract,
IEEE,conferencePaper,2023,Assessing the Impact of Refactoring Energy-Inefficient Code Patterns on Software Sustainability: An Industry Case Study,ASE - Automated Software Engineering,A*,"Advances in technologies like artificial intelligence and metaverse have led to a proliferation of software systems in business and everyday life. With this widespread penetration, the carbon emissions of software are rapidly growing as well, thereby negatively impacting the long-term sustainability of our environment. Hence, optimizing software from a sustainability standpoint becomes more crucial than ever. We believe that the adoption of automated tools that can identify energy-inefficient patterns in the code and guide appropriate refactoring can significantly assist in this optimization. In this extended abstract, we present an industry case study that evaluates the sustainability impact of refactoring energy -inefficient code patterns identified by automated software sustainability assessment tools for a large application. Preliminary results highlight a positive impact on the application's sustainability post-refactoring, leading to a 29% decrease in per-user per-month energy consumption.",,Abstract,
ACM DL,conferencePaper,2022,PredART: Towards Automatic Oracle Prediction of Object Placements in Augmented Reality Testing,ASE - Automated Software Engineering,A*,"While the emerging Augmented Reality (AR) technique allows a lot of new application opportunities, from education and communication to gaming, current augmented apps often have complaints about their usability and/or user experience due to placement errors of virtual objects. Therefore, identifying noticeable placement errors is an important goal in the testing of AR apps. However, placement errors can only be perceived by human beings and may need to be confirmed by multiple users, making automatic testing very challenging. In this paper, we propose PredART, a novel approach to predict human ratings of virtual object placements that can be used as test oracles in automated AR testing. PredART is based on automatic screenshot sampling, crowd sourcing, and a hybrid neural network for image regression. The evaluation on a test set of 480 screenshots shows that our approach can achieve an accuracy of 85.0% and a mean absolute error, mean squared error, and root mean squared error of 0.047, 0.008, and 0.091, respectively.",Augmented Reality; Placement Error; Virtual Objects,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,A Role Based Model Template for Specifying Virtual Reality Software,ASE - Automated Software Engineering,A*,"Research in hardware and software support for Virtual Reality (VR) has significantly increased over the last decade. Given the software platform fragmentation and hardware volatility, there is an apparent disconnect among practitioners while building applications in the VR domain. This paper proposes a role-based model template as a meta-model to specify the bare minimum VR software system. We conducted a grounded-theory-based qualitative study on prevailing and phased-out VR SDKs and standards to propose this meta-model. This model template can help VR practitioners build open-source tools to develop, design, and test VR software systems.",Virtual Reality; Meta model; Grounded-theory; VR Model Template; VR SDK,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,A Study of User Privacy in Android Mobile AR Apps,ASE - Automated Software Engineering,A*,"With the development of augmented reality (AR) technology, the use of mobile AR applications (MAR apps) is rising rapidly in various aspects of people’s everyday lives, such as games, shopping, and education. When compared to traditional apps, AR apps typically need access to the smartphone’s camera all the time and collect and analyze significantly more data, such as sensor data, geolocation, and biometric information. Due to the sensitivity and volume of data collected by MAR apps, new privacy concerns are raised. In this paper, we describe a preliminary empirical study of Android MAR apps in terms of the sensitive data collected by MAR apps, whether the collected data is well protected, and whether the data practice is publicly available so that users can learn about the data safety and make informed decisions when deciding which apps to install. In this study, we analyzed 390 real-world MAR apps and reported the dangerous permissions they requested, the data leaks detected in them, and the availability of their data safety.",user privacy; mobile application; data safety; privacy leak,Abstract,
ACM DL,conferencePaper,2022,DyTRec: A Dynamic Testing Recommendation tool for Unity-based Virtual Reality Software,ASE - Automated Software Engineering,A*,"Virtual Reality (VR) technology has been utilized in other fields besides gaming, such as education, training, arts, shopping, and e-commerce. However, the technical support of VR software is not growing as fast as its market size, especially for testing. Because of the immersive feature that requires VR apps to act and react to all the interactions dynamically, the traditional static testing techniques such as unit test generation cannot fully guarantee the correctness of the tested functions. In this paper, we proposed a Dynamic Testing Recommendation tool (DyTRec) to suggest the potential types of dynamic testing for the target VR projects. Specifically, we categorize the dynamic testing types by analyzing the official APIs from the VR engine documentation and then apply the extracting and searching on all VR script files. We evaluated DyTRec on 20 VR projects and successfully reported 39 suggested results.",Virtual Reality; Software Testing; Unity,Abstract_Keywords_Title,
IEEE,conferencePaper,2020,Towards Immersive Comprehension of Software Systems Using Augmented Reality - An Empirical Evaluation,ASE - Automated Software Engineering,A*,"While traditionally, software comprehension relies on approaches like reading through the code or looking at charts on screens, which are 2D mediums, there have been some recent approaches that advocate exploring 3D approaches like Augmented or Virtual Reality (AR/VR) to have a richer experience towards understanding software and its internal relationships. However, there is a dearth of objective studies that compare such 3D representations with their traditional 2D counterparts in the context of software comprehension. In this paper, we present an evaluation study to quantitatively and qualitatively compare 2D and 3D software representations with respect to typical comprehension tasks. For the 3D medium, we utilize an AR-based approach for 3D visualizations of a software system (XRaSE), while the 2D medium comprises of textual IDEs and 2D graph representations. The study, which has been conducted using 20 professional developers, shows that for most comprehension tasks, the developers perform much better using the 3D representation, especially in terms of velocity and recollection, while also displaying reduced cognitive load and better engagement.",Software Visualization;Augmented Reality;3D Software;User Study,Abstract_Keywords_Title,
IEEE,conferencePaper,2020,Edge4Real: A Cost-Effective Edge Computing based Human Behaviour Recognition System for Human-Centric Software Engineering,ASE - Automated Software Engineering,A*,"Recognition of human behaviours including body motions and facial expressions plays a significant role in human-centric software engineering. However, due to the data and computation intensive nature of human behaviour recognition through video analytics, expensive powerful machines are often required, which could hinder the research and application in human-centric software engineering. To address such an issue, this paper proposes a cost-effective human behaviour recognition system named Edge4Real which can be easily deployed in an edge computing environment with commodity machines. Compared with existing centralised solutions, Edge4Real has three major advantages including cost-effectiveness, easy-to-use, and realtime. Specifically, Edge4Real adopts a distributed architecture where components such as motion capturing, human behaviour recognition, data decoding and extraction, and the application of the recognition result, can be deployed on separated end devices and edge nodes in an edge computing environment. Using a virtual reality application which can capture a user's motion and translate into the motion of a 3D avatar in real time, we successfully validate the effectiveness of the system and demonstrate its promising value to the research and application of human-centric software engineering. The demo video can be found at https://youtu.be/tnEshD8j-kA.",Software and its engineering â†’Software creation and management;Human Behaviour Recognition;Human Centric Software Engineering;Edge Computing;Virtual Reality,Abstract_Keywords,
IEEE,conferencePaper,2019,XRaSE: Towards Virtually Tangible Software using Augmented Reality,ASE - Automated Software Engineering,A*,"Software engineering has seen much progress in recent past including introduction of new methodologies, new paradigms for software teams, and from smaller monolithic applications to complex, intricate, and distributed software applications. However, the way we represent, discuss, and collaborate on software applications throughout the software development life cycle is still primarily using the source code, textual representations, or charts on 2D computer screens - the confines of which have long limited how we visualize and comprehend software systems. In this paper, we present XRaSE, a novel prototype implementation that leverages augmented reality to visualize a software application as a virtually tangible entity. This immersive approach is aimed at making activities like application comprehension, architecture analysis, knowledge communication, and analysis of a software's dynamic aspects, more intuitive, richer and collaborative.","Augmented Reality, Immersive Experience, Software Visualization, Software Comprehension",Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Towards immersive software archaeology: regaining legacy systems’ design knowledge via interactive exploration in virtual reality,FSE - Foundations of Software Engineering,A*,"Many of today's software systems will become the legacy systems of tomorrow, comprised of outdated technology and inaccurate design documents. Preparing for their eventual re-engineering requires engineers to regain lost design knowledge and discover re-engineering opportunities. While tools and visualizations exist, comprehending an unfamiliar code base remains challenging. Hence, software archaeology suffers from a considerable entry barrier as it requires expert knowledge, significant diligence, tenacity, and stamina. In this paper, we propose a paradigm shift in how legacy systems' design knowledge can be regained by presenting our vision for an immersive explorable software visualization in virtual reality (VR). We propose innovative concepts leveraging benefits of VR for a) immersion in an exoteric visualization metaphor, b) effective navigation and orientation, c) guiding exploration, and d) maintaining a link to the implementation. By enabling immersive and playful legacy system exploration, we strive for lowering the entry barrier, fostering long-term engagement, strengthening mental-model building, and improving knowledge retention in an effort to ease coping with the increased number of tomorrow's legacy systems.",Legacy Software; Software Archaeology; Software Engineering; Software Re-Engineering; Software Visualization; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Software documentation and augmented reality: love or arranged marriage?,FSE - Foundations of Software Engineering,A*,"There is a significant rise in the availability, development and size of software projects in the present day. Many open source projects are reused or updated for various purposes that include fixing bugs in existing projects, development and maintenance of project extensions. Developers who interact with the projects might require documentation for better comprehension of the project and to develop extensions. Most of the software projects currently do not have sufficient documentation or it is not updated along with the project. If some projects have reasonably sufficient documentation, it is usually difficult to comprehend it either for maintenance or for reuse purposes. Considering the usefulness of Augmented Reality (AR) towards comprehension, we propose the vision of integrating the domains of augmented reality and software documentation, and specifically, visualization of software documentation using AR. In this paper, we present some of the directions that could be explored towards this vision and also present an example visualization scenario for API documentation using neural system metaphor. We see this paper as a basis for the future research direction of leveraging AR towards making documentation as a primary artifact in the software development process.",Augmented Reality; Software Documentation; Visualization,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Metamorphic Shader Fusion for Testing Graphics Shader Compilers,ICSE - International Conference on Software Engineering,A*,"Computer graphics are powered by graphics APIs (e.g., OpenGL, Direct3D) and their associated shader compilers, which render high-quality images by compiling and optimizing user-written high-level shader programs into GPU machine code. Graphics rendering is extensively used in production scenarios like virtual reality (VR), gaming, autonomous driving, and robotics. Despite the development by industrial manufacturers such as Intel, Nvidia, and AMD, shader compilers — like traditional software — may produce ill-rendered outputs. In turn, these errors may result in negative results, from poor user experience in entertainment to accidents in driving assistance systems.This paper introduces FSHADER, a metamorphic testing (MT) framework designed specifically for shader compilers to uncover erroneous compilations and optimizations. FSHADER tests shader compilers by mutating input shader programs via four carefully-designed metamorphic relations (MRs). In particular, FSHADER fuses two shader programs via an MR and checks the visual consistency between the image rendered from the fused shader program with the output of fusing individually rendered images. Our study of 12 shader compilers covers five mainstream GPU vendors, including Intel, AMD, Nvidia, ARM, and Apple. We successfully uncover over 16K error-triggering inputs that generate incorrect rendering outputs. We manually locate and characterize buggy optimization places, and developers have confirmed representative bugs.",,Abstract,
ACM DL,conferencePaper,2021,How Developers Optimize Virtual Reality Applications: A Study of Optimization Commits in Open Source Unity Projects,ICSE - International Conference on Software Engineering,A*,"Virtual Reality (VR) is an emerging technique that provides immersive experience for users. Due to the high computation cost of rendering real-time animation twice (for both eyes) and the resource limitation of wearable devices, VR applications often face performance bottlenecks and performance optimization plays an important role in VR software development. Performance optimizations of VR applications can be very different from those in traditional software as VR involves more elements such as graphics rendering and real-time animation. In this paper, we present the first empirical study on 183 real-world performance optimizations from 45 VR software projects. In particular, we manually categorized the optimizations into 11 categories, and applied static analysis to identify how they affect different life-cycle phases of VR applications. Furthermore, we studied the complexity and design / behavior effects of performance optimizations, and how optimizations are different between large organizational software projects and smaller personal software projects. Our major findings include: (1) graphics simplification (24.0%), rendering optimization (16.9%), language / API optimization (15.3%), heap avoidance (14.8%), and value caching (12.0%) are the most common categories of performance optimization in VR applications; (2) game logic updates (30.4%) and before-scene initialization (20.0%) are the most common life-cycle phases affected by performance issues; (3) 45.9% of the optimizations have behavior and design effects and 39.3% of the optimizations are systematic changes; (4) the distributions of optimization classes are very different between organizational VR projects and personal VR projects.",Virtual Reality; Empirical Study; Performance Optimization,Abstract_Keywords_Title,
ACM DL,conferencePaper,2015,"Virtual reality in software engineering: affordances, applications, and challenges",ICSE - International Conference on Software Engineering,A*,"Software engineers primarily interact with source code using a keyboard and mouse, and typically view software on a small number of 2D monitors. This interaction paradigm does not take advantage of many affordances of natural human movement and perception. Virtual reality (VR) can use these affordances more fully than existing developer environments to enable new creative opportunities and potentially result in higher productivity, lower learning curves, and increased user satisfaction. This paper describes the affordances offered by VR; demonstrates the benefits of VR and software engineering in prototypes for live coding and code review; and discusses future work, open questions, and the challenges of VR.",,Abstract_Title,
ACM DL,conferencePaper,2023,Virtual and Augmented Reality for Environmental Sustainability: A Systematic Review,CHI - Human Factors in Computing Systems,A*,"In recent years, extended reality (XR) technology has seen a rise in use in environmental subjects, i.e., climate change or biodiversity loss, as a potential tool to inform and engage the public with current and future environmental issues. However, research on the potential of XR technology for environmental sustainability is still in the early stages, and there is no clear synthesis of the methods studied in this field. To provide a clearer view of existing approaches and research objectives, we systematically reviewed current literature dealing with XR use in environmental topics. Although the results indicate that the volume of literature exploring XR in environmental applications is increasing, empirical evidence of its impact is limited, hindering the possibility of presently drawing significant conclusions on its potential benefits. Based on our analyses, we identified thematic, theoretical, and methodological knowledge gaps and provide a guideline to aid future research in the field.",Augmented Reality; Climate Change; Environmental Sustainability; Extended Reality; Systematic Literature Review; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,I think I don’t feel sick: Exploring the Relationship Between Cognitive Demand and Cybersickness in Virtual Reality using fNIRS,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) applications commonly use the illusion of self-motion (vection) to simulate experiences such as running, driving, or flying. However, this can lead to cybersickness, which diminishes the experience of users, and can even lead to disengagement with this platform. In this paper we present a study in which we show that users performing a cognitive task while experiencing a VR rollercoaster reported reduced symptoms of cybersickness. Furthermore, we collected and analysed brain activity data from our participants during their experience using functional near infra-red spectroscopy (fNIRS): preliminary analysis suggests the possibility that this technology may be able to detect the experience of cybersickness. Together, these results can assist the creators of VR experiences, both through mitigation of cybersickness in the design process, and by better understanding the experiences of their users.",cognitive demand; cybersickness; fNIRS; HMD; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,InfinitePaint: Painting in Virtual Reality with Passive Haptics Using Wet Brushes and a Physical Proxy Canvas,CHI - Human Factors in Computing Systems,A*,"Digital painting interfaces require an input fidelity that preserves the artistic expression of the user. Drawing tablets allow for precise and low-latency sensing of pen motions and other parameters like pressure to convert them to fully digitized strokes. A drawback is that those interfaces are rigid. While soft brushes can be simulated in software, the haptic sensation of the rigid pen input device is different compared to using a soft wet brush on paper. We present InfinitePaint, a system that supports digital painting in Virtual Reality on real paper with a real wet brush. We use special paper that turns black wherever it comes into contact with water and turns blank again upon drying. A single camera captures those temporary strokes and digitizes them while applying properties like color or other digital effects. We tested our system with artists and compared the subjective experience with a drawing tablet.",brush input; Digital painting; mixed reality; passive haptics; traditional art; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,InstruMentAR: Auto-Generation of Augmented Reality Tutorials for Operating Digital Instruments Through Recording Embodied Demonstration,CHI - Human Factors in Computing Systems,A*,"Augmented Reality tutorials, which provide necessary context by directly superimposing visual guidance on the physical referent, represent an effective way of scaffolding complex instrument operations. However, current AR tutorial authoring processes are not seamless as they require users to continuously alternate between operating instruments and interacting with virtual elements. We present InstruMentAR, a system that automatically generates AR tutorials through recording user demonstrations. We design a multimodal approach that fuses gestural information and hand-worn pressure sensor data to detect and register the user’s step-by-step manipulations on the control panel. With this information, the system autonomously generates virtual cues with designated scales to respective locations for each step. Voice recognition and background capture are employed to automate the creation of text and images as AR content. For novice users receiving the authored AR tutorials, we facilitate immediate feedback through haptic modules. We compared InstruMentAR with traditional systems in the user study.",Augmented Reality; Embodied Demonstration; Haptic Feedback; Immersive Authoring; Tangible Interaction; wearable Devices,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Location-Aware Adaptation of Augmented Reality Narratives,CHI - Human Factors in Computing Systems,A*,"The recent popularity of augmented reality (AR) devices has enabled players to participate in interactive narratives through virtual events and characters populated in a real-world environment, where different actions may lead to different story branches. In this paper, we propose a novel approach to adapt narratives to real spaces for AR experiences. Our optimization-based approach automatically assigns contextually compatible locations to story events, synthesizing a navigation graph to guide players through different story branches while considering their walking experiences. We validated the effectiveness of our approach for adapting AR narratives to different scenes through experiments and user studies.",augmented reality; interactive narratives; path generation; storytelling,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,PointShopAR: Supporting Environmental Design Prototyping Using Point Cloud in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"We present PointShopAR, a novel tablet-based system for AR environmental design using point clouds as the underlying representation. It integrates point cloud capture and editing in a single AR workflow to help users quickly prototype design ideas in their spatial context. We hypothesize that point clouds are well suited for prototyping, as they can be captured more rapidly than textured meshes and then edited immediately in situ on the capturing device. We based the design of PointShopAR on the practical needs of six architects in a formative study. Our system supports a variety of point cloud editing operations in AR, including selection, transformation, hole filling, drawing, morphing, and animation. We evaluate PointShopAR through a remote study on usability and an in-person study on environmental design support. Participants were able to iterate design rapidly, showing the merits of an integrated capture and editing workflow with point clouds in AR environmental design.",AR design; capture and editing; point cloud,Title,
ACM DL,conferencePaper,2023,User-Driven Constraints for Layout Optimisation in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Automatic layout optimisation allows users to arrange augmented reality content in the real-world environment without the need for tedious manual interactions. This optimisation is often based on modelling the intended content placement as constraints, defined as cost functions. Then, applying a cost minimization algorithm leads to a desirable placement. However, such an approach is limited by the lack of user control over the optimisation results. In this paper we explore the concept of user-driven constraints for augmented reality layout optimisation. With our approach users can define and set up their own constraints directly within the real-world environment. We first present a design space composed of three dimensions: the constraints, the regions of interest and the constraint parameters. Then we explore which input gestures can be employed to define the user-driven constraints of our design space through a user elicitation study. Using the results of the study, we propose a holistic system design and implementation demonstrating our user-driven constraints, which we evaluate in a final user study where participants had to create several constraints at the same time to arrange a set of virtual contents.",,Abstract_Title,
ACM DL,conferencePaper,2023,VRGit: A Version Control System for Collaborative Content Creation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Immersive authoring tools allow users to intuitively create and manipulate 3D scenes while immersed in Virtual Reality (VR). Collaboratively designing these scenes is a creative process that involves numerous edits, explorations of design alternatives, and frequent communication with collaborators. Version Control Systems (VCSs) help users achieve this by keeping track of the version history and creating a shared hub for communication. However, most VCSs are unsuitable for managing the version history of VR content because their underlying line differencing mechanism is designed for text and lacks the semantic information of 3D content; and the widely adopted commit model is designed for asynchronous collaboration rather than real-time awareness and communication in VR. We introduce VRGit, a new collaborative VCS that visualizes version history as a directed graph composed of 3D miniatures, and enables users to easily navigate versions, create branches, as well as preview and reuse versions directly in VR. Beyond individual uses, VRGit also facilitates synchronous collaboration in VR by providing awareness of users’ activities and version history through portals and shared history visualizations. In a lab study with 14 participants (seven groups), we demonstrate that VRGit enables users to easily manage version history both individually and collaboratively in VR.",Collaboration; Version Control System; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Full-hand Electro-Tactile Feedback without Obstructing Palmar Side of Hand,CHI - Human Factors in Computing Systems,A*,"We present a technique to render tactile feedback to the palmar side of the hand while keeping it unobstructed and, thus, preserving manual dexterity during interactions with physical objects. We implement this by applying electro-tactile stimulation only to the back of the hand and to the wrist. In our approach, there are no electrodes on the palmar side, yet that is where tactile sensations are felt. While we place electrodes outside the user's palm, we do so in strategic locations that conduct the electrical currents to the median/ulnar nerves, causing tactile sensations on the palmar side of the hand. In our user studies, we demonstrated that our approach renders tactile sensations to 11 different locations on the palmar side while keeping users’ palms free for dexterous manipulations. Our approach enables new applications such as tactile notifications during dexterous activities or VR experiences that rely heavily on physical props.",Electro-tactile; Haptics; Mixed Reality; Virtual Reality,Keywords,
ACM DL,conferencePaper,2023,Generating Haptic Motion Effects for Multiple Articulated Bodies for Improved 4D Experiences: A Camera Space Approach,CHI - Human Factors in Computing Systems,A*,"Motion effects are indispensable for improving 4D experiences in highly interactive applications, such as amusement parks, 4D theaters, and virtual reality games. Their recent emergence calls for effective algorithms generating motion effects synchronized with audiovisual content. This paper presents an automatic algorithm for synthesizing the object-based motion effects that express the movements of multiple articulated bodies inclusively when the objects’ motion trajectories are available in the 3D camera space. By taking the visual velocities and sizes of all object parts, our method computes a motion proxy that represents the objects’ movements by one point and converts the motion proxy to a motion command through a motion cueing algorithm. The motion proxy is determined by linearly combining the velocities, and its best combination was selected from several candidates by user studies. The results of user studies indicate that our algorithm can produce compelling object-based motion effects that enhance the multisensory experience.",4D; articulated body; automatic generation; haptics; motion cueing; motion effects; mulsemedia; multiple sensorial media; synthesis; vestibular sense; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2023,JumpMod: Haptic Backpack that Modifies Users’ Perceived Jump,CHI - Human Factors in Computing Systems,A*,"Vertical force-feedback is extremely rare in mainstream interactive experiences. This happens because existing haptic devices capable of sufficiently strong forces that would modify a user's jump require grounding (e.g., motion platforms or pulleys) or cumbersome actuators (e.g., large propellers attached or held by the user). To enable interactive experiences to feature jump-based haptics without sacrificing wearability, we propose JumpMod, an untethered backpack that modifies one's sense of jumping. JumpMod achieves this by moving a weight up/down along the user's back, which modifies perceived jump momentum—creating accelerated &amp; decelerated jump sensations. In our second study, we empirically found that our device can render five effects: jump higher, land harder/softer, pulled higher/lower. Based on these, we designed four jumping experiences for VR &amp; sports. Finally, in our third study, we found that participants preferred wearing our device in an interactive context, such as one of our jump-based VR applications.",backpack; full-body; haptics; jumping; virtual reality; wearable,Keywords,
ACM DL,conferencePaper,2023,The Effects of Body Location and Biosignal Feedback Modality on Performance and Workload Using Electromyography in Virtual&nbsp;Reality,CHI - Human Factors in Computing Systems,A*,"Using biosignals through electromyography (EMG) and rendering them as feedback for hands-free interaction finally migrates to engaging virtual reality (VR) experiences for health and fitness-related applications. Previous work proposes various body locations as input sources and different output modalities for creating effective biofeedback loops. However, it is currently unknown which muscles and sensory modalities can provide optimal real-time interaction regarding the performance and perceived workload of the users. In two VR studies (N=18 and N=40) based on a Fitts’ law target selection task, we explored sensor placement at different body locations and investigate auditory, tactile, and visual feedback modalities. Objective and subjective results indicate that input performance can be improved by presenting muscle tension as simultaneous tactile and visual feedback. We contribute with recommendations for registration of isometric muscle contraction at different body locations and conclude that reproducing physiological feedback through multimodal channels can assist users interacting with EMG devices.",Accessibility; Biofeedback; Electromyography; Physiological Sensing; Virtual Reality,Abstract_Keywords,
ACM DL,conferencePaper,2023,Collaborating Across Realities: Analytical Lenses for Understanding Dyadic Collaboration in Transitional Interfaces,CHI - Human Factors in Computing Systems,A*,"Transitional Interfaces are a yet underexplored, emerging class of cross-reality user interfaces that enable users to freely move along the reality-virtuality continuum during collaboration. To analyze and understand how such collaboration unfolds, we propose four analytical lenses derived from an exploratory study of transitional collaboration with 15 dyads. While solving a complex spatial optimization task, participants could freely switch between three contexts, each with different displays (desktop screens, tablet-based augmented reality, head-mounted virtual reality), input techniques (mouse, touch, handheld controllers), and visual representations (monoscopic and allocentric 2D/3D maps, stereoscopic egocentric views). Using the rich qualitative and quantitative data from our study, we evaluated participants’ perceptions of transitional collaboration and identified commonalities and differences between dyads. We then derived four lenses including metrics and visualizations to analyze key aspects of transitional collaboration: (1) place and distance, (2) temporal patterns, (3) group use of contexts, (4) individual use of contexts.",analytical lenses; transitional collaboration; transitional interfaces; user study,Abstract,
ACM DL,conferencePaper,2023,Eliciting Security &amp; Privacy-Informed Sharing Techniques for Multi-User Augmented Reality,CHI - Human Factors in Computing Systems,A*,"The HCI community has explored new interaction designs for collaborative AR interfaces in terms of usability and feasibility; however, security &amp; privacy (S&amp;P) are often not considered in the design process and left to S&amp;P professionals. To produce interaction proposals with S&amp;P in mind, we extend the user-driven elicitation method with a scenario-based approach that incorporates a threat model involving access control in multi-user AR. We conducted an elicitation study in two conditions, pairing AR/AR experts in one condition and AR/S&amp;P experts in the other, to investigate the impact of each pairing. We contribute a set of expert-elicited interactions for sharing AR content enhanced with access control provisions, analyze the benefits and tradeoffs of pairing AR and S&amp;P experts, and present recommendations for designing future multi-user AR interactions that better balance competing design goals of usability, feasibility, and S&amp;P in collaborative AR.",elicitation studies; threat modeling,Title,
ACM DL,conferencePaper,2023,Evaluating the Extension of Wall Displays with AR for Collaborative Work,CHI - Human Factors in Computing Systems,A*,"Wall displays are well suited for collaborative work and are often placed in rooms with ample space in front of them that remains largely unused. Augmented Reality (AR) headsets can seamlessly extend the collaboration space around the Wall. Nevertheless, it is unclear if extending Walls with AR is effective and how it may affect collaboration. We first present a prototype combining a Wall and AR headsets to extend the Wall workspace. We then use this prototype to study how users utilize the virtual space created in AR. In an experiment with 24 participants, we compare how pairs solve collaborative tasks with the Wall alone and with Wall+AR. Our qualitative and quantitative results highlight that with Wall+AR, participants use the physical space in front and around the Wall extensively, and while this creates interaction overhead, it does not impact performance and improves the user experience.",Augmented Reality; Collaboration; Empirical Study; Wall Display,Abstract_Keywords,
ACM DL,conferencePaper,2023,Social Virtual Reality as a Mental Health Tool: How People Use VRChat to Support Social Connectedness and Wellbeing,CHI - Human Factors in Computing Systems,A*,"Social virtual reality (VR) platforms have increased in popularity with many people turning to these platforms to experience social connection, including a rapid influx of users during the COVID-19 pandemic. However, there is limited understanding of how people appropriate and use emerging social VR applications to actively support their mental health and wellbeing in daily life. Through an online questionnaire and exploratory interviews conducted within the social VR app VRChat during the COVID-19 pandemic, we document how social VR is being used explicitly as a mental health support tool. Participants reported positive wellbeing benefits, mostly attributed to the anonymity provided by avatars and perceived safety within digital worlds and communities of practice. We also report how people use social VR to practice social interaction, reduce negative thoughts and form strong social bonds and connections with others.",,Abstract_Title,
ACM DL,conferencePaper,2023,Volumetric Mixed Reality Telepresence for Real-time Cross Modality Collaboration,CHI - Human Factors in Computing Systems,A*,"Mixed-reality telepresence allows local and remote users feel as if they are present together in the same space. In this paper we report on a mixed-reality volumetric telepresence system that is adaptable, multi-user and cross-modal, i.e. combining augmented and virtual reality technologies with face-to-face interactions. The system extends state-of-art by creating full-body and environmental volumetric renderings in real-time over local enterprise networks. We report findings of an evaluation in a training scenario which was adapted for remote delivery and led by an industry professional. Analysis of interviews and observed behaviours identify varying attitudes towards virtually mediated full-body experiences and highlight the impact of volumetric mixed-reality telepresence to facilitate personal experiences of co-presence and to ground communication with interlocutors.",augmented reality; avatars; collaboration; conversational grounding; Extended reality; telepresence; virtual reality; volumetric capture,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Wish You Were Here: Mental and Physiological Effects of Remote Music Collaboration in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"With face-to-face music collaboration being severely limited during the recent pandemic, mixed reality technologies and their potential to provide musicians a feeling of ""being there"" with their musical partner can offer tremendous opportunities. In order to assess this potential, we conducted a laboratory study in which musicians made music together in real-time while simultaneously seeing their jamming partner’s mixed reality point cloud via a head-mounted display and compared mental effects such as flow, affect, and co-presence to an audio-only baseline. In addition, we tracked the musicians’ physiological signals and evaluated their features during times of self-reported flow. For users jamming in mixed reality, we observed a significant increase in co-presence. Regardless of the condition (mixed reality or audio-only), we observed an increase in positive affect after jamming remotely. Furthermore, we identified heart rate and HF/LF as promising features for classifying the flow state musicians experienced while making music together.",Augmented Reality; Co-Presence; Head-mounted Displays; Mixed Reality; Networked Music Performance; Physiological Signal Processing; Psychophysiology; Remote Collaboration; Social Presence,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Visual Captions: Augmenting Verbal Communication with On-the-fly Visuals,CHI - Human Factors in Computing Systems,A*,"Video conferencing solutions like Zoom, Google Meet, and Microsoft Teams are becoming increasingly popular for facilitating conversations, and recent advancements such as live captioning help people better understand each other. We believe that the addition of visuals based on the context of conversations could further improve comprehension of complex or unfamiliar concepts. To explore the potential of such capabilities, we conducted a formative study through remote interviews (N=10) and crowdsourced a dataset of over 1500 sentence-visual pairs across a wide range of contexts. These insights informed Visual Captions, a real-time system that integrates with a video conferencing platform to enrich verbal communication. Visual Captions leverages a fine-tuned large language model to proactively suggest relevant visuals in open-vocabulary conversations. We present findings from a lab study (N=26) and an in-the-wild case study (N=10), demonstrating how Visual Captions can help improve communication through visual augmentation in various scenarios.",AI agent; augmented communication; augmented reality; collaborative work; dataset; large language models; online meeting; text-to-visual; video-mediated communication,Keywords,
ACM DL,conferencePaper,2023,Here and Now: Creating Improvisational Dance Movements with a Mixed Reality Mirror,CHI - Human Factors in Computing Systems,A*,"This paper explores using mixed reality (MR) mirrors for supporting improvisational dance making. Motivated by the prevalence of mirrors in dance studios and inspired by Forsythe’s Improvisation Technologies, we conducted workshops with 13 dancers and choreographers to inform the design of future MR visualisation and annotation tools for dance. The workshops involved using a prototype MR mirror as a technology probe that reveals the spatial and temporal relationships between the reflected dancing body and its surroundings during improvisation; speed dating group interviews around future design ideas; follow-up surveys and extended interviews with a digital media dance artist and a dance educator. Our findings highlight how the MR mirror enriches dancers’ temporal and spatial perception, creates multi-layered presence, and affords appropriation by dancers. We also discuss the unique place of MR mirrors in the theoretical context of dance and in the history of movement visualisation, and distil lessons for broader HCI research.",augmented reality; dance; improvisation; mirror; mixed reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,transPAF: Rendering Omnidirectional Impact Feedback with Dynamic Point of Application of Force All Round a Controller,CHI - Human Factors in Computing Systems,A*,"Impact force is common haptic feedback on virtual reality (VR) controllers, such as hitting objects with weapons or rackets. It applies to different points of application of force (PAFs) and directions in varied scenarios. For example, using weapons with different shapes, e.g.,&nbsp;a sword and a pickaxe, using a weapon in different ways, e.g.,&nbsp;stabbing and slashing with a sword, or a ball flying and hitting a racket in different directions cause different PAFs and/or force directions. Therefore, to achieve realistic VR experiences, rendering dynamic PAF and force direction is essential. Although previous works have proposed the concept of dynamic PAF, the PAF is only in a limited space and without dynamic force direction. Therefore, we propose a controller, transPAF, to render omnidirectional impact feedback with dynamic PAF all round the controller for versatile VR scenarios. transPAF consists of a controller, a semicircular track, a linear track, and an impactor, which are all rotatable. The impactor can move to any position in a sphere, which means the whole 3D space all round the controller, and rotate in any direction. Therefore, dynamic PAF and force direction are achieved and independent to each other. We conducted a just-noticeable difference (JND) study to understand users’ distinguishability in position and direction, separately. A VR experience study was further performed to verify that feedback from transPAF with dynamic PAF and force direction enhances the VR experiences and demonstrate some applications for transPAF.",Haptic feedback; impact force feedback; point of application of force; virtual reality.,Abstract_Keywords,
ACM DL,conferencePaper,2023,XAIR: A Framework of Explainable AI in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Explainable AI (XAI) has established itself as an important component of AI-driven interactive systems. With Augmented Reality (AR) becoming more integrated in daily lives, the role of XAI also becomes essential in AR because end-users will frequently interact with intelligent services. However, it is unclear how to design effective XAI experiences for AR. We propose XAIR, a design framework that addresses when, what, and how to provide explanations of AI output in AR. The framework was based on a multi-disciplinary literature review of XAI and HCI research, a large-scale survey probing 500+ end-users’ preferences for AR-based explanations, and three workshops with 12 experts collecting their insights about XAI design in AR. XAIR’s utility and effectiveness was verified via a study with 10 designers and another study with 12 end-users. XAIR can provide guidelines for designers, inspiring them to identify new design opportunities and achieve effective XAI designs in AR.",Augmented Reality; Design Framework; Explainable AI,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,ChameleonControl: Teleoperating Real Human Surrogates through Mixed Reality Gestural Guidance for Remote Hands-on Classrooms,CHI - Human Factors in Computing Systems,A*,"We present ChameleonControl, a real-human teleoperation system for scalable remote instruction in hands-on classrooms. In contrast to existing video or AR/VR-based remote hands-on education, ChameleonControl uses a real human as a surrogate of a remote instructor. Building on existing human-based telepresence approaches, we contribute a novel method to teleoperate a human surrogate through synchronized mixed reality hand gestural navigation and verbal communication. By overlaying the remote instructor’s virtual hands in the local user’s MR view, the remote instructor can guide and control the local user as if they were physically present. This allows the local user/surrogate to synchronize their hand movements and gestures with the remote instructor, effectively teleoperating a real human. We deploy and evaluate our system in classrooms of physiotherapy training, as well as other application domains such as mechanical assembly, sign language and cooking lessons. The study results confirm that our approach can increase engagement and the sense of co-presence, showing potential for the future of remote hands-on classrooms.",Hands-on Training; Human Surrogates; Mixed Reality; Remote Collaboration; Remote Guidance; Telepresence; Visual Cue,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Investigating Eyes-away Mid-air Typing in Virtual Reality using Squeeze haptics-based Postural Reinforcement,CHI - Human Factors in Computing Systems,A*,"In this paper, we investigate postural reinforcement haptics for mid-air typing using squeeze actuation on the wrist. We propose and validate eye-tracking based objective metrics that capture the impact of haptics on the user’s experience, which traditional performance metrics like speed and accuracy are not able to capture. To this end, we design four wrist-based haptic feedback conditions: no haptics, vibrations on keypress, squeeze+vibrations on keypress, and squeeze posture reinforcement + vibrations on keypress. We conduct a text input study with 48 participants to compare the four conditions on typing and gaze metrics. Our results show that for expert qwerty users, posture reinforcement haptics significantly benefit typing by reducing the visual attention on the keyboard by up to 44% relative to no haptics, thus enabling eyes-away behaviors.",,Title,
ACM DL,conferencePaper,2023,The Effects of Avatar and Environment on Thermal Perception and Skin Temperature in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Humans’ thermal regulation and subjective perception of temperature is highly plastic and depends on the visual appearance of the surrounding environment. Previous work shows that an environment’s color temperature affects the experienced temperature. As virtual reality (VR) enables visual immersion, recent work suggests that a VR scene’s color temperature also affects experienced temperature. It is, however, unclear if an avatar’s appearance also affects users’ thermal perception and if a change in thermal perception even influences the body temperature. Therefore, we conducted a study with 32 participants performing a task in an ice or fire world while having ice or fire hands. We show that being in a fire world or having fire hands increases the perceived temperature. We even show that having fire hands decreases the hand temperature compared to having ice hands. We discuss the implications for the design of VR systems and future research directions.",embodiment; skin temperature; thermal perception; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,The Benefits of Passive Haptics and Perceptual Manipulation for Extended Reality Interactions in Constrained Passenger Spaces,CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) technology brings exciting possibilities for aeroplane passengers, allowing them to escape their limited cabin space. Using nearby physical surfaces enables a connection with the real world while improving the XR experience through touch. However, available surfaces may be located in awkward positions, reducing comfort and input performance and thus limiting their long-term use. We explore the usability of passive haptic surfaces in different orientations, assessing their effects on input performance, user experience and comfort. We then overcome ergonomic issues caused by the confined space by using perceptual manipulation techniques that remap the position and rotation of physical surfaces and user movements, assessing their effects on task workload, comfort and presence. Our results show that the challenges posed by constrained seating environments can be overcome by a combination of passive haptics and remapping the workspace with moderate translation and rotation manipulations. These manipulations allow for good input performance, low workload and comfortable interaction, opening up XR use while in transit.",3D User Interfaces; aeroplane; airplane; confined spaces; passive haptics; selection; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,"UndoPort: Exploring the Influence of Undo-Actions for Locomotion in Virtual Reality on the Efficiency, Spatial Understanding and User Experience",CHI - Human Factors in Computing Systems,A*,"When we get lost in Virtual Reality (VR) or want to return to a previous location, we use the same methods of locomotion for the way back as for the way forward. This is time-consuming and requires additional physical orientation changes, increasing the risk of getting tangled in the headsets’ cables. In this paper, we propose the use of undo actions to revert locomotion steps in VR. We explore eight different variations of undo actions as extensions of point&amp;teleport, based on the possibility to undo position and orientation changes together with two different visualizations of the undo step (discrete and continuous). We contribute the results of a controlled experiment with 24 participants investigating the efficiency and orientation of the undo techniques in a radial maze task. We found that the combination of position and orientation undo together with a discrete visualization resulted in the highest efficiency without increasing orientation errors.",Locomotion; Teleport; Undo; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,"How Space is Told: Linking Trajectory, Narrative, and Intent in Augmented Reality Storytelling for Cultural Heritage Sites",CHI - Human Factors in Computing Systems,A*,"We report on a qualitative study in which 22 participants created Augmented Reality (AR) stories for outdoor cultural heritage sites. As storytelling is a crucial strategy for AR content aimed at providing meaningful experiences, the emphasis has been on what storytelling does, rather than how it is done, the end user’s needs prioritized over the author’s. To address this imbalance, we identify how recurring patterns in the spatial trajectories and narrative compositions of AR stories for cultural heritage sites are linked to the author’s intent and creative process: While authors tend to bind story arcs tightly to confined trajectories for narrative delivery, the need for spatial exploration results in thematic content mapped loosely onto encompassing trajectories. Based on our analysis, we present design recommendations for site-specific AR storytelling tools that can support authors in delivering their intent while leveraging the placeness of cultural heritage sites as a creative resource.",Augmented Reality; authoring tools; cultural heritage; Mixed Reality; storytelling,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,A Fitts’ Law Study of Gaze-Hand Alignment for Selection in 3D User Interfaces,CHI - Human Factors in Computing Systems,A*,"Gaze-Hand Alignment has recently been proposed for multimodal selection in 3D. The technique takes advantage of gaze for target pre-selection, as it naturally precedes manual input. Selection is then completed when manual input aligns with gaze on the target, without need for an additional click method. In this work we evaluate two alignment techniques, Gaze&amp;Finger and Gaze&amp;Handray, combining gaze with image plane pointing versus raycasting, in comparison with hands-only baselines and Gaze&amp;Pinch as established multimodal technique. We used Fitts’ Law study design with targets presented at different depths in the visual scene, to assess effect of parallax on performance. The alignment techniques outperformed their respective hands-only baselines. Gaze&amp;Finger is efficient when targets are close to the image plane but less performant with increasing target depth due to parallax.",augmented reality; gaze interaction; eye-tracking; mid-air gestures; menu selection; pointing,Keywords,
ACM DL,conferencePaper,2023,Classifying Head Movements to Separate Head-Gaze and Head Gestures as Distinct Modes of Input,CHI - Human Factors in Computing Systems,A*,"Head movement is widely used as a uniform type of input for human-computer interaction. However, there are fundamental differences between head movements coupled with gaze in support of our visual system, and head movements performed as gestural expression. Both Head-Gaze and Head Gestures are of utility for interaction but differ in their affordances. To facilitate the treatment of Head-Gaze and Head Gestures as separate types of input, we developed HeadBoost as a novel classifier, achieving high accuracy in classifying gaze-driven versus gestural head movement (F1-Score: 0.89). We demonstrate the utility of the classifier with three applications: gestural input while avoiding unintentional input by Head-Gaze; target selection with Head-Gaze while avoiding Midas Touch by head gestures; and switching of cursor control between Head-Gaze for fast positioning and Head Gesture for refinement. The classification of Head-Gaze and Head Gesture allows for seamless head-based interaction while avoiding false activation.",Virtual Reality; Machine Learning; Eye Tracking; Computational Interaction; Eye-head Coordination; Head Gestures; XGBoost,Keywords,
ACM DL,conferencePaper,2023,Embodying Physics-Aware Avatars in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Embodiment toward an avatar in virtual reality (VR) is generally stronger when there is a high degree of alignment between the user’s and self-avatar’s motion. However, one-to-one mapping between the two is not always ideal when user interacts with the virtual environment. On these occasions, the user input often leads to unnatural behavior without physical realism (e.g., objects penetrating virtual body, body unmoved by hitting stimuli). We investigate how adding physics correction to self-avatar motion impacts embodiment. Physics-aware self-avatar preserves the physical meaning of the movement but introduces discrepancies between the user’s and self-avatar’s motion, whose contingency is a determining factor for embodiment. To understand its impact, we conducted an in-lab study (n = 20) where participants interacted with obstacles on their upper bodies in VR with and without physics correction. Our results showed that, rather than compromising embodiment level, physics-responsive self-avatar improved embodiment compared to no-physics condition in both active and passive interactions.",Virtual Reality; Embodiment; Physics Avatars,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Induce a Blink of the Eye: Evaluating Techniques for Triggering Eye Blinks in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"As more and more virtual reality (VR) headsets support eye tracking, recent techniques started to use eye blinks to induce unnoticeable manipulations to the virtual environment, e.g., to redirect users’ actions. However, to exploit their full potential, more control over users’ blinking behavior in VR is required. To this end, we propose a set of reflex-based blink triggers that are suited specifically for VR. In accordance with blink-based techniques for redirection, we formulate (i) effectiveness, (ii) efficiency, (iii) reliability, and (iv) unobtrusiveness as central requirements for successful triggers. We implement the soft- and hardware-based methods and compare the four most promising approaches in a user study. Our results highlight the pros and cons of the tested triggers, and show those based on the menace, corneal, and dazzle reflexes to perform best. From these results, we derive recommendations that help choosing suitable blink triggers for VR applications.",virtual reality; blink triggers; change blindness; eye blinks,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Vergence Matching: Inferring Attention to Objects in 3D Environments for Gaze-Assisted Selection,CHI - Human Factors in Computing Systems,A*,"Gaze pointing is the de facto standard to infer attention and interact in 3D environments but is limited by motor and sensor limitations. To circumvent these limitations, we propose a vergence-based motion correlation method to detect visual attention toward very small targets. Smooth depth movements relative to the user are induced on 3D objects, which cause slow vergence eye movements when looked upon. Using the principle of motion correlation, the depth movements of the object and vergence eye movements are matched to determine which object the user is focussing on. In two user studies, we demonstrate how the technique can reliably infer gaze attention on very small targets, systematically explore how different stimulus motions affect attention detection, and show how the technique can be extended to multi-target selection. Finally, we provide example applications using the concept and design guidelines for small target and accuracy-independent attention detection in 3D environments.",Virtual Reality; Attention Detection; Gaze; Motion Correlation; Selection; Small Targets; Vergence,Keywords,
ACM DL,conferencePaper,2023,PumpVR: Rendering the Weight of Objects and Avatars through Liquid Mass Transfer in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Perceiving objects’ and avatars’ weight in Virtual Reality (VR) is important to understand their properties and naturally interact with them. However, commercial VR controllers cannot render weight. Controllers presented by previous work are single-handed, slow, or only render a small mass. In this paper, we present PumpVR that renders weight by varying the controllers’ mass according to the properties of virtual objects or bodies. Using a bi-directional pump and solenoid valves, the system changes the controllers’ absolute weight by transferring water in or out with an average error of less than 5%. We implemented VR use cases with objects and avatars of different weights to compare the system with standard controllers. A study with 24 participants revealed significantly higher realism and enjoyment when using PumpVR to interact with virtual objects. Using the system to render body weight had significant effects on virtual embodiment, perceived exertion, and self-perceived fitness.",virtual reality; haptic controllers; virtual embodiment; weight interface; weight perception,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,“I normally wouldn’t talk with strangers”: Introducing a Socio-Spatial Interface for Fostering Togetherness Between Strangers,CHI - Human Factors in Computing Systems,A*,"Interacting with strangers can be beneficial but also challenging. Fortunately, these challenges can lead to design opportunities. In this paper, we present the design and evaluation of a socio-spatial interface, SocialStools, that leverages the human propensity for embodied interaction to foster togetherness between strangers. SocialStools is an installation of three responsive stools on caster wheels that generate sound and imagery in the near environment as three strangers sit on them, move them, and rotate them relative to each other. In our study with 12 groups of three strangers, we found a sense of togetherness emerged through interaction, evidenced by different patterns of socio-spatial movements, verbal communication, non-verbal behavior, and interview responses. We present our findings, articulate reasons for the cultivation of togetherness, consider the unique social affordances of our spatial interface in shifting attention during interpersonal communication, and provide design implications. This research contributes insights toward designing cyber-physical interfaces that foster interaction and togetherness among strangers at a time when cultivating togetherness is especially critical.",Mixed Reality; Embodied Interaction; Proxemics; Small Groups; Socio-Spatial Interface,Keywords,
ACM DL,conferencePaper,2023,Understanding Context to Capture when Reconstructing Meaningful Spaces for Remote Instruction and Connecting in XR,CHI - Human Factors in Computing Systems,A*,"Recent technological advances are enabling HCI researchers to explore interaction possibilities for remote XR collaboration using high-fidelity reconstructions of physical activity spaces. However, creating these reconstructions often lacks user involvement with an overt focus on capturing sensory context that does not necessarily augment an informal social experience. This work seeks to understand social context that can be important for reconstruction to enable XR applications for informal instructional scenarios. Our study involved the evaluation of an XR remote guidance prototype by 8 intergenerational groups of closely related gardeners using reconstructions of personally meaningful spaces in their gardens. Our findings contextualize physical objects and areas with various motivations related to gardening and detail perceptions of XR that might affect the use of reconstructions for remote interaction. We discuss implications for user involvement to create reconstructions that better translate real-world experience, encourage reflection, incorporate privacy considerations, and preserve shared experiences with XR as a medium for informal intergenerational activities.",metaverse; Extended Reality; 3D reconstruction; contextual capture; gardening; hobby activities; intergenerational study; remote instruction,Keywords,
ACM DL,conferencePaper,2023,Predicting Gaze-based Target Selection in Augmented Reality Headsets based on Eye and Head Endpoint Distributions,CHI - Human Factors in Computing Systems,A*,"Target selection is a fundamental task in interactive Augmented Reality (AR) systems. Predicting the intended target of selection in such systems can provide users with a smooth, low-friction interaction experience. Our work aims to predict gaze-based target selection in AR headsets with eye and head endpoint distributions, which describe the probability distribution of eye and head 3D orientation when a user triggers a selection input. We first conducted a user study to collect users’ eye and head behavior in a gaze-based pointing selection task with two confirmation mechanisms (air tap and blinking). Based on the study results, we then built two models: a unimodal model using only eye endpoints and a multimodal model using both eye and head endpoints. Results from a second user study showed that the pointing accuracy is improved by approximately 32% after integrating our models into gaze-based selection techniques.",Augmented reality; error prediction; eye input; selection modeling; target selection,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Resemblance of religion and pervasive games: A study among church employees and gamers,CHI - Human Factors in Computing Systems,A*,"Previous research suggests that the experience and practices related to gaming and extended realities, and religion and spiritualism, share similarities. In this study, we explore how both the employees of the Evangelical Lutheran Church (n=156) and pervasive game players (n=98) perceive and make sense of these connections. We approach the qualitative data from the perspective of Durkheim, who, similarly to how game theorists view games, views religion as a multi-faceted system that incorporates the rules, practices and communities that comprise the religion. From the data emerges the following prominent connection as perceived by both groups of informants: systems of (1) shared premise, (2) resilience and restoration, (3) symbolism, (4) extended reality and (5) day-to-day structuring. A numerical view of the data shows that 42,5% of the participants did not perceive similarities, and examination of these responses suggested that while religion and pervasive games share functional similarities, they are further apart from a substantive perspective.",Augmented Reality; Gamification; Metaverse; Video games; Pervasive games; Religion; Techno-Spirituality,Abstract_Keywords,
ACM DL,conferencePaper,2023,Using Virtual Reality to Shape Humanity’s Return to the Moon: Key Takeaways from a Design Study,CHI - Human Factors in Computing Systems,A*,"Revived interest in lunar exploration is heralding a new generation of design solutions in support of human operations on the Moon. While space system design has traditionally been guided by prototype deployments in analogue studies, the resource-intensive nature of this approach has largely precluded application of proficient user-centered design (UCD) methods from human-computer interaction (HCI). This paper explores possible use of Virtual Reality (VR) to simulate analogue studies in lab settings and thereby bring to bear UCD in this otherwise engineering-dominated field. Drawing on the ongoing development of the European Large Logistics Lander, we have recreated a prospective lunar operational scenario in VR and evaluated it with a group of astronauts and space experts (n=20). Our qualitative findings demonstrate the efficacy of VR in facilitating UCD, enabling efficient contextual inquiries and improving project team coordination. We conclude by proposing future directions to further exploit VR in lunar systems design.",Virtual Reality; user centered design; HCI research; ergonomics; human factors; lunar lander; space system engineering,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,"Grab It, While You Can: A VR Gesture Evaluation of a Co-Designed Traditional Narrative by Indigenous People",CHI - Human Factors in Computing Systems,A*,"Recent developments in Virtual Reality (VR) applications, such as hand gesture tracking, provide new opportunities to create embodied user experiences. Numerous gesture elicitation studies have been conducted. However, in most instances they lack validation of implemented gestures, as well diversity of participant groups. Our research explores the digitalization of intangible cultural heritage in collaboration with one of the San tribes in Southern Africa. The focus is on particular gestures as embodied interactions of a VR implementation of a traditional San hunting story. In this paper, we present a gesture study, which entails an in-situ elicitation of natural gestures, a co-designed integration, a VR story implementation with grasping and three mid-air gestures, and a user evaluation. Based on our findings, we discuss the anthropological value of gesture implementations determined by an indigenous community, the local usability of a grasping gesture, and in-VR gesture elicitation, as an extension of existing methods.",Virtual Reality; Namibia; Diversity; Community; Hand Gestures; Indigenous People; Natural Interaction; User Experiences,Abstract_Keywords,
ACM DL,conferencePaper,2023,HandAvatar: Embodying Non-Humanoid Virtual Avatars through Hands,CHI - Human Factors in Computing Systems,A*,"We propose HandAvatar to enable users to embody non-humanoid avatars using their hands. HandAvatar leverages the high dexterity and coordination of users’ hands to control virtual avatars, enabled through our novel approach for automatically-generated joint-to-joint mappings. We contribute an observation study to understand users’ preferences on hand-to-avatar mappings on eight avatars. Leveraging insights from the study, we present an automated approach that generates mappings between users’ hands and arbitrary virtual avatars by jointly optimizing control precision, structural similarity, and comfort. We evaluated HandAvatar on static posing, dynamic animation, and creative exploration tasks. Results indicate that HandAvatar enables more precise control, requires less physical effort, and brings comparable embodiment compared to a state-of-the-art body-to-avatar control method. We demonstrate HandAvatar’s potential with applications including non-humanoid avatar based social interaction in VR, 3D animation composition, and VR scene design with physical proxies. We believe that HandAvatar unlocks new interaction opportunities, especially for usage in Virtual Reality, by letting users become the avatar in applications including virtual social interaction, animation, gaming, or education.",Mixed Reality; embodiment; virtual avatar; gestural interaction,Abstract_Keywords,
ACM DL,conferencePaper,2023,HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using Inertial Sensing,CHI - Human Factors in Computing Systems,A*,"Current Virtual Reality systems are designed for interaction under visual control. Using built-in cameras, headsets track the user’s hands or hand-held controllers while they are inside the field of view. Current systems thus ignore the user’s interaction with off-screen content—virtual objects that the user could quickly access through proprioception without requiring laborious head motions to bring them into focus. In this paper, we present HOOV, a wrist-worn sensing method that allows VR users to interact with objects outside their field of view. Based on the signals of a single wrist-worn inertial sensor, HOOV continuously estimates the user’s hand position in 3-space to complement the headset’s tracking as the hands leave the tracking range. Our novel data-driven method predicts hand positions and trajectories from just the continuous estimation of hand orientation, which by itself is stable based solely on inertial observations. Our inertial sensing simultaneously detects finger pinching to register off-screen selection events, confirms them using a haptic actuator inside our wrist device, and thus allows users to select, grab, and drop virtual content. We compared HOOV’s performance with a camera-based optical motion capture system in two folds. In the first evaluation, participants interacted based on tracking information from the motion capture system to assess the accuracy of their proprioceptive input, whereas in the second, they interacted based on HOOV’s real-time estimations. We found that HOOV’s target-agnostic estimations had a mean tracking error of 7.7&nbsp;cm, which allowed participants to reliably access virtual objects around their body without first bringing them into focus. We demonstrate several applications that leverage the larger input space HOOV opens up for quick proprioceptive interaction, and conclude by discussing the potential of our technique.",Virtual Reality; Eyes-free Interaction; Hand Tracking; Inertial Sensing; Inertial Tracking; Proprioceptive Interaction; Sensor Fusion,Abstract_Keywords,
ACM DL,conferencePaper,2023,SelVReflect: A Guided VR Experience Fostering Reflection on Personal Challenges,CHI - Human Factors in Computing Systems,A*,"Reflecting on personal challenges can be difficult. Without encouragement, the reflection process often remains superficial, thus inhibiting deeper understanding and learning from past experiences. To allow people to immerse themselves in and deeply reflect on past challenges, we developed SelVReflect, a VR experience which offers active voice-based guidance and a space to freely express oneself. SelVReflect was developed in an iterative design process (N=5) and evaluated in a user study with N=20 participants. We found that SelVReflect enabled participants to approach their challenge and its (emotional) components from different perspectives and to discover new relationships between these components. By making use of the spatial possibilities in VR, participants developed a better understanding of the situation and of themselves. We contribute empirical evidence of how a guided VR experience can support reflection. We discuss opportunities and design requirements for guided VR experiences that aim to foster deeper reflection.",Virtual Reality; Emotion; Reflection; Creativity; Expression; Guidance; Self-care; Well-being,Keywords,
ACM DL,conferencePaper,2023,Supporting Collaborative Discussions In Surgical Teleconsulting Through Augmented Reality Head Mounted Displays,CHI - Human Factors in Computing Systems,A*,"Although Augmented Reality (AR) has been touted as the future of surgery, its contribution to distributed collaboration such as in surgical teleconsulting has not been articulated. We propose AR-Head Mounted Displays (AR-HMD) to tackle two previously-identified challenges: operating surgeons needing to view and interact with imaging systems that reside away from the operative field, and, their lack of gesturing tools to point and annotate on the shared images and physical environment. We report on a controlled lab experiment where 12 expert gynecology surgeons perform a tumor localisation task guided by a remote radiologist (confederate) via an AR-HMD. We find that bringing the shared images to the place of work reduces the need for clarifications and provides opportunistic access to information when required, and, that pointing and annotating provides opportunities to further support verbal instruction in deictic communication. Our results inform the design of intraoperative AR-HMD systems for surgical telecollaboration.",augmented reality; remote collaboration; teleconsulting,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,AutoVis: Enabling Mixed-Immersive Analysis of Automotive User Interface Interaction Studies,CHI - Human Factors in Computing Systems,A*,"Automotive user interface (AUI) evaluation becomes increasingly complex due to novel interaction modalities, driving automation, heterogeneous data, and dynamic environmental contexts. Immersive analytics may enable efficient explorations of the resulting multilayered interplay between humans, vehicles, and the environment. However, no such tool exists for the automotive domain. With AutoVis, we address this gap by combining a non-immersive desktop with a virtual reality view enabling mixed-immersive analysis of AUIs. We identify design requirements based on an analysis of AUI research and domain expert interviews (N=5). AutoVis supports analyzing passenger behavior, physiology, spatial interaction, and events in a replicated study environment using avatars, trajectories, and heatmaps. We apply context portals and driving-path events as automotive-specific visualizations. To validate AutoVis against real-world analysis tasks, we implemented a prototype, conducted heuristic walkthroughs using authentic data from a case study and public datasets, and leveraged a real vehicle in the analysis process.",virtual reality; automotive user interfaces; visualization; Immersive analytics; interaction analysis,Abstract_Keywords,
ACM DL,conferencePaper,2023,DataDancing: An Exploration of the Design Space For Visualisation View Management for 3D Surfaces and Spaces,CHI - Human Factors in Computing Systems,A*,"Recent studies have explored how users of immersive visualisation systems arrange data representations in the space around them. Generally, these have focused on placement centred at eye-level in absolute room coordinates. However, work in HCI exploring full-body interaction has identified zones relative to the user’s body with different roles. We encapsulate the possibilities for visualisation view management into a design space (called “DataDancing”). From this design space we extrapolate a variety of view management prototypes, each demonstrating a different combination of interaction techniques and space use. The prototypes are enabled by a full-body tracking system including novel devices for torso and foot interaction. We explore four of these prototypes, encompassing standard wall and table-style interaction as well as novel foot interaction, in depth through a qualitative user study. Learning from the results, we improve the interaction techniques and propose two hybrid interfaces that demonstrate interaction possibilities of the design space.",virtual reality; immersive analytics; design space exploration; 3D surfaces and spaces; visualisation view management,Keywords,
ACM DL,conferencePaper,2023,GestureExplorer: Immersive Visualisation and Exploration of Gesture Data,CHI - Human Factors in Computing Systems,A*,"This paper presents the design and evaluation of GestureExplorer, an Immersive Analytics tool that supports the interactive exploration, classification and sensemaking with large sets of 3D temporal gesture data. GestureExplorer features 3D skeletal and trajectory visualisations of gestures combined with abstract visualisations of clustered sets of gestures. By leveraging the large immersive space afforded by a Virtual Reality interface our tool allows free navigation and control of viewing perspective for users to gain a better understanding of gestures. We explored a selection of classification methods to provide an overview of the dataset that was linked to a detailed view of the data that showed different visualisation modalities. We evaluated GestureExplorer with two user studies and collected feedback from participants with diverse visualisation and analytics backgrounds. Our results demonstrated the promising capability of GestureExplorer for providing a useful and engaging experience in exploring and analysing gesture data.",virtual reality; immersive analytics; gesture elicitation study,Abstract_Keywords,
ACM DL,conferencePaper,2023,Pearl: Physical Environment based Augmented Reality Lenses for In-Situ Human Movement Analysis,CHI - Human Factors in Computing Systems,A*,"This paper presents Pearl, a mixed-reality approach for the analysis of human movement data in situ. As the physical environment shapes human motion and behavior, the analysis of such motion can benefit from the direct inclusion of the environment in the analytical process. We present methods for exploring movement data in relation to surrounding regions of interest, such as objects, furniture, and architectural elements. We introduce concepts for selecting and filtering data through direct interaction with the environment, and a suite of visualizations for revealing aggregated and emergent spatial and temporal relations. More sophisticated analysis is supported through complex queries comprising multiple regions of interest. To illustrate the potential of Pearl, we developed an Augmented Reality-based prototype and conducted expert review sessions and scenario walkthroughs in a simulated exhibition. Our contribution lays the foundation for leveraging the physical environment in the in-situ analysis of movement data.",affordance; augmented/mixed reality; Immersive Analytics; In-situ visualization; movement data analysis; physical referents,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms,CHI - Human Factors in Computing Systems,A*,"Many collaborative data analysis situations benefit from collaborators utilizing different platforms. However, maintaining group awareness between team members using diverging devices is difficult, not least because common ground diminishes. A person using head-mounted VR cannot physically see a user on a desktop computer even while co-located, and the desktop user cannot easily relate to the VR user’s 3D workspace. To address this, we propose the “eyes-and-shoes” principles for group awareness and abstract them into four levels of techniques. Furthermore, we evaluate these principles with a qualitative user study of 6 participant pairs synchronously collaborating across distributed desktop and VR head-mounted devices. In this study, we vary the group awareness techniques between participants and explore two visualization contexts within participants. The results of this study indicate that the more visual metaphors and views of participants diverge, the greater the level of group awareness is needed. A copy of this paper, the study preregistration, and all supplemental materials required to reproduce the study are available on OSF (link).",virtual reality; immersive analytics; Asymmetric collaboration; ubiquitous analytics.,Keywords_Title,
ACM DL,conferencePaper,2023,Challenges of Moderating Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Recent years have seen a rise in social virtual reality (VR) platforms that allow people to interact in real-time through voice and gestures. The ephemeral nature of communication on these platforms can enable new forms of harmful behavior and new challenges for moderators. We performed virtual field research on three VR environments (AltspaceVR, Horizon Worlds, Rec Room). Based on observing 100 scheduled events, our analysis uncovered 13 distinct types of potentially harmful behaviors enabled by real-time voice, embodied interactions, and platform affordances. We witnessed potential harm at 45% of our observed events; only 24% of these incidents were addressed by moderators. To understand moderation practices, we conducted interviews with 11 moderators to investigate how they assess real-time interactions and how they operate within the current state of moderation tools. Our work sheds light on how moderation tools and practices must evolve to meet the new challenges of social VR.",Virtual Reality; Interview; Content Moderation; Ephemeral Social Spaces; Virtual Ethnography,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,"""I Am a Mirror Dweller"": Probing the Unique Strategies Users Take to Communicate in the Context of Mirrors in Social Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Increasingly popular social virtual reality (VR) platforms like VRChat created new ways for people to interact with each other, generating dedicated user communities with unique idioms of socializing in an alternative world. In VRChat, users frequently gather in front of mirrors en masse during online interactions. Understanding how user communities deal with the mirror’s unique interactions can generate insights for supporting communication in social VR. In this study, we investigated the mirror’s synergistic effect with avatars on behaviors and dedicated user conversational performance. Qualitative findings indicate that avatar-mediated communication through mirrors provides functions like ensuring synchronization of incarnations, increasing immersion, and enhancing idealized embodiment to express bolder behaviors anonymously. Quantitative studies show that while mirrors improve self-perception, it has a potentially adverse effect on conversational performance, similar to the role of self-viewing in video conferencing. Studying how users interact with mirrors in an immersive environment allows us to explore how digital environments affect spatialized interactions when transported from physical to digital domains.",mirror; Social VR; avatar-mediated communication; body illusion,Abstract_Title,
ACM DL,conferencePaper,2023,Investigating Wrist Deflection Scrolling Techniques for Extended Reality,CHI - Human Factors in Computing Systems,A*,"Scrolling in extended reality (XR) is currently performed using handheld controllers or vision-based arm-in-front gestures, which have the limitations of encumbering the user’s hands or requiring a specific arm posture, respectively. To address these limitations, we investigate freehand, posture-independent scrolling driven by wrist deflection. We propose two novel techniques: Wrist Joystick, which uses rate control, and Wrist Drag, which uses position control. In an empirical study of a rapid item acquisition task and a casual browsing task, both Wrist Drag and Wrist Joystick performed on par with a comparable state-of-the-art technique on one of the two tasks. Further, using a relaxed arm-at-side posture, participants retained their arm-in-front performance for both wrist techniques. Finally, we analyze behavioral and ergonomic data to provide design insights for wrist deflection scrolling. Our results demonstrate that wrist deflection provides a promising method for performant scrolling controls while offering additional benefits over existing XR interaction techniques.",virtual reality; extended reality; user study; freehand; position control; rate control; scrolling; wrist deflection; wristband,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Using Pseudo-Stiffness to Enrich the Haptic Experience in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Providing users with a haptic sensation of the hardness and softness of objects in virtual reality is an open challenge. While physical props and haptic devices help, their haptic properties do not allow for dynamic adjustments. To overcome this limitation, we present a novel technique for changing the perceived stiffness of objects based on a visuo-haptic illusion. We achieved this by manipulating the hands’ Control-to-Display (C/D) ratio in virtual reality while pressing down on an object with fixed stiffness. In the first study (N=12), we determine the detection thresholds of the illusion. Our results show that we can exploit a C/D ratio from 0.7 to 3.5 without user detection. In the second study (N=12), we analyze the illusion’s impact on the perceived stiffness. Our results show that participants perceive the objects to be up to 28.1% softer and 8.9% stiffer, allowing for various haptic applications in virtual reality.",virtual reality; haptic illusions; pseudo-haptics,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Using Virtual Reality and Co-Design to Study the Design of Large-scale Shape-Changing Interfaces,CHI - Human Factors in Computing Systems,A*,"Large-scale shape-changing interfaces (SCIs) such as shape-changing walls offer opportunities for enhancing user experiences within buildings, e.g., for navigation. However, due to the embryonic nature of SCI technologies, designing and explaining the shape features that are beneficial to users is challenging. Previous work used virtual platforms (2D video or Projected Augmented Reality) to design SCI. This paper explores how Virtual Reality (VR) can provide an immersive experience that can help in designing large-scale SCI. We follow a co-design approach in which we use VR to obtain users’ impressions of shape-changing walls. Then, we conduct co-design sessions to understand how shape-changing walls can be designed to become ambient and blend with the environment. We report our results to guide the design of shape-changing walls as well as discuss how our approach can provide valuable insights into how a VR experience, prior to design, and can help in the design process.",Virtual Reality; Co-design; Large-Scale Interfaces; Shape-Changing Walls,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,“Awesomely freaky!” The impact of type on children's social-emotional perceptions of virtual reality characters,CHI - Human Factors in Computing Systems,A*,"While VR, through decades of research, has shown to successfully improve young children's lives, more research needs to examine the appropriateness of VR for children, including its design. The type of character in combination with the perceptual realism of virtual reality (VR) may influence children's perceptions of VR experiences. A within-participant experiment examined 5- to 9-year-old children's (N = 25) perceptions of three different character types in VR (i.e., human, animal, and anthropomorphized creature) based on their level of social realism. Results showed that character type impacted children's (a) social-emotional descriptions of the VR experience, (b) if VR's realism was an asset or a hindrance, and (c) primed thoughts about fantasy versus reality. However, children experienced the embodiment and personification of the characters similarly across all character types. Finally, children recalled the salient aspects of the characters they remembered and identified elements to improve the VR characters’ design.",virtual reality; child development; parasocial relationships; social-emotional; uncanny valley,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Imagine That! Imaginative Suggestibility Affects Presence in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Personality characteristics can affect how much presence an individual experiences in virtual reality, and researchers have explored how it may be possible to prime users to increase their sense of presence. A personality characteristic that has yet to be explored in the VR literature is imaginative suggestibility, the ability of an individual to successfully experience an imaginary scenario as if it were real. In this paper, we explore how suggestibility and priming affect presence when consulting an ancient oracle in VR as part of an educational experience – a common VR application. We show for the first time how imaginative suggestibility is a major factor which affects presence and emotions experienced in VR, while priming cues have no effect on participants’ (n=128) user experience, contrasting results from prior work. We consider the impacts of these findings for VR design and provide guidelines based on our results.",virtual reality; presence; imaginative suggestibility; priming,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Investigating the Effects of Individual Spatial Abilities on Virtual Reality Object Manipulation,CHI - Human Factors in Computing Systems,A*,"Object manipulation in 3D space, meaning translating, rotating, and scaling, is ubiquitous in virtual reality (VR), and several interaction techniques have been developed in the past to optimize the task performance and usability. However, preliminary research indicates that individual spatial abilities also have an impact. Yet, it was never investigated if users’ spatial abilities influence VR object manipulation. We assessed this in a user study (N=66) using 21 manipulation tasks defined in a Fitts’ law-related approach. As interaction techniques, we chose gizmos for simultaneously manipulating 1 and 3 degrees of freedom (DOF) and a handle bar metaphor for 7 DOF. Higher spatial abilities resulted in significantly shorter task completion time and more targeted manipulations, while task accuracy was unaffected. However, an optimized interaction technique could compensate individual disadvantages. We propose seven guidelines on spatial abilities in interaction technique design and research to personalize and improve VR applications.",virtual reality; mixed reality; Fitts’ law; docking task; individual characteristics; interaction technique; object manipulation; spatial abilities,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Realism and Field of View Affect Presence in VR but Not the Way You Think,CHI - Human Factors in Computing Systems,A*,"Presence is one of the most studied and most important variables in immersive virtual reality (VR) and it influences the effectiveness of many VR applications. Separate bodies of research indicate that presence is determined by (1) technical factors such as the visual realism of a virtual environment (VE) and the field of view (FoV), and (2) human factors such as emotions and agency. However, it remains unknown how technical and human factors may interact in the presence formation process. We conducted a user study (n=360) to investigate the effects of visual realism (high/low), FoV (high/low), emotions (focusing on fear) and agency (yes/no) on presence. Counter to previous assumptions, technical factors did not affect presence directly but were moderated through human factors. We propose TAP-Fear, a structural equation model that describes how design decisions, technical factors and human factors combine and interact in the formation of presence.",virtual reality; emotions; presence; agency; field of view; level of detail,Abstract_Keywords,
ACM DL,conferencePaper,2023,Tailor Twist: Assessing Rotational Mid-Air Interactions for Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Mid-air gestures, widely used in today’s Augmented Reality (AR) applications, are prone to the “gorilla arm” effect, leading to discomfort with prolonged interactions. While prior work has proposed metrics to quantify this effect and means to improve comfort and ergonomics, these works usually only consider simplistic, one-dimensional AR interactions, like reaching for a point or pushing a button. However, interacting with AR environments also involves far more complex tasks, such as rotational knobs, potentially impacting ergonomics. This paper advances the understanding of the ergonomics of rotational mid-air interactions in AR. For this, we contribute the results of a controlled experiment exposing the participants to a rotational task in the interaction space defined by their arms’ reach. Based on the results, we discuss how novel future mid-air gesture modalities benefit from our findings concerning ergonomic-aware rotational interaction.",Augmented Reality; Mid-Air Gesture; Rotational Interaction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,TurnAhead: Designing 3-DoF Rotational Haptic Cues to Improve First-person Viewing (FPV) Experiences,CHI - Human Factors in Computing Systems,A*,"First-Person View (FPV) drone is a recently developed category of drones designed for precision flying and for capturing exhilarating experiences that could not be captured before, such as navigating through tight indoor spaces and flying extremely close to subjects of interest. FPV viewing experiences, while exhilarating, typically have frequent rotations that can lead to visually induced discomfort. We present TurnAhead, which uses 3-DoF rotational haptic cues that correspond to camera rotations to improve the comfort, immersion, and enjoyment of FPV experiences. It uses headset-mounted air jets to provide ungrounded rotational forces and is the first device to support rotation around all 3 axes: yaw, pitch, and roll. We conducted a series of perception and formative studies to explore the design space of timing and intensity of haptic cues, followed by user experience evaluation, for a combined total of 44 participants (n=12, 8, 6, 18). Results showed that TurnAhead significantly improved overall comfort, immersion, and enjoyment, and was preferred by 89% of participants.",Virtual Reality; First-person Viewing Video; Haptic Device; User Experience Design,Keywords,
ACM DL,conferencePaper,2023,"The Effect of Movement Direction, Hand Dominance, and Hemispace on Reaching Movement Kinematics in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"When users reach their arms to different locations in physical space, they often adapt how they move (i.e., kinematic properties of their reaches) depending on the: (1) direction they move, (2) hand they use, and (3) side of the body where the movement occurs. However, it is not yet clear if and how these three properties of reaching tasks may interact to influence users’ behavior when they reach to objects in VR. To address this question, we had users perform virtual hand reaches in five different directions, on both sides of their bodies, using both their dominant and non-dominant hands. The results revealed that users adapted their virtual hand reaching movements in response to changes in all three properties. The findings provide practitioners insights on how to measure and interpret users’ movements, which has applicability in emerging contexts that include detecting VR usability issues and using VR for stroke rehabilitation.",virtual reality; pointing; movement kinematics; reaching; virtual hand,Keywords_Title,
ACM DL,conferencePaper,2023,Let’s Face It: Influence of Facial Expressions on Social Presence in Collaborative Virtual Reality,CHI - Human Factors in Computing Systems,A*,"As the world becomes more interconnected, physical separation between people increases. Existing collaborative Virtual Reality (VR) applications, designed to bridge this distance, are not yet sufficient in providing a sense of social connection comparable to face-to-face interactions. Possible reasons are the limited multimodality of VR systems and the lack of non-verbal cues in VR avatars. We systematically investigated how facial expressions influence Social Presence in two collaborative VR tasks. We explored four types of facial expressions: eyes and mouth movements, their combination, and no expressions, for two types of explanations: verbal and graphical. To examine how these expressions influence Social Presence, we conducted a controlled VR experiment (N = 48), in which participants had to explain a specific term to their counterpart. Our results demonstrate that eye and mouth movements positively influence Social Presence in VR. Particularly, combining verbal explanations and eye movements induces the highest feeling of co-presence.",virtual reality; collaboration; facial expressions; social presence,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Bridging the Generational Gap: Exploring How Virtual Reality Supports Remote Communication Between Grandparents and Grandchildren,CHI - Human Factors in Computing Systems,A*,"When living apart, grandparents and grandchildren often use audio-visual communication approaches to stay connected. However, these approaches seldom provide sufficient companionship and intimacy due to a lack of co-presence and spatial interaction, which can be fulfilled by immersive virtual reality (VR). To understand how grandparents and grandchildren might leverage VR to facilitate their remote communication and better inform future design, we conducted a user-centered participatory design study with twelve pairs of grandparents and grandchildren. Results show that VR affords casual and equal communication by reducing the generational gap, and promotes conversation by offering shared activities as bridges for connection. Participants preferred resemblant appearances on avatars for conveying well-being but created ideal selves for gaining playfulness. Based on the results, we contribute eight design implications that inform future VR-based grandparent-grandchild communications.",VR; virtual reality; older adults; aging; generational gap; grandchildren; grandparents; inter-generational communication,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,"Designing Immersive, Narrative-Based Interfaces to Guide Outdoor Learning",CHI - Human Factors in Computing Systems,A*,"Outdoor learning experiences, such as field trips, can improve children’s science achievement and engagement, but these experiences are often difficult to deliver without extensive support. Narrative in educational experiences can provide needed structure, while also increasing engagement. We created a narrative-based, mobile application to investigate how to guide young learners in interacting with their local, outdoor environment. In a second variant, we added augmented reality and image classification to explore the value of these features. A study (n = 44) found that participants using our system demonstrated learning gains and found the experience engaging. Our findings identified several major themes, including participant excitement for hands-on interactions with nature, curiosity about the characters, and enthusiasm toward typing their thoughts and observations. We offer a set of design implications for supporting narrative-based, outdoor learning with immersive technology.",augmented reality; ubiquitous computing; machine learning; computer vision; mobile computing; immersive technology; outdoor learning,Abstract_Keywords,
ACM DL,conferencePaper,2023,"From Artifacts to Outcomes: Comparison of HMD VR, Desktop, and Slides Lectures for Food Microbiology Laboratory Instruction",CHI - Human Factors in Computing Systems,A*,"Despite the value of VR (Virtual Reality) for educational purposes, the instructional power of VR in Biology Laboratory education remains under-explored. Laboratory lectures can be challenging due to students’ low motivation to learn abstract scientific concepts and low retention rate. Therefore, we designed a VR-based lecture on fermentation and compared its effectiveness with lectures using PowerPoint slides and a desktop application. Grounded in the theory of distributed cognition and motivational theories, our study examined how learning happens in each condition from students’ learning outcomes, behaviors, and perceptions. Our result indicates that VR facilitates students’ long-term retention to learn by cultivating their longer visual attention and fostering a higher sense of immersion, though students’ short-term retention remains the same across all conditions. This study extends current research on VR studies by identifying the characteristics of each teaching artifact and providing design implications for integrating VR technology into higher education.",Virtual Reality; Educational Technology; Immersive Visualization Design; Laboratory Instruction; Learning Theories,Abstract_Keywords,
ACM DL,conferencePaper,2023,LearnIoTVR: An End-to-End Virtual Reality Environment Providing Authentic Learning Experiences for Internet of Things,CHI - Human Factors in Computing Systems,A*,"The rapid growth of Internet-of-Things (IoT) applications has generated interest from many industries and a need for graduates with relevant knowledge. An IoT system is comprised of spatially distributed interactions between humans and various interconnected IoT components. These interactions are contextualized within their ambient environment, thus impeding educators from recreating authentic tasks for hands-on IoT learning. We propose LearnIoTVR, an end-to-end virtual reality (VR) learning environment which helps students to acquire IoT knowledge through immersive design, programming, and exploration of real-world environments empowered by IoT (e.g., a smart house). The students start the learning process by installing virtual IoT components we created in different locations inside the VR environment so that the learning will be situated in the same context where the IoT is applied. With our custom-designed 3D block-based language, students can program IoT behaviors directly within VR and get immediate feedback on their programming outcome. In the user study, we evaluated the learning outcomes among students using LearnIoTVR with a pre- and post-test to understand to what extent does engagement in LearnIoTVR lead to gains in learning programming skills and IoT competencies. Additionally, we examined what aspects of LearnIoTVR support usability and learning of programming skills compared to a traditional desktop-based learning environment. The results from these studies were promising. We also acquired insightful user feedback which provides inspiration for further expansions of this system.",IoT; Virtual Reality; Embodied Interaction; Block-based Programming; Immersive Programming; Project-based Learning,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,MR.Brick: Designing A Remote Mixed-reality Educational Game System for Promoting Children’s Social &amp; Collaborative Skills,CHI - Human Factors in Computing Systems,A*,"Children are one of the groups most influenced by COVID-19-related social distancing, and a lack of contact with peers can limit their opportunities to develop social and collaborative skills. However, remote socialization and collaboration as an alternative approach is still a great challenge for children. This paper presents MR.Brick, a Mixed Reality (MR) educational game system that helps children adapt to remote collaboration. A controlled experimental study involving 24 children aged six to ten was conducted to compare MR.Brick with the traditional video game by measuring their social and collaborative skills and analyzing their multi-modal playing behaviours. The results showed that MR.Brick was more conducive to children’s remote collaboration experience than the traditional video game. Given the lack of training systems designed for children to collaborate remotely, this study may inspire interaction design and educational research in related fields.",children; mixed reality; tangible user interface; remote collaboration; educational game; social and collaborative skill,Abstract_Keywords,
ACM DL,conferencePaper,2023,Tutor In-sight: Guiding and Visualizing Students’ Attention with Mixed Reality Avatar Presentation Tools,CHI - Human Factors in Computing Systems,A*,"Remote conferencing systems are increasingly used to supplement or even replace in-person teaching. However, prevailing conferencing systems restrict the teacher’s representation to a webcam live-stream, hamper the teacher’s use of body-language, and result in students’ decreased sense of co-presence and participation. While Virtual Reality (VR) systems may increase student engagement, the teacher may not have the time or expertise to conduct the lecture in VR. To address this issue and bridge the requirements between students and teachers, we have developed Tutor In-sight, a Mixed Reality (MR) avatar augmented into the student’s workspace based on four design requirements derived from the existing literature, namely: integrated virtual with physical space, improved teacher’s co-presence through avatar, direct attention with auto-generated body language, and usable workflow for teachers. Two user studies were conducted from the perspectives of students and teachers to determine the advantages of Tutor In-sight in comparison to two existing conferencing systems, Zoom (video-based) and Mozilla Hubs (VR-based). The participants of both studies favoured Tutor In-sight. Among others, this main finding indicates that Tutor In-sight satisfied the needs of both teachers and students. In addition, the participants’ feedback was used to empirically determine the four main teacher requirements and the four main student requirements in order to improve the future design of MR educational tools.",Augmented Reality; Remote Presentation; Virtual Avatar,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Partially Blended Realities: Aligning Dissimilar Spaces for Distributed Mixed Reality Meetings,CHI - Human Factors in Computing Systems,A*,"Mixed Reality allows for distributed meetings where people’s local physical spaces are virtually aligned into blended interaction spaces. In many cases, people’s physical rooms are dissimilar, making it challenging to design a coherent blended space. We introduce the concept of Partially Blended Realities (PBR) — using Mixed Reality to support remote collaborators in partially aligning their physical spaces. As physical surfaces are central in collaborative work, PBR supports users in transitioning between different configurations of tables and whiteboard surfaces. In this paper, we 1) describe the design space of PBR, 2) present RealityBlender to explore interaction techniques for how users may configure and transition between blended spaces, and 3) provide insights from a study on how users experience transitions in a remote collaboration task. With this work, we demonstrate new potential for using partial solutions to tackle the alignment problem of dissimilar spaces in distributed Mixed Reality meetings.",mixed reality; remote collaboration; augmented and virtual reality; blended realities; proxemic transitions,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,ProObjAR: Prototyping Spatially-aware Interactions of Smart Objects with AR-HMD,CHI - Human Factors in Computing Systems,A*,"The rapid advances in technologies have brought new interaction paradigms of smart objects (e.g., digital devices) beyond digital device screens. By utilizing spatial properties, configurations, and movements of smart objects, designing spatial interaction, which is one of the emerging interaction paradigms, efficiently promotes engagement with digital content and physical facility. However, as an important phase of design, prototyping such interactions still remains challenging, since there is no ad-hoc approach for this emerging paradigm. Designers usually rely on methods that require fixed hardware setup and advanced coding skills to script and validate early-stage concepts. These requirements restrict the design process to a limited group of users in indoor scenes. To facilitate the prototyping to general usages, we aim to figure out the design difficulties and underlying needs of current design processes for spatially-aware object interactions by empirical studies. Besides, we explore the design space of the spatial interaction for smart objects and discuss the design space in an input-output spatial interaction model. Based on these findings, we present ProObjAR, an all-in-one novel prototyping system with an Augmented Reality Head Mounted Display (AR-HMD). Our system allows designers to easily obtain the spatial data of smart objects being prototyped, specify spatially-aware interactive behaviors from an input-output event triggering workflow, and test the prototyping results in situ. From the user study, we find that ProObjAR&nbsp;simplifies the design procedure and increases design efficiency to a large extent and thus advancing the development of spatially-aware applications in smart ecosystems.",smart objects; AR prototyping; spatial interaction,Abstract,
ACM DL,conferencePaper,2023,Reality Rifts: Wonder-ful Interfaces by Disrupting Perceptual Causality,CHI - Human Factors in Computing Systems,A*,"Reality Rifts are interfaces between the physical and the virtual reality, where incoherent observations of physical behavior lead users to imagine comprehensive and plausible end-to-end dynamics. Reality Rifts emerge in interactive physical systems that lack one or more components that are central to their operation, yet where the physical end-to-end interaction persists with plausible outcomes. Even in the presence of a Reality Rift, users can still interact with a system—much like they would with the unaltered and complete counterpart—leading them to implicitly infer the existence and imagine the behavior of the lacking components from observable phenomena and outcomes. Therefore, dynamic systems with Reality Rifts trigger doubt, curiosity, and rumination—a sense of wonder that users experience when observing a Reality Rift due to their innate curiosity. In this paper, we explore how interactive systems can elicit and guide the user’s imagination by integrating Reality Rifts. We outline the design process for opening a Reality Rift in interactive physical systems, describe the resulting design space, and explore it through six characteristic prototypes. To understand to what extent and with which qualities these prototypes indeed induce a sense of wonder during an interaction, we evaluated Reality Rifts in the form of a field deployment with 50 participants. We discuss participants’ behavior and derive factors for the implementation of future wonder-ful experiences.",Mixed reality; immersive experiences; physical displays,Abstract_Keywords,
ACM DL,conferencePaper,2023,Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching,CHI - Human Factors in Computing Systems,A*,"This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.",Augmented Reality; Mixed Reality; Human-Centered Machine Learning; Everyday Objects; Interactive Machine Teaching; Prototyping Tools; Tangible Interactions,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Ubi Edge: Authoring Edge-Based Opportunistic Tangible User Interfaces in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Edges are one of the most ubiquitous geometric features of physical objects. They provide accurate haptic feedback and easy-to-track features for camera systems, making them an ideal basis for Tangible User Interfaces (TUI) in Augmented Reality (AR). We introduce Ubi Edge, an AR authoring tool that allows end-users to customize edges on daily objects as TUI inputs to control varied digital functions. We develop an integrated AR-device and an integrated vision-based detection pipeline that can track 3D edges and detect the touch interaction between fingers and edges. Leveraging the spatial-awareness of AR, users can simply select an edge by sliding fingers along it and then make the edge interactive by connecting it to various digital functions. We demonstrate four use cases including multi-function controllers, smart homes, games, and TUI-based tutorials. We also evaluated and proved our system’s usability through a two-session user study, where qualitative and quantitative results are positive.",Augmented Reality; Tangible User Interface; immersive authoring,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,FlowAR: How Different Augmented Reality Visualizations of Online Fitness Videos Support Flow for At-Home Yoga Exercises,CHI - Human Factors in Computing Systems,A*,"Online fitness video tutorials are an increasingly popular way to stay fit at home without a personal trainer. However, to keep the screen playing the video in view, users typically disrupt their balance and break the motion flow — two main pillars for the correct execution of yoga poses. While past research partially addressed this problem, these approaches supported only a limited view of the instructor and simple movements. To enable the fluid execution of complex full-body yoga exercises, we propose FlowAR, an augmented reality system for home workouts that shows training video tutorials as always-present virtual static and dynamic overlays around the user. We tested different overlay layouts in a study with 16 participants, using motion capture equipment for baseline performance. Then, we iterated the prototype and tested it in a furnished lab simulating home settings with 12 users. Our results highlight the advantages of different visualizations and the system’s general applicability.",augmented reality; fitness video; home workouts; yoga,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Are Embodied Avatars Harmful to our Self-Experience? The Impact of Virtual Embodiment on Body Awareness,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) allows us to replace our visible body with a virtual self-representation (avatar) and to explore its effects on our body perception. While the feeling of owning and controlling a virtual body is widely researched, how VR affects the awareness of internal body signals (body awareness) remains open. Forty participants performed moving meditation tasks in reality and VR, either facing their mirror image or not. Both the virtual environment and avatars photorealistically matched their real counterparts. We found a negative effect of VR on body awareness, mediated by feeling embodied in and changed by the avatar. Further, we revealed a negative effect of a mirror on body awareness. Our results indicate that assessing body awareness should be essential in evaluating VR designs and avatar embodiment aiming at mental health, as even a scenario as close to reality as possible can distract users from their internal body signals.",virtual reality; body ownership; agency; body perception; interoception; Sense of embodiment; virtual humans,Abstract_Keywords,
ACM DL,conferencePaper,2023,Drifting Off in Paradise: Why People Sleep in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Sleep is important for humans, and past research has considered methods of improving sleep through technologies such as virtual reality (VR). However, there has been limited research on how such VR technology may affect the experiential and practical aspects of sleep, especially outside of a clinical lab setting. We consider this research gap through the lens of individuals that voluntarily engage in the practice of sleeping in VR. Semi-structured interviews with 14 participants that have slept in VR reveal insights regarding the motivations, actions, and experiential factors that uniquely define this practice. We find that participant motives can be largely categorized through either the experiential or social affordances of VR. We tie these motives into findings regarding the unique customs of sleeping in VR, involving set-up both within the physical and virtual space. Finally, we identify current and future challenges for sleeping in VR, and propose prospective design directions.",sleep; social VR; VRChat,Abstract_Title,
ACM DL,conferencePaper,2023,"Meeting Your Virtual Twin: Effects of Photorealism and Personalization on Embodiment, Self-Identification and Perception of Self-Avatars in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Embodying virtual twins – photorealistic and personalized avatars – will soon be easily achievable in consumer-grade VR. For the first time, we explored how photorealism and personalization impact self-identification, as well as embodiment, avatar perception and presence. Twenty participants were individually scanned and, in a two-hour session, embodied four avatars (high photorealism personalized, low photorealism personalized, high photorealism generic, low photorealism generic). Questionnaire responses revealed stronger mid-immersion body ownership for the high photorealism personalized avatars compared to all other avatar types, and stronger embodiment for high photorealism compared to low photorealism avatars and for personalized compared to generic avatars. In a self-other face distinction task, participants took significantly longer to pause the face morphing videos of high photorealism personalized avatars, suggesting a stronger self-identification bias with these avatars. Photorealism and personalization were perceptually positive features; how employing these avatars in VR applications impacts users over time requires longitudinal investigation.",virtual reality; personalization; photorealism; self-identification,Keywords_Title,
ACM DL,conferencePaper,2023,No Pie in the (Digital) Sky: Co-Imagining the Food Metaverse,CHI - Human Factors in Computing Systems,A*,"Human behaviour and habits co-evolve with technology, and the metaverse is poised to become a key player in reshaping how we live our everyday life. Given the importance of food in our daily lives, we ask: how will our relationships with food be transformed by the metaverse, and what are the promises and pitfalls of this technology? To answer this, we propose a co-design study that reveals the important elements people value in their daily interactions with food. We then present a speculative catalogue of novel metaverse food experiences, and insights from discussing these ideas with food designers, anthropologists and metaverse experts. Our work aims to provide designers with inspirations for building a metaverse that: provides inclusive opportunities for the future of food; helps re-discover the forgotten or lost knowledge about food; facilitates the exploration, excitement and joy of eating; and reinvigorates the ways that food can soothe and heal.",metaverse; co-design; speculative design; food; human food technology interaction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,"Towards a Metaverse Workspace: Opportunities, Challenges, and Design Implications",CHI - Human Factors in Computing Systems,A*,"Both enterprises and their employees have globally experienced remote work at an unprecedented scale since the outbreak of COVID-19. As the pandemic becomes less of a threat, some companies have called their employees back to a physical office, citing issues related to working remotely, but many employees have refused to return. Thus, working in the metaverse has gained much attention as an alternative that could complement the weaknesses of completely remote work or even offline work. However, we do not know yet what benefits and drawbacks the metaverse has as a legitimate workspace, because there are few real cases of 1) working in the metaverse and 2) working remotely at such an unprecedented scale. Thus, this paper aims to identify real challenges and opportunities the metaverse workspace presents when compared to remote work by conducting semi-structured interviews and participatory workshops with various employees and company stakeholders (e.g., HR managers and CEOs) who have experienced at least two of three work types: working in a physical office, remotely, or in the metaverse. Consequently, we identified 1) advantages and disadvantages of remote work and 2) opportunities and challenges of the metaverse. We further discuss design implications that may overcome the identified challenges of working in the metaverse.",Future of Work; Hybrid Work; Metaverse; Remote Work; Stakeholder-Centered Metaverse Design; Videoconferencing; Virtual Environment; Virtual Workspace,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Be Our Guest: Intercultural Heritage Exchange through Augmented Reality (AR),CHI - Human Factors in Computing Systems,A*,"This paper explores how interactive applications can help mitigate the adversity of facing cultural differences between migrants and the host community, and between migrants of diverse backgrounds to foster intercultural exchange. Based on literature about situated cognition, immersive theater, and affordance, we designed and built Be Our Guest: an augmented reality application where a user is invited to the houses of people from different cultures and is asked to help with one of their cultural rituals around simple everyday objects. We detail the various phases we took to collect the cultural stories and construct the application. We then report the results of a user study with the developed application. Our findings show that participants were easily immersed in the augmented space due to the app’s narrative, visuals, and interactive nature. Moreover, they enjoyed exploring cultural rituals, including their own, and felt more confident connecting with people from other cultures.",Augmented Reality; AR; HCI; communication; exploration; immigrant; migrant; culture; heritage; host community; immersive theater,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,DreamVR: Curating an Interactive Exhibition in Social VR Through an Autobiographical Design Study,CHI - Human Factors in Computing Systems,A*,"Virtual exhibitions have long been regarded as an extension of information delivery for physical exhibitions. However, what virtual exhibitions can offer audiences as a novel experience independently from physical exhibitions has been largely unexplored. In this study, we aim to understand the promises and challenges of experiencing and curating exhibitions in VR by interviewing nine expert curators. Drawing from expert insights, we summarized a set of design guidelines to inform what we can learn and adapt from physical exhibitions when curating in VR. Then, using an autobiographical design approach, we curated an interactive exhibition in VRChat to explore novel interaction techniques. We also hosted an open tour guide in the user study to validate our design guidelines with thirty participants. Results show that our approach of curating an exhibition in VRChat provided the participants with engaging and novel experiences interacting with the exhibits and other audiences.",social virtual reality; interview; Virtual exhibition; autobiographical design study; virtual museum,Keywords,
ACM DL,conferencePaper,2023,“Picture the Audience...”: Exploring Private AR Face Filters for Online Public Speaking,CHI - Human Factors in Computing Systems,A*,"Faced with public speaking anxiety, one common piece of advice is to picture the audience in a new light, using your mind’s eye. With Augmented Reality (AR) face filters, it becomes possible to literally change how one sees oneself or others. In this paper, we explore privately applied AR filters during online public speaking. Private means that these effects are only visible to the speaker. To investigate this possibly controversial concept, we conducted an online survey with 100 respondents to gather a diverse set of initial impressions, possible boundaries, and guidelines. Following this, we built a prototype of a private AR web-based video-calling application, and pilot-tested it with 16 participants to gain more in-depth insights. Based on our results, we outline key user perspectives and opportunities for the private application of AR face filters during online public speaking and discuss them in the context of previous literature on this topic.",augmented reality; communication; video conferencing; self-perception; camera filters; public speaking,Abstract_Keywords,
ACM DL,conferencePaper,2023,Tangible Immersive Trauma Simulation: Is Mixed Reality the next level of medical skills training?,CHI - Human Factors in Computing Systems,A*,"In medical simulation training two approaches are currently rather disjunct: realistic manikins are used to teach physical skills and procedures and VR systems are used to train situation assessment and decision making. We propose a mixed reality approach, which allows trainees to use real tools and their hands when interacting with a physical manikin overlaid with a responsive virtual avatar. In close exchange with first responder organizations, we developed and evaluated an MR training scenario. In the scenario, users can talk to injured people in a car accident, assess the threat of the environment, and utilize real medical equipment. Participants experienced high levels of physical- and self-presence, increased stress levels, and reported a high technology acceptance. The proposed main requirements of first responders regarding haptic multi-sensory skill training in MR and the lessons learned from the workshop aim to guide the design of training solutions for medical training in MR.",mixed reality; haptic feedback; presence; training; first responder,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Towards Leveraging AI-based Moderation to Address Emergent Harassment in Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Extensive HCI research has investigated how to prevent and mitigate harassment in virtual spaces, particularly by leveraging human-based and Artificial Intelligence (AI)-based moderation. However, social Virtual Reality (VR) constitutes a novel social space that faces both intensified harassment challenges and a lack of consensus on how moderation should be approached to address such harassment. Drawing on 39 interviews with social VR users with diverse backgrounds, we investigate the perceived opportunities and limitations for leveraging AI-based moderation to address emergent harassment in social VR, and how future AI moderators can be designed to enhance such opportunities and address limitations. We provide the first empirical investigation into re-envisioning AI’s new roles in innovating content moderation approaches to better combat harassment in social VR. We also highlight important principles for designing future AI-based moderation incorporating user-human-AI collaboration to achieve safer and more nuanced online spaces.",artificial intelligence; online harassment; content moderation; social VR,Abstract_Title,
ACM DL,conferencePaper,2023,"""We Cried on Each Other’s Shoulders"": How LGBTQ+ Individuals Experience Social Support in Social Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Although social support can be a vital component of gender and sexual identity formation, many LGBTQ+ individuals often lack offline social networks for such support. Traditional online technologies also reveal several challenges in providing LGBTQ+ individuals with effective social support. Therefore, social VR, as a unique online space for immersive and embodied experiences, is becoming popular within LGBTQ+ communities for supportive online interactions. Drawing on 29 LGBTQ+ social VR users’ experiences, we investigate the types of social support LGBTQ+ users have experienced through social VR and how they leverage unique social VR features to experience such support. We provide one of the first empirical evidence of how social VR innovates traditional online support mechanisms to empower LGBTQ+ individuals but leads to new safety and equality concerns. We also propose important principles for rethinking social VR design to provide all users, rather than just the privileged few, with supportive experiences.",social VR; LGBTQ+; online social support,Title,
ACM DL,conferencePaper,2023,ARound the Smartphone: Investigating the Effects of Virtually-Extended Display Size on Spatial Memory,CHI - Human Factors in Computing Systems,A*,"Smartphones conveniently place large information spaces in the palms of our hands. While research has shown that larger screens positively affect spatial memory, workload, and user experience, smartphones remain fairly compact for the sake of device ergonomics and portability. Thus, we investigate the use of hybrid user interfaces to virtually increase the available display size by complementing the smartphone with an augmented reality head-worn display. We thereby combine the benefits of familiar touch interaction with the near-infinite visual display space afforded by augmented reality. To better understand the potential of virtually-extended displays and the possible issues of splitting the user’s visual attention between two screens (real and virtual), we conducted a within-subjects experiment with 24 participants completing navigation tasks using different virtually-augmented display sizes. Our findings reveal that a desktop monitor size represents a “sweet spot” for extending smartphones with augmented reality, informing the design of hybrid user interfaces.",augmented reality; spatial memory; hybrid user interfaces,Abstract_Keywords,
ACM DL,conferencePaper,2023,HandyCast: Phone-based Bimanual Input for Virtual Reality in Mobile and Space-Constrained Settings via Pose-and-Touch Transfer,CHI - Human Factors in Computing Systems,A*,"Despite the potential of Virtual Reality as the next computing platform for general purposes, current systems are tailored to stationary settings to support expansive interaction in mid-air. However, in mobile scenarios, the physical constraints of the space surrounding the user may be prohibitively small for spatial interaction in VR with classical controllers. In this paper, we present HandyCast, a smartphone-based input technique that enables full-range 3D input with two virtual hands in VR while requiring little physical space, allowing users to operate large virtual environments in mobile settings. HandyCast defines a pose-and-touch transfer function that fuses the phone’s position and orientation with touch input to derive two individual 3D hand positions. Holding their phone like a gamepad, users can thus move and turn it to independently control their virtual hands. Touch input using the thumbs fine-tunes the respective virtual hand position and controls object selection. We evaluated HandyCast in three studies, comparing its performance with that of Go-Go, a classic bimanual controller technique. In our open-space study, participants required significantly less physical motion using HandyCast with no decrease in completion time or body ownership. In our space-constrained study, participants achieved significantly faster completion times, smaller interaction volumes, and shorter path lengths with HandyCast compared to Go-Go. In our technical evaluation, HandyCast’s fully standalone inside-out 6D tracking performance again incurred no decrease in completion time compared to an outside-in tracking baseline.",Virtual reality; interaction techniques; 3D controller; bimanual interaction.; VR input,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Don’t Panic! - Influence of Virtual Stressor Representations from the ICU Context on Perceived Stress Levels,CHI - Human Factors in Computing Systems,A*,"Intensive care nurses are prone to suffering from chronic stress due to constant exposure to two main profession-related stressors: interruption and time pressure. These stressors have detrimental effects on the well-being of the nursing staff and, by proxy, the patients. To alleviate stress, increase safety, and support the training of stressful scenarios, we investigate the impact these stressors have on subjective and objective stress levels in a virtual environment. We designed an intensive care unit in which participants (n=26, 18 healthcare professionals) perform common tasks, e.g. refilling an infusion pump, whilst being exposed to interruptions and time pressure. Results from our between-subjects study provide data indicating stress increase in both stressor conditions, suggesting that artificially evoking work-related stressors for stress inoculation training (SIT) is a possible extension to simulation training during nursing education. This knowledge is helpful for designing training scenarios of safety-critical situations early in the professional apprenticeship.",Virtual Reality; Stress; Nursing,Keywords,
ACM DL,conferencePaper,2023,Perspective and Geometry Approaches to Mouse Cursor Control in Spatial Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Spatial augmented reality (SAR) can extend desktop computing out of the monitor and into our surroundings, but extending the standard style of mouse input is challenging due to real-world geometry irregularity, gaps, and occlusion. We identify two general approaches for controlling a mouse cursor in SAR: perspective-based approaches based on raycasting, such as Nacenta et. al’s Perspective Cursor, and geometry-based approaches that closely associate cursor movement with surface topology. For the latter, we introduce Everywhere Cursor, a geometry-based approach for indirect mouse cursor control for complex 3D surface geometry in SAR. A controlled experiment compares approaches. Results show the geometry-based Everywhere Cursor improves accuracy and precision by 29% to 60% on average in a tracing task, but when traversing long distances, the perspective-based Perspective Cursor and Raycasting techniques are 22% to 49% faster, albeit with 4% to 10% higher error rates.",Empirical Study; Interaction Techniques,Abstract_Title,
ACM DL,conferencePaper,2023,Towards Applied Remapped Physical-Virtual Interfaces: Synchronization Methods for Resolving Control State Conflicts,CHI - Human Factors in Computing Systems,A*,"User interfaces in virtual reality enable diverse interactions within the virtual world, though they typically lack the haptic cues provided by physical interface controls. Haptic retargeting enables flexible mapping between dynamic virtual interfaces and physical controls to provide real haptic feedback. This investigation aims to extend these remapped interfaces to support more diverse control types. Many interfaces incorporate sliders, switches, and knobs. These controls hold fixed states between interactions creating potential conflicts where a virtual control has a different state from the physical control. This paper presents two methods, “manual” and “automatic”, for synchronizing physical and virtual control states and explores the effects of these methods on the usability of remapped interfaces. Results showed that interfaces without retargeting were the ideal configuration, but they lack the flexibility that remapped interfaces provide. Automatic synchronization was faster and more usable; however, manual synchronization is suitable for a broader range of physical interfaces.",virtual reality; interaction; user interfaces; haptic retargeting; remapped interfaces,Abstract_Keywords,
ACM DL,conferencePaper,2023,Visuo-haptic Crossmodal Shape Perception Model for Shape-Changing Handheld Controllers Bridged by Inertial Tensor,CHI - Human Factors in Computing Systems,A*,"We present a visuo-haptic crossmodal model of shape perception designed for shape-changing handheld controllers. The model uses the inertia tensor of an object to bridge the two senses. The model was constructed from the results of three perceptual experiments. In the first two experiments, we validate that the primary moment and product of inertia (MOI and POI) in the inertia tensor have critical effects on the haptic perception of object length and asymmetry. Then, we estimate a haptic-to-visual shape matching model using MOI and POI as two link variables from the results of the third experiment for crossmodal magnitude production. Finally, we validate in a summative user study that the inverse of the shape matching model is effective for pairing a perceptually-congruent haptic object from a virtual object—the functionality we need for shape-changing handheld interfaces to afford perceptually-fulfilling sensory experiences in virtual reality.",Virtual Reality; Dynamic Touch; Handheld Controller; Perception Model,Abstract_Keywords,
ACM DL,conferencePaper,2023,Eye-Perspective View Management for Optical See-Through Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"Optical see-through (OST) head-mounted displays (HMDs) enable users to experience Augmented Reality (AR) support in the form of helpful real-world annotations. Unfortunately, the blend of the environment with virtual augmentations due to semitransparent OST displays often deteriorates the contrast and legibility of annotations. View management algorithms adapt the annotations’ layout to improve legibility based on real-world information, typically captured by built-in HMD cameras. However, the camera views are different from the user’s view through the OST display which decreases the final layout quality. We present eye-perspective view management that synthesizes high-fidelity renderings of the user’s view to optimize annotation placement. Our method significantly improves over traditional camera-based view management in terms of annotation placement and legibility. Eye-perspective optimizations open up opportunities for further research on use cases relying on the user’s true view through OST HMDs.",augmented reality; head-mounted displays; label placement; legibility; optical see-through,Abstract_Keywords,
ACM DL,conferencePaper,2023,Interactive AR Applications for Nonspeaking Autistic People? - A Usability Study,CHI - Human Factors in Computing Systems,A*,"About one-third of autistic people are nonspeaking, and most are never provided access to an effective alternative to speech. Thoughtfully designed AR applications could provide members of this population with structured learning opportunities, including training on skills that underlie alternative forms of communication. A fundamental step toward creating such opportunities, however, is to investigate nonspeaking autistic people’s ability to tolerate a head-mounted AR device and to interact with virtual objects. We present the first study to examine the usability of an interactive AR-based application by this population. We recruited 17 nonspeaking autistic subjects to play a HoloLens 2 game we developed that involved holographic animations and buttons. Almost all subjects tolerated the device long enough to begin the game, and most completed increasingly challenging tasks that involved pressing holographic buttons. Based on the results, we discuss best practice design and process recommendations. Our findings contradict prevailing assumptions about nonspeaking autistic people and thus open up exciting possibilities for AR-based solutions for this understudied and underserved population.",augmented reality; assistive technology; usability study; nonspeaking autistic people,Keywords,
ACM DL,conferencePaper,2023,The Impact of Navigation Aids on Search Performance and Object Recall in Wide-Area Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Head-worn augmented reality (AR) is a hotly pursued and increasingly feasible contender paradigm for replacing or complementing smartphones and watches for continual information consumption. Here, we compare three different AR navigation aids (on-screen compass, on-screen radar and in-world vertical arrows) in a wide-area outdoor user study (n=24) where participants search for hidden virtual target items amongst physical and virtual objects. We analyzed participants’ search task performance, movements, eye-gaze, survey responses and object recall. There were two key findings. First, all navigational aids enhanced search performance relative to a control condition, with some benefit and strongest user preference for in-world arrows. Second, users recalled fewer physical objects than virtual objects in the environment, suggesting reduced awareness of the physical environment. Together, these findings suggest that while navigational aids presented in AR can enhance search task performance, users may pay less attention to the physical environment, which could have undesirable side-effects.",Mobile Augmented Reality; Behavior; User Study; Lighting Conditions; Navigation Aids; Perception; Wide-Area,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,User Onboarding in Virtual Reality: An Investigation of Current Practices,CHI - Human Factors in Computing Systems,A*,"Explaining to novice users how to interact in immersive VR applications may be challenging. This is in particular due to the fact that the learners are isolated from the real world, and they are asked to manipulate hardware and software objects they are not used to. Consequently, the onboarding phase, which consists in teaching the user how to interact with the application is particularly crucial. In this paper, we aim at giving a better understanding of current VR onboarding methods, their benefits and challenges. We performed 21 VR tutorial ergonomic reviews and 15 interviews with VR experts with experience in VR onboarding. Building on the results, we propose a conceptual framework for VR onboarding and discuss important research directions to explore the design of future efficient onboarding solutions adapted to VR.",virtual reality; learning; computer assisted; human assisted; user onboarding,Keywords_Title,
ACM DL,conferencePaper,2023,"You spin me right round, baby, right round: Examining the Impact of Multi-Sensory Self-Motion Cues on Motion Sickness During a VR Reading Task",CHI - Human Factors in Computing Systems,A*,"Motion sickness is a problem for many in everyday travel and will become more prevalent with the rise of automated vehicles. Virtual Reality (VR) headsets have shown significant promise in-transit, enabling passengers to engage in immersive entertainment and productivity experiences. In a controlled multi-session motion sickness study using an actuated rotating chair, we examine the potential of multi-sensory visual and auditory motion cues, presented during a VR reading task, for mitigating motion sickness. We found that visual cues are most efficient in reducing symptoms, with auditory cues showing some beneficial effects when combined with the visual. Motion sickness had negative effects on presence as well as task performance, and despite the cognitive demand and multi-sensory cues, motion sickness still reached problematic levels. Our work emphasises the need for effective mitigations and the design of stronger multi-sensory motion cues if VR is to fulfil its potential for passengers.",automated vehicles; virtual reality; motion sickness; rotation,Abstract_Keywords,
ACM DL,conferencePaper,2023,Collaborative Online Learning with VR Video: Roles of Collaborative Tools and Shared Video Control,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) has a noteworthy educational potential by providing immersive and collaborative environments. As an alternative but cost-effective way of delivering realistic environments in VR, using 360-degree videos in immersive VR (VR videos) received more attention. Although many studies reported positive learning experiences with VR videos, little is known about how collaborative learning performs on VR video viewing systems. In this study, we implemented two collaborative VR video viewing modes based on the way of group video control, synchronized or shared (Sync mode) and non-synchronized or individual (Non-sync mode) video control, against a conventional VR video viewing setting (Basic mode). We conducted a within-subject study (N = 54) in a lab-simulated remote learning environment. Our results show that collaborative VR video modes (Sync and Non-sync mode) improve users’ learning experiences and collaboration quality, especially with shared video control. Our findings provide directions for designing and employing collaborative VR video tools in online learning environments.",collaborative learning; Virtual Reality; social VR; 360-degree video; educational VR,Abstract_Keywords,
ACM DL,conferencePaper,2023,Dancing with the Avatars: Minimal Avatar Customisation Enhances Learning in a Psychomotor Task,CHI - Human Factors in Computing Systems,A*,"Virtual environments can support psychomotor learning by allowing learners to observe instructor avatars. Instructor avatars that look like the learner hold promise in enhancing learning; however, it is unclear whether this works for psychomotor tasks and how similar avatars need to be. We investigated ‘minimal’ customisation of instructor avatars, approximating a learner’s appearance by matching only key visual features: gender, skin-tone, and hair colour. These avatars can be created easily and avoid problems of highly similar avatars. Using modern dancing as a skill to learn, we compared the effects of visually similar and dissimilar avatars, considering both learning on a screen (n=59) and in VR (n=38). Our results indicate that minimal avatar customisation leads to significantly more vivid visual imagery of the dance moves than dissimilar avatars. We analyse variables affecting interindividual differences, discuss the results in relation to theory, and derive design implications for psychomotor training in virtual environments.",Virtual Reality; Avatar Customisation; Psychomotor; Skills Training; Virtual Environments,Keywords,
ACM DL,conferencePaper,2023,When XR and AI Meet - A Scoping Review on Extended Reality and Artificial Intelligence,CHI - Human Factors in Computing Systems,A*,"Research on Extended Reality (XR) and Artificial Intelligence (AI) is booming, which has led to an emerging body of literature in their intersection. However, the main topics in this intersection are unclear, as are the benefits of combining XR and AI. This paper presents a scoping review that highlights how XR is applied in AI research and vice versa. We screened 2619 publications from 203 international venues published between 2017 and 2021, followed by an in-depth review of 311 papers. Based on our review, we identify five main topics at the intersection of XR and AI, showing how research at the intersection can benefit each other. Furthermore, we present a list of commonly used datasets, software, libraries, and models to help researchers interested in this intersection. Finally, we present 13 research opportunities and recommendations for future work in XR and AI research.",artificial intelligence; extended reality; scoping review,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,4Doodle: 4D Printing Artifacts Without 3D Printers,CHI - Human Factors in Computing Systems,A*,"4D printing encodes transformability over time, which empowers users to create artifacts by on-demand deformation. The creative process of 4D printing shape-changing artifacts can be challenging because of its discontinuous fabrication steps, such as digital designing, specific path planning, automatic printing and manual triggering. We hypothesize that switching from typical 4D printing reliant on 3D printers to a more “handcrafted” method can allow users to understand and continuously reflect upon the artifact and its transformability. Towards this vision, we introduce 4Doodle, a hybrid craft approach that integrates unique deformation controllability and five techniques for freehand 4D printing, using a 3D pen. To tackle the shape-changing challenges of uncertain hands-on fabrication, we develop a mixed reality system to help novices master the manual skills of 4D printing. We also demonstrate a series of 4D printed artifacts with fully human intervention. Finally, our user study shows that 4Doodle lowers the skill-acquisition barrier associated with handcrafting 4D printed artifacts, and it has great potential for creative production and spatial ability.",3D pens; 4D printing; Hybrid craft; Shape-changing behavior,Abstract,
ACM DL,conferencePaper,2023,"Electrical, Vibrational, and Cooling Stimuli-Based Redirected Walking: Comparison of Various Vestibular Stimulation-Based Redirected Walking Systems",CHI - Human Factors in Computing Systems,A*,"Redirected walking (RDW) is a technology that enables users to walk seamlessly in an enormous virtual space within a narrow real space while avoiding collisions with physical elements. Although RDW provides accurate proprioceptive sensations, redirection performance is limited by visual–vestibular inconsistencies. This study aims to support seamless walking in a VR environment by alleviating inconsistencies using four vestibular stimulations: noisy and directional galvanic vestibular stimulation, bone-conduction vibration, and caloric vestibular stimulation. The user study demonstrated that the stimulations successfully enable spatial expansion without impairing immersion and presence. Non-electrical stimulations (bone-conduction vibration and caloric vestibular stimulation) expanded the detection threshold, making them alternatives to electrical stimulations, and direction-based stimulation (directional galvanic vestibular stimulation) improved the user’s gait stability in RDW. Finally, the findings suggested improving the user experience for vestibular stimulation RDW either by lowering audio interference or increasing the synchronization between the RDW gain and the stimulation intensity.",Virtual Reality; Locomotion; Haptic Device; Gait Stability; Redirected Walking; Vestibular Stimulation,Keywords,
ACM DL,conferencePaper,2023,"Interaction Effects of Pedestrian Behavior, Smartphone Distraction and External Communication of Automated Vehicles on Crossing and Gaze Behavior",CHI - Human Factors in Computing Systems,A*,"External communication of automated vehicles is proposed to replace driver-pedestrian communication in ambiguous crossing situations. So far, research has focused on simpler scenarios with one attentive pedestrian and one automated vehicle. This virtual reality study (N=115) investigates a more complex scenario with other crossing pedestrians, a distracting task on the smartphone, and external communication by the automated vehicle. Interaction effects were found for crossing duration, gaze behavior, and subjective measures. For attentive pedestrians, the external communication resulted in shorter crossing durations, higher perceived safety, as well as lower perceived criticality, cognitive workload, and effort. These positive effects were not found when pedestrians were distracted. Instead, distracted pedestrians benefited from other crossing pedestrians because they looked less at the stopping vehicle, felt safer, perceived the situation as less critical, and reported lower cognitive workload and effort. Pedestrians initiated crossings earlier with a group or external communication and later with a smartphone.",automated vehicles; virtual reality; eye tracking; eHMI; pedestrian group; smartphone distraction; unsignalized crossing,Abstract_Keywords,
ACM DL,conferencePaper,2023,Sensorimotor Simulation of Redirected Reaching using Stochastic Optimal Feedback Control,CHI - Human Factors in Computing Systems,A*,"Illusory VR interaction techniques such as hand redirection work because humans use vision to adjust their motor commands during movement (e.g., reaching). Existing simulations of redirected reaching are limited, however, and have not yet incorporated important stochastic characteristics like sensorimotor noise, nor captured redirection’s effect on movement duration. In this work, we propose adapting a stochastic optimal feedback control (SOFC) model of normal reach to simulate redirection by augmenting sensory feedback at run-time. We present a summary of our simulation and validate it against user data gathered in multiple redirection conditions. We also evaluate the impacts of visual attention on the effectiveness of redirection in real users and replicate the effects in simulation. Our results show that an infinite-horizon SOFC model is able to reproduce key characteristics of redirected reaches and highlight the benefits of SOFC as a tool for simulating, evaluating, and gaining insights about redirection techniques.",Virtual Reality; Modeling; Hand Redirection; Optimal Control; Sensorimotor Control; Stochastic Simulation,Keywords,
ACM DL,conferencePaper,2023,Dementia Eyes: Co-Design and Evaluation of a Dementia Education Augmented Reality Experience for Medical Workers,CHI - Human Factors in Computing Systems,A*,"Dementia describes a syndrome of cognitive degeneration, and Behavioural and Psychological Symptoms of Dementia (BPSD) is the non-cognitive symptom. BPSD can be improved by care services. To aid better care service, we explore the potential of using Augmented Reality (AR) to support dementia education for medical workers in three steps: (1) We explore medical workers’ perspective on dementia care lived experience and XR, (2) we co-design an educational experience containing an AR-based application and a 5-min activity with medical workers, (3) we evaluate the effectiveness of the system through a mixed method study. Our result shows that the AR experience successfully touches participants, and motivates them to reflect on the provision of care service. On this basis, we discuss the elements and challenges of designing XR-enabled dementia education for users unfamiliar with novel technology, and the potential of using XR in clinical education.",augmented reality; education; dementia; experience-centred design,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Designing Virtual Environments for Social Engagement in Older Adults: A Qualitative Multi-site Study,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) is increasingly used as a platform for social interaction, including as a means for elders to maintain engagement. However, not many empirical studies have been conducted to examine features of social VR that are most relevant to elders’ experiences. The current study qualitatively analyzed the behavior of older adults in a collaborative VR environment and evaluated aspects of design that affected their engagement outcomes. We paired 36 participants over the age of 60 from three diverse geographic locations to interact in collaborative VR modules. Video-based observation methods and thematic analyses were used to study their interactions. The results indicated a strong link between conversations about personal lives in VR and social engagement, highlighting the need for social VR to encourage users to create their own stories and share their life experiences. The study provides new insights into design guidelines that could improve social VR for older adults.",virtual reality; older adults; social VR; social Engagement,Abstract_Keywords,
ACM DL,conferencePaper,2023,Toward Inclusive Mindsets: Design Opportunities to Represent Neurodivergent Work Experiences to Neurotypical Co-Workers in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Inclusive workplaces require mutual efforts between neurotypical (NT) and neurodivergent (ND) employees to understand one another’s viewpoints and experiences. Currently, the majority of inclusivity training places the burden of change on NDs to conform to NT social-behavioral standards. Our research examines moving toward a more equal effort distribution by exploring virtual reality (VR) design opportunities to build NTs’ understanding of ND workplace experiences. Using participatory design, including generative toolkits and design meetings, we surfaced two main themes that could bridge gaps in understanding: (1) NTs’ recognition of NDs’ strengths and efforts at work, and (2) NTs’ understanding of NDs’ differences. We present a strengths-based assessment of ND traits in the workplace, focusing on how workplaces can support NDs’ success. Finally, we propose VR simulation designs that communicate these themes to represent ND experiences, emphasizing their strengths and viewpoints so that NT co-workers can better empathize and accommodate them.",virtual reality; empathy-building; neurodiversity; workplace,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Bubbleu: Exploring Augmented Reality Game Design with Uncertain AI-based Interaction,CHI - Human Factors in Computing Systems,A*,"Object detection, while being an attractive interaction method for Augmented Reality (AR), is fundamentally error-prone due to the probabilistic nature of the underlying AI models, resulting in sub-optimal user experiences. In this paper, we explore the effect of three game design concepts, Ambiguity, Transparency, and Controllability, to provide better gameplay experiences in AR games that use error-prone object detection-based interaction modalities. First, we developed a base AR pet breeding game, called Bubbleu that uses object detection as a key interaction method. We then implemented three different variants, each according to the three concepts, to investigate the impact of each design concept on the overall user experience. Our user study results show that each design has its own strengths and can improve player experiences in different ways such as decreasing perceived errors (Ambiguity), explaining the system (Transparency), and enabling users to control the rate of uncertainties (Controllability).",computer vision; Human-AI Interaction; vision sensing,Abstract_Title,
ACM DL,conferencePaper,2023,"Going, Going, Gone: Exploring Intention Communication for Multi-User Locomotion in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Exploring virtual worlds together with others adds a social component to the Virtual Reality (VR) experience that increases connectedness. In the physical world, joint locomotion comes naturally through implicit intention communication and subsequent adjustments of the movement patterns. In VR, however, discrete locomotion techniques such as point&amp;teleport come without prior intention communication, hampering the collective experience. Related work proposes fixed groups, with a single person controlling the group movement, resulting in the loss of individual movement capabilities. To close the gap and mediate between these two extremes, we introduce three intention communication methods and explore them with two baseline methods. We contribute the results of a controlled experiment (n=20) investigating these methods from the perspective of a leader and a follower in a dyadic locomotion task. Our results suggest shared visualizations support the understanding of movement intentions, increasing the group feeling while maintaining individual freedom of movement.",Virtual Reality; Teleportation; Locomotion; Connectedness; Multi-User; SocialVR,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Never Skip Leg Day Again: Training the Lower Body with Vertical Jumps in a Virtual Reality Exergame,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) exergames can increase engagement in and motivation for physical activities. Most VR exergames focus on the upper body because many VR setups only track the users’ heads and hands. To become a serious alternative to existing exercise programs, VR exergames must provide a balanced workout and train the lower limbs, too. To address this issue, we built a VR exergame focused on vertical jump training to explore full-body exercise applications. To create a safe and effective training, nine domain experts participated in our prototype design. Our mixed-methods study confirms that the jump-centered exercises provided a worthy challenge and positive player experience, indicating long-term retention. Based on our findings, we present five design implications to guide future work: avoid an unintended forward drift, consider technical constraints, address safety concerns in full-body VR exergames, incorporate rhythmic elements with fluent movement patterns, adapt difficulty to players’ fitness progression status.",VR; virtual reality; serious games; health; training; dynamic difficulty; exergame; sport; vertical jump,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Playful Reflection: Impact of Gamification on a Virtual Reality Simulation of Breastfeeding,CHI - Human Factors in Computing Systems,A*,"Gamification is a popular technique to improve task engagement, and has broadly been deployed in health and education to a point where many users now expect gameful experiences in these settings. However, gamification has been criticised for being a potential obstacle to the experience of reflection. Motivated by this tension, our work examines how the addition of gamification to a Virtual Reality simulation of breastfeeding impacts player experience and reflection. Using a within-subjects design, we invited 34 participants to take part in a mixed-methods evaluation of a gamified and non-gamified variant of the simulation that included questionnaires and semi-structured interviews. Results show that gamification improved player experience and encouraged players to reflect on goal achievement and performance. However, it also diverted players’ attention from nuances within the act of nursing. Drawing on our findings, we contribute considerations for the application of gamification in personal and sensitive settings such as breastfeeding.",Virtual Reality; Simulation; Gamification; Reflection; Breastfeeding,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Supporting Piggybacked Co-Located Leisure Activities via Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Technology, especially the smartphone, is villainized for taking meaning and time away from in-person interactions and secluding people into “digital bubbles”. We believe this is not an intrinsic property of digital gadgets, but evidence of a lack of imagination in technology design. Leveraging augmented reality (AR) toward this end allows us to create experiences for multiple people, their pets, and their environments. In this work, we explore the design of AR technology that “piggybacks” on everyday leisure to foster co-located interactions among close ties (with other people and pets). We designed, developed, and deployed three such AR applications, and evaluated them through a 41-participant and 19-pet user study. We gained key insights about the ability of AR to spur and enrich interaction in new channels, the importance of customization, and the challenges of designing for the physical aspects of AR devices (e.g., holding smartphones). These insights guide design implications for the novel research space of co-located AR.",embodied interaction; augmented/mixed reality; co-located interaction; everyday leisure; human–pet–computer interaction; piggybacking,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,VR Almost There: Simulating Co-located Multiplayer Experiences in Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Consumer social virtual reality (VR) applications have recently started to enable social interactions at a distance. Yet it is still relatively unknown if and to what extent such applications provide meaningful social experiences in cases where in-person leisure activities are not feasible. To explore this, we developed a custom social VR application and conducted an exploratory lab study with 25 dyads in which we compared an in-person and a virtual version of a co-located multiplayer scenario. Our mixed-methods analysis revealed that both scenarios created a socially rich atmosphere and strengthened the social closeness between players. However, the lack of facial animations, limited body language, and a low field of view led to VR’s main social experiential limitations: a reduced mutual awareness and emotional understanding compared to the in-person scenario. We derive implications for social VR design and research as well as game user research.",social virtual reality; player experience; social interaction; social presence; multiplayer games,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Working with Forensic Practitioners to Understand the Opportunities and Challenges for Mixed-Reality Digital Autopsy,CHI - Human Factors in Computing Systems,A*,"Forensic practitioners analyse intrinsic 3D data daily on 2D screens. We explore novel immersive visualisation techniques that enable digital autopsy through analysis of 3D imagery. We employ a user-centred design process involving four rounds of user feedback: (1) formative interviews eliciting opportunities and requirements for mixed-reality digital autopsies; (2) a larger workshop identifying our prototype’s limitations and further use-cases and interaction ideas; (3+4) two rounds of qualitative user validation of successive prototypes of novel interaction techniques for pathologist sensemaking. Overall, we find MR holds great potential to enable digital autopsy, initially to supplement physical autopsy, but ultimately to replace it. We found that experts were able to use our tool to perform basic virtual autopsy tasks, MR setup promotes exploration and sense making of cause of death, and subject to limitations of current MR technology, the proposed system is a valid option for digital autopsies, according to experts’ feedback. – Warning: This paper contains sensitive images which are 3D visualisation of deceased people.",mixed reality; user-centred design; autopsy; pathology; forensics,Keywords,
ACM DL,conferencePaper,2023,FingerMapper: Mapping Finger Motions onto Virtual Arms to Enable Safe Virtual Reality Interaction in Confined Spaces,CHI - Human Factors in Computing Systems,A*,"Whole-body movements enhance the presence and enjoyment of Virtual Reality (VR) experiences. However, using large gestures is often uncomfortable and impossible in confined spaces (e.g., public transport). We introduce FingerMapper, mapping small-scale finger motions onto virtual arms and hands to enable whole-body virtual movements in VR. In a first target selection study (n=13) comparing FingerMapper to hand tracking and ray-casting, we found that FingerMapper can significantly reduce physical motions and fatigue while having a similar degree of precision. In a consecutive study (n=13), we compared FingerMapper to hand tracking inside a confined space (the front passenger seat of a car). The results showed participants had significantly higher perceived safety and fewer collisions with FingerMapper while preserving a similar degree of presence and enjoyment as hand tracking. Finally, we present three example applications demonstrating how FingerMapper could be applied for locomotion and interaction for VR in confined spaces.",Body Re-Association in VR; Confined Spaces; FingerMapper,Abstract_Title,
ACM DL,conferencePaper,2023,Memory Manipulations in Extended Reality,CHI - Human Factors in Computing Systems,A*,"Human memory has notable limitations (e.g., forgetting) which have necessitated a variety of memory aids (e.g., calendars). As we grow closer to mass adoption of everyday Extended Reality (XR), which is frequently leveraging perceptual limitations (e.g., redirected walking), it becomes pertinent to consider how XR could leverage memory limitations (forgetting, distorting, persistence) to induce memory manipulations. As memories highly impact our self-perception, social interactions, and behaviors, there is a pressing need to understand XR Memory Manipulations (XRMMs). We ran three speculative design workshops (n=12), with XR and memory researchers creating 48 XRMM scenarios. Through thematic analysis, we define XRMMs, present a framework of their core components and reveal three classes (at encoding, pre-retrieval, at retrieval). Each class differs in terms of technology (AR, VR) and impact on memory (influencing quality of memories, inducing forgetting, distorting memories). We raise ethical concerns and discuss opportunities of perceptual and memory manipulations in XR.",Augmented Reality; Virtual Reality; Extended Reality; Speculative Design; Perceptual Manipulations; XR Memory Manipulations,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,Re-Evaluating VR User Awareness Needs During Bystander Interactions,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) users are often around bystanders, i.e. people in the real world the VR user may want to interact with. To facilitate bystander-VR user interactions, technology-mediated awareness systems have been introduced to increase a user’s awareness of bystanders. However, while prior works have found effective means of facilitating bystander-VR user interactions, it is unclear when and why one awareness system should be used over another. We reviewed, and selected, a breadth of bystander awareness systems from the literature and investigated their usability, and how they could be holistically used together to support varying awareness needs across 14 bystander-VR user interactions. Our results demonstrate VR users do not manage bystander awareness based solely on the usability of awareness systems but rather on the demands of social context weighted against desired immersion in VR (something existing evaluations fail to capture) and show the need for socially intelligent bystander awareness systems.",Augmented Reality; Virtual Reality; Mixed Reality; Awareness; Interruptions; Bystander-VR User Interactions; Context Awareness,Abstract_Keywords,
ACM DL,conferencePaper,2023,Towards a Bedder Future: A Study of Using Virtual Reality while Lying Down,CHI - Human Factors in Computing Systems,A*,"Most contemporary Virtual Reality (VR) experiences are made for standing users. However, when a user is lying down—either by choice or necessity—it is unclear how they can walk around, dodge obstacles, or grab distant objects. We rotate the virtual coordinate space to study the movement requirements and user experience of using VR while lying down. Fourteen experienced VR users engaged with various popular VR applications for 40 minutes in a study using a think-aloud protocol and semi-structured interviews. Thematic analysis of captured videos and interviews reveals that using VR while lying down is comfortable and usable and that the virtual perspective produces a potent illusion of standing up. However, commonplace movements in VR are surprisingly difficult when lying down, and using alternative interactions is fatiguing and hampers performance. To conclude, we discuss design opportunities to tackle the most significant challenges and to create new experiences.",user experience; virtual reality; bed; lying down; movement; room-scale; supine,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,UnMapped: Leveraging Experts’ Situated Experiences to Ease Remote Guidance in Collaborative Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Collaborative Mixed Reality (MR) systems that help extend expertise for physical tasks to remote environments often situate experts in an immersive view of the task environment to bring the collaboration closer to collocated settings. In this paper, we design UnMapped, an alternative interface for remote experts that combines a live 3D view of the active space within the novice’s environment with a static 3D recreation of the expert’s own workspace to leverage their existing spatial memories within it. We evaluate the impact of this approach on single and repeated use of collaborative MR systems for remote guidance through a comparative study. Our results indicate that despite having a limited understanding of the novice’s environment, using an UnMapped interface increased performance and communication efficiency while reducing experts’ task load. We also outline the various affordances of providing remote experts with a familiar and spatially-stable environment to assist novices.",Augmented Reality; Virtual Reality; Mixed Reality; Remote Collaboration; Physical Tasks,Abstract_Keywords_Title,
ACM DL,conferencePaper,2023,What does it mean to cycle in Virtual Reality? Exploring Cycling Fidelity and Control of VR Bicycle Simulators,CHI - Human Factors in Computing Systems,A*,"Creating highly realistic Virtual Reality (VR) bicycle experiences can be time-consuming and expensive. Moreover, it is unclear what hardware parts are necessary to design a bicycle simulator and whether a bicycle is needed at all. In this paper, we investigated cycling fidelity and control of VR bicycle simulators. For this, we developed and evaluated three cycling simulators: (1) cycling without a bicycle (bikeless), (2) cycling on a fixed (stationary) and (3) moving bicycle (tandem) with four levels of control (no control, steering, pedaling, and steering + pedaling). To evaluate all combinations of fidelity and control, we conducted a controlled experiment (N = 24) in indoor and outdoor settings. We found that the bikeless setup provides the highest feeling of safety, while the tandem leads to the highest realism without increasing motion sickness. Moreover, we discovered that bicycles are not essential for cycling in VR.",virtual reality; locomotion; cycling; bicycle simulators,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Immersive Speculative Enactments: Bringing Future Scenarios and Technology to Life Using Virtual Reality,CHI - Human Factors in Computing Systems,A*,"In this paper we present Immersive Speculative Enactments (ISEs), a novel concept that extends conventional Speculative Enactments to Virtual Reality. Through ISEs, participants are immersed in a speculative world depicted by the designers and can engage with it in its truest envisioned form. We explore this concept via four scenarios with increasing technological uncertainty: a glimpse in the daily life of the parent of a newborn baby; a Mixed Reality experience supporting hybrid classrooms; two wearable devices that present a pet’s emotional state and needs; and an enactment on the effect of communication delay across interplanetary distances. We discuss the concept of ISEs and contrast them to other forms of speculation, provide guidelines on how to design them, as well as reflecting on the challenges, limitations, and potential associated with the role of ISEs in the HCI discourse.",Childcare; Cross-Reality; Design Fiction; Quantified Pets; Space Exploration.; Speculative Enactments; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Mood Worlds: A Virtual Environment for Autonomous Emotional Expression,CHI - Human Factors in Computing Systems,A*,"Immersive interactive technologies such as virtual reality (VR) have the potential to foster well-being. While VR applications have been successfully used to evoke positive emotions through the presetting of light, colour and scenery, the experiential potential of allowing users to independently create a virtual environment (VE) has not yet been sufficiently addressed. To that end, we explore how the autonomous design of a VE can affect emotional engagement and well-being. We present Mood Worlds – a VR application allowing users to visualise their emotions by self-creating a VE. In an exploratory evaluation (N=16), we found that Mood Worlds is an effective tool supporting emotional engagement. Additionally, we found that an autonomous creation process in VR increases positive emotions and well-being. Our work shows that VR can be an effective tool to visualise emotions, thereby increasing positive affect. We discuss opportunities and design requirements for VR as positive technology.",Emotion Regulation; Emotions; Happiness; Positive Technology; Virtual Reality; Well-being,Abstract_Keywords,
ACM DL,conferencePaper,2022,AvatAR: An Immersive Analysis Environment for Human Motion Data Combining Interactive 3D Avatars and Trajectories,CHI - Human Factors in Computing Systems,A*,"Analysis of human motion data can reveal valuable insights about the utilization of space and interaction of humans with their environment. To support this, we present AvatAR, an immersive analysis environment for the in-situ visualization of human motion data, that combines 3D trajectories with virtual avatars showing people’s detailed movement and posture. Additionally, we describe how visualizations can be embedded directly into the environment, showing what a person looked at or what surfaces they touched, and how the avatar’s body parts can be used to access and manipulate those visualizations. AvatAR combines an AR HMD with a tablet to provide both mid-air and touch interaction for system control, as well as an additional overview device to help users navigate the environment. We implemented a prototype and present several scenarios to show that AvatAR can enhance the analysis of human motion data by making data not only explorable, but experienceable.",analysing space utilization; augmented/mixed reality; human motion data; Immersive Analytics; In-situ visualisation; motion analysis,Keywords,
ACM DL,conferencePaper,2022,ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies,CHI - Human Factors in Computing Systems,A*,"The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.",data visualization; Immersive analytics; virtual reality.; visual analytics,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,A Design Space For Data Visualisation Transformations Between 2D And 3D In Mixed-Reality Environments,CHI - Human Factors in Computing Systems,A*,"As mixed-reality (MR) technologies become more mainstream, the delineation between data visualisations displayed on screens or other surfaces and those floating in space becomes increasingly blurred. Rather than the choice of using either a 2D surface or the 3D space for visualising data being a dichotomy, we argue that users should have the freedom to transform visualisations seamlessly between the two as needed. However, the design space for such transformations is large, and practically uncharted. To explore this, we first establish an overview of the different states that a data visualisation can take in MR, followed by how transformations between these states can facilitate common visualisation tasks. We then describe a design space of how these transformations function, in terms of the different stages throughout the transformation, and the user interactions and input parameters that affect it. This design space is then demonstrated with multiple exemplary techniques based in MR.",animated transitions; direct manipulation; Immersive Analytics; mixed reality; visualisation,Keywords,
ACM DL,conferencePaper,2022,A Model Predictive Control Approach for Reach Redirection in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Reach redirection is an illusion-based virtual reality (VR) interaction technique where a user’s virtual hand is shifted during a reach in order to guide their real hand to a physical location. Prior works have not considered the underlying sensorimotor processes driving redirection. In this work, we propose adapting a sensorimotor model for goal-directed reach to obtain a model for visually-redirected reach, specifically by incorporating redirection as a sensory bias in the state estimate used by a minimum jerk motion controller. We validate and then leverage this model to develop a Model Predictive Control (MPC) approach for reach redirection, enabling the real-time generation of spatial warping according to desired optimization criteria (e.g., redirection goals) and constraints (e.g., sensory thresholds). We illustrate this approach with two example criteria – redirection to a desired point and redirection along a desired path – and compare our approach against existing techniques in a user evaluation.",Haptic Retargeting; Model Predictive Control; Optimization; Reach; Redirection; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Squeezy-Feely: Investigating Lateral Thumb-Index Pinching as an Input Modality,CHI - Human Factors in Computing Systems,A*,"From zooming on smartphones and mid-air gestures to deformable user interfaces, thumb-index pinching grips are used in many interaction techniques. However, there is still a lack of systematic understanding of how the accuracy and efficiency of such grips are affected by various factors such as counterforce, grip span, and grip direction. Therefore, in this paper, we contribute an evaluation (N = 18) of thumb-index pinching performance in a visual targeting task using scales up to 75 items. As part of our findings, we conclude that the pinching interaction between the thumb and index finger is a promising modality also for one-dimensional input on higher scales. Furthermore, we discuss and outline implications for future user interfaces that benefit from pinching as an additional and complementary interaction modality.",Deformation; Input; Mixed Reality; Pinching; Thumb-to-finger; User Studies,Keywords,
ACM DL,conferencePaper,2022,ScalAR: Authoring Semantically Adaptive Augmented Reality Experiences in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) experiences tightly associate virtual contents with environmental entities. However, the dissimilarity of different environments limits the adaptive AR content behaviors under large-scale deployment. We propose ScalAR, an integrated workflow enabling designers to author semantically adaptive AR experiences in Virtual Reality (VR). First, potential AR consumers collect local scenes with a semantic understanding technique. ScalAR then synthesizes numerous similar scenes. In VR, a designer authors the AR contents’ semantic associations and validates the design while being immersed in the provided scenes. We adopt a decision-tree-based algorithm to fit the designer’s demonstrations as a semantic adaptation model to deploy the authored AR experience in a physical scene. We further showcase two application scenarios authored by ScalAR and conduct a two-session user study where the quantitative results prove the accuracy of the AR content rendering and the qualitative results show the usability of ScalAR.",Adaptation; Augmented Reality; Immersive Authoring; Semantic Understanding; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,immersivePOV: Filming How-To Videos with a Head-Mounted 360° Action Camera,CHI - Human Factors in Computing Systems,A*,"How-to videos are often shot using camera angles that may not be optimal for learning motor tasks, with a prevalent use of third-person perspective. We present immersivePOV, an approach to film how-to videos from an immersive first-person perspective using a head-mounted 360° action camera. immersivePOV how-to videos can be viewed in a Virtual Reality headset, giving the viewer an eye-level viewpoint with three Degrees of Freedom. We evaluated our approach with two everyday motor tasks against a baseline first-person perspective and a third-person perspective. In a between-subjects study, participants were assigned to watch the task videos and then replicate the tasks. Results suggest that immersivePOV reduced perceived cognitive load and facilitated task learning. We discuss how immersivePOV can also streamline the video production process for content creators. Altogether, we conclude that immersivePOV is an effective approach to film how-to videos for learners and content creators alike.",360° video; how-to video; online learning; POV; YouTube,Abstract,
ACM DL,conferencePaper,2022,XR-OOM: MiXed Reality driving simulation with real cars for research and design,CHI - Human Factors in Computing Systems,A*,"High-fidelity driving simulators can act as testbeds for designing in-vehicle interfaces or validating the safety of novel driver assistance features. In this system paper, we develop and validate the safety of a mixed reality driving simulator system that enables us to superimpose virtual objects and events into the view of participants engaging in real-world driving in unmodified vehicles. To this end, we have validated the mixed reality system for basic driver cockpit and low-speed driving tasks, comparing the use of the system with non-headset and with the headset driving conditions, to ensure that participants behave and perform similarly using this system as they would otherwise. This paper outlines the operational procedures and protocols for using such systems for cockpit tasks (like using the parking brake, reading the instrument panel, and turn signaling) as well as basic low-speed driving exercises (such as steering around corners, weaving around obstacles, and stopping at a fixed line) in ways that are safe, effective, and lead to accurate, repeatable data collection about behavioral responses in real-world driving tasks.",automotive; design; driving simulation; mixed reality; safety; user studies; XR,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,ControllerPose: Inside-Out Body Capture with VR Controller Cameras,CHI - Human Factors in Computing Systems,A*,"We present a new and practical method for capturing user body pose in virtual reality experiences: integrating cameras into handheld controllers, where batteries, computation and wireless communication already exist. By virtue of the hands operating in front of the user during many VR interactions, our controller-borne cameras can capture a superior view of the body for digitization. Our pipeline composites multiple camera views together, performs 3D body pose estimation, uses this data to control a rigged human model with inverse kinematics, and exposes the resulting user avatar to end user applications. We developed a series of demo applications illustrating the potential of our approach and more leg-centric interactions, such as balancing games and kicking soccer balls. We describe our proof-of-concept hardware and software, as well as results from our user study, which point to imminent feasibility.",Motion Capture; Pose Tracking; Virtual Reality,Abstract_Keywords,
ACM DL,conferencePaper,2022,Designing and Assessing a Virtual Reality Simulation to Build Resilience to Street Harassment,CHI - Human Factors in Computing Systems,A*,"Street harassment is a widespread problem that can constrain people’s freedom to enjoy public spaces safely, along with many other negative psychological impacts. However, very little research has looked at how immersive technology can help in addressing it. We conducted three studies to investigate the design decisions, ethical issues and efficacy of an immersive simulation of street harassment: an online design study (n=20), an interview study with experts working in the area (n=9), and a comparative lab study investigating design, ethics and efficacy (n=44). Our results deepen understanding of the design decisions that contribute to a realistic psychological experience, such as the effects of screen-based video vs passive VR vs interactive VR. They also highlight important ethical issues such as traumatisation and potential for victim blaming, and how they can be approached in an ethical manner. Finally, they provide insights into efficacy in terms of perceived usefulness, competence and empathy.",design; ethics; street harassment; virtual reality,Keywords_Title,
ACM DL,conferencePaper,2022,(Re)discovering the Physical Body Online: Strategies and Challenges to Approach Non-Cisgender Identity in Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The contemporary understanding of gender continues to highlight the complexity and variety of gender identities beyond a binary dichotomy regarding one’s biological sex assigned at birth. The emergence and popularity of various online social spaces also makes the digital presentation of gender even more sophisticated. In this paper, we use non-cisgender as an umbrella term to describe diverse gender identities that do not match people’s sex assigned at birth, including Transgender, Genderfluid, and Non-binary. We especially explore non-cisgender individuals’ identity practices and their challenges in novel social Virtual Reality (VR) spaces where they can present, express, and experiment their identity in ways that traditional online social spaces cannot provide. We provide one of the first empirical evidence of how social VR platforms may introduce new and novel phenomena and practices of approaching diverse gender identities online. We also contribute to re-conceptualizing technology-supported identity practices by highlighting the role of (re)discovering the physical body online and informing the design of the emerging metaverse for supporting diverse gender identities in the future.",embodied interaction; gender presentation online; non-cisgender; social virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,ProGesAR: Mobile AR Prototyping for Proxemic and Gestural Interactions with Real-world IoT Enhanced Spaces,CHI - Human Factors in Computing Systems,A*,"Real-world IoT enhanced spaces involve diverse proximity- and gesture-based interactions between users and IoT devices/objects. Prototyping such interactions benefits various applications like the conceptual design of ubicomp space. AR (Augmented Reality) prototyping provides a flexible way to achieve early-stage designs by overlaying digital contents on real objects or environments. However, existing AR prototyping approaches have focused on prototyping AR experiences or context-aware interactions from the first-person view instead of full-body proxemic and gestural (pro-ges&nbsp;for short) interactions of real users in the real world. In this work, we conducted interviews to figure out the challenges of prototyping pro-ges interactions in real-world IoT enhanced spaces. Based on the findings, we present ProGesAR, a mobile AR tool for prototyping pro-ges interactions of a subject in a real environment from a third-person view, and examining the prototyped interactions from both the first- and third- person views. Our interface supports the effects of virtual assets dynamically triggered by a single subject, with the triggering events based on four features: location, orientation, gesture, and distance. We conduct a preliminary study by inviting participants to prototype in a freeform manner using ProGesAR. The early-stage findings show that with ProGesAR, users can easily and quickly prototype their design ideas about pro-ges interactions.",AR prototyping; Gestural interaction; Mobile augmented reality; Proxemic interaction,Abstract_Keywords,
ACM DL,conferencePaper,2022,ExposAR: Bringing Augmented Reality to the Computational Thinking Agenda through a Collaborative Authoring Tool,CHI - Human Factors in Computing Systems,A*,"There is a growing focus on computational thinking (CT) in terms of supporting children’s understanding of everyday technologies. But unlike other technologies, Augmented Reality (AR) has received limited attention. In this paper, we present ExposAR – a collaborative cross-device AR system enabling children to create, use, and reflect on AR technologies through the co-creation of a simple AR application. With ExposAR, we explore three core design principles: 1) reify computational concepts, 2) support collaborative cross-device authoring, and 3) incorporate the user’s own world. These principles were developed through a co-design process with teachers and evaluated with 46 primary school students. We found that the collaborative authoring process with ExposAR supported students in understanding AR concepts and challenged their perspectives on AR. With these results, we bring AR to the CT agenda and contribute novel design principles for exposing the underlying mechanisms and implications of AR.",Augmented reality; computational thinking; education; interactive learning system,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,The Normal Natural Troubles of Virtual Reality in Mixed-Reality Performances,CHI - Human Factors in Computing Systems,A*,"Performing with technology is a complex and challenging task. Artists who use novel technologies, such as Virtual Reality, have to develop strategies of monitoring, maintenance, and recovery from errors with as minimal impact on the ongoing performance as possible. In this paper we draw on two case studies of mixed-reality performances and document strategies of Stage Managing VR Performance, Choreographing for Cables, Consistency &amp; Charging, Improvising Interventions, and Priming Participants. We discuss how these practices expose areas ripe with potential for tool development, and how they can also be used to inform the design of interaction with other technologies, such as the Internet of Things.",Artistic Interaction; Mixed-reality performance; Virtual Reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Virtual Transcendent Dream: Empowering People through Embodied Flying in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Flying dreams have the potential to evoke a feeling of empowerment (or self-efficacy, confidence in our ability to succeed) and self-transcendent experience (STE), which have been shown to contribute to an individual’s overall well-being. However, these exceptional dreaming experiences remain difficult to induce at will. Inspired by the potential of Virtual Reality (VR) to support profound emotional experiences, we explored if a VR flying interface with more embodied self-motion cues could contribute to the benefits associated with flying dreams (i.e., STE and empowerment). Our results indicated that a flying interface with more self-motion cues indeed better supported STE and empowerment. We derived several design considerations: obscurity, extraordinary light and supportive setting. Our results contribute to the discourse around design guidelines for self-transcendence and empowerment in VR, which may further be applied to the improvement of mental well-being.",Dreaming; empowerment; flight simulation; gravity imagery; self-transcendence; transcendent dream; vection; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,“It Would Be Cool to Get Stampeded by Dinosaurs”: Analyzing Children's Conceptual Model of AR Headsets Through Co-Design,CHI - Human Factors in Computing Systems,A*,"Children are being presented with augmented reality (AR) in different contexts, such as education and gaming. However, little is known about how children conceptualize AR, especially AR headsets. Prior work has shown that children's interaction behaviors and expectations of technological devices can be quite different from adults’. It is important to understand children's mental models of AR headsets to design more effective experiences for them. To elicit children's perceptions, we conducted four participatory design sessions with ten children on designing content for imaginary AR headsets. We found that children expect AR systems to be highly intelligent and to recognize and virtually transform surroundings to create immersive environments. Also, children are in favor of using these devices for difficult tasks but prefer to work on their own for easy tasks. Our work contributes new understanding on how children comprehend AR headsets and provides recommendations for designing future headsets for children.",Augmented reality; children; co-design; conceptual model; participatory design,Abstract_Keywords,
ACM DL,conferencePaper,2022,Interactive Robotic Plastering: Augmented Interactive Design and Fabrication for On-site Robotic Plastering,CHI - Human Factors in Computing Systems,A*,"This paper presents Interactive Robotic Plastering (IRoP), a system enabling designers and skilled workers to engage intuitively with an in-situ robotic plastering process. The research combines three elements: interactive design tools, an augmented reality interface, and a robotic spraying system. Plastering is a complex process relying on tacit knowledge and craftsmanship, making it difficult to simulate and automate. However, our system utilizes a controller-based interaction system to enable diverse users to interactively create articulated plasterwork in-situ. A customizable computational toolset converts human intentions into robotic motions while respecting robotic and material constraints. To accomplish this, we developed both an interactive computational model to translate the data from a motion-tracking system into robotic trajectories using design and editing tools as well as an audio-visual guidance system for in-situ projection. We then conducted two user-studies of designers and skilled workers who used IRoP to design and fabricate a full-scale demonstrator.",augmented reality; digital fabrication; robot; interactive fabrication,Abstract_Keywords,
ACM DL,conferencePaper,2022,"AirRacket: Perceptual Design of Ungrounded, Directional Force Feedback to Improve Virtual Racket Sports Experiences",CHI - Human Factors in Computing Systems,A*,"We present AirRacket, perceptual modeling and design of ungrounded, directional force feedback for virtual racket sports. Using compressed air propulsion jets to provide directional impact forces, we iteratively designed for three popular sports that span a wide range of force magnitudes: ping-pong, badminton, and tennis. To address the limited force magnitude of ungrounded force feedback technologies, we conducted a perception study which discovered the novel illusion that users perceive larger impact force magnitudes with longer impact duration, by an average factor of 2.57x. Through a series of formative, perceptual, and user experience studies with a combined total of 72 unique participants, we explored several perceptual designs using force magnitude scaling and duration scaling methods to expand the dynamic range of perceived force magnitude. Our user experience evaluation showed that perceptual designs can significantly improve realism and preference vs. physics-based designs for ungrounded force feedback systems.",Haptics; virtual reality.; air propulsion; force perception; perceptual design; ungrounded force feedback,Keywords,
ACM DL,conferencePaper,2022,Predicting Opportune Moments to Deliver Notifications in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) has increasingly been used in many areas, and the need to deliver notifications in VR is also expected to increase accordingly. However, untimely interruptions could largely impact the experience in VR. Identifying opportune times to deliver notifications to users allows for notifications to be scheduled in a way that minimizes disruption. We conducted a study to investigate the use of sensor data available on an off-the-shelf VR device and additional contextual information, including current activity and engagement of users, to predict opportune moments for sending notifications using deep learning models. Our analysis shows that using mainly sensor features could achieve 72% recall, 71% precision and 0.86 area under receiver operating characteristic (AUROC); performance can be further improved to 81% recall, 82% precision, and 0.93 AUROC if information about activity and summarized user engagement is included.",Virtual Reality; Predictive Models; Notifications; Interruptibility,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Reducing Virtual Reality Sickness for Cyclists in VR Bicycle Simulators,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) bicycle simulations aim to recreate the feeling of riding a bicycle and are commonly used in many application areas. However, current solutions still create mismatches between the visuals and physical movement, which causes VR sickness and diminishes the cycling experience. To reduce VR sickness in bicycle simulators, we conducted two controlled lab experiments addressing two main causes of VR sickness: (1) steering methods and (2) cycling trajectory. In the first experiment (N = 18) we compared handlebar, HMD, and upper-body steering methods. In the second experiment (N = 24) we explored three types of movement in VR (1D, 2D, and 3D trajectories) and three countermeasures (airflow, vibration, and dynamic Field-of-View) to reduce VR sickness. We found that handlebar steering leads to the lowest VR sickness without decreasing cycling performance and airflow suggests to be the most promising method to reduce VR sickness for all three types of trajectories.",virtual reality; cycling; bicycle simulators; VR sickness,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Vibing Together: Dance Experiences in Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Dancing is a universal human activity, and also a domain of enduring significance in Human-Computer Interaction (HCI) research. However, there has been limited investigation into how computing supports the experiences of recreational dancers. Concurrently, a diverse and sizeable dance community has been emerging in VRChat. Little is known about these dancers’ experiences, motivations, and practices. Yet shedding light into these could inform both VR technology development and the design of systems that better support embodied and complex social interactions. To bridge this gap, we interviewed participants active in the VRChat dance scene. Through thematic analysis, we identified six central facets of their experiences related to freedom, community, dance as an individual experience, dance as a shared experience, dance as a performance, and self-expression and -exploration. Based on these findings, we discuss emerging tensions and highlight beneficial impacts of dancing in VR as well as problems that still await resolving.",social VR; VRChat; dancing,Title,
ACM DL,conferencePaper,2022,Mixing in Reverse Optical Flow to Mitigate Vection and Simulation Sickness in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Simulator sickness has been one of the major obstacles toward making virtual reality (VR) widely accepted and used. For example, virtual navigation produces vection, which is the illusion of self-motion as one perceives bodily motion despite no movement actually occurs. This, in turn, causes a sensory conflict between visual and actual (or vestibular) motion and sickness. In this study, we explore a method to reduce simulator sickness by visually mixing the optical flow patterns that are in the reverse direction of the virtual visual motion. As visual motion is mainly detected and perceived by the optical flow, artificial mixing in the reverse flow is hypothesized to induce a cancellation effect, thereby reducing the degree of the conflict with the vestibular sense and sickness. To validate our hypothesis, we developed a real-time algorithm to visualize the reverse optical flow and conducted experiments by comparing the before and after sickness levels in seven virtual navigation conditions. The experimental results confirmed the proposed method was effective for reducing the simulator sickness in a statistically significant manner. However, no dependency to the motion type or degrees of freedom were found. Significant distraction and negative influence to the sense of presence and immersion were observed only when the the artificially added reverse optical flow patterns were rather visually marked with high contrast to the background content.",Virtual Reality; Optical Flow; Simulator Sickness; Vection,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Visual Delegate Generalization Frame – Evaluating Impact of Visual Effects and Elements on Player and User Experiences in Video Games and Interactive Virtual Environments,CHI - Human Factors in Computing Systems,A*,"Visual effects and elements in video games and interactive virtual environments can be applied to transfer (or delegate) non-visual perceptions (e.g., proprioception, presence, pain) to players and users, thus increasing perceptual diversity via the visual modality. Such elements or effects are referred to as visual delegates (VDs). Current findings on the experiences that VDs can elicit relate to specific VDs, not to VDs in general. Deductive and comprehensive VD evaluation frameworks are lacking. We analyzed VDs in video games to generalize VDs in terms of their visual properties. We conducted a systematic paper analysis to explore player and user experiences observed in association with specific VDs in user studies. We conducted semi-structured interviews with expert players to determine their preferences and the impact of VD properties. The resulting VD framework (VD-frame) contributes to a more strategic approach to identifying the impact of VDs on player and user experiences.",user experience; virtual reality; literature review; game analysis; graphical user interface; semi-structured interview; visual delegates; visual perception,Keywords,
ACM DL,conferencePaper,2022,"Evaluating Singing for Computer Input Using Pitch, Interval, and Melody",CHI - Human Factors in Computing Systems,A*,"In voice-based interfaces, non-verbal features represent a simple and underutilized design space for hands-free, language-agnostic interactions. We evaluate the performance of three fundamental types of voice-based musical interactions: pitch, interval, and melody. These interactions involve singing or humming a sequence of one or more notes. A 21-person study evaluates the feasibility and enjoyability of these interactions. The top performing participants were able to perform all interactions reasonably quickly (&lt;5s) with average error rates between 1.3% and 8.6% after training. Others improved with training but still had error rates as high as 46% for pitch and melody interactions. The majority of participants found all tasks enjoyable. Using these results, we propose design considerations for using singing interactions as well as potential use cases for both standard computers and augmented reality glasses.",music; non-verbal vocal interactions,Abstract,
ACM DL,conferencePaper,2022,Keep the VRhythm going: A musician-centred study investigating how Virtual Reality can support creative musical practice,CHI - Human Factors in Computing Systems,A*,"The acoustic and visual experiences of musicians in the spaces they perform in are complex and organic in nature, entailing a continuous interaction with the environment. With this project, we leverage the power of Virtual Reality (VR) to support musicians in their creative practice by transporting them to novel sonic and visual worlds. For this, we developed a musician-centred VR system, featuring various acoustic and visual virtual environments, VR Rehearse &amp; Perform, based on design requirements gathered with musicians and performance experts. To investigate how VR can be designed to support music-makers in their creative musical practice, we performed iterative tests with 19 musicians followed by semi-structured interviews. Our findings suggest that VR has the potential to support different aspects of the creative musical practice, such as rehearsing, performing and improvising. Our research provides insights and inspirations toward designing musician-centred VR experiences for various musical activities.",Virtual Reality; Music; Creativity; Improvisation; Musical practice; Performing; Rehearsing,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Janus Screen: Screen with Switchable Projection Surfaces Using Wire Grid Polarizer,CHI - Human Factors in Computing Systems,A*,"In this paper, we present a novel screen system employing polarizers that allow switching of the projection surface to the front, rear, or both sides using only two projectors on one side. In this system, we propose a method that employs two projectors equipped with polarizers and a multi-layered screen comprising an anti-reflective plate, transparent screen, and wire grid polarizer. The multi-layered screen changes whether the projected image is shown on the front or rear side of the screen depending on the polarization direction of the incident light. Hence, the proposed method can project images on the front, rear, or both sides of the screen by projecting images from either or both projectors using polarizers. In addition, the proposed method can be easily deployed by simply attaching multiple optical films. We implement a prototype and confirm that the proposed method can selectively switch the projection surface.",ubiquitous computing; spatial augmented reality; projection screen,Keywords,
ACM DL,conferencePaper,2022,Electrical Head Actuation: Enabling Interactive Systems to Directly Manipulate Head Orientation,CHI - Human Factors in Computing Systems,A*,"We propose a novel interface concept in which interactive systems directly manipulate the user's head orientation. We implement this using electrical-muscle-stimulation (EMS) of the neck muscles, which turns the head around its yaw (left/right) and pitch (up/down) axis. As the first exploration of EMS for head actuation, we characterized which muscles can be robustly actuated. Second, we evaluated the accuracy of our system for actuating participants' head orientation towards static targets and trajectories. Third, we demonstrated how it enables interactions not possible before by building a range of applications, such as (1) synchronizing head orientations of two users, which enables a user to communicate head nods to another user while listening to music, and (2) directly changing the user's head orientation to locate objects in AR. Finally, in our second study, participants felt that our head actuation contributed positively to their experience in four distinct applications.",Augmented Reality; Virtual Reality; Haptics; Electrical Muscle Stimulation,Keywords,
ACM DL,conferencePaper,2022,STRAIDE: A Research Platform for Shape-Changing Spatial Displays based on Actuated Strings,CHI - Human Factors in Computing Systems,A*,"We present STRAIDE, a string-actuated interactive display environment that allows to explore the promising potential of shape-changing interfaces for casual visualizations. At the core, we envision a platform that spatially levitates elements to create dynamic visual shapes in space. We conceptualize this type of tangible mid-air display and discuss its multifaceted design dimensions. Through a design exploration, we realize a physical research platform with adjustable parameters and modular components. For conveniently designing and implementing novel applications, we provide developer tools ranging from graphical emulators to in-situ augmented reality representations. To demonstrate STRAIDE’s reconfigurability, we further introduce three representative physical setups as a basis for situated applications including ambient notifications, personal smart home controls, and entertainment. They serve as a technical validation, lay the foundations for a discussion with developers that provided valuable insights, and encourage ideas for future usage of this type of appealing interactive installation.",Casual Visualization; Data Physicalization; Prototyping Platform; Shape-Changing Interface; Spatial Display; Tangible Interaction,Abstract,
ACM DL,conferencePaper,2022,"InfraredTags: Embedding Invisible AR Markers and Barcodes Using Low-Cost, Infrared-Based 3D Printing and Imaging Tools",CHI - Human Factors in Computing Systems,A*,"Existing approaches for embedding unobtrusive tags inside 3D&nbsp;objects require either complex fabrication or high-cost imaging equipment. We present InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye that can be 3D printed as part of objects, and detected rapidly by low-cost near-infrared cameras. We achieve this by printing objects from an infrared-transmitting filament, which infrared cameras can see through, and by having air gaps inside for the tag’s bits, which appear at a different intensity in the infrared image. We built a user interface that facilitates the integration of common tags (QR codes, ArUco markers) with the object geometry to make them 3D printable as InfraredTags. We also developed a low-cost infrared imaging module that augments existing mobile devices and decodes tags using our image processing pipeline. Our evaluation shows that the tags can be detected with little near-infrared illumination (0.2lux) and from distances as far as 250cm. We demonstrate how our method enables various applications, such as object tracking and embedding metadata for augmented reality and tangible interactions.",augmented reality; computer vision; tracking; identification; 3D printing; personal fabrication; infrared imaging; markers; unobtrusive tags,Abstract_Keywords,
ACM DL,conferencePaper,2022,AirRes Mask: A Precise and Robust Virtual Reality Breathing Interface Utilizing Breathing Resistance as Output Modality,CHI - Human Factors in Computing Systems,A*,"Increased levels of interactivity and multi-sensory stimulation have been shown to enhance the immersion of Virtual Reality experiences. We present the AirRes mask that enables users to utilize their breathing for precise natural interactions with the virtual environment without suffering from limitations of the sensing equipment such as motion artifacts. Furthermore, the AirRes mask provides breathing resistance as novel output modality that can be adjusted in real-time by the application. In a user study, we demonstrate the mask’s precision measurements for interaction as well as its ability to use breathing resistance to communicate contextual information such as adverse environmental conditions that affect the user’s virtual avatar. Our results show that the AirRes mask enhances virtual experiences and has the potential to create more immersive scenarios for applications by enforcing the perception of danger or improving situational awareness in training simulations, or for psychotherapy by providing additional physical stimuli.",Virtual Reality; Avatar; Natural Interaction; Breathing Interface; Environment; Resistance,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Mouth Haptics in VR using a Headset Ultrasound Phased Array,CHI - Human Factors in Computing Systems,A*,"Today’s consumer virtual reality (VR) systems offer limited haptic feedback via vibration motors in handheld controllers. Rendering haptics to other parts of the body is an open challenge, especially in a practical and consumer-friendly manner. The mouth is of particular interest, as it is a close second in tactile sensitivity to the fingertips, offering a unique opportunity to add fine-grained haptic effects. In this research, we developed a thin, compact, beamforming array of ultrasonic transducers, which can render haptic effects onto the mouth. Importantly, all components are integrated into the headset, meaning the user does not need to wear an additional accessory, or place any external infrastructure in their room. We explored several effects, including point impulses, swipes, and persistent vibrations. Our haptic sensations can be felt on the lips, teeth and tongue, which can be incorporated into new and interesting VR experiences.",,Abstract,
ACM DL,conferencePaper,2022,ShapeFindAR: Exploring In-Situ Spatial Search for Physical Artifact Retrieval using Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Personal fabrication is made more accessible through repositories like Thingiverse, as they replace modeling with retrieval. However, they require users to translate spatial requirements to keywords, which paints an incomplete picture of physical artifacts: proportions or morphology are non-trivially encoded through text only. We explore a vision of in-situ spatial search for (future) physical artifacts, and present ShapeFindAR, a mixed-reality tool to search for 3D models using in-situ sketches blended with textual queries. With ShapeFindAR, users search for geometry, and not necessarily precise labels, while coupling the search process to the physical environment (e.g., by sketching in-situ, extracting search terms from objects present, or tracing them). We developed ShapeFindAR for HoloLens 2, connected to a database of 3D-printable artifacts. We specify in-situ spatial search, describe its advantages, and present walkthroughs using ShapeFindAR, which highlight novel ways for users to articulate their wishes, without requiring complex modeling tools or profound domain knowledge.",Mixed Reality; Personal Fabrication; 3D-Printing; In-Situ Search; Model Repositories; Physical Artifact Retrieval; Spatial Search,Keywords_Title,
ACM DL,conferencePaper,2022,Consent in the Age of AR: Investigating The Comfort With Displaying Personal Information in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Social Media (SM) has shown that we adapt our communication and disclosure behaviors to available technological opportunities. Head-mounted Augmented Reality (AR) will soon allow to effortlessly display the information we disclosed not isolated from our physical presence (e.g., on a smartphone) but visually attached to the human body. In this work, we explore how the medium (AR vs. Smartphone), our role (being augmented vs. augmenting), and characteristics of information types (e.g., level of intimacy, self-disclosed vs. non-self-disclosed) impact the users’ comfort when displaying personal information. Conducting an online survey (N=148), we found that AR technology and being augmented negatively impacted this comfort. Additionally, we report that AR mitigated the effects of information characteristics compared to those they had on smartphones. In light of our results, we discuss that information augmentation should be built on consent and openness, focusing more on the comfort of the augmented rather than the technological possibilities.",Augmented Reality; Mixed Reality; Data Glasses; Personal Information; Social Acceptability; User Acceptance; Comfort; Consent; Disclosure; Public Experiences,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Elements of XR Prototyping: Characterizing the Role and Use of Prototypes in Augmented and Virtual Reality Design,CHI - Human Factors in Computing Systems,A*,"Current research in augmented, virtual, and mixed reality (XR) reveals a lack of tool support for designing and, in particular, prototyping XR applications. While recent tools research is often motivated by studying the requirements of non-technical designers and end-user developers, the perspective of industry practitioners is less well understood. In an interview study with 17 practitioners from different industry sectors working on professional XR projects, we establish the design practices in industry, from early project stages to the final product. To better understand XR design challenges, we characterize the different methods and tools used for prototyping and describe the role and use of key prototypes in the different projects. We extract common elements of XR prototyping, elaborating on the tools and materials used for prototyping and establishing different views on the notion of fidelity. Finally, we highlight key issues for future XR tools research.",augmented reality; interaction design; virtual reality; XR; mixed reality; prototyping; authoring; interface design.,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Unmaking as Agonism: Using Participatory Design with Youth to Surface Difference in an Intergenerational Urban Context,CHI - Human Factors in Computing Systems,A*,"Design has been used to contest existing socio-technical arrangements, provoke conversations around matters of concern, and operationalize radical theories such as agonism, which embraces difference and contention. However, the focus is usually on creating something new: a product, interface or artifact. In this paper, we investigate what happens when critical unmaking is deployed as a deliberate design strategy in an intergenerational, agonistic urban context. Specifically, we report on how youth in a six-week design internship used unmaking as a design move to subvert conventional narratives about their surrounding urban context. We analyze how this led to conflictual encounters at the local senior center, and compare it to the other, making-centric proposals which received favorable feedback but failed to raise the same important discussions. Through this ethnographic account, we argue that critical unmaking is important yet overlooked, and should be in the repertoire of design moves available for agonism and provocation.",virtual reality; making; participatory design; older adults; civic engagement; youth; unmaking; agonism; critical unmaking,Keywords,
ACM DL,conferencePaper,2022,TapGazer: Text Entry with Finger Tapping and Gaze-directed Word Selection,CHI - Human Factors in Computing Systems,A*,"While using VR, efficient text entry is a challenge: users cannot easily locate standard physical keyboards, and keys are often out of reach, e.g. when standing. We present TapGazer, a text entry system where users type by tapping their fingers in place. Users can tap anywhere as long as the identity of each tapping finger can be detected with sensors. Ambiguity between different possible input words is resolved by selecting target words with gaze. If gaze tracking is unavailable, ambiguity is resolved by selecting target words with additional taps. We evaluated TapGazer for seated and standing VR: seated novice users using touchpads as tap surfaces reached 44.81 words per minute (WPM), 79.17% of their QWERTY typing speed. Standing novice users tapped on their thighs with touch-sensitive gloves, reaching 45.26 WPM (71.91%). We analyze TapGazer with a theoretical performance model and discuss its potential for text input in future AR scenarios.",Virtual Reality; Eye Tracking; Text Entry; Input Techniques; Typing,Keywords,
ACM DL,conferencePaper,2022,HybridTrak: Adding Full-Body Tracking to VR Using an Off-the-Shelf Webcam,CHI - Human Factors in Computing Systems,A*,"Full-body tracking in virtual reality improves presence, allows interaction via body postures, and facilitates better social expression among users. However, full-body tracking systems today require a complex setup fixed to the environment (e.g., multiple lighthouses/cameras) and a laborious calibration process, which goes against the desire to make VR systems more portable and integrated. We present HybridTrak, which provides accurate, real-time full-body tracking by augmenting inside-out1 upper-body VR tracking systems with a single external off-the-shelf RGB web camera. HybridTrak uses a full-neural solution to convert and transform users’ 2D full-body poses from the webcam to 3D poses leveraging the inside-out upper-body tracking data. We showed HybridTrak is more accurate than RGB or depth-based tracking methods on the MPI-INF-3DHP dataset. We also tested HybridTrak in the popular VRChat app and showed that body postures presented by HybridTrak are more distinguishable and more natural than a solution using an RGBD camera.",virtual reality; computer vision.; full-body tracking,Abstract_Keywords,
ACM DL,conferencePaper,2022,In-Depth Mouse: Integrating Desktop Mouse into Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) has potential for productive knowledge work, however, midair pointing with controllers or hand gestures does not offer the precision and comfort of traditional 2D mice. Directly integrating mice into VR is difficult as selecting targets in a 3D space is negatively impacted by binocular rivalry, perspective mismatch, and improperly calibrated control-display (CD) gain. To address these issues, we developed Depth-Adaptive Cursor&nbsp;, a 2D-mouse driven pointing technique for 3D selection with depth-adaptation that continuously interpolates the cursor depth by inferring what users intend to select based on the cursor position, the viewpoint, and the selectable objects. Depth-Adaptive Cursor&nbsp;uses a novel CD gain tool to compute a usable range of CD gains for general mouse-based pointing in VR. A user study demonstrated that Depth-Adaptive Cursor&nbsp;significantly improved performance compared with an existing mouse-based pointing technique without depth-adaption in terms of time (21.2%), error (48.3%), perceived workload, and user satisfaction.",Virtual Reality; target selection; 3D pointing; virtual workspace,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,OVRlap: Perceiving Multiple Locations Simultaneously to Improve Interaction in VR,CHI - Human Factors in Computing Systems,A*,"We introduce OVRlap, a VR interaction technique that lets the user perceive multiple places at the same time from a first-person perspective. OVRlap achieves this by overlapping viewpoints. At any time, only one viewpoint is active, meaning that the user may interact with objects therein. Objects seen from the active viewpoint are opaque, whereas objects seen from passive viewpoints are transparent. This allows users to perceive multiple locations at once and easily switch to the one in which they want to interact. We compare OVRlap and a single-viewpoint technique in a study where 20 participants complete object-collection and monitoring tasks. We find that in both tasks, participants are significantly faster and move their head significantly less with OVRlap. We propose how the technique might be improved through automated switching of the active viewpoint and intelligent viewpoint rendering.",virtual reality; user studies; interaction techniques; large environments,Keywords,
ACM DL,conferencePaper,2022,ImpactVest: Rendering Spatio-Temporal Multilevel Impact Force Feedback on Body in VR,CHI - Human Factors in Computing Systems,A*,"Rendering instant and intense impact feedback on users’ hands, limbs and head to enhance realism in virtual reality (VR) has been proposed in previous works, but impact on the body is still less discussed. With the body’s large surface area to utilize, numerous impact patterns can be rendered in versatile VR applications, e.g.,&nbsp;being shot, blasted, punched or slashed on body in VR games. Herein we propose ImpactVest to render spatio-temporal multilevel impact force feedback on body. By independently controlling nine impactors in a 3 × 3 layout using elastic force, impact is generated at different levels, positions and time sequences for versatile spatial and temporal combinations. We conducted a just-noticeable difference (JND) study to understand users’ impact level distinguishability on the body. A time interval threshold study was then performed to ascertain what time interval thresholds between two impact stimuli should be used to distinguish from simultaneous impact, a continuous impact stroke and two discrete impact stimuli. Based on the results, we conducted a VR experience study to verify that impact feedback from ImpactVest enhances VR realism.",Haptics; impact; virtual reality.; force feedback; wearable device,Abstract_Keywords,
ACM DL,conferencePaper,2022,Kuiper Belt: Utilizing the “Out-of-natural Angle” Region in the Eye-gaze Interaction for Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The maximum physical range of horizontal human eye movement is approximately 45°. However, in a natural gaze shift, the difference in the direction of the gaze relative to the frontal direction of the head rarely exceeds 25°. We name this region of 25° − 45° the “Kuiper Belt” in the eye-gaze interaction. We try to utilize this region to solve the Midas touch problem to enable a search task while reducing false input in the Virtual Reality environment. In this work, we conduct two studies to figure out the design principle of how we place menu items in the Kuiper Belt as an “out-of-natural angle” region of the eye-gaze movement, and determine the effectiveness and workload of the Kuiper Belt-based method. The results indicate that the Kuiper Belt-based method facilitated the visual search task while reducing false input. Finally, we present example applications utilizing the findings of these studies.",Virtual Reality; Eye Tracking; Eye-gaze Interface; Menu Item Selection,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Beyond Being Real: A Sensorimotor Control Perspective on Interactions in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We can create Virtual Reality (VR) interactions that have no equivalent in the real world by remapping spacetime or altering users’ body representation, such as stretching the user’s virtual arm for manipulation of distant objects or scaling up the user’s avatar to enable rapid locomotion. Prior research has leveraged such approaches, what we call beyond-real techniques, to make interactions in VR more practical, efficient, ergonomic, and accessible. We present a survey categorizing prior movement-based VR interaction literature as reality-based, illusory, or beyond-real interactions. We survey relevant conferences (CHI, IEEE VR, VRST, UIST, and DIS) while focusing on selection, manipulation, locomotion, and navigation in VR. For beyond-real interactions, we describe the transformations that have been used by prior works to create novel remappings. We discuss open research questions through the lens of the human sensorimotor control system and highlight challenges that need to be addressed for effective utilization of beyond-real interactions in future VR applications, including plausibility, control, long-term adaptation, and individual differences.",interaction design; virtual reality; framework; sensorimotor control,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,There Is No First- or Third-Person View in Virtual Reality: Understanding the Perspective Continuum,CHI - Human Factors in Computing Systems,A*,"Modern games make creative use of First- and Third-person perspectives (FPP and TPP) to allow the player to explore virtual worlds. Traditionally, FPP and TPP perspectives are seen as distinct concepts. Yet, Virtual Reality (VR) allows for flexibility in choosing perspectives. We introduce the notion of a perspective continuum in VR, which is technically related to the camera position and conceptually to how users perceive their environment in VR. A perspective continuum enables adapting and manipulating the sense of agency and involvement in the virtual world. This flexibility of perspectives broadens the design space of VR experiences through deliberately manipulating perception. In a study, we explore users’ attitudes, experiences and perceptions while controlling a virtual character from the two known perspectives. Statistical analysis of the empirical results shows the existence of a perspective continuum in VR. Our findings can be used to design experiences based on shifts of perception.",Virtual Reality; Embodiment; First Person; Perspective; Third Person,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,BikeAR: Understanding Cyclists’ Crossing Decision-Making at Uncontrolled Intersections using Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Cycling has become increasingly popular as a means of transportation. However, cyclists remain a highly vulnerable group of road users. According to accident reports, one of the most dangerous situations for cyclists are uncontrolled intersections, where cars approach from both directions. To address this issue and assist cyclists in crossing decision-making at uncontrolled intersections, we designed two visualizations that: (1) highlight occluded cars through an X-ray vision and (2) depict the remaining time the intersection is safe to cross via a Countdown. To investigate the efficiency of these visualizations, we proposed an Augmented Reality simulation as a novel evaluation method, in which the above visualizations are represented as AR, and conducted a controlled experiment with 24 participants indoors. We found that the X-ray ensures a fast selection of shorter gaps between cars, while the Countdown facilitates a feeling of safety and provides a better intersection overview.",augmented reality; crossing decision-making; cyclist safety,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,"Effects of Pedestrian Behavior, Time Pressure, and Repeated Exposure on Crossing Decisions in Front of Automated Vehicles Equipped with External Communication",CHI - Human Factors in Computing Systems,A*,"Automated vehicles are expected to substitute driver-pedestrian communication via LED strips or displays. This communication is expected to improve trust and the crossing process in general. However, numerous factors such as other pedestrians’ behavior, perceived time pressure, or previous experience influence crossing decisions. Therefore, we report the results of a triply subdivided Virtual Reality study (N=18) evaluating these. Results show that external communication was perceived as hedonically pleasing, increased perceived safety and trust, and also that pedestrians’ behavior affected participants’ behavior. A timer did not alter crossing behavior, however, repeated exposure increased trust and reduced crossing times, showing a habituation effect. Our work helps better to integrate research on external communication in ecologically valid settings.",Autonomous vehicles; Chicken Game; eHMI.; External communication; Pedestrian Behavior,Abstract,
ACM DL,conferencePaper,2022,Paper Trail: An Immersive Authoring System for Augmented Reality Instructional Experiences,CHI - Human Factors in Computing Systems,A*,"Prior work has demonstrated augmented reality’s benefits to education, but current tools are difficult to integrate with traditional instructional methods. We present Paper Trail, an immersive authoring system designed to explore how to enable instructors to create AR educational experiences, leaving paper at the core of the interaction and enhancing it with various forms of digital media, animations for dynamic illustrations, and clipping masks to guide learning. To inform the system design, we developed five scenarios exploring the benefits that hand-held and head-worn AR can bring to STEM instruction and developed a design space of AR interactions enhancing paper based on these scenarios and prior work. Using the example of an AR physics handout, we assessed the system’s potential with PhD-level instructors and its usability with XR design experts. In an elicitation study with high-school teachers, we study how Paper Trail could be used and extended to enable flexible use cases across various domains. We discuss benefits of immersive paper for supporting diverse student needs and challenges for making effective use of AR for learning.",augmented reality; educational technology; immersive paper; STEM.,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Smooth as Steel Wool: Effects of Visual Stimuli on the Haptic Perception of Roughness in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Haptic Feedback is essential for lifelike Virtual Reality (VR) experiences. To provide a wide range of matching sensations of being touched or stroked, current approaches typically need large numbers of different physical textures. However, even advanced devices can only accommodate a limited number of textures to remain wearable. Therefore, a better understanding is necessary of how expectations elicited by different visualizations affect haptic perception, to achieve a balance between physical constraints and great variety of matching physical textures. In this work, we conducted an experiment (N=31) assessing how the perception of roughness is affected within VR. We designed a prototype for arm stroking and compared the effects of different visualizations on the perception of physical textures with distinct roughnesses. Additionally, we used the visualizations’ real-world materials, no-haptics and vibrotactile feedback as baselines. As one result, we found that two levels of roughness can be sufficient to convey a realistic illusion.",virtual reality; perception; haptic feedback; stroke; touch; caress; roughness,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,The Role of Staff in Facilitating Immersive Virtual Reality for Enrichment in Aged Care: An Ethic of Care Perspective,CHI - Human Factors in Computing Systems,A*,"Immersive virtual reality (VR) is being used as an enriching experience for people living in residential aged care, or nursing homes, where care staff play a critical role supporting clients to use VR. In HCI research concerned with technology use in aged care, however, the role of formal caregivers has received limited attention. We conducted interviews with 11 caregivers working in care homes that have implemented VR as part of the social program offered to residents. Our findings highlight tensions between the opportunities created by the immersive VR experience and the risks and challenges full immersion presents for people in aged care. In this paper, we draw on an ethics of care framework to make visible the care practices involved in facilitating VR in aged care homes, highlighting the care required to ensure that older adults experience benefits when using immersive VR, while risks and challenges are carefully managed.",Virtual Reality; Aging; Aged Care; Ethics of Care,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Proxemics for Human-Agent Interaction in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) embeds virtual content in physical spaces, including virtual agents that are known to exert a social presence on users. Existing design guidelines for AR rarely consider the social implications of an agent’s personal space (PS) and that it can impact user behavior and arousal. We report an experiment (N=54) where participants interacted with agents in an AR art gallery scenario. When participants approached six virtual agents (i.e., two males, two females, a humanoid robot, and a pillar) to ask for directions, we found that participants respected the agents’ PS and modulated interpersonal distances according to the human-like agents’ perceived gender. When participants were instructed to walk through the agents, we observed heightened skin-conductance levels that indicate physiological arousal. These results are discussed in terms of proxemic theory that result in design recommendations for implementing pervasive AR experiences with virtual agents.",Augmented Reality; Proxemics; Perception; Human-Agent Interaction; Personal Space; Virtual Agents,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Understanding and Designing Avatar Biosignal Visualizations for Social Virtual Reality Entertainment,CHI - Human Factors in Computing Systems,A*,"Visualizing biosignals can be important for social Virtual Reality (VR), where avatar non-verbal cues are missing. While several biosignal representations exist, designing effective visualizations and understanding user perceptions within social VR entertainment remains unclear. We adopt a mixed-methods approach to design biosignals for social VR entertainment. Using survey (N=54), context-mapping (N=6), and co-design (N=6) methods, we derive four visualizations. We then ran a within-subjects study (N=32) in a virtual jazz-bar to investigate how heart rate (HR) and breathing rate (BR) visualizations, and signal rate, influence perceived avatar arousal, user distraction, and preferences. Findings show that skeuomorphic visualizations for both biosignals allow differentiable arousal inference; skeuomorphic and particles were least distracting for HR, whereas all were similarly distracting for BR; biosignal perceptions often depend on avatar relations, entertainment type, and emotion inference of avatars versus spaces. We contribute HR and BR visualizations, and considerations for designing social VR entertainment biosignal visualizations.",virtual reality; design; perception; visualization; social VR; Biosignals; entertainment,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Hand Interfaces: Using Hands to Imitate Objects in AR/VR for Expressive Interactions,CHI - Human Factors in Computing Systems,A*,"Augmented reality (AR) and virtual reality (VR) technologies create exciting new opportunities for people to interact with computing resources and information. Less exciting is the need for holding hand controllers, which limits applications that demand expressive, readily available interactions. Prior research investigated freehand AR/VR input by transforming the user’s body into an interaction medium. In contrast to previous work that has users’ hands grasp virtual objects, we propose a new interaction technique that lets users’ hands become virtual objects by imitating the objects themselves. For example, a thumbs-up hand pose is used to mimic a joystick. We created a wide array of interaction designs around this idea to demonstrate its applicability in object retrieval and interactive control tasks. Collectively, we call these interaction designs Hand Interfaces. From a series of user studies comparing Hand Interfaces against various baseline techniques, we collected quantitative and qualitative feedback, which indicates that Hand Interfaces are effective, expressive, and fun to use.",Interaction design; AR/VR; Embodiment; Free-hand interactions; Imitation; On-body interactions,Abstract,
ACM DL,conferencePaper,2022,FingerX: Rendering Haptic Shapes of Virtual Objects Augmented by Real Objects using Extendable and Withdrawable Supports on Fingers,CHI - Human Factors in Computing Systems,A*,"Interacting with not only virtual but also real objects, or even virtual objects augmented by real objects becomes a trend of virtual reality (VR) interactions and is common in augmented reality (AR). However, current haptic shape rendering devices generally focus on feedback of virtual objects, and require the users to put down or take off those devices to perceive real objects. Therefore, we propose FingerX to render haptic shapes and enable users to touch, grasp and interact with virtual and real objects simultaneously. An extender on the fingertip extends to a corresponding height to support between the fingertip and the real objects or the hand, to render virtual shapes. A ring rotates and withdraws the extender behind the fingertip when touching real objects. By independently controlling four extenders and rings on each finger with the exception of the pinky finger, FingerX renders feedback in three common scenarios, including touching virtual objects augmented by real environments (e.g.,&nbsp;a desk), grasping virtual objects augmented by real objects (e.g.,&nbsp;a bottle) and grasping virtual objects in the hand. We conducted a shape recognition study to evaluate the recognition rates for these three scenarios and obtained an average recognition rate of 76.59% with shape visual feedback. We then performed a VR study to observe how users interact with virtual and real objects simultaneously and verify that FingerX significantly enhances VR realism, compared to current vibrotactile methods.",virtual reality; Haptic feedback; wearable device; augmented reality.; shape rendering,Abstract_Keywords,
ACM DL,conferencePaper,2022,Haptic Fidelity Framework: Defining the Factors of Realistic Haptic Feedback for Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Providing haptic feedback in virtual reality to make the experience more realistic has become a strong focus of research in recent years. The resulting haptic feedback systems differ greatly in their technologies, feedback possibilities, and overall realism making it challenging to compare different systems. We propose the Haptic Fidelity Framework providing the means to describe, understand and compare haptic feedback systems. The framework locates a system in the spectrum of providing realistic or abstract haptic feedback using the Haptic Fidelity dimension. It comprises 14 criteria that either describe foundational or limiting factors. A second Versatility dimension captures the current trade-off between highly realistic but application-specific and more abstract but widely applicable feedback. To validate the framework, we compared the Haptic Fidelity score to the perceived feedback realism of evaluations from 38 papers and found a strong correlation suggesting the framework accurately describes the realism of haptic feedback.",user experience; realism; haptics; immersion; framework; haptic feedback; feedback; fidelity; versatility; virtual environment,Abstract_Title,
ACM DL,conferencePaper,2022,SpinOcchio: Understanding Haptic-Visual Congruency of Skin-Slip in VR with a Dynamic Grip Controller,CHI - Human Factors in Computing Systems,A*,"This paper’s goal is to understand the haptic-visual congruency perception of skin-slip on the fingertips given visual cues in Virtual Reality (VR). We developed SpinOcchio (Spin for the spinning mechanism used, Occhio for the Italian word “eye”), a handheld haptic controller capable of rendering the thickness and slipping of a virtual object pinched between two fingers. This is achieved using a mechanism with spinning and pivoting disks that apply a tangential skin-slip movement to the fingertips. With SpinOcchio, we determined the baseline haptic discrimination threshold for skin-slip, and, using these results, we tested how haptic realism of motion and thickness is perceived with varying visual cues in VR. Surprisingly, the results show that in all cases, visual cues dominate over haptic perception. Based on these results, we suggest applications that leverage skin-slip and grip interaction, contributing further to realistic experiences in VR.",Virtual Reality; Haptics; controller; normal force; skin-slip,Abstract_Keywords,
ACM DL,conferencePaper,2022,Virtual Feed: Design and Evaluation of a Virtual Reality Simulation Addressing the Lived Experience of Breastfeeding,CHI - Human Factors in Computing Systems,A*,"Breastfeeding can be challenging, but it is difficult for antenatal education to convey issues associated with the lived experience of breastfeeding. In our work, we explore the potential of interactive simulations to support antenatal education, and present Virtual Feed, a Virtual Reality breastfeeding simulation for parents-to-be developed following a three-step process. (1) We created an experience prototype that features basic VR scenarios and a tangible baby, (2) we engaged in design sessions with 19 parents and parents-to-be to derive design implications to further refine the simulation, and (3) we evaluated the system through case studies to examine the perspectives of parents and parents-to-be on the simulation. Our results show that the simulation successfully engaged users and sparked curiosity, while also encouraging reflection about the challenges of breastfeeding. On this basis, we discuss challenges for the design of simulations with the purpose of supplementing antenatal education.",qualitative research; virtual reality; co-design; breastfeeding,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,TapType: Ten-finger text entry on everyday surfaces via Bayesian inference,CHI - Human Factors in Computing Systems,A*,"Despite the advent of touchscreens, typing on physical keyboards remains most efficient for entering text, because users can leverage all fingers across a full-size keyboard for convenient typing. As users increasingly type on the go, text input on mobile and wearable devices has had to compromise on full-size typing. In this paper, we present TapType, a mobile text entry system for full-size typing on passive surfaces—without an actual keyboard. From the inertial sensors inside a band on either wrist, TapType decodes and relates surface taps to a traditional QWERTY keyboard layout. The key novelty of our method is to predict the most likely character sequences by fusing the finger probabilities from our Bayesian neural network classifier with the characters’ prior probabilities from an n-gram language model. In our online evaluation, participants on average typed 19 words per minute with a character error rate of 0.6% after 30 minutes of training. Expert typists thereby consistently achieved more than 25&nbsp;WPM at a similar error rate. We demonstrate applications of TapType in mobile use around smartphones and tablets, as a complement to interaction in situated Mixed Reality outside visual control, and as an eyes-free mobile text input method using an audio feedback-only interface.",virtual reality; Bayesian inference; Bayesian neural network; invisible interfaces; mobile text entry; n-gram language model,Abstract_Keywords,
ACM DL,conferencePaper,2022,Enhanced Videogame Livestreaming by Reconstructing an Interactive 3D Game View for Spectators,CHI - Human Factors in Computing Systems,A*,"Many videogame players livestream their gameplay so remote spectators can watch for enjoyment, fandom, and to learn strategies and techniques. Current approaches capture the player’s rendered RGB view of the game, and then encode and stream it as a 2D live video feed. We extend this basic concept by also capturing the depth buffer, camera pose, and projection matrix from the rendering pipeline of the videogame and package them all within a MPEG-4 media container. Combining these additional data streams with the RGB view, our system builds a real-time, cumulative 3D representation of the live game environment for spectators. This enables each spectator to individually control a personal game view in 3D. This means they can watch the game from multiple perspectives, enabling a new kind of videogame spectatorship experience.",virtual reality; 3D video; graphics hacking; videogame streaming,Keywords,
ACM DL,conferencePaper,2022,SABLIER : a Tangible Interactor to Navigate through Space and Time,CHI - Human Factors in Computing Systems,A*,"Historians use spatio-temporal navigation for their models and studies of historical evolutions and events. Their findings can then be exhibited in cultural mediation centers or museums. The latter, both to facilitate the transmission of knowledge and to make their exhibitions more attractive, are now exploiting new technologies. Indeed, digital systems allow, among other things, visitors to navigate spatially and temporally in virtual reconstructions of historical environments. We propose to combine these virtual representations with a tangible interface to provide visitors with an immersive experience and engaging interactions. To do so, we have set up a co-design process involving cultural mediation actors (museum directors, historians, etc.). The result is SABLIER, a tangible interactor to navigate through space and time based on the interaction metaphors and natural affordance of an hourglass. Finally, we have conducted an evaluation of the acceptability of our interactor, whose results are positive.",Virtual Reality; Tangible User Interface; Cultural Mediation,Keywords,
ACM DL,conferencePaper,2022,Tangible Globes for Data Visualisation in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Head-mounted augmented reality (AR) displays allow for the seamless integration of virtual visualisation with contextual tangible references, such as physical (tangible) globes. We explore the design of immersive geospatial data visualisation with AR and tangible globes. We investigate the “tangible-virtual interplay” of tangible globes with virtual data visualisation, and propose a conceptual approach for designing immersive geospatial globes. We demonstrate a set of use cases, such as augmenting a tangible globe with virtual overlays, using a physical globe as a tangible input device for interacting with virtual globes and maps, and linking an augmented globe to an abstract data visualisation. We gathered qualitative feedback from experts about our use case visualisations, and compiled a summary of key takeaways as well as ideas for envisioned future improvements. The proposed design space, example visualisations and lessons learned aim to guide the design of tangible globes for data visualisation in AR.",augmented reality; immersive analytics; tangible user interface; geographic visualisation,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,SkyPort: Investigating 3D Teleportation Methods in Virtual Environments,CHI - Human Factors in Computing Systems,A*,"Teleportation has become the de facto standard of locomotion in Virtual Reality (VR) environments. However, teleportation with parabolic and linear target aiming methods is restricted to horizontal 2D planes and it is unknown how they transfer to the 3D space. In this paper, we propose six 3D teleportation methods in virtual environments based on the combination of two existing aiming methods (linear and parabolic) and three types of transitioning to a target (instant, interpolated and continuous). To investigate the performance of the proposed teleportation methods, we conducted a controlled lab experiment (N = 24) with a mid-air coin collection task to assess accuracy, efficiency and VR sickness. We discovered that the linear aiming method leads to faster and more accurate target selection. Moreover, a combination of linear aiming and instant transitioning leads to the highest efficiency and accuracy without increasing VR sickness.",virtual reality; locomotion; teleportation; virtual environments,Abstract_Keywords,
ACM DL,conferencePaper,2022,Understanding User Experiences Across VR Walking-in-place Locomotion Methods,CHI - Human Factors in Computing Systems,A*,"Navigating large-scale virtual spaces is a major challenge in Virtual Reality (VR) applications due to real-world spatial limitations. Walking-in-place (WIP) locomotion solutions may provide a natural approach for VR use cases that require locomotion to share similar qualities with walking in real-life. However, there is limited knowledge on the range of experiences across common WIP methods to inform the design of usable WIP solutions using consumer-accessible components. This paper contributes to this knowledge via a user study with 40 participants that experienced several easy-to-setup WIP methods in a VR commuting simulation. A nuanced understanding of cybersickness and exertion relationships and walking affordances based on different tracker setups were among the findings derived from a corroborated analysis of think-aloud, interview, and observational data, supplemented with self-reports of VR sickness, presence and flow. Practical design insights were then constructed along the dimensions of cybersickness, affordances, space and user interfaces.",Virtual Reality; Immersion; Locomotion; Walking-In-Place,Abstract_Keywords,
ACM DL,conferencePaper,2022,Designing for Inaccessible Emergency Medical Service Contexts: Development and Evaluation of the Contextual Secondary Video Toolkit,CHI - Human Factors in Computing Systems,A*,"Designing technology for emergency medical services (EMS) can be difficult, for example, due to limited access to domain experts. To support designers who aim to engage in a participatory design process in EMS environments, we created and evaluated the Contextual Secondary Video Toolkit (CSVT). This method uses secondary video material and design cards that allow domain experts to identify and prioritise challenges in their work environment and generate design ideas that address them. We illustrate the effects of the CSVT on design processes by analysing four workshops during which aeromedical EMS staff explored the potential of augmented reality to support their work. Our results indicate that the CSVT can support reflection about work practices, aid the generation of design ideas, and facilitate genuine participation. Furthermore, our data indicates that the use of secondary video in design projects is appropriate and even has certain advantages compared to primary field video.",Design method; Participatory design; Augmented reality; Emergency medical services; Head-worn display; Secondary video,Abstract_Keywords,
ACM DL,conferencePaper,2022,Exploring Spatial UI Transition Mechanisms with Head-Worn Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Imagine in the future people comfortably wear augmented reality (AR) displays all day, how do we design interfaces that adapt to the contextual changes as people move around? In current operating systems, the majority of AR content defaults to staying at a fixed location until being manually moved by the users. However, this approach puts the burden of user interface (UI) transition solely on users. In this paper, we first ran a bodystorming design workshop to capture the limitations of existing manual UI transition approaches in spatially diverse tasks. Then we addressed these limitations by designing and evaluating three UI transition mechanisms with different levels of automation and controllability (low-effort manual, semi-automated, fully-automated). Furthermore, we simulated imperfect contextual awareness by introducing prediction errors with different costs to correct them. Our results provide valuable lessons about the trade-offs between UI automation levels, controllability, user agency, and the impact of prediction errors.",automation; agency; adaptive interfaces; controllability,Abstract_Title,
ACM DL,conferencePaper,2022,Dually Noted: Layout-Aware Annotations with Smartphone Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Sharing annotations encourages feedback, discussion, and knowledge passing among readers and can be beneficial for personal and public use. Prior augmented reality (AR) systems have expanded these benefits to both digital and printed documents. However, despite smartphone AR now being widely available, there is a lack of research about how to use AR effectively for interactive document annotation. We propose Dually Noted, a smartphone-based AR annotation system that recognizes the layout of structural elements in a printed document for real-time authoring and viewing of annotations. We conducted experience prototyping with eight users to elicit potential benefits and challenges within smartphone AR, and this informed the resulting Dually Noted system and annotation interactions with the document elements. AR annotation is often unwieldy, but during a 12-user empirical study our novel structural understanding component allows Dually Noted to improve precise highlighting and annotation interaction accuracy by 13%, increase interaction speed by 42%, and significantly lower cognitive load over a baseline method without document layout understanding. Qualitatively, participants commented that Dually Noted was a swift and portable annotation experience. Overall, our research provides new methods and insights for how to improve AR annotations for physical documents.",augmented reality; text; smartphone; Annotation; document interaction; layout structure; paper,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces,CHI - Human Factors in Computing Systems,A*,"This paper contributes to a taxonomy of augmented reality and robotics based on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have emerged as a new way to enhance human-robot interaction (HRI) and robotic interfaces (e.g., actuated and shape-changing interfaces). Recently, an increasing number of studies in HCI, HRI, and robotics have demonstrated how AR enables better interactions between people and robots. However, often research remains focused on individual explorations and key design strategies, and research questions are rarely analyzed systematically. In this paper, we synthesize and categorize this research field in the following dimensions: 1) approaches to augmenting reality; 2) characteristics of robots; 3) purposes and benefits; 4) classification of presented information; 5) design components and strategies for visual augmentation; 6) interaction techniques and modalities; 7) application domains; and 8) evaluation strategies. We formulate key challenges and opportunities to guide and inform future research in AR and robotics.",augmented reality; mixed reality; human-robot interaction; robotics; actuated tangible UI; AR-HRI; shape-changing UI; VAM-HRI,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Overcoming Legacy Bias: Re-Designing Gesture Interactions in Virtual Reality With a San Community in Namibia,CHI - Human Factors in Computing Systems,A*,"Recent improvements in hand-tracking technologies support novel applications and developments of gesture interactions in virtual reality (VR). Current implementations are mostly convention-based, originating in a Western technological context, thereby creating a legacy bias in gesture interaction implementations. With expanding application contexts and growing user groups and contexts, the design and selection of gestures need to be diversified. In this paper we present an exploration of natural gestures, followed by their implementation in a VR application and co-design of new gestures with a marginalized San community in Namibia. This study contributes to the still scarce empirical work in user-driven gesture design research, aiming to reduce legacy bias, on a methodological and technical level as well as through engaging non-WEIRD participants. Our findings confirm the applicability of our method, combined with Partner and Priming suggested by Morris et al., to the design of gestures inspired by natural interactions. We also consider the implementation of user-designed gestures to be necessary to asses usability, usefulness and technical issues in VR. Furthermore, the research directly advances the HCI agenda for diversity, through an ongoing research and design partnership with an indigenous community in Southern Africa, thereby challenging systemic bias and promoting design for the pluriverse.",Virtual Reality; Namibia; Diversity; Community; Hand Gestures; Indigenous People; Natural Interaction; User Experiences; San,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,How Will VR Enter University Classrooms? Multi-stakeholders Investigation of VR in Higher Education,CHI - Human Factors in Computing Systems,A*,"VR has received increased attention as an educational tool and many argue it is destined to influence educational practices, especially with the emergence of the Metaverse. Most prior research on educational VR reports on applications or systems designed for specified educational or training objectives. However, it is also crucial to understand current practices and attitudes across disciplines, having a holistic view to extend the body of knowledge in terms of VR adoption in an authentic setting. Taking a higher-level perception of people in different roles, we conducted a qualitative analysis based on 23 interviews with major stakeholders and a series of participatory design workshops with instructors and students. We identified the stakeholders who need to be considered for using VR in higher education, and highlighted the challenges and opportunities critical for VR current and potential practices in the university classroom. Finally, we discussed the design implications based on our findings. This study contributes a detailed description of current perceptions and considerations from a multi-stakeholder perspective, providing new empirical insights for designing novel VR and HCI technologies in higher education.",Virtual Reality; collaboration; higher education; social VR; educational VR; multi-stakeholder,Abstract_Keywords,
ACM DL,conferencePaper,2022,Bringing Patient Mannequins to Life: 3D Projection Enhances Nursing Simulation,CHI - Human Factors in Computing Systems,A*,"Mannequin-based simulations are widely used to train novice nurses. However, current mannequins have no dynamic facial expressions, which decreases the mannequins’ fidelity and impacts students’ learning outcomes and experience. This study proposes a projection-based AR system for overlaying dynamic facial expressions on a mannequin and implements the system in a stroke simulation. Thirty-six undergraduate nursing students participated in the study and were equally divided into the control (without the system) and experimental group (with the system). The participants’ gaze behavior, simulation performance, and subjective evaluation were measured. Results illustrated that the participants focused more on the face-animated mannequin than the traditional mannequin during the simulation. Nursing experts believed that the face-animated mannequin increased the participants’ performance in recognizing deviations but decreased their performance in seeking additional information. Moreover, the participants reported that the face-animated mannequin was more interactive and helpful for performing appropriate assessments than the traditional mannequin.",interactive mannequin; mixed/augmented reality; nursing simulation,Keywords,
ACM DL,conferencePaper,2022,Towards Modeling of Virtual Reality Welding Simulators to Promote Accessible and Scalable Training,CHI - Human Factors in Computing Systems,A*,"The US manufacturing industry is currently facing a welding workforce shortage which is largely due to inadequacy of widespread welding training. To address this challenge, we present a Virtual Reality (VR)-based training system aimed at transforming state-of-the-art-welding simulations and in-person instruction into a widely accessible and engaging platform. We applied backward design principles to design a low-cost welding simulator in the form of modularized units through active consulting with welding training experts. Using a minimum viable prototype, we conducted a user study with 24 novices to test the system’s usability. Our findings show (1) greater effectiveness of the system in transferring skills to real-world environments as compared to accessible video-based alternatives and, (2) the visuo-haptic guidance during virtual welding enhances performance and provides a realistic learning experience to users. Using the solution, we expect inexperienced users to achieve competencies faster and be better prepared to enter actual work environments.",Virtual Reality; Backward design; Manufacturing; Virtual Reality Training; Virtual Reality Welding Simulators; Welding,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,ZenVR: Design Evaluation of a Virtual Reality Learning System for Meditation,CHI - Human Factors in Computing Systems,A*,"Meditation has become a popular option to manage stress. Though studies examine technologies to assist in meditation, few explore how technology supports development of such skills for independent practice. From a two-phase mixed-methods study, we contribute learner-centered insights from 36 participants in a virtual reality environment designed to teach meditation skills to novices. In Phase I, we gathered affective and behavioral learner needs from 21 meditation novices, experts, and instructors to synthesize insights for learning. We then designed ZenVR: an interactive system to deliver an eight-lesson meditation curriculum to support learners’ progress. In Phase II, we conducted a 6-week longitudinal lab-based evaluation with 15 novice meditation learners. We found statistically significant improvements in mindfulness and self-reported meditation ability. Their insights from a self-managed practice, two weeks after the study ended, offered opportunities to understand how technology can be designed to offer progressive support without creating dependence in technology-mediated meditation practice.",interaction design; Virtual reality; meditation; wellness; self-guided learning,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Towards Collaborative Learning in Virtual Reality: A Comparison of Co-Located Symmetric and Asymmetric Pair-Learning,CHI - Human Factors in Computing Systems,A*,"Pair-learning is beneficial for learning outcome, motivation, and social presence, and so is virtual reality (VR) by increasing immersion, engagement, motivation, and interest of students. Nevertheless, there is a research gap if the benefits of pair-learning and VR can be combined. Furthermore, it is not clear which influence it has if only one or both peers use VR. To investigate these aspects, we implemented two types of VR pair-learning systems, a symmetric system with both peers using VR and an asymmetric system with one using a tablet. In a user study (N=46), the symmetric system statistically significantly provided higher presence, immersion, player experience, and lower intrinsic cognitive load, which are all important for learning. Symmetric and asymmetric systems performed equally well regarding learning outcome, highlighting that both are valuable learning systems. We used these findings to define guidelines on how to design co-located VR pair-learning applications, including characteristics for symmetric and asymmetric systems.",collaborative learning; virtual reality; pair-learning; signaling; symmetric and asymmetric system,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,VRception: Rapid Prototyping of Cross-Reality Systems in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Cross-reality systems empower users to transition along the reality-virtuality continuum or collaborate with others experiencing different manifestations of it. However, prototyping these systems is challenging, as it requires sophisticated technical skills, time, and often expensive hardware. We present VRception, a concept and toolkit for quick and easy prototyping of cross-reality systems. By simulating all levels of the reality-virtuality continuum entirely in Virtual Reality, our concept overcomes the asynchronicity of realities, eliminating technical obstacles. Our VRception Toolkit leverages this concept to allow rapid prototyping of cross-reality systems and easy remixing of elements from all continuum levels. We replicated six cross-reality papers using our toolkit and presented them to their authors. Interviews with them revealed that our toolkit sufficiently replicates their core functionalities and allows quick iterations. Additionally, remote participants used our toolkit in pairs to collaboratively implement prototypes in about eight minutes that they would have otherwise expected to take days.",Augmented Reality; Virtual Reality; Prototyping; Cross-Reality Systems; Transitional Interfaces,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,The Dark Side of Perceptual Manipulations in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"“Virtual-Physical Perceptual Manipulations” (VPPMs) such as redirected walking and haptics expand the user’s capacity to interact with Virtual Reality (VR) beyond what would ordinarily physically be possible. VPPMs leverage knowledge of the limits of human perception to effect changes in the user’s physical movements, becoming able to (perceptibly and imperceptibly) nudge their physical actions to enhance interactivity in VR. We explore the risks posed by the malicious use of VPPMs. First, we define, conceptualize and demonstrate the existence of VPPMs. Next, using speculative design workshops, we explore and characterize the threats/risks posed, proposing mitigations and preventative recommendations against the malicious use of VPPMs. Finally, we implement two sample applications to demonstrate how existing VPPMs could be trivially subverted to create the potential for physical harm. This paper aims to raise awareness that the current way we apply and publish VPPMs can lead to malicious exploits of our perceptual vulnerabilities.",VR security; physical harm; virtual-physical perceptual manipulation; VPPM,Abstract_Title,
ACM DL,conferencePaper,2022,Effects of Field of View on Egocentric Distance Perception in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We performed a mixed-design study with 56 participants to compare the effect of horizontal FOV (hfov) and vertical FOV (vfov) on egocentric distance perception in four different realistic virtual environments (VEs). We also compared VE attributes of indoor/outdoor and cluttered/uncluttered. The participants blind-walked towards four different targets at 3m, 4m, 5m, and 6m distance while wearing a backpack computer and a wide FOV head-mounted display (HMD). The combinations of 165°, 110° and 45° hfovs, and 110° and 35° vfovs was simulated in the same HMD. The results indicated more accurate distance judgement with larger hfov with no significant effect of vfov. More accurate distance judgement in indoor VEs compared to outdoor VEs was observed. Also, participants judged distances more accurately in cluttered environments versus uncluttered environments. These results highlight that the environment is important in distance-critical VR applications and wider hfov should be considered for an improved distance judgment.",VR; virtual reality; field of view; virtual environment; clutter; distance perception; FOV; horizontal FOV; indoor; outdoor; vertical FOV,Keywords_Title,
ACM DL,conferencePaper,2022,Where Should We Put It? Layout and Placement Strategies of Documents in Augmented Reality for Collaborative Sensemaking,CHI - Human Factors in Computing Systems,A*,"Future offices are likely reshaped by Augmented Reality (AR) extending the display space while maintaining awareness of surroundings, and thus promise to support collaborative tasks such as brainstorming or sensemaking. However, it is unclear how physical surroundings and co-located collaboration influence the spatial organization of virtual content for sensemaking. Therefore, we conducted a study (N=28) to investigate the effect of office environments and work styles during a document classification task using AR with regard to content placement, layout strategies, and sensemaking workflows. Results show that participants require furniture, especially tables and whiteboards, to assist sensemaking and collaboration regardless of room settings, while generous free spaces (e.g., walls) are likely used when available. Moreover, collaborating participants tend to use furniture despite personal layout preferences. We identified different placement and layout strategies, as well as the transitions in-between. Finally, we propose design implications for future immersive sensemaking applications and beyond.",Augmented Reality; Mixed Reality; affordance; sensemaking; collaborative sensemaking; content organization; qualitative user study; spatial layout; spatiality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,Understanding AR Activism: An Interview Study with Creators of Augmented Reality Experiences for Social Change,CHI - Human Factors in Computing Systems,A*,"The rise of consumer augmented reality (AR) technology has opened up new possibilities for interventions intended to disrupt and subvert cultural conventions. From defacing corporate logos to erecting geofenced digital monuments, more and more people are creating AR experiences for social causes. We sought to understand this new form of activism, including why people use AR for these purposes, opportunities and challenges in using it, and how well it can support activist goals. We conducted semi-structured interviews with twenty people involved in projects that used AR for a social cause across six different countries. We found that AR can overcome physical world limitations of activism to convey immersive, multilayered narratives that aim to reveal invisible histories and perspectives. At the same time, people experienced challenges in creating, maintaining, and distributing their AR experiences to audiences. We discuss open questions and opportunities for creating AR tools and experiences for social change.",augmented reality; activism; social change,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,"Something Personal from the Metaverse: Goals, Topics, and Contextual Factors of Self-Disclosure in Commercial Social VR",CHI - Human Factors in Computing Systems,A*,"Current Social VR literature provides limited insight on one of the most critical behaviors for developing and maintaining interpersonal relationships: self-disclosure. Therefore, we present an online survey (N = 126) investigating how users disclose personal information to each other in Social VR. Our results indicate that many participants see in Social VR access to authentic connections with others despite tending towards skepticism and privacy concerns. Most users disclose sexuality-related information, lifestyle preferences, and personal goals. In contrast, information that breaks anonymity, such as real names and more intimate aspects of oneself, are shared less commonly. Thereby, self-disclosure decisions depend on factors like the relationship to or age of disclosure recipients, the privacy of a virtual environment, the group size, or the activity context, and is driven by different goals, i.a., relational development or exploration of oneself. These insights advance the understanding of current Social VR users and their behavior by directing future research on self-disclosure-based relationship building in Social VR and outlying broader design implications for the future metaverse.",social virtual reality; self-disclosure; online social interaction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2022,The Effect of the Vergence-Accommodation Conflict on Virtual Hand Pointing in Immersive Displays,CHI - Human Factors in Computing Systems,A*,"Previous work hypothesized that for Virtual Reality (VR) and Augmented Reality (AR) displays a mismatch between disparities and optical focus cues, known as the vergence and accommodation conflict (VAC), affects depth perception and thus limits user performance in 3D selection tasks within arm’s reach (peri-personal space). To investigate this question, we built a multifocal stereo display, which can eliminate the influence of the VAC for pointing within the investigated distances. In a user study, participants performed a virtual hand 3D selection task with targets arranged laterally or along the line of sight, with and without a change in visual depth, in display conditions with and without the VAC. Our results show that the VAC influences 3D selection performance in common VR and AR stereo displays and that multifocal displays have a positive effect on 3D selection performance with a virtual hand.",selection; virtual hand; 3D pointing; Fitts’ Law; vergence-accommodation conflict,Abstract,
ACM DL,conferencePaper,2022,Causality-preserving Asynchronous Reality,CHI - Human Factors in Computing Systems,A*,"Mixed Reality is gaining interest as a platform for collaboration and focused work to a point where it may supersede current office settings in future workplaces. At the same time, we expect that interaction with physical objects and face-to-face communication will remain crucial for future work environments, which is a particular challenge in fully immersive Virtual Reality. In this work, we reconcile those requirements through a user’s individual Asynchronous Reality, which enables seamless physical interaction across time. When a user is unavailable, e.g., focused on a task or in a call, our approach captures co-located or remote physical events in real-time, constructs a causality graph of co-dependent events, and lets immersed users revisit them at a suitable time in a causally accurate way. Enabled by our system AsyncReality, we present a workplace scenario that includes walk-in interruptions during a person’s focused work, physical deliveries, and transient spoken messages. We then generalize our approach to a use-case agnostic concept and system architecture. We conclude by discussing the implications of Asynchronous Reality for future offices.",Mixed Reality; Collaboration; Asynchronous communication; Camera networks; Immersive workspaces; Interruption in workplaces,Abstract_Keywords,
ACM DL,conferencePaper,2022,"Designing Visuo-Haptic Illusions with Proxies in Virtual Reality: Exploration of Grasp, Movement Trajectory and Object Mass",CHI - Human Factors in Computing Systems,A*,"Visuo-haptic illusions are a method to expand proxy-based interactions in VR by introducing unnoticeable discrepancies between the virtual and real world. Yet how different design variables affect the illusions with proxies is still unclear. To unpack a subset of variables, we conducted two user studies with 48 participants to explore the impact of (1) different grasping types and movement trajectories, and (2) different grasping types and object masses on the discrepancy which may be introduced. Our Bayes analysis suggests that grasping types and object masses (≤ 500 g) did not noticeably affect the discrepancy, but for movement trajectory, results were inconclusive. Further, we identified a significant difference between (un)restricted movement trajectories. Our data shows considerable differences in participants’ proprioceptive accuracy, which seem to correlate with their prior VR experience. Finally, we illustrate the impact of our key findings on the visuo-haptic illusion design process by showcasing a new design workflow.",Grasp; Movement Trajectory; Object Mass; Visuo-Haptic Illusions,Title,
ACM DL,conferencePaper,2022,DreamStream: Immersive and Interactive Spectating in VR,CHI - Human Factors in Computing Systems,A*,"Today spectating and streaming virtual reality (VR) activities typically involves spectators viewing a 2D stream of the VR user’s view. Streaming 2D videos of the game play is popular and well-supported by platforms such as Twitch. However, the generic streaming of full 3D representations is less explored. Thus, while the VR player’s experience may be fully immersive, spectators are limited to 2D videos. This asymmetry lessens the overall experience for spectators, who themselves may be eager to spectate in VR. DreamStream puts viewers in the virtual environment of the VR application, allowing them to look “over the shoulder” of the VR player. Spectators can view streamed VR content immersively in 3D, independently explore the VR scene beyond what the VR player sees and ultimately cohabit the virtual environment alongside the VR player. For the VR player, DreamStream provides a spatial awareness of all their spectators. DreamStream retrofits and works with existing VR applications. We discuss the design and implementation of DreamStream, and carry out three qualitative informal evaluations. These evaluations shed light on the strengths and weakness of using DreamStream for the purpose of interactive spectating. Our participants found that DreamStream’s VR viewer interface offered increased immersion, and made it easier to communicate and interact with the VR player.",Virtual Reality; Streaming; Collaborative Interactions,Abstract_Keywords,
ACM DL,conferencePaper,2021,AtaTouch: Robust Finger Pinch Detection for a VR Controller Using RF Return Loss,CHI - Human Factors in Computing Systems,A*,"Handheld controllers are an essential part of VR systems. Modern sensing techniques enable them to track users’ finger movements to support natural interaction using hands. The sensing techniques, however, often fail to precisely determine whether two fingertips touch each other, which is important for the robust detection of a pinch gesture. To address this problem, we propose AtaTouch, which is a novel, robust sensing technique for detecting the closure of a finger pinch. It utilizes a change in the coupled impedance of an antenna and human fingers when the thumb and finger form a loop. We implemented a prototype controller in which AtaTouch detects the finger pinch of the grabbing hand. A user test with the prototype showed a finger-touch detection accuracy of 96.4%. Another user test with the scenarios of moving virtual blocks demonstrated low object-drop rate (2.75%) and false-pinch rate (4.40%). The results and feedback from the participants support the robustness and sensitivity of AtaTouch.",Antenna; Hand gesture; RF Sensing; Touch segmentation; Virtual Reality (VR),Keywords,
ACM DL,conferencePaper,2021,XRStudio: A Virtual Production and Live Streaming System for Immersive Instructional Experiences,CHI - Human Factors in Computing Systems,A*,"There is increased interest in using virtual reality in education, but it often remains an isolated experience that is difficult to integrate into current instructional experiences. In this work, we adapt virtual production techniques from filmmaking to enable mixed reality capture of instructors so that they appear to be standing directly in the virtual scene. We also capitalize on the growing popularity of live streaming software for video conferencing and live production. With XRStudio, we develop a pipeline for giving lectures in VR, enabling live compositing using a variety of presets and real-time output to traditional video and more immersive formats. We present interviews with media designers experienced in film and MOOC production that informed our design. Through walkthrough demonstrations of XRStudio with instructors experienced with VR, we learn how it could be used in a variety of domains. In end-to-end evaluations with students, we analyze and compare differences of traditional video vs. more immersive lectures with XRStudio.",immersive instruction.; mixed reality capture; virtual production,Abstract_Keywords,
ACM DL,conferencePaper,2021,A Design Space Exploration of Worlds in Miniature,CHI - Human Factors in Computing Systems,A*,"Worlds-in-Miniature (WiMs) are interactive worlds within a world and combine the advantages of an input space, a cartographic map, and an overview+detail interface. They have been used across the extended virtuality spectrum for a variety of applications. Building on an analysis of examples of WiMs from the research literature we contribute a design space for WiMs based on seven design dimensions. Further, we expand upon existing definitions of WiMs to provide a definition that applies across the extended reality spectrum. We identify the design dimensions of size-scope-scale, abstraction, geometry, reference frame, links, multiples, and virtuality. Using our framework we describe existing Worlds-in-Miniature from the research literature and reveal unexplored research areas. Finally, we generate new examples of WiMs using our framework to fill some of these gaps. With our findings, we identify opportunities that can guide future research into WiMs.",Meta-Analysis/Literature Survey; Virtual/Augmented Reality,Abstract_Keywords,
ACM DL,conferencePaper,2021,Armstrong: An Empirical Examination of Pointing at Non-Dominant Arm-Anchored UIs in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"In virtual reality (VR) environments, asymmetric bimanual interaction techniques can increase users’ input bandwidth by complementing their perceptual and motor systems (e.g., using the dominant hand to select 3D UI controls anchored around the non-dominant arm). However, it is unclear how to optimize the layout of such 3D UI controls for near-body and mid-air interactions. We evaluate the performance and limitations of non-dominant arm-anchored 3D UIs in VR environments through a bimanual pointing study. Results demonstrated that targets appearing closer to the skin, located around the wrist, or placed on the medial side of the forearm could be selected more quickly than targets farther away from the skin, located around the elbow, or on the lateral side of the forearm. Based on these results, we developed Armstrong guidelines, demonstrated through a Unity plugin to enable designers to create performance-optimized arm-anchored 3D UI layouts.",Arm-anchored UIs; Bimanual interaction; Virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,GamesBond: Bimanual Haptic Illusion of Physically Connected Objects for Immersive VR Using Grip Deformation,CHI - Human Factors in Computing Systems,A*,"Virtual Reality experiences, such as games and simulations, typically support the usage of bimanual controllers to interact with virtual objects. To recreate the haptic sensation of holding objects of various shapes and behaviors with both hands, previous researchers have used mechanical linkages between the controllers that render adjustable stiffness. However, the linkage cannot quickly adapt to simulate dynamic objects, nor it can be removed to support free movements. This paper introduces GamesBond, a pair of 4-DoF controllers without physical linkage but capable to create the illusion of being connected as a single device, forming a virtual bond. The two controllers work together by dynamically displaying and physically rendering deformations of hand grips, and so allowing users to perceive a single connected object between the hands, such as a jumping rope. With a user study and various applications we show that GamesBond increases the realism, immersion, and enjoyment of bimanual interaction.",bimanual interaction; grip deformation; Haptics; shape-changing; Virtual Reality,Abstract_Keywords,
ACM DL,conferencePaper,2021,Locomotion Vault: the Extra Mile in Analyzing VR Locomotion Techniques,CHI - Human Factors in Computing Systems,A*,"Numerous techniques have been proposed for locomotion in virtual reality (VR). Several taxonomies consider a large number of attributes (e.g., hardware, accessibility) to characterize these techniques. However, finding the appropriate locomotion technique (LT) and identifying gaps for future designs in the high-dimensional space of attributes can be quite challenging. To aid analysis and innovation, we devised Locomotion Vault (https://locomotionvault.github.io/), a database and visualization of over 100 LTs from academia and industry. We propose similarity between LTs as a metric to aid navigation and visualization. We show that similarity based on attribute values correlates with expert similarity assessments (a method that does not scale). Our analysis also highlights an inherent trade-off between simulation sickness and accessibility across LTs. As such, Locomotion Vault shows to be a tool that unifies information on LTs and enables their standardization and large-scale comparison to help understand the space of possibilities in VR locomotion.",database; locomotion method; locomotion technique; movement; navigation; traveling; visualization; VR,Abstract,
ACM DL,conferencePaper,2021,Phonetroller: Visual Representations of Fingers for Precise Touch Input with Mobile Phones in VR,CHI - Human Factors in Computing Systems,A*,"Smartphone touch screens are potentially attractive for interaction in virtual reality (VR). However, the user cannot see the phone or their hands in a fully immersive VR setting, impeding their ability for precise touch input. We propose mounting a mirror above the phone screen such that the front-facing camera captures the thumbs on or near the screen. This enables the creation of semi-transparent overlays of thumb shadows and inference of fingertip hover points with deep learning, which help the user aim for targets on the phone. A study compares the effect of visual feedback on touch precision in a controlled task and qualitatively evaluates three example applications demonstrating the potential of the technique. The results show that the enabled style of feedback is effective for thumb-size targets, and that the VR experience can be enriched by using smartphones as VR controllers supporting precise touch input.",deep learning; mobile phone; touch input; visual feedback; VR,Abstract,
ACM DL,conferencePaper,2021,Ninja Hands: Using Many Hands to Improve Target Selection in VR,CHI - Human Factors in Computing Systems,A*,"Selection and manipulation in virtual reality often happen using an avatar’s hands. However, objects outside the immediate reach require effort to select. We develop a target selection technique called Ninja Hands. It maps the movement of a single real hand to many virtual hands, decreasing the distance to targets. We evaluate Ninja Hands in two studies. The first study shows that compared to a single hand, 4 and 8 hands are significantly faster for selecting targets. The second study complements this finding by using a larger target layout with many distractors. We find no decrease in selection time across 8, 27, and 64 hands, but an increase in the time spent deciding which hand to use. Thereby, net movement time still decreases significantly. In both studies, the physical motion exerted also decreases significantly with more hands. We discuss how these findings can inform future implementations of the Ninja Hands technique.",many hands; target selection; user studies; virtual hands; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2021,Comparison of Different Types of Augmented Reality Visualizations for Instructions,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) is increasingly being used for providing guidance and supporting troubleshooting in industrial settings. While the general application of AR has been shown to provide clear benefits regarding physical tasks, it is important to understand how different visualization types influence user’s performance during the execution of the tasks. Previous studies evaluating AR and user’s performance compared different media types or types of AR hardware as opposed to different types of visualization for the same hardware type. This paper provides details of our comparative study in which we identified the influence of visualization types on the performance of complex machine set-up processes. Although our results show clear advantages to using concrete rather than abstract visualizations, we also find abstract visualizations coupled with videos leads to similar user performance as with concrete visualizations.",Augmented Reality; User Study; Visualization; Instructions,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,vMirror: Enhancing the Interaction with Occluded or Distant Objects in VR with Virtual Mirrors,CHI - Human Factors in Computing Systems,A*,"Interacting with out of reach or occluded VR objects can be cumbersome. Although users can change their position and orientation, such as via teleporting, to help observe and select, doing so frequently may cause loss of spatial orientation or motion sickness. We present vMirror, an interactive widget leveraging reflection of mirrors to observe and select distant or occluded objects. We first designed interaction techniques for placing mirrors and interacting with objects through mirrors. We then conducted a formative study to explore a semi-automated mirror placement method with manual adjustments. Next, we conducted a target-selection experiment to measure the effect of the mirror’s orientation on users’ performance. Results showed that vMirror can be as efficient as direct target selection for most mirror orientations. We further compared vMirror with teleport technique in a virtual treasure hunt game and measured participants’ task performance and subjective experiences. Finally, we discuss vMirorr user experience and present future directions.",VR; Virtual Reality; Virtual mirror; target selection; DOF; occlusion; out of reach; raycasting; vMirror,Keywords,
ACM DL,conferencePaper,2021,"HairTouch: Providing Stiffness, Roughness and Surface Height Differences Using Reconfigurable Brush Hairs on a VR Controller",CHI - Human Factors in Computing Systems,A*,"Tactile feedback is widely used to enhance realism in virtual reality (VR). When touching virtual objects, stiffness and roughness are common and obvious factors perceived by the users. Furthermore, when touching a surface with complicated surface structure, differences from not only stiffness and roughness but also surface height are crucial. To integrate these factors, we propose a pin-based handheld device, HairTouch, to provide stiffness differences, roughness differences, surface height differences and their combinations. HairTouch consists of two pins for the two finger segments close to the index fingertip, respectively. By controlling brush hairs’ length and bending direction to change the hairs’ elasticity and hair tip direction, each pin renders various stiffness and roughness, respectively. By further independently controlling the hairs’ configuration and pins’ height, versatile stiffness, roughness and surface height differences are achieved. We conducted a perception study to realize users’ distinguishability of stiffness and roughness on each of the segments. Based on the results, we performed a VR experience study to verify that the tactile feedback from HairTouch enhances VR realism.",Haptic; Tactile; Hair; Handheld Device; Roughness; Stiffness; Virtual Reality.,Abstract_Keywords,
ACM DL,conferencePaper,2021,GuideBand: Intuitive 3D Multilevel Force Guidance on a Wristband in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"For haptic guidance, vibrotactile feedback is a commonly-used mechanism, but requires users to interpret its complicated patterns especially in 3D guidance, which is not intuitive and increases their mental effort. Furthermore, for haptic guidance in virtual reality (VR), not only guidance performance but also realism should be considered. Since vibrotactile feedback interferes with and reduces VR realism, it may not be proper for VR haptic guidance. Therefore, we propose a wearable device, GuideBand, to provide intuitive 3D multilevel force guidance upon the forearm, which reproduces an effect that the forearm is pulled and guided by a virtual guider or telepresent person in VR. GuideBand uses three motors to pull a wristband at different force levels in 3D space. Such feedback usually requires much larger and heavier robotic arms or exoskeletons. We conducted a just-noticeable difference study to understand users’ force level distinguishability. Based on the results, we performed a study to verify that compared with state-of-the-art vibrotactile guidance, GuideBand is more intuitive, needs a lower level of mental effort, and achieves similar guidance performance. We further conducted a VR experience study to observe how users combine and complement visual and force guidance, and prove that GuideBand enhances realism in VR guidance.",virtual reality; Haptic feedback; force feedback; force guidance; motion guidance; wearable device.,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,"Effects of Semantic Segmentation Visualization on Trust, Situation Awareness, and Cognitive Load in Highly Automated Vehicles",CHI - Human Factors in Computing Systems,A*,"Autonomous vehicles could improve mobility, safety, and inclusion in traffic. While this technology seems within reach, its successful introduction depends on the intended user’s acceptance. A substantial factor for this acceptance is trust in the autonomous vehicle’s capabilities. Visualizing internal information processed by an autonomous vehicle could calibrate this trust by enabling the perception of the vehicle’s detection capabilities (and its failures) while only inducing a low cognitive load. Additionally, the simultaneously raised situation awareness could benefit potential take-overs. We report the results of two comparative online studies on visualizing semantic segmentation information for the human user of autonomous vehicles. Effects on trust, cognitive load, and situation awareness were measured using a simulation (N=32) and state-of-the-art panoptic segmentation on a pre-recorded real-world video (N=41). Results show that the visualization using Augmented Reality increases situation awareness while remaining low cognitive load.",Autonomous vehicles; self-driving vehicles; semantic segmentation.,Abstract,
ACM DL,conferencePaper,2021,Impact of Task on Attentional Tunneling in Handheld Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Attentional tunneling describes a phenomenon in Augmented Reality (AR) where users excessively focus on virtual content while neglecting their physical surroundings. This leads to the concern that users could neglect hazardous situations when using AR applications. However, studies have often confounded the role of the virtual content with the role of the associated task in inducing attentional tunneling. In this paper, we disentangle the impact of the associated task and of the virtual content on the attentional tunneling effect by measuring reaction times to events in two user studies. We found that presenting virtual content did not significantly increase user reaction times to events, but adding a task to the content did. This work contributes towards our understanding of the attentional tunneling effect on handheld AR devices, and highlights the need to consider both task and context when evaluating AR application usage.",Augmented Reality; Mobile Devices; Attentional Tunneling,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Visuo-haptic Illusions for Linear Translation and Stretching using Physical Proxies in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Providing haptic feedback when manipulating virtual objects is an essential part of immersive virtual reality experiences; however, it is challenging to replicate all of an object's properties and characteristics. We propose the use of visuo-haptic illusions alongside physical proxies to enhance the scope of proxy-based interactions with virtual objects. In this work, we focus on two manipulation techniques, linear translation and stretching across different distances, and investigate how much discrepancy between the physical proxy and the virtual object may be introduced without participants noticing. In a study with 24 participants, we found that manipulation technique and travel distance significantly affect the detection thresholds, and that visuo-haptic illusions impact performance and accuracy. We show that this technique can be used to enable functional proxy objects that act as stand-ins for multiple virtual objects, illustrating the technique through a showcase VR-DJ application.",Virtual Reality; Haptics; Proxy Objects; Tangible Interfaces; Visuo-haptic Illusions,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Proxemics and Social Interactions in an Instrumented Virtual Reality Workshop,CHI - Human Factors in Computing Systems,A*,"Virtual environments (VEs) can create collaborative and social spaces, which are increasingly important in the face of remote work and travel reduction. Recent advances, such as more open and widely available platforms, create new possibilities to observe and analyse interaction in VEs. Using a custom instrumented build of Mozilla Hubs to measure position and orientation, we conducted an academic workshop to facilitate a range of typical workshop activities. We analysed social interactions during a keynote, small group breakouts, and informal networking/hallway conversations. Our mixed-methods approach combined environment logging, observations, and semi-structured interviews. The results demonstrate how small and large spaces influenced group formation, shared attention, and personal space, where smaller rooms facilitated more cohesive groups while larger rooms made small group formation challenging but personal space more flexible. Beyond our findings, we show how the combination of data and insights can fuel collaborative spaces’ design and deliver more effective virtual workshops.",Virtual Environments; Social Signal Processing; Interviews.; Virtual Meetings,Title,
ACM DL,conferencePaper,2021,Experiencing Simulated Confrontations in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The use of virtual reality (VR) to simulate confrontational human behaviour has significant potential for use in training, where the recreation of uncomfortable feelings may help users to prepare for challenging real-life situations. In this paper we present a user study (n=68) in which participants experienced simulated confrontational behaviour performed by a virtual character either in immersive VR, or on a 2D display. Participants reported a higher elevation in anxiety in VR, which correlated positively with a perceived sense of physical space. Character believability was influenced negatively by visual elements of the simulation, and positively by behavioural elements, which complements findings from previous work. We recommend the use of VR for simulations of confrontational behaviour, where a realistic emotional response is part of the intended experience. We also discuss incorporation of domain knowledge of human behaviours, and carefully crafted motion-captured sequences, to increase users’ sense of believability.",Virtual reality; confrontational behaviour; virtual character,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Towards “Avatar-Friendly” 3D Manipulation Techniques: Bridging the Gap Between Sense of Embodiment and Interaction in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Avatars, the users’ virtual representations, are becoming ubiquitous in virtual reality applications. In this context, the avatar becomes the medium which enables users to manipulate objects in the virtual environment. It also becomes the users’ main spatial reference, which can not only alter their interaction with the virtual environment, but also the perception of themselves. In this paper, we review and analyse the current state-of-the-art for 3D object manipulation and the sense of embodiment. Our analysis is twofold. First, we discuss the impact that the avatar can have on object manipulation. Second, we discuss how the different components of a manipulation technique (i.e. input, control and feedback) can influence the user’s sense of embodiment. Throughout the analysis, we crystallise our discussion with practical guidelines for VR application designers and we propose several research topics towards “avatar-friendly’’ manipulation techniques.",virtual reality; interaction; embodiment; avatars; guidelines,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,"Understanding, Detecting and Mitigating the Effects of Coactivations in Ten-Finger Mid-Air Typing in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Typing with ten fingers on a virtual keyboard in virtual or augmented reality exposes a challenging input interpretation problem. There are many sources of noise in this interaction context and these exacerbate the challenge of accurately translating human actions into text. A particularly challenging input noise source arises from the physiology of the hand. Intentional finger movements can produce unintentional coactivations in other fingers. On a physical keyboard, the resistance of the keys alleviates this issue. On a virtual keyboard, coactivations are likely to introduce spurious input events under a naïve solution to input detection. In this paper we examine the features that discriminate intentional activations from coactivations. Based on this analysis, we demonstrate three alternative coactivation detection strategies with high discrimination power. Finally, we integrate coactivation detection into a probabilistic decoder and demonstrate its ability to further reduce uncorrected character error rates by approximately 10% relative and 0.9% absolute.",virtual reality; text entry,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Gesture Knitter: A Hand Gesture Design Tool for Head-Mounted Mixed Reality Applications,CHI - Human Factors in Computing Systems,A*,"Hand gestures are a natural and expressive input method enabled by modern mixed reality headsets. However, it remains challenging for developers to create custom gestures for their applications. Conventional strategies to bespoke gesture recognition involve either hand-crafting or data-intensive deep-learning. Neither approach is well suited for rapid prototyping of new interactions. This paper introduces a flexible and efficient alternative approach for constructing hand gestures. We present Gesture Knitter: a design tool for creating custom gesture recognizers with minimal training data. Gesture Knitter allows the specification of gesture primitives that can then be combined to create more complex gestures using a visual declarative script. Designers can build custom recognizers by declaring them from scratch or by providing a demonstration that is automatically decoded into its primitive components. Our developer study shows that Gesture Knitter achieves high recognition accuracy despite minimal training data and delivers an expressive and creative design experience.",augmented reality; virtual reality; gestures,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,We Can Do More to Save Guqin: Design and Evaluate Interactive Systems to Make Guqin More Accessible to the General Public,CHI - Human Factors in Computing Systems,A*,"Guqin is a plucked seven-string traditional Chinese musical instrument that exists for over 3,000 years. However, as an Intangible World Cultural Heritage, the inheritance of Guqin and its culture in modern society is in deep danger. According to our study with 1,006 Chinese worldwide, Guqin as an instrument is not well-known and barely accessible. To better promote Guqin, we developed two interactive systems: VirGuqin and MRGuqin. VirGuqin was developed using a low-cost motion tracking device and was tested in a museum. 89% of 308 participants expressed an increase in interest in learning Guqin after using our system. MRGuqin was developed as a mixed reality learning environment to reduce the entry barrier to Guqin, and was tested by 16 participants, allowing them to learn Guqin significantly faster and perform better than the current practice. Our study demonstrates how technology can be used to help the inheritance of this dying art.",Augmented Reality; Cultural Preservation; Guqin Art; Interactive Design,Abstract_Keywords,
ACM DL,conferencePaper,2021,Identifying Manipulative Advertising Techniques in XR Through Scenario Construction,CHI - Human Factors in Computing Systems,A*,"As Extended Reality (XR) devices and applications become more mainstream, so too will XR advertising&nbsp;—&nbsp;advertising that takes place in XR mediums. Due to the defining features of XR devices, such as the immersivity of the medium and the ability of XR devices to simulate reality, there are fears that these features could be exploited to create manipulative XR ads that trick consumers into buying products they do not need or might harm them. Using scenario construction, we investigate potential future incarnations of manipulative XR advertising and their harms. We identify five key mechanisms of manipulative XR advertising: misleading experience marketing; inducing artificial emotions in consumers; sensing and targeting people when they are vulnerable; emotional manipulation through hyperpersonalization; and distortion of reality. We discuss research challenges and questions in order to address and mitigate manipulative XR advertising risks.",augmented reality; privacy; virtual reality; mixed reality; advertising; computer ethics.; Extended reality; scenario construction,Abstract_Keywords,
ACM DL,conferencePaper,2021,Virtual Reality Esports - Understanding Competitive Players’ Perceptions of Location Based VR Esports,CHI - Human Factors in Computing Systems,A*,"Competitive VR gaming has emerged as a new trend in recent years, due to the availability of consumer grade VR technologies and the rise of esports as a billion dollar industry. Despite the considerable attention to competitive VR gaming, there is a lack of research on attitudes and experiences that players have with these games. In this qualitative study with a pre-post interview design, we recruited eight competitive Counter-Strike: Global Offensive players from a university esports club. We aimed to understand their attitudes towards VR esports and their experiences playing a representative location based VR esports game. Findings showed that players had visceral and positive affective experiences in the game, such as how players map physical movements to the game. These findings can help design future competitive VR esports, while also further contributing to HCI as the first exploration on player experiences with VR esports, laying groundwork for future studies.",Qualitative Study; Player Experiences; Virtual Reality Esports,Keywords_Title,
ACM DL,conferencePaper,2021,SpatialProto: Exploring Real-World Motion Captures for Rapid Prototyping of Interactive Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Spatial computing devices that blend virtual and real worlds have the potential to soon become ubiquitous. Yet, creating experiences for spatial computing is non-trivial and needs skills in programming and 3D content creation, rendering them inaccessible to a wider group of users. We present SpatialProto, an in-situ spatial prototyping system for lowering the barrier to engage in spatial prototyping. With a depth-sensing capable mixed reality headset, SpatialProto lets users record animated objects of the real-world environment (e.g. paper, clay, people, or any other prop), extract only the relevant parts, and directly place and transform these recordings in their physical environment. We describe the design and implementation of SpatialProto, a user study evaluating the system’s prototype with non-expert users (n = 9), and demonstrate applications where multiple captures are fused for compelling augmented reality experiences.",mixed reality; interaction; animation; Spatial prototyping,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Context-Based Interface Prototyping: Understanding the Effect of Prototype Representation on User Feedback,CHI - Human Factors in Computing Systems,A*,"The rise of autonomous systems in cities, such as automated vehicles (AVs), requires new approaches for prototyping and evaluating how people interact with those systems through context-based user interfaces, such as external human-machine interfaces (eHMIs). In this paper, we present a comparative study of three prototype representations (real-world VR, computer-generated VR, real-world video) of an eHMI in a mixed-methods study with 42 participants. Quantitative results show that while the real-world VR representation results in higher sense of presence, no significant differences in user experience and trust towards the AV itself were found. However, interview data shows that participants focused on different experiential and perceptual aspects in each of the prototype representations. These differences are linked to spatial awareness and perceived realism of the AV behaviour and its context, affecting in turn how participants assess trust and the eHMI. The paper offers guidelines for prototyping and evaluating context-based interfaces through simulations.",automated vehicles; virtual reality; user studies; prototyping; human-machine interfaces; prototype representation,Keywords,
ACM DL,conferencePaper,2021,You’re Making Me Sick: A Systematic Review of How Virtual Reality Research Considers Gender &amp; Cybersickness,CHI - Human Factors in Computing Systems,A*,"While multiple studies suggest that female-identified participants are more likely to experience cybersickness in virtual reality (VR), our systematic review of 71 eligible VR publications (59 studies and 12 surveys) pertaining to gender and cybersickness reveals a number of confounding factors in study design (e.g., a variety of technical specifications, tasks, content), a lack of demographic data, and a bias in participant recruitment. Our review shows an ongoing need within VR research to more consistently include and report on women’s experiences in VR to better understand the gendered possibility of cybersickness. Based on the gaps identified in our systematic review, we contribute study design recommendations for future work, arguing that gender considerations are necessary at every stage of VR study design, even when the study is not ‘about’ gender.",systematic review; virtual reality; simulator sickness; cybersickness; sex; gender; virtual environments,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,DistanciAR: Authoring Site-Specific Augmented Reality Experiences for Remote Environments,CHI - Human Factors in Computing Systems,A*,"Most augmented reality (AR) authoring tools only support the author’s current environment, but designers often need to create site-specific experiences for a different environment. We propose DistanciAR, a novel tablet-based workflow for remote AR authoring. Our baseline solution involves three steps. A remote environment is captured by a camera with LiDAR; then, the author creates an AR experience from a different location using AR interactions; finally, a remote viewer consumes the AR content on site. A formative study revealed understanding and navigating the remote space as key challenges with this solution. We improved the authoring interface by adding two novel modes: Dollhouse, which renders a bird’s-eye view, and Peek, which creates photorealistic composite images using captured images. A second study compared this improved system with the baseline, and participants reported that the new modes made it easier to understand and navigate the remote scene.",augmented reality; 3D scanning; remote authoring; spatial design,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,HandPainter - 3D Sketching in VR with Hand-based Physical Proxy,CHI - Human Factors in Computing Systems,A*,"3D sketching in virtual reality (VR) enables users to create 3D virtual objects intuitively and immersively. However, previous studies showed that mid-air drawing may lead to inaccurate sketches. To address this issue, we propose to use one hand as a canvas proxy and the index finger of the other hand as a 3D pen. To this end, we first perform a formative study to compare two-handed interaction with tablet-pen interaction for VR sketching. Based on the findings of this study, we design HandPainter, a VR sketching system which focuses on the direct use of two hands for 3D sketching without requesting any tablet, pen, or VR controller. Our implementation is based on a pair of VR gloves, which provide hand tracking and gesture capture. We devise a set of intuitive gestures to control various functionalities required during 3D sketching, such as canvas panning and drawing positioning. We show the effectiveness of HandPainter by presenting a number of sketching results and discussing the outcomes of a user study-based comparison with mid-air drawing and tablet-based sketching tools.",VR; 3D sketching; hand-based interaction,Abstract,
ACM DL,conferencePaper,2021,AdapTutAR: An Adaptive Tutoring System for Machine Tasks in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Modern manufacturing processes are in a state of flux, as they adapt to increasing demand for flexible and self-configuring production. This poses challenges for training workers to rapidly master new machine operations and processes, i.e. machine tasks. Conventional in-person training is effective but requires time and effort of experts for each worker trained and not scalable. Recorded tutorials, such as video-based or augmented reality (AR), permit more efficient scaling. However, unlike in-person tutoring, existing recorded tutorials lack the ability to adapt to workers’ diverse experiences and learning behaviors. We present AdapTutAR, an adaptive task tutoring system that enables experts to record machine task tutorials via embodied demonstration and train learners with different AR tutoring contents adapting to each user’s characteristics. The adaptation is achieved by continually monitoring learners’ tutorial-following status and adjusting the tutoring content on-the-fly and in-situ. The results of our user study evaluation have demonstrated that our adaptive system is more effective and preferable than the non-adaptive one.",adaptation; manufacturing; training/learning; user state recognition,Abstract_Title,
ACM DL,conferencePaper,2021,"Effect of Gameplay Uncertainty, Display Type, and Age on Virtual Reality Exergames",CHI - Human Factors in Computing Systems,A*,"Uncertainty is widely acknowledged as an engaging gameplay element but rarely used in exergames. In this research, we explore the role of uncertainty in exergames and introduce three uncertain elements (false-attacks, misses, and critical hits) to an exergame. We conducted a study under two conditions (uncertain and certain), with two display types (virtual reality and large display) and across young and middle-aged adults to measure their effect on game performance, experience, and exertion. Results show that (1) our designed uncertain elements are instrumental in increasing exertion levels; (2) when playing a motion-based first-person perspective exergame, virtual reality can improve performance, while maintaining the same motion sickness level as a large display; and (3) exergames for middle-aged adults should be designed with age-related declines in mind, similar to designing for elderly adults. We also framed two design guidelines for exergames that have similar features to the game used in this research.",virtual reality; uncertainty; young adults; exergame; middle-aged adults,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Exploring the Design Space of Immersive Social Fitness Games: The ImSoFit Games Model,CHI - Human Factors in Computing Systems,A*,"The design space of social exergames remains narrow despite the many benefits of playing and exercising together. Towards opening this design space, we followed a Research through Design (RtD) approach focused on exergames that can be fun and immersive social training experiences. Through embodied sketching activities with designers and 10 pairs of players, we explored future games for the ExerCube, an immersive exergame platform. Our work contributes with forms of intermediate-level knowledge: a design space model (the Immersive Social Fitness—ImSoFit—Games model); and a novel design vocabulary including new bodily orientations in co-located physical interaction. We illustrate their use and value scrutinizing three of our games and applying three analytical lenses to 1) understand how design choices impact how players move together; 2) evaluate design expectations and analyze players’ behavior in relation to design choices; and 3) potentially extend the design space of immersive co-located social fitness games.",design space; mixed reality; exergame; embodied sketching; fitness game; multiplayer; social immersion,Keywords,
ACM DL,conferencePaper,2021,Feels like Team Spirit: Biometric and Strategic Interdependence in Asymmetric Multiplayer VR Games,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) multiplayer games increasingly use asymmetry (e.g., differences in a person’s capability or the user interface) and resulting interdependence between players to create engagement even when one player has no access to a head-mounted display (HMD). Previous work shows this enhances player experience (PX). Until now, it remains unclear whether and how an asymmetric game design with interdependences creates comparably enjoyable PX for both an HMD and a non-HMD player. In this work, we designed and implemented an asymmetric VR game (different in its user interface) with two types of interdependence: strategic (difference in game information/player capability) and biometric (difference in player’s biometric influence). Our mixed-methods user study (N=30) shows that asymmetries positively impact PX for both player roles, that interdependence strongly affects players’ perception of agency, and that biometric feedback—while subjective—is a valuable game mechanic.",VR; virtual reality; biometric; multiplayer; asymmetry; interdependence; strategic,Abstract_Keywords,
ACM DL,conferencePaper,2021,Towards the Next Generation of Gaming Wearables,CHI - Human Factors in Computing Systems,A*,"Recent studies on gaming wearables show that wearables can contribute to the gaming experience by bolstering performativity, facilitating social interaction, and accommodating distinct interaction modalities. Still, these studies focused on contexts such as role-playing, casual, or festival games. Stakeholder-oriented research that explores the integration of wearables for mainstream gaming platforms such as game consoles is scarce. To fill this gap, we have conducted an exploratory study through 6 participatory design workshops focusing on different aspects of wearables with 33 participants from different stakeholders. As a result, we have created fifteen design themes and three gaming wearable concepts that led to seven actionable design implications which can be adopted by designers and researchers for designing gaming wearables.",Wearables; Virtual Reality; Bioadaptive; Costuming; Game Controllers; Game Research; LARP; Movement-Based Games; Role Playing; RPG; Social Interaction,Keywords,
ACM DL,conferencePaper,2021,Streaming VR Games to the Broad Audience: A Comparison of the First-Person and Third-Person Perspectives,CHI - Human Factors in Computing Systems,A*,"The spectatorship experience for virtual reality (VR) games differs strongly from its non-VR precursor. When watching non-VR games on platforms such as Twitch, spectators just see what the player sees, as the physical interaction is mostly unimportant for the overall impression. In VR, the immersive full-body interaction is a crucial part of the player experience. Hence, content creators, such as streamers, often rely on green screens or similar solutions to offer a mixed-reality third-person view to disclose their full-body actions. Our work compares the most popular realizations of the first-person and the third-person perspective in an online survey (N&nbsp;=&nbsp;217) with three different VR games. Contrary to the current trend to stream in third-person, our key result is that most viewers prefer the first-person version, which they attribute mostly to the better focus on in-game actions and higher involvement. Based on the study insights, we provide design recommendations for both perspectives.",virtual reality; games; streaming; first-person; perspective; spectator; third-person,Abstract_Keywords,
ACM DL,conferencePaper,2021,"Current Practices, Challenges, and Design Implications for Collaborative AR/VR Application Development",CHI - Human Factors in Computing Systems,A*,"Augmented/Virtual Reality (AR/VR) is still a fragmented space to design for due to the rapidly evolving hardware, the interdisciplinarity of teams, and a lack of standards and best practices. We interviewed 26 professional AR/VR designers and developers to shed light on their tasks, approaches, tools, and challenges. Based on their work and the artifacts they generated, we found that AR/VR application creators fulfill four roles: concept developers, interaction designers, content authors, and technical developers. One person often incorporates multiple roles and faces a variety of challenges during the design process from the initial contextual analysis to the deployment. From analysis of their tool sets, methods, and artifacts, we describe critical key challenges. Finally, we discuss the importance of prototyping for the communication in AR/VR development teams and highlight design implications for future tools to create a more usable AR/VR tool chain.",Augmented Reality; XR; AR design; AR/VR; MR; authoring tools; AR development; practitioners,Abstract_Keywords,
ACM DL,conferencePaper,2021,The Role of Social Presence for Cooperation in Augmented Reality on Head Mounted Devices: A Literature Review,CHI - Human Factors in Computing Systems,A*,"With growing interest regarding cooperation support using Augmented Reality (AR), social presence has become a popular measure of its quality. While this concept is established throughout cooperation research, its role in AR is still unclear: Some work uses social presence as an indicator for support quality, while others found no impact at all. To clarify this role, we conducted a literature review of recent publications that empirically investigated social presence in cooperative AR. After a thorough selection procedure, we analyzed 19&nbsp;publications according to factors influencing social presence and the impact of social presence on cooperation support. We found that certain interventions support social presence better than others, that social presence has an influence on user's preferences and that the relation between social presence and cooperation quality may depend on the symmetry of the cooperation task. This contributes to existing research by clarifying the role of social presence for cooperative AR and deriving corresponding design recommendations.",Augmented Reality; User Study; Literature Review; Cooperation; Social Presence; Head Mounted Devices; Presence Questionnaire,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Tele-Immersive Improv: Effects of Immersive Visualisations on Rehearsing and Performing Theatre Online,CHI - Human Factors in Computing Systems,A*,"Performers acutely need but lack tools to remotely rehearse and create live theatre, particularly due to global restrictions on social interactions during the Covid-19 pandemic. No studies, however, have heretofore examined how remote video-collaboration affects performance. This paper presents the findings of a field study with 16 domain experts over six weeks investigating how tele-immersion affects the rehearsal and performance of improvisational theatre. To conduct the study, an original media server was developed for co-locating remote performers into shared virtual 3D environments which were accessed through popular video conferencing software. The results of this qualitative study indicate that tele-immersive environments uniquely provide performers with a strong sense of co- presence, feelings of physical connection, and an increased ability to enter the social-flow states required for improvisational theatre. Based on our observations, we put forward design recommendations for video collaboration tools tailored to the unique demands of live performance.",immersive communication; mixed reality; creativity; presence; improvisation; tele-immersion; tele-presence,Keywords,
ACM DL,conferencePaper,2021,Grand Challenges in Immersive Analytics,CHI - Human Factors in Computing Systems,A*,"Immersive Analytics is a quickly evolving field that unites several areas such as visualisation, immersive environments, and human-computer interaction to support human data analysis with emerging technologies. This research has thrived over the past years with multiple workshops, seminars, and a growing body of publications, spanning several conferences. Given the rapid advancement of interaction technologies and novel application domains, this paper aims toward a broader research agenda to enable widespread adoption. We present 17 key research challenges developed over multiple sessions by a diverse group of 24 international experts, initiated from a virtual scientific workshop at ACM CHI 2020. These challenges aim to coordinate future work by providing a systematic roadmap of current directions and impending hurdles to facilitate productive and effective applications for Immersive Analytics.",augmented reality; virtual reality; data visualisation; grand research challenges; Immersive analytics,Keywords,
ACM DL,conferencePaper,2021,Towards an Understanding of Situated AR Visualization for Basketball Free-Throw Training,CHI - Human Factors in Computing Systems,A*,"We present an observational study to compare co-located and situated real-time visualizations in basketball free-throw training. Our goal is to understand the advantages and concerns of applying immersive visualization to real-world skill-based sports training and to provide insights for designing AR sports training systems. We design both a situated 3D visualization on a head-mounted display and a 2D visualization on a co-located display to provide immediate visual feedback on a player’s shot performance. Using a within-subject study design with experienced basketball shooters, we characterize user goals, report on qualitative training experiences, and compare the quantitative training results. Our results show that real-time visual feedback helps athletes refine subsequent shots. Shooters in our study achieve greater angle consistency with our visual feedback. Furthermore, AR visualization promotes an increased focus on body form in athletes. Finally, we present suggestions for the design of future sports AR studies.",Augmented Reality; Data Visualization; Immersive Analytics; Situated Analytics; SportsXR,Keywords,
ACM DL,conferencePaper,2021,MARVIS: Combining Mobile Devices and Augmented Reality for Visual Data Analysis,CHI - Human Factors in Computing Systems,A*,"We present Marvis, a conceptual framework that combines mobile devices and head-mounted Augmented Reality (AR) for visual data analysis. We propose novel concepts and techniques addressing visualization-specific challenges. By showing additional 2D and 3D information around and above displays, we extend their limited screen space. AR views between displays as well as linking and brushing are also supported, making relationships between separated visualizations plausible. We introduce the design process and rationale for our techniques. To validate Marvis’ concepts and show their versatility and widespread applicability, we describe six implemented example use cases. Finally, we discuss insights from expert hands-on reviews. As a result, we contribute to a better understanding of how the combination of one or more mobile devices with AR can benefit visual data analysis. By exploring this new type of visualization environment, we hope to provide a foundation and inspiration for future mobile data visualizations.",mobile devices; data visualization; immersive analytics; augmented displays; cross-device interaction; data analysis; head-mounted augmented reality; mobile data visualization,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,STREAM: Exploring the Combination of Spatially-Aware Tablets with Augmented Reality Head-Mounted Displays for Immersive Analytics,CHI - Human Factors in Computing Systems,A*,"Recent research in the area of immersive analytics demonstrated the utility of head-mounted augmented reality devices for visual data analysis. However, it can be challenging to use the by default supported mid-air gestures to interact with visualizations in augmented reality (e.g. due to limited precision). Touch-based interaction (e.g. via mobile devices) can compensate for these drawbacks, but is limited to two-dimensional input. In this work we present STREAM: Spatially-aware Tablets combined with Augmented Reality Head-Mounted Displays for the multimodal interaction with 3D visualizations. We developed a novel eyes-free interaction concept for the seamless transition between the tablet and the augmented reality environment. A user study reveals that participants appreciated the novel interaction concept, indicating the potential for spatially-aware tablets in augmented reality. Based on our findings, we provide design insights to foster the application of spatially-aware touch devices in augmented reality and research implications indicating areas that need further investigation.",augmented reality; mobile devices; immersive analytics; multimodal interaction; visualizations,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,MIRIA: A Mixed Reality Toolkit for the In-Situ Visualization and Analysis of Spatio-Temporal Interaction Data,CHI - Human Factors in Computing Systems,A*,"In this paper, we present MIRIA, a Mixed Reality Interaction Analysis toolkit designed to support the in-situ visual analysis of user interaction in mixed reality and multi-display environments. So far, there are few options to effectively explore and analyze interaction patterns in such novel computing systems. With MIRIA, we address this gap by supporting the analysis of user movement, spatial interaction, and event data by multiple, co-located users directly in the original environment. Based on our own experiences and an analysis of the typical data, tasks, and visualizations used in existing approaches, we identify requirements for our system. We report on the design and prototypical implementation of MIRIA, which is informed by these requirements and offers various visualizations such as 3D movement trajectories, position heatmaps, and scatterplots. To demonstrate the value of MIRIA for real-world analysis tasks, we conducted expert feedback sessions using several use cases with authentic study data.",augmented reality; visualization; human-computer interaction; immersive analytics; interaction analysis; in-situ analysis; in-situ visualization,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Scene-Aware Behavior Synthesis for Virtual Pets in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Virtual pets are an alternative to real pets, providing a substitute for people with allergies or preparing people for adopting a real pet. Recent advancements in mixed reality pave the way for virtual pets to provide a more natural and seamless experience for users. However, one key challenge is embedding environmental awareness into the virtual pet (e.g., identifying the food bowl’s location) so that they can behave naturally in the real world. We propose a novel approach to synthesize virtual pet behaviors by considering scene semantics, enabling a virtual pet to behave naturally in mixed reality. Given a scene captured from the real world, our approach synthesizes a sequence of pet behaviors (e.g., resting after eating). Then, we assign each behavior in the sequence to a location in the real scene. We conducted user studies to evaluate our approach, which showed the efficacy of our approach in synthesizing natural virtual pet behaviors.",Behavior Synthesis; Scene Semantics; Virtual Pets,Abstract_Title,
ACM DL,conferencePaper,2021,RobotAR: An Augmented Reality Compatible Teleconsulting Robotics Toolkit for Augmented Makerspace Experiences,CHI - Human Factors in Computing Systems,A*,"Distance learning is facing a critical moment finding a balance between high quality education for remote students and engaging them in hands-on learning. This is particularly relevant for project-based classrooms and makerspaces, which typically require extensive trouble-shooting and example demonstrations from instructors. We present RobotAR, a teleconsulting robotics toolkit for creating Augmented Reality (AR) makerspaces. We present the hardware and software for an AR-compatible robot, which behaves as a student’s voice assistant and can be embodied by the instructor for teleconsultation. As a desktop-based teleconsulting agent, the instructor has control of the robot’s joints and position to better focus on areas of interest inside the workspace. Similarly, the instructor has access to the student’s virtual environment and the capability to create AR content to aid the student with problem-solving. We also performed a user study which compares current techniques for distance hands-on learning and an implementation of our toolkit.",augmented reality; robotics; robot; voice; teleconsulting; makerspaces,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Pedagogical Agents in Educational VR: An in the Wild Study,CHI - Human Factors in Computing Systems,A*,"Pedagogical agents are theorized to increase humans’ effort to understand computerized instructions. Despite the pedagogical promises of VR, the usefulness of pedagogical agents in VR remains uncertain. Based on this gap, and inspired by global efforts to advance remote learning during the COVID-19 pandemic, we conducted an educational VR study in-the-wild (N = 161). With a 2 × 2 + 1 between subjects design, we manipulated the appearance and behavior of a virtual museum guide in an exhibition about viruses. Factual and conceptual learning outcomes as well as subjective learning experience measures were collected. In general, participants reported high enjoyment and had significant knowledge acquisition. We found that the agent’s appearance and behavior impacted factual knowledge gain. We also report an interaction effect between behavioral and visual realism for conceptual knowledge gain. Our findings nuance classical multimedia learning theories and provide directions for employing agents in immersive learning environments.",Learning; Educational Technology; Cognitive Load; Immersive Virtual Reality; Pedagogical Agents,Keywords,
ACM DL,conferencePaper,2021,Digital Transformations of Classrooms in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"With rapid developments in consumer-level head-mounted displays and computer graphics, immersive VR has the potential to take online and remote learning closer to real-world settings. However, the effects of such digital transformations on learners, particularly for VR, have not been evaluated in depth. This work investigates the interaction-related effects of sitting positions of learners, visualization styles of peer-learners and teachers, and hand-raising behaviors of virtual peer-learners on learners in an immersive VR classroom, using eye tracking data. Our results indicate that learners sitting in the back of the virtual classroom may have difficulties extracting information. Additionally, we find indications that learners engage with lectures more efficiently if virtual avatars are visualized with realistic styles. Lastly, we find different eye movement behaviors towards different performance levels of virtual peer-learners, which should be investigated further. Our findings present an important baseline for design decisions for VR classrooms.",education; perception; eye tracking; avatars; immersive virtual reality,Keywords_Title,
ACM DL,conferencePaper,2021,HulaMove: Using Commodity IMU for Waist Interaction,CHI - Human Factors in Computing Systems,A*,"We present HulaMove, a novel interaction technique that leverages the movement of the waist as a new eyes-free and hands-free input method for both the physical world and the virtual world. We first conducted a user study (N=12) to understand users’ ability to control their waist. We found that users could easily discriminate eight shifting directions and two rotating orientations, and quickly confirm actions by returning to the original position (quick return). We developed a design space with eight gestures for waist interaction based on the results and implemented an IMU-based real-time system. Using a hierarchical machine learning model, our system could recognize waist gestures at an accuracy of 97.5%. Finally, we conducted a second user study (N=12) for usability testing in both real-world scenarios and virtual reality settings. Our usability study indicated that HulaMove significantly reduced interaction time by 41.8% compared to a touch screen method, and greatly improved users’ sense of presence in the virtual world. This novel technique provides an additional input method when users’ eyes or hands are busy, accelerates users’ daily operations, and augments their immersive experience in the virtual world.",on-device sensing; Waist interaction,Abstract,
ACM DL,conferencePaper,2021,Little Road Driving HUD: Heads-Up Display Complexity Influences Drivers’ Perceptions of Automated Vehicles,CHI - Human Factors in Computing Systems,A*,"Modern vehicles are using AI and increasingly sophisticated sensor suites to improve Advanced Driving Assistance Systems (ADAS) and support automated driving capabilities. Heads-Up-Displays (HUDs) provide an opportunity to visually inform drivers about vehicle perception and interpretation of the driving environment. One approach to HUD design may be to reveal to drivers the vehicle’s full contextual understanding, though it is not clear if the benefits of additional information outweigh the drawbacks of added complexity, or if this balance holds across drivers. We designed and tested an Augmented Reality (AR) HUD in an online study (N = 298), focusing on the influence of HUD visualizations on drivers’ situation awareness and perceptions. Participants viewed two driving scenes with one of three HUD conditions. Results were nuanced: situation awareness declined with increasing driving context complexity, and contrary to expectation, also declined with the presence of a HUD compared to no HUD. Significant differences were found by varying HUD complexity, which led us to explore different characterizations of complexity, including counts of scene items, item categories, and illuminated pixels. Our analysis finds that driving style interacts with driving context and HUD complexity, warranting further study.",augmented reality; interaction design; user interface; heads-up-display; situation awareness; vehicle interface,Abstract_Keywords,
ACM DL,conferencePaper,2021,From FOMO to JOMO: Examining the Fear and Joy of Missing Out and Presence in a 360° Video Viewing Experience,CHI - Human Factors in Computing Systems,A*,"Cinematic Virtual Reality (CVR), or 360° video, engages users in immersive viewing experiences. However, as users watch one part of the 360° view, they will necessarily miss out on events happening in other parts of the sphere. Consequently, fear of missing out (FOMO) is unavoidable. However, users can also experience the joy of missing out (JOMO). In a repeated measures, mixed methods design, we examined the fear and joy of missing out (FOMO and JOMO) and sense of presence in two repeat viewings of a 360° film using a head-mounted display. We found that users experienced both FOMO and JOMO. FOMO was caused by the users’ awareness of parallel events in the spherical view, but users also experienced JOMO. FOMO did not compromise viewers’ sense of presence, and FOMO also decreased in the second viewing session, while JOMO remained constant. The findings suggest that FOMO and JOMO can be two integral qualities in an immersive video viewing experience and that FOMO may not be as negative a factor as previously thought.",virtual reality; storytelling; embodiment; presence; immersive technologies; 360° video; FOMO; JOMO,Abstract_Keywords,
ACM DL,conferencePaper,2021,"RCEA-360VR: Real-time, Continuous Emotion Annotation in 360° VR Videos for Collecting Precise Viewport-dependent Ground Truth Labels",CHI - Human Factors in Computing Systems,A*,"Precise emotion ground truth labels for 360° virtual reality (VR) video watching are essential for fine-grained predictions under varying viewing behavior. However, current annotation techniques either rely on post-stimulus discrete self-reports, or real-time, continuous emotion annotations (RCEA) but only for desktop/mobile settings. We present RCEA for 360° VR videos (RCEA-360VR), where we evaluate in a controlled study (N=32) the usability of two peripheral visualization techniques: HaloLight and DotSize. We furthermore develop a method that considers head movements when fusing labels. Using physiological, behavioral, and subjective measures, we show that (1) both techniques do not increase users’ workload, sickness, nor break presence (2) our continuous valence and arousal annotations are consistent with discrete within-VR and original stimuli ratings (3) users exhibit high similarity in viewing behavior, where fused ratings perfectly align with intended labels. Our work contributes usable and effective techniques for collecting fine-grained viewport-dependent emotion labels in 360° VR.",virtual reality; Emotion; real-time; annotation; 360° video; continuous; ground truth; labels; viewport-dependent,Abstract_Keywords,
ACM DL,conferencePaper,2021,Do You Really Need to Know Where “That” Is? Enhancing Support for Referencing in Collaborative Mixed Reality Environments,CHI - Human Factors in Computing Systems,A*,"Mixed Reality has been shown to enhance remote guidance and is especially well-suited for physical tasks. Conversations during these tasks are heavily anchored around task objects and their spatial relationships in the real world, making referencing - the ability to refer to an object in a way that is understood by others - a crucial process that warrants explicit support in collaborative Mixed Reality systems. This paper presents a 2x2 mixed factorial experiment that explores the effects of providing spatial information and system-generated guidance to task objects. It also investigates the effects of such guidance on the remote collaborator’s need for spatial information. Our results show that guidance increases performance and communication efficiency while reducing the need for spatial information, especially in unfamiliar environments. Our results also demonstrate a reduced need for remote experts to be in immersive environments, making guidance more scalable, and expertise more accessible.",Mixed Reality; Collaboration; Remote Guidance; Referencing,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Understanding User Identification in Virtual Reality Through Behavioral Biometrics and the Effect of Body Normalization,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) is becoming increasingly popular both in the entertainment and professional domains. Behavioral biometrics have recently been investigated as a means to continuously and implicitly identify users in VR. Applications in VR can specifically benefit from this, for example, to adapt virtual environments and user interfaces as well as to authenticate users. In this work, we conduct a lab study (N = 16) to explore how accurately users can be identified during two task-driven scenarios based on their spatial movement. We show that an identification accuracy of up to 90% is possible across sessions recorded on different days. Moreover, we investigate the role of users’ physiology in behavioral biometrics by virtually altering and normalizing their body proportions. We find that body normalization in general increases the identification rate, in some cases by up to 38%; hence, it improves the performance of identification systems.",usable security; virtual reality; identification; task-driven biometrics,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Physiological and Perceptual Responses to Athletic Avatars while Cycling in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Avatars in virtual reality (VR) enable embodied experiences and induce the Proteus effect—a shift in behavior and attitude to mimic one’s digital representation. Previous work found that avatars associated with physical strength can decrease users’ perceived exertion when performing physical tasks. However, it is unknown if an avatar’s appearance can also influence the user’s physiological response to exercises. Therefore, we conducted an experiment with 24 participants to investigate the effect of avatars’ athleticism on heart rate and perceived exertion while cycling in VR following a standardized protocol. We found that the avatars’ athleticism has a significant and systematic effect on users’ heart rate and perceived exertion. We discuss potential moderators such as body ownership and users’ level of fitness. Our work contributes to the emerging area of VR exercise systems.",virtual reality; virtual embodiment; body ownership illusion; health intervention; perception of effort; Proteus effect; VR cycling,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Investigating the Impact of Real-World Environments on the Perception of 2D Visualizations in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"In this work we report on two comprehensive user studies investigating the perception of Augmented Reality (AR) visualizations influenced by real-world backgrounds. Since AR is an emerging technology, it is important to also consider productive use cases, which is why we chose an exemplary and challenging industry 4.0 environment. Our basic perceptual research focuses on both the visual complexity of backgrounds as well as the influence of a secondary task. In contrast to our expectation, data of our 34 study participants indicate that the background has far less influence on the perception of AR visualizations. Moreover, we observed a mismatch between measured and subjectively reported performance. We discuss the importance of the background and recommendations for visual real-world augmentations. Overall, our results suggest that AR can be used in many visually challenging environments without losing the ability to productively work with the visualizations shown.",Industry 4.0; Augmented Reality; User Study; Immersive Analytics; AR Visualization; In-Situ Visualization; Industrial Scenario; Visual Perception,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Remote and Collaborative Virtual Reality Experiments via Social VR Platforms,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) researchers struggle to conduct remote studies. Previous work has focused on working around limitations imposed by traditional crowdsourcing methods. However, the potential for leveraging social VR platforms for HCI evaluations is largely unexplored. These platforms have large VR-ready user populations, distributed synchronous virtual environments, and support for user-generated content. We demonstrate how social VR platforms can be used to practically and ethically produce valid research results by replicating two studies using one such platform (VRChat): a quantitative study on Fitts’ Law and a qualitative study on tabletop collaboration. Our replication studies exhibited analogous results to the originals, indicating the research validity of this approach. Moreover, we easily recruited experienced VR users with their own hardware for synchronous, remote, and collaborative participation. We further provide lessons learned for future researchers experimenting using social VR platforms. This paper and all supplemental materials are available at osf.io/c2amz.",Virtual Reality; Qualitative study; Social VR; Crowdsourcing; Quantitative Study; Replication Study; Transferability Study,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Exploring Text Revision with Backspace and Caret in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Current VR systems provide various text input methods that enable users to enter text efficiently with virtual keyboards. However, little attention has been paid to facilitate text revision during the VR text input process. We first summarized existing text revision solutions in current VR text input research and found that backspace is the only tool available for text revision with virtual keyboards with few mentioning designs for caret control. To systematically explore VR text revision designs, we presented a design space for VR text revision based on backspace and caret. With the proposed design space, we further analyzed the feasibility of the combined usage of backspace and caret by proposing and evaluating four VR text revision techniques. Outcomes of this research can provide a fundamental understanding of VR text revision solutions (with backspace and caret) and a comparable basis for evaluating future VR text revision techniques.",Virtual reality; Text input; Design space; Text revision,Keywords_Title,
ACM DL,conferencePaper,2021,Dynamic Field of View Restriction in 360° Video: Aligning Optical Flow and Visual SLAM to Mitigate VIMS,CHI - Human Factors in Computing Systems,A*,"Head-Mounted Display based Virtual Reality is proliferating. However, Visually Induced Motion Sickness (VIMS), which prevents many from using VR without discomfort, bars widespread adoption. Prior work has shown that limiting the Field of View (FoV) can reduce VIMS at a cost of also reducing presence. Systems that dynamically adjust a user’s FoV may be able to balance these concerns. To explore this idea, we present a technique for standard 360° video that shrinks FoVs only during VIMS inducing scenes. It uses Visual Simultaneous Localization and Mapping and peripheral optical flow to compute camera movements and reduces FoV during rapid motion or optical flow. A user study (N=23) comparing 360° video with unrestricted-FoVs (90°), reduced fixed-FoVs (40°) and dynamic-FoVs (40°-90°) revealed that dynamic-FoVs mitigate VIMS while maintaining presence. We close by discussing the user experience of dynamic-FoVs and recommendations for how they can help make VR comfortable and immersive for all.",Optical Flow; Cinematic Virtual Reality; Field of View Restriction; Simultaneous Localization and Mapping; Visually Induced Motion Sickness,Abstract_Keywords,
ACM DL,conferencePaper,2021,"Bad Breakdowns, Useful Seams, and Face Slapping: Analysis of VR Fails on YouTube",CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) is increasingly used in complex social and physical settings outside of the lab. However, not much is known about how these settings influence use, nor how to design for them. We analyse 233 YouTube videos of VR Fails to: (1) understand when breakdowns occur, and (2) reveal how the seams between VR use and the social and physical setting emerge. The videos show a variety of fails, including users flailing, colliding with surroundings, and hitting spectators. They also suggest causes of the fails, including fear, sensorimotor mismatches, and spectator participation. We use the videos as inspiration to generate design ideas. For example, we discuss more flexible boundaries between the real and virtual world, ways of involving spectators, and interaction designs to help overcome fear. Based on the findings, we further discuss the ‘moment of breakdown’ as an opportunity for designing engaging and enhanced VR experiences.",Virtual Reality; VR Breakdowns; VR Fails,Abstract_Keywords,
ACM DL,conferencePaper,2021,Extended Reality (XR) Remote Research: a Survey of Drawbacks and Opportunities,CHI - Human Factors in Computing Systems,A*,"Extended Reality (XR) technology - such as virtual and augmented reality - is now widely used in Human Computer Interaction (HCI), social science and psychology experimentation. However, these experiments are predominantly deployed in-lab with a co-present researcher. Remote experiments, without co-present researchers, have not flourished, despite the success of remote approaches for non-XR investigations. This paper summarises findings from a 30-item survey of 46 XR researchers to understand perceived limitations and benefits of remote XR experimentation. Our thematic analysis identifies concerns common with non-XR remote research, such as participant recruitment, as well as XR-specific issues, including safety and hardware variability. We identify potential positive affordances of XR technology, including leveraging data collection functionalities builtin to HMDs (e.g. hand, gaze tracking) and the portability and reproducibility of an experimental setting. We suggest that XR technology could be conceptualised as an interactive technology and a capable data-collection device suited for remote experimentation.",Augmented Reality; Virtual Reality; Extended Reality; literature review; expert interviews,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Distance Perception with a Video See-Through Head-Mounted Display,CHI - Human Factors in Computing Systems,A*,"In recent years, pass-through cameras have resurfaced as inclusions for virtual reality (VR) hardware. With modern cameras that now have increased resolution and frame rate, Video See-Through (VST) Head-Mounted Displays (HMD) can be used to provide an Augmented Reality (AR) experience. However, because users see their surroundings through video capture and HMD lenses, there is question surrounding how people perceive their environment with these devices. We conducted a user study with 26 participants to help understand if distance perception is altered when viewing surroundings with a VST HMD. Although previous work shows that distance estimation in VR with an HTC Vive is comparable to that in the real world, our results show that the inclusion of a ZED Mini pass-through camera causes a significant difference between normal, unrestricted viewing and that through a VST HMD.",Virtual Reality; User Study; Distance Perception; Pass-Through Camera; Video See-Through,Abstract_Keywords,
ACM DL,conferencePaper,2021,Effects of Emotion and Agency on Presence in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Arguably one of the most important characteristics of virtual reality (VR) is its ability to induce higher feelings of presence. Still, research has remained inconclusive on how presence is affected by human factors such as emotion and agency. Here we adopt a novel design to investigate their effects by testing virtual environments inducing either happiness or fear, with or without user agency. Results from 121 participants showed that the dominant emotion induced by a virtual environment is positively correlated with presence. In addition, agency had a significant positive effect on presence and, furthermore, moderated the effect of emotion on presence. We show for the first time that the effects of emotion and agency on presence are not straightforward but they can be modelled by separating design factors from subjective measures. We discuss how these findings can explain seemingly conflicting results of related work and their implications for VR design.",emotion; virtual reality; presence; agency.,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,A Critical Assessment of the Use of SSQ as a Measure of General Discomfort in VR Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"Based on a systematic literature review of more than 300 papers published over the last 10 years, we provide indicators that the simulator sickness questionnaire (SSQ) is extensively used and widely accepted as a general discomfort measure in virtual reality (VR) research – although it actually only accounts for one category of symptoms. This results in important other categories (digital eye strain (DES) and ergonomics) being largely neglected. To contribute to a more comprehensive picture of discomfort in VR head-mounted displays, we further conducted an online study (N=352) on the severity and relevance of all three symptom categories. Most importantly, our results reveal that symptoms of simulator sickness are significantly less severe and of lower prevalence than those of DES and ergonomics. In light of these findings, we critically discuss the current use of SSQ as the only discomfort measure and propose a more comprehensive factor model that also includes DES and ergonomics.",virtual reality; digital eye strain; discomfort; head-mounted displays; simulator sickness; SSQ,Abstract_Keywords,
ACM DL,conferencePaper,2021,Reading in VR: The Effect of Text Presentation Type and Location,CHI - Human Factors in Computing Systems,A*,"Reading is a fundamental activity to obtain information both in the real and the digital world. Virtual reality (VR) allows novel approaches for users to view, read, and interact with a text. However, for efficient reading, it is necessary to understand how a text should be displayed in VR without impairing the VR experience. Therefore, we conducted a study with 18 participants to investigate text presentation type and location in VR. We compared world-fixed, edge-fixed, and head-fixed text locations. Texts were displayed using Rapid Serial Visual Presentation (RSVP) or as a paragraph. We found that RSVP is a promising presentation type for reading short texts displayed in edge-fixed or head-fixed location in VR. The paragraph presentation type using world-fixed or edge-fixed location is promising for reading long text if movement in the virtual environment is not required. Insights from our study inform the design of reading interfaces for VR applications.",Virtual reality; reading in VR.; RSVP; text in VR,Abstract_Keywords,
ACM DL,conferencePaper,2021,Poros: Configurable Proxies for Distant Interactions in VR,CHI - Human Factors in Computing Systems,A*,"A compelling property of virtual reality is that it allows users to interact with objects as they would in the real world. However, such interactions are limited to space within reach. We present Poros, a system that allows users to rearrange space. After marking a portion of space, the distant marked space is mirrored in a nearby proxy. Thereby, users can arrange what is within their reachable space, making it easy to interact with multiple distant spaces as well as nearby objects. Proxies themselves become part of the scene and can be moved, rotated, scaled, or anchored to other objects. Furthermore, they can be used in a set of higher-level interactions such as alignment and action duplication. We show how Poros enables a variety of tasks and applications and also validate its effectiveness through an expert evaluation.",virtual reality; 3D user interface; portals; worlds in miniature,Abstract_Keywords,
ACM DL,conferencePaper,2021,How to Evaluate Object Selection and Manipulation in VR? Guidelines from 20 Years of Studies,CHI - Human Factors in Computing Systems,A*,"The VR community has introduced many object selection and manipulation techniques during the past two decades. Typically, they are empirically studied to establish their benefits over the state-of-the-art. However, the literature contains few guidelines on how to conduct such studies; standards developed for evaluating 2D interaction often do not apply. This lack of guidelines makes it hard to compare techniques across studies, to report evaluations consistently, and therefore to accumulate or replicate findings. To build such guidelines, we review 20 years of studies on VR object selection and manipulation. Based on the review, we propose recommendations for designing studies and a checklist for reporting them. We also identify research directions for improving evaluation methods and offer ideas for how to make studies more ecologically valid and rigorous.",virtual reality; experiments; object selection and manipulation,Keywords,
ACM DL,conferencePaper,2021,RepliCueAuth: Validating the Use of a Lab-Based Virtual Reality Setup for Evaluating Authentication Systems,CHI - Human Factors in Computing Systems,A*,"Evaluating novel authentication systems is often costly and time-consuming. In this work, we assess the suitability of using Virtual Reality (VR) to evaluate the usability and security of real-world authentication systems. To this end, we conducted a replication study and built a virtual replica of CueAuth [52], a recently introduced authentication scheme, and report on results from: (1) a lab-based in-VR usability study (N=20) evaluating user performance; (2) an online security study (N=22) evaluating system’s observation resistance through virtual avatars; and (3) a comparison between our results and those previously reported in the real-world evaluation. Our analysis indicates that VR can serve as a suitable test-bed for human-centred evaluations of real-world authentication schemes, but the used VR technology can have an impact on the evaluation. Our work is a first step towards augmenting the design and evaluation spectrum of authentication systems and offers ground work for more research to follow.",Virtual Reality; Usable Security; Authentication; Research Method,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Covert Embodied Choice: Decision-Making and the Limits of Privacy Under Biometric Surveillance,CHI - Human Factors in Computing Systems,A*,"Algorithms engineered to leverage rich behavioral and biometric data to predict individual attributes and actions continue to permeate public and private life. A fundamental risk may emerge from misconceptions about the sensitivity of such data, as well as the agency of individuals to protect their privacy when fine-grained (and possibly involuntary) behavior is tracked. In this work, we examine how individuals adjust their behavior when incentivized to avoid the algorithmic prediction of their intent. We present results from a virtual reality task in which gaze, movement, and other physiological signals are tracked. Participants are asked to decide which card to select without an algorithmic adversary anticipating their choice. We find that while participants use a variety of strategies, data collected remains highly predictive of choice (80% accuracy). Additionally, a significant portion of participants became more predictable despite efforts to obfuscate, possibly indicating mistaken priors about the dynamics of algorithmic prediction.",privacy; biometrics; virtual reality; surveillance; prediction,Abstract_Keywords,
ACM DL,conferencePaper,2021,CakeVR: A Social Virtual Reality (VR) Tool for Co-designing Cakes,CHI - Human Factors in Computing Systems,A*,"Cake customization services allow clients to collaboratively personalize cakes with pastry chefs. However, remote (e.g., email) and in-person co-design sessions are prone to miscommunication, due to natural restrictions in visualizing cake size, decoration, and celebration context. This paper presents the design, implementation, and expert evaluation of a social VR application (CakeVR) that allows a client to remotely co-design cakes with a pastry chef, through real-time realistic 3D visualizations. Drawing on expert semi-structured interviews (4 clients, 5 pastry chefs), we distill and incorporate 8 design requirements into our CakeVR prototype. We evaluate CakeVR with 10 experts (6 clients, 4 pastry chefs) using cognitive walkthroughs, and find that it supports ideation and decision making through intuitive size manipulation, color/flavor selection, decoration design, and custom celebration theme fitting. Our findings provide recommendations for enabling co-design in social VR and highlight CakeVR’s potential to transform product design communication through remote interactive and immersive co-design.",Social Virtual Reality; Co-design; Remote Collaboration; Cake Design,Keywords_Title,
ACM DL,conferencePaper,2021,A Critical Examination of Virtual Reality Technology in the Context of the Minority Body,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) holds the promise of immersing people in virtual worlds. However, initial work on the relationship between VR and disability suggests that VR is a body-centric technology that poses barriers for disabled users. We supplement this work with a theoretical analysis of immersive VR through the lens of Surrogate Body theory, a concept from media theory for the structured examination of interactive media in use. Leveraging Critical Disability Studies, particularly the theory of the Minority Body, we explore the assumptions about bodies inherent in VR, and we reflect on implications of these assumptions when disabled people engage with the technology. Our findings show that VR is an inherently ableist technology that assumes a ‘corporeal standard’ (i.e., an ‘ideal’, non-disabled human body), and fails to adequately accommodate disabled people. We conclude with implications for HCI research on VR, and discuss design approaches that foster inclusive technology development.",Disability Studies; Surrogate Body Theory,Abstract_Title,
ACM DL,conferencePaper,2021,Floral Tribute Ritual in Virtual Reality: Design and Validation of SenseVase with Virtual Memorial,CHI - Human Factors in Computing Systems,A*,"While floral tributes are commonly used for the public commemoration of victims of disasters, war, and other accidents, flowers in vases color everyday life. In this research, these features of flowers are intertwined with the recent phenomenon of online memorials to develop a virtual floral tribute concept that includes physical rituals. We designed SenseVase, a smart vase to detect flowers placed in it, and a 3DCG Virtual Memorial that illustrates floral tributes given by people using SenseVases at home. This paper describes how we developed our design concept by reviewing previous literature and social aspects, and presents a video illustrating the concept. To validate the current concept, we interviewed several experts knowledgeable in public commemorations, virtual and online communities, and the floral business. Through a discussion of our findings from the design process and interviews, we propose a new direction for how HCI technology can contribute to public commemoration in addition to personal memorialization.",Research through Design; Commemoration; Death Rituals; Memorialization; Mourning; Online Memorial; Techno-spiritual Practices; Thanatosensitive Design,Title,
ACM DL,conferencePaper,2021,ARTEMIS: A Collaborative Mixed-Reality System for Immersive Surgical Telementoring,CHI - Human Factors in Computing Systems,A*,"Traumatic injuries require timely intervention, but medical expertise is not always available at the patient’s location. Despite recent advances in telecommunications, surgeons still have limited tools to remotely help inexperienced surgeons. Mixed Reality hints at a future where remote collaborators work side-by-side as if co-located; however, we still do not know how current technology can improve remote surgical collaboration. Through role-playing and iterative-prototyping, we identify collaboration practices used by expert surgeons to aid novice surgeons as well as technical requirements to facilitate these practices. We then introduce ARTEMIS, an AR-VR collaboration system that supports these key practices. Through an observational study with two expert surgeons and five novice surgeons operating on cadavers, we find that ARTEMIS supports remote surgical mentoring of novices through synchronous point, draw, and look affordances and asynchronous video clips. Most participants found that ARTEMIS facilitates collaboration despite existing technology limitations explored in this paper.",Augmented Reality; Virtual Reality; Mixed Reality; Collaboration; Surgery; Telementoring,Abstract_Keywords,
ACM DL,conferencePaper,2021,Bringing the Jury to the Scene of the Crime: Memory and Decision-Making in a Simulated Crime Scene,CHI - Human Factors in Computing Systems,A*,"This paper investigates the use of immersive virtual reconstructions as an aid for jurors during a courtroom trial. The findings of a between-participant user study on memory and decision-making are presented in the context of viewing a simulated hit-run-death scenario. Participants listened to the opening statement of a prosecutor and a defence attorney before viewing the crime scene in Virtual Reality (VR) or as still images. We compare the effects on cognition and usability of using VR over images presented on a screen. We found several significant improvements, including that VR led to more consistent decision-making among participants. This shows that VR could provide a promising solution for the court to present crime scenes when site visitations are not possible.",Virtual Reality; spatial memory; 3D Reconstruction; crime scene; interactive virtual environment; jury,Abstract_Keywords,
ACM DL,conferencePaper,2021,A User-Oriented Approach to Space-Adaptive Augmentation: The Effects of Spatial Affordance on Narrative Experience in an Augmented Reality Detective Game,CHI - Human Factors in Computing Systems,A*,"Space-adaptive algorithms aim to effectively align the virtual with the real to provide immersive user experiences for Augmented Reality(AR) content across various physical spaces. While such measures are reliant on real spatial features, efforts to understand those features from the user’s perspective and reflect them in designing adaptive augmented spaces have been lacking. For this, we compared factors of narrative experience in six spatial conditions during the gameplay of Fragments, a space-adaptive AR detective game. Configured by size and furniture layout, each condition afforded disparate degrees of traversability and visibility. Results show that whereas centered furniture clusters are suitable for higher presence in sufficiently large rooms, the same layout leads to lower narrative engagement. Based on our findings, we suggest guidelines that can enhance the effects of space adaptivity by considering how users perceive and navigate augmented space generated from different physical environments.",Augmented Reality; storytelling; Head Mounted Displays; narrative experience; space adaptivity; spatial affordance; spatial mapping,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Augmented Reality and Older Adults: A Comparison of Prompting Types,CHI - Human Factors in Computing Systems,A*,"Older adults can benefit from technologies that help them to complete everyday tasks. However, they are an often-under-represented population in augmented reality (AR) research. We present the results of a study in which people aged 50 years or older were asked to perform actions by interpreting visual AR prompts in a lab setting. Our results show that users were less successful at completing actions when using ARROW and HIGHLIGHT augmentations than when using ghosted OBJECT or GHOSTHAND augmentations. We found that user confidence in performing actions varied according to action and augmentation type. Users preferred combined AUDIO+TEXT prompts (our control condition) overall, but the GHOSTHAND was the most preferred visual prompt. We discuss reasons for these differences and provide insight for developers of AR content for older adults. Our work provides the first comparative study of AR with older adults in a non-industrial context.",augmented reality; older adults; prompting tools; task prompting,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Assisting Manipulation and Grasping in Robot Teleoperation with Augmented Reality Visual Cues,CHI - Human Factors in Computing Systems,A*,"Teleoperating industrial manipulators in co-located spaces can be challenging. Facilitating robot teleoperation by providing additional visual information about the environment and the robot affordances using augmented reality (AR), can improve task performance in manipulation and grasping. In this paper, we present two designs of augmented visual cues, that aim to enhance the visual space of the robot operator through hints about the position of the robot gripper in the workspace and in relation to the target. These visual cues aim to improve the distance perception and thus, the task performance. We evaluate both designs against a baseline in an experiment where participants teleoperate a robotic arm to perform pick-and-place tasks. Our results show performance improvements in different levels, reflecting in objective and subjective measures with trade-offs in terms of time, accuracy, and participants’ views of teleoperation. These findings show the potential of AR not only in teleoperation, but in understanding the human-robot workspace.",augmented reality; human-robot interaction; visual cues; robot teleoperation,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Augmented Reality Glasses as an Orientation and Mobility Aid for People with Low Vision: a Feasibility Study of Experiences and Requirements,CHI - Human Factors in Computing Systems,A*,"People with low vision experience reduced mobility that affects their physical and mental wellbeing. With augmented reality (AR) glasses, there are new opportunities to provide visual and auditory information that can improve mobility for this vulnerable group. Current research into AR-based mobility aids has focused mainly on the technical aspects, and less emphasis has been placed on understanding the usability and suitability of these aids in people with various levels of visual impairment. In this paper, we present the results of qualitative interviews with 18 participants using HoloLens v1 and eight prototype augmentations to understand how these enhancements are perceived by people with low vision and how these aids should be adjusted to suit their needs. Our results suggested that participants with moderate vision loss could potentially perceive the most benefit from glasses and underlined the importance of extensive customizability to accommodate the needs of a highly varied low vision population.",augmented reality; low vision; mobility aids; vision enhancement,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Exploring Augmented Visual Alterations in Interpersonal Communication,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) glasses equip users with the tools to modify the visual appearance of their surrounding environment. This might severely impact interpersonal communication, as the conversational partners will no longer share the same visual perception of reality. Grounded in color-in-context theory, we present a potential AR application scenario in which users can modify the color of the environment to achieve subconscious benefits. In a consecutive online survey (N=64), we measured the user’s comfort, acceptance of altering and being altered, and how it is impacted by being able to perceive or not perceive the alteration. We identified significant differences depending on (1) who or what is the target of the alteration, (2) which body part is altered, and (3) which relationship the conversational partners share. In light of our quantitative and qualitative findings, we discuss ethical and practical implications for future devices and applications that employ visual alterations.",Augmented Reality; interpersonal communication; social acceptability; color-in-context; visual alterations,Abstract_Keywords,
ACM DL,conferencePaper,2021,Crowdsourcing Design Guidance for Contextual Adaptation of Text Content in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) can deliver engaging user experiences that seamlessly meld virtual content with the physical environment. However, building such experiences is challenging due to the developer’s inability to assess how uncontrolled deployment contexts may influence the user experience. To address this issue, we demonstrate a method for rapidly conducting AR experiments and real-world data collection in the user’s own physical environment using a privacy-conscious mobile web application. The approach leverages the large number of distinct user contexts accessible through crowdsourcing to efficiently source diverse context and perceptual preference data. The insights gathered through this method complement emerging design guidance and sample-limited lab-based studies. The utility of the method is illustrated by re-examining the design challenge of adapting AR text content to the user’s environment. Finally, we demonstrate how gathered design insight can be operationalized to provide adaptive text content functionality in an AR headset.",Augmented Reality; Privacy; Crowdsourcing,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Gaze-Supported 3D Object Manipulation in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"This paper investigates integration, coordination, and transition strategies of gaze and hand input for 3D object manipulation in VR. Specifically, this work aims to understand whether incorporating gaze input can benefit VR object manipulation tasks, and how it should be combined with hand input for improved usability and efficiency. We designed four gaze-supported techniques that leverage different combination strategies for object manipulation and evaluated them in two user studies. Overall, we show that gaze did not offer significant performance benefits for transforming objects in the primary working space, where all objects were located in front of the user and within the arm-reach distance, but can be useful for a larger environment with distant targets. We further offer insights regarding combination strategies of gaze and hand input, and derive implications that can help guide the design of future VR systems that incorporate gaze input for 3D object manipulation.",multimodal interface; 3D object manipulation; gaze input,Title,
ACM DL,conferencePaper,2021,Touch&amp;Fold: A Foldable Haptic Actuator for Rendering Touch in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"We propose a nail-mounted foldable haptic device that provides tactile feedback to mixed reality (MR) environments by pressing against the user's fingerpad when a user touches a virtual object. What is novel in our device is that it quickly tucks away when the user interacts with real-world objects. Its design allows it to fold back on top of the user's nail when not in use, keeping the user's fingerpad free to, for instance, manipulate handheld tools and other objects while in MR. To achieve this, we engineered a wireless and self-contained haptic device, which measures 24×24×41&nbsp;mm and weighs 9.5&nbsp;g. Furthermore, our foldable end-effector also features a linear resonant actuator, allowing it to render not only touch contacts (i.e., pressure) but also textures (i.e., vibrations). We demonstrate how our device renders contacts with MR surfaces, buttons, low- and high-frequency textures. In our first user study, we found that participants perceived our device to be more realistic than a previous haptic device that also leaves the fingerpad free (i.e., fingernail vibration). In our second user study, we investigated the participants’ experience while using our device in a real-world task that involved physical objects. We found that our device allowed participants to use the same finger to manipulate handheld tools, small objects, and even feel textures and liquids, without much hindrance to their dexterity, while feeling haptic feedback when touching MR interfaces.",Wearable; Mixed Reality; Haptics,Abstract_Keywords_Title,
ACM DL,conferencePaper,2021,Radi-Eye: Hands-Free Radial Interfaces for 3D Interaction using Gaze-Activated Head-Crossing,CHI - Human Factors in Computing Systems,A*,"Eye gaze and head movement are attractive for hands-free 3D interaction in head-mounted displays, but existing interfaces afford only limited control. Radi-Eye is a novel pop-up radial interface designed to maximise expressiveness with input from only the eyes and head. Radi-Eye provides widgets for discrete and continuous input and scales to support larger feature sets. Widgets can be selected with Look &amp; Cross, using gaze for pre-selection followed by head-crossing as trigger and for manipulation. The technique leverages natural eye-head coordination where eye and head move at an offset unless explicitly brought into alignment, enabling interaction without risk of unintended input. We explore Radi-Eye in three augmented and virtual reality applications, and evaluate the effect of radial interface scale and orientation on performance with Look &amp; Cross. The results show that Radi-Eye provides users with fast and accurate input while opening up a new design space for hands-free fluid interaction.",Augmented Reality; Virtual Reality; Eye tracking; Eye-Head Coordination; Gaze interaction; Radial Interface,Abstract_Keywords,
ACM DL,conferencePaper,2021,HoloBar: Rapid Command Execution for Head-Worn AR Exploiting Around the Field-of-View Interaction,CHI - Human Factors in Computing Systems,A*,"Inefficient menu interfaces lead to system and application commands being tedious to execute in Immersive Environments. HoloBar is a novel approach to ease the interaction with multi-level menus in immersive environments: with HoloBar, the hierarchical menu splits between the field of view (FoV) of the Head Mounted Display and the smartphone (SP). Command execution is based on around-the-FoV interaction with the SP, and touch input on the SP display. The HoloBar offers a unique combination of features, namely rapid mid-air activation, implicit selection of top-level items and preview of second-level items on the SP, ensuring rapid access to commands. In a first study we validate its activation method, which consists in bringing the SP within an activation distance from the FoV. In a second study, we compare the HoloBar to two alternatives, including the standard HoloLens menu. Results show that the HoloBar shortens each step of a multi-level menu interaction (menu activation, top-level item selection, second-level item selection and validation), with a high success rate. A follow-up study confirms that these results remain valid when compared with the two validation mechanisms of HoloLens (Air-Tap and clicker).",Augmented reality; Menu interaction; Smartphone based interactions,Keywords,
ACM DL,conferencePaper,2020,Mouillé: Exploring Wetness Illusion on Fingertips to Enhance Immersive Experience in VR,CHI - Human Factors in Computing Systems,A*,"Providing users with rich sensations is beneficial to enhance their immersion in Virtual Reality (VR) environments. Wetness is one such imperative sensation that affects users' sense of comfort and helps users adjust grip force when interacting with objects. Researchers have recently begun to explore ways to create wetness illusions, primarily on a user's face or body skin. In this work, we extended this line of research by creating wetness illusion on users' fingertips. We first conducted a user study to understand the effect of thermal and tactile feedback on users' perceived wetness sensation. Informed by the findings, we designed and evaluated a prototype—Mouillé—that provides various levels of wetness illusions on fingertips for both hard and soft items when users squeeze, lift, or scratch it. Study results indicated that users were able to feel wetness with different levels of temperature changes and they were able to distinguish three levels of wetness for simulated VR objects. We further presented applications that simulated an ice cube, an iced cola bottle, and a wet sponge, etc, to demonstrate its use in VR.",prototype; user study; virtual reality; wetness illusion,Abstract_Keywords,
ACM DL,conferencePaper,2020,Replicate and Reuse: Tangible Interaction Design for Digitally-Augmented Physical Media Objects,CHI - Human Factors in Computing Systems,A*,"Technology has transformed our physical interactions into infinitely more scalable and flexible digital ones. We can peruse an infinite number of photos, news articles, and books. However, these digital experiences lack the physical experience of paging through an album, reading a newspaper, or meandering through a bookshelf. Overlaying physical objects with digital content using augmented reality is a promising avenue towards bridging this gap. In this paper, we investigate the interaction design for such digital-overlaid physical objects and their varying levels of tangibility. We first conduct a user evaluation of a physical photo album that uses tangible interactions to support physical and digital operations. We further prototype multiple objects including bookshelves and newspapers and probe users on their usage, capabilities, and interactions. We then conduct a qualitative investigation of three interaction designs with varying tangibility that use three different input modalities. Finally, we discuss the insights from our investigations and recommend design guidelines.",augmented reality; design; physical objects; tangible interaction,Abstract_Keywords,
ACM DL,conferencePaper,2020,Breaking The Experience: Effects of Questionnaires in VR User Studies,CHI - Human Factors in Computing Systems,A*,"Questionnaires are among the most common research tools in virtual reality (VR) evaluations and user studies. However, transitioning from virtual worlds to the physical world to respond to VR experience questionnaires can potentially lead to systematic biases. Administering questionnaires in VR (inVRQs) is becoming more common in contemporary research. This is based on the intuitive notion that inVRQs may ease participation, reduce the Break in Presence (BIP) and avoid biases. In this paper, we perform a systematic investigation into the effects of interrupting the VR experience through questionnaires using physiological data as a continuous and objective measure of presence. In a user study (n=50), we evaluated question-asking procedures using a VR shooter with two different levels of immersion. The users rated their player experience with a questionnaire either inside or outside of VR. Our results indicate a reduced BIP for the employed inVRQ without affecting the self-reported player experience.",biosignals; break in presence; in-vr questionnaires; invrqs; research methods; surveys; user studies; virtual reality; vr,Abstract_Keywords,
ACM DL,conferencePaper,2020,Meta-AR-App: An Authoring Platform for Collaborative Augmented Reality in STEM Classrooms,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) has become a valuable tool for education and training processes. Meanwhile, cloud-based technologies can foster collaboration and other interaction modalities to enhance learning. We combine the cloud capabilities with AR technologies to present Meta-AR-App, an authoring platform for collaborative AR, which enables authoring between instructors and students. Additionally, we introduce a new application of an established collaboration process, the pull-based development model, to enable sharing and retrieving of AR learning content. We customize this model and create two modalities of interaction for the classroom: local (student to student) and global (instructor to class) pull. Based on observations from our user studies, we organize a four-category classroom model which implements our system: Work, Design, Collaboration, and Technology. Further, our system enables an iterative improvement workflow of the class content and enables synergistic collaboration that empowers students to be active agents in the learning process.",augmented reality; authoring; classroom; collaboration; electrical circuitry; git; pull-based model; stem; version control,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Pronto: Rapid Augmented Reality Video Prototyping Using Sketches and Enaction,CHI - Human Factors in Computing Systems,A*,"Designers have limited tools to prototype AR experiences rapidly. Can lightweight, immediate tools let designers prototype dynamic AR interactions while capturing the nuances of a 3D experience? We interviewed three AR experts and identified several recurring issues in AR design: creating and positioning 3D assets, handling the changing user position, and orchestrating multiple animations. We introduce PROJECT PRONTO, a tablet-based video prototyping system that combines 2D video with 3D manipulation. PRONTO supports four intertwined activities: capturing 3D spatial information alongside a video scenario, positioning and sketching 2D drawings in a 3D world, and enacting animations with physical interactions. An observational study with professional designers shows that participants can use PRONTO to prototype diverse AR experiences. All participants performed two tasks: replicating a sample non-trivial AR experience and prototyping their open-ended designs. All participants completed the replication task and found PRONTO easy to use. Most participants found that PRONTO encourages more exploration of designs than their current practices.",ar; design by enaction; sketching; video prototyping,Title,
ACM DL,conferencePaper,2020,Therminator: Understanding the Interdependency of Visual and On-Body Thermal Feedback in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Recent advances have made Virtual Reality (VR) more realistic than ever before. This improved realism is attributed to today's ability to increasingly appeal to human sensations, such as visual, auditory or tactile. While research also examines temperature sensation as an important aspect, the interdependency of visual and thermal perception in VR is still underexplored. In this paper, we propose Therminator, a thermal display concept that provides warm and cold on-body feedback in VR through heat conduction of flowing liquids with different temperatures. Further, we systematically evaluate the interdependency of different visual and thermal stimuli on the temperature perception of arm and abdomen with 25 participants. As part of the results, we found varying temperature perception depending on the stimuli, as well as increasing involvement of users during conditions with matching stimuli.",virtual reality; haptics; temperature; thermal feedback,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,"The Effects of Explicit Intention Communication, Conspicuous Sensors, and Pedestrian Attitude in Interactions with Automated Vehicles",CHI - Human Factors in Computing Systems,A*,"In this paper, we investigate the effect of an external human-machine interface (eHMI) and a conspicuous external vehicle appearance due to visible sensors on pedestrian interactions with automated vehicles (AVs). Recent research shows that AVs may need to explicitly communicate with the environment due to the absence of a driver. Furthermore, in interaction situations, an AV that looks different and conspicuous owing to an extensive sensor system may potentially lead to hesitation stemming from mistrust in automation. Thus, we evaluated in a virtual reality study how pedestrian attitude, the presence/absence of an eHMI, and a conspicuous sensor system affect their willingness to cross the road. Results recommend the use of an eHMI. A conspicuous appearance of automated-driving capability had no effect for the sample as a whole, although it led to more efficient crossing decisions for those with a more negative attitude towards AVs. Our findings contribute towards the effective design of future AV interfaces.",automated vehicles; ehmi; automated driving; vulnerable road users; pedestrians; external appearance; vehicle-pedestrian interaction; visible sensors,Abstract,
ACM DL,conferencePaper,2020,A Palette of Deepened Emotions: Exploring Emotional Challenge in Virtual Reality Games,CHI - Human Factors in Computing Systems,A*,"Recent work introduced the notion of 'emotional challenge' promising for understanding more unique and diverse player experiences (PX). Although emotional challenge has immediately attracted HCI researchers' attention, the concept has not been experimentally explored, especially in virtual reality (VR), one of the latest gaming environments. We conducted two experiments to investigate how emotional challenge affects PX when separately from or jointly with conventional challenge in VR and PC conditions. We found that relatively exclusive emotional challenge induced a wider range of different emotions in both conditions, while the adding of emotional challenge broadened emotional responses only in VR. In both experiments, VR significantly enhanced the measured PX of emotional responses, appreciation, immersion and presence. Our findings indicate that VR may be an ideal medium to present emotional challenge and also extend the understanding of emotional (and conventional) challenge in video games.",emotion; virtual reality; player experience; games; emotional challenge,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Bridging the Virtual and Real Worlds: A Preliminary Study of Messaging Notifications in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) platforms provide their users with immersive virtual environments, but disconnect them from real-world events. The increasing length of VR sessions can therefore be expected to boost users' needs to obtain information about external occurrences such as message arrival. Yet, how and when to present these real-world notifications to users engaged in VR activities remains underexplored. We conducted an experiment to investigate individuals' receptivity during four VR activities (Loading, 360 Video, Treasure Hunt, Rhythm Game) to message notifications delivered using three types of displays (head-mounted, controller, and movable panel). While higher engagement generally led to higher perceptions that notifications were ill-timed and/or disruptive, the suitability of notification displays to VR activities was influenced by the time-sensitiveness of VR content, overlapping use of modalities for delivering alerts, the display locations, and a requirement that the display be moved for notifications to be seen. Specific design suggestions are also provided.",virtual reality; eye-tracking; notification systems; interruptibility; receptivity,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,BISHARE: Exploring Bidirectional Interactions Between Smartphones and Head-Mounted Augmented Reality,CHI - Human Factors in Computing Systems,A*,"In pursuit of a future where HMD devices can be used in tandem with smartphones and other smart devices, we present BISHARE, a design space of cross-device interactions between smartphones and ARHMDs. Our design space is unique in that it is bidirectional in nature, as it examines how both the HMD can be used to enhance smartphone tasks, and how the smartphone can be used to enhance HMD tasks. We then present an interactive prototype that enables cross-device interactions across the proposed design space. A 12-participant user study demonstrates the promise of the design space and provides insights, observations, and guidance for the future.",augmented reality; smartphones; cross-device computing; mixed-reality computing,Keywords_Title,
ACM DL,conferencePaper,2020,JumpVR: Jump-Based Locomotion Augmentation for Virtual Reality,CHI - Human Factors in Computing Systems,A*,"One of the great benefits of virtual reality (VR) is the implementation of features that go beyond realism. Common ""unrealistic"" locomotion techniques (like teleportation) can avoid spatial limitation of tracking, but minimize potential benefits of more realistic techniques (e.g. walking). As an alternative that combines realistic physical movement with hyper-realistic virtual outcome, we present JumpVR, a jump-based locomotion augmentation technique that virtually scales users' physical jumps. In a user study (N=28), we show that jumping in VR (regardless of scaling) can significantly increase presence, motivation and immersion compared to teleportation, while largely not increasing simulator sickness. Further, participants reported higher immersion and motivation for most scaled jumping variants than forward-jumping. Our work shows the feasibility and benefits of jumping in VR and explores suitable parameters for its hyper-realistic scaling. We discuss design implications for VR experiences and research.",virtual reality; immersion; jumping; vr; hyper realism; super human,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,ARMath: Augmenting Everyday Life with Math Learning,CHI - Human Factors in Computing Systems,A*,"We introduce ARMath, a mobile Augmented Reality (AR) system that allows ch ildren to discover mathematical concepts in familiar, ord inary objects and engage with math problems in meaningful contexts. Leveraging advanced computer vision, ARMath recognizes everyday objects, visualizes their mathematical attributes, and turns them into tangible or virtual manipulatives. Using the manipulatives, children can solve problems that situate math operations or concepts in specific everyday contexts. Informed by four participatory design sessions with teachers and children, we developed five ARMath modules to support basic arithmetic and 2D geometry. We also conducted an exploratory evaluation of ARMath with 27 children (ages 5-8) at a local children's museum. Our findings demonstrate how ARMath engages children in math learning, how failures in AI can be used as learning opportunities, and challenges that children face when using ARMath.",augmented reality; learning; human-ai interaction,Abstract_Keywords,
ACM DL,conferencePaper,2020,"Race Yourselves: A Longitudinal Exploration of Self-Competition Between Past, Present, and Future Performances in a VR Exergame",CHI - Human Factors in Computing Systems,A*,"Participating in competitive races can be a thrilling experience for athletes, involving a rush of excitement and sensations of flow, achievement, and self-fulfilment. However, for non-athletes, the prospect of competition is often a scary one which affects intrinsic motivation negatively, especially for less fit, less competitive individuals. We propose a novel method making the positive racing experience accessible to non-athletes using a high-intensity cycling VR exergame: by recording and replaying all their previous gameplay sessions simultaneously, including a projected future performance, players can race against a crowd of ""ghost"" avatars representing their individual fitness journey. The experience stays relevant and exciting as every race adds a new competitor. A longitudinal study over four weeks and a cross-sectional study found that the new method improves physical performance, intrinsic motivation, and flow compared to a non-competitive exergame. Additionally, the longitudinal study provides insights into the longer-term effects of VR exergames.",performance; exergame; ghosts; intrinsic motivation; longitudinal; self-competition; virtual reality (vr),Keywords,
ACM DL,conferencePaper,2020,Examining Design Choices of Questionnaires in VR User Studies,CHI - Human Factors in Computing Systems,A*,"Questionnaires are among the most common research tools in virtual reality (VR) user studies. Transitioning from virtuality to reality for giving self-reports on VR experiences can lead to systematic biases. VR allows to embed questionnaires into the virtual environment which may ease participation and avoid biases. To provide a cohesive picture of methods and design choices for questionnaires in VR (inVRQ), we discuss 15 inVRQ studies from the literature and present a survey with 67 VR experts from academia and industry. Based on the outcomes, we conducted two user studies in which we tested different presentation and interaction methods of inVRQs and evaluated the usability and practicality of our design. We observed comparable completion times between inVRQs and questionnaires outside VR (nonVRQs) with higher enjoyment but lower usability for inVRQs. These findings advocate the application of inVRQs and provide an overview of methods and considerations that lay the groundwork for inVRQ design.",virtual reality; research methods; user studies; in-vr questionnaires; invrqs; vr,Abstract_Keywords,
ACM DL,conferencePaper,2020,Virtual Reality Games for People Using Wheelchairs,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) holds the promise of providing engaging embodied experiences, but little is known about how people with disabilities engage with it. We explore challenges and opportunities of VR gaming for wheelchair users. First, we present findings from a survey that received 25 responses and gives insights into wheelchair users' motives to (non-) engage with VR and their experiences. Drawing from this survey, we derive design implications which we tested through implementation and qualitative evaluation of three full-body VR game prototypes with 18 participants. Our results show that VR gaming engages wheelchair users, though nuanced consideration is required for the design of embodied immersive experiences for minority bodies, and we illustrate how designers can create meaningful, positive experiences.",accessibility; virtual reality; games,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,MoveVR: Enabling Multiform Force Feedback in Virtual Reality using Household Cleaning Robot,CHI - Human Factors in Computing Systems,A*,"Haptic feedback can significantly enhance the realism and immersiveness of virtual reality (VR) systems. In this paper, we propose MoveVR, a technique that enables realistic, multiform force feedback in VR leveraging commonplace cleaning robots. MoveVR can generate tension, resistance, impact and material rigidity force feedback with multiple levels of force intensity and directions. This is achieved by changing the robot's moving speed, rotation, position as well as the carried proxies. We demonstrated the feasibility and effectiveness of MoveVR through interactive VR gaming. In our quantitative and qualitative evaluation studies, participants found that MoveVR provides more realistic and enjoyable user experience when compared to commercially available haptic solutions such as vibrotactile haptic systems.",virtual reality; human-robot interaction; robotics; haptic feedback; force feedback; vr; cleaning robot,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Mixed Reality Light Fields for Interactive Remote Assistance,CHI - Human Factors in Computing Systems,A*,"Remote assistance represents an important use case for mixed reality. With the rise of handheld and wearable devices, remote assistance has become practical in the wild. However, spontaneous provisioning of remote assistance requires an easy, fast and robust approach for capturing and sharing of unprepared environments. In this work, we make a case for utilizing interactive light fields for remote assistance. We demonstrate the advantages of object representation using light fields over conventional geometric reconstruction. Moreover, we introduce an interaction method for quickly annotating light fields in 3D space without requiring surface geometry to anchor annotations. We present results from a user study demonstrating the effectiveness of our interaction techniques, and we provide feedback on the usability of our overall system.",augmented reality; mixed reality; interaction; telepresence; 3d user interfaces; annotations; light field; remote assistance,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Knock on Wood: Combining Redirected Touching and Physical Props for Tool-Based Interaction in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"When physical props serve as proxies for virtual tools used to manipulate the virtual environment, it is challenging to provide appropriate haptic feedback. Redirected tool-mediated manipulation addresses this challenge by distorting the mapping between physical and virtual tools to provide a sensation of manipulating the virtual environment, when the physical tool comes into contact with another physical prop. For example, a virtual hammer's position can be offset to ensure that physical impacts accompany each strike of a virtual nail. We demonstrate the idea by showing that it can be used to create sensations of impact and resistance when driving a virtual nail into a surface, when tightening a virtual screw, and when sawing through a virtual plank. The results of a user study indicate that the proposed approach is perceived as more realistic than interaction with a single physical prop or controller and no notable detriments to precision were observed.",virtual reality; passive haptics; redirected touching,Keywords_Title,
ACM DL,conferencePaper,2020,Gripmarks: Using Hand Grips to Transform In-Hand Objects into Mixed Reality Input,CHI - Human Factors in Computing Systems,A*,"We introduce Gripmarks, a system that enables users to opportunistically use objects they are already holding as input surfaces for mixed reality head-mounted displays (HMD). Leveraging handheld objects reduces the need for users to free up their hands or acquire a controller to interact with their HMD. Gripmarks associate a particular hand grip with the shape primitive of the physical object without the need of object recognition or instrumenting the object. From the grip pose and shape primitive we can infer the surface of the object. With an activation gesture, we can enable the object for use as input to the HMD. With five gripmarks we demonstrate a recognition rate of 94.2%; we show that our grip detection benefits from the physical constraints of holding an object. We explore two categories of input objects 1) tangible surfaces and 2) tangible tools and present two representative applications. We discuss the design and technical challenges for expanding the concept.",mixed reality; grip recognition; gripmarks; tangible objects,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,MRAT: The Mixed Reality Analytics Toolkit,CHI - Human Factors in Computing Systems,A*,"Significant tool support exists for the development of mixed reality (MR) applications; however, there is a lack of tools for analyzing MR experiences. We elicit requirements for future tools through interviews with 8 university research, instructional, and media teams using AR/VR in a variety of domains. While we find a common need for capturing how users perform tasks in MR, the primary differences were in terms of heuristics and metrics relevant to each project. Particularly in the early project stages, teams were uncertain about what data should, and even could, be collected with MR technologies. We designed the Mixed Reality Analytics Toolkit (MRAT) to instrument MR apps via visual editors without programming and enable rapid data collection and filtering for visualizations of MR user sessions. With MRAT, we contribute flexible interaction tracking and task definition concepts, an extensible set of heuristic techniques and metrics to measure task success, and visual inspection tools with in-situ visualizations in MR. Focusing on a multi-user, cross-device MR crisis simulation and triage training app as a case study, we then show the benefits of using MRAT, not only for user testing of MR apps, but also performance tuning throughout the design process.",augmented/virtual reality; interaction tracking; user testing,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Investigating the Effects of Self-Avatars and Story-Relevant Avatars on Children's Creative Storytelling,CHI - Human Factors in Computing Systems,A*,"Storytelling is a critical step in the cognitive development of children. Particularly, this requires children to mentally project into the story context and to identify with the thoughts of the characters in their stories. We propose to support free imagination in creative storytelling through an enactment-based approach that allows children to embody an avatar and perform as the story character. We designed our story creation interface with two modes of avatar: the story-relevant avatar and the self-avatar, to investigate the effects of avatar design on the quality of children's creative products. In our study with 20 child participants, the results indicate that self-avatars can create a stronger sense of identification and embodied presence, while story-relevant avatars can provide a scaffold for mental projection.",virtual reality; storytelling; embodied interaction; creativity; expressive writing,Keywords,
ACM DL,conferencePaper,2020,Improving Humans' Ability to Interpret Deictic Gestures in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Collaborative Virtual Environments (CVEs) offer unique opportunities for human communication. Humans can interact with each other over a distance in any environment and visual embodiment they want. Although deictic gestures are especially important as they can guide other humans' attention, humans make systematic errors when using and interpreting them. Recent work suggests that the interpretation of vertical deictic gestures can be significantly improved by warping the pointing arm. In this paper, we extend previous work by showing that models enable to also improve the interpretation of deictic gestures at targets all around the user. Through a study with 28 participants in a CVE, we analyzed the errors users make when interpreting deictic gestures. We derived a model that rotates the arm of a pointing user's avatar to improve the observing users' accuracy. A second study with 24 participants shows that we can improve observers' accuracy by 22.9%. As our approach is not noticeable for users, it improves their accuracy without requiring them to learn a new interaction technique or distracting from the experience.",virtual reality; correction model; deictic; ray tracing,Keywords_Title,
ACM DL,conferencePaper,2020,Virtual Reality Without Vision: A Haptic and Auditory White Cane to Navigate Complex Virtual Worlds,CHI - Human Factors in Computing Systems,A*,"Current Virtual Reality (VR) technologies focus on rendering visuospatial effects, and thus are inaccessible for blind or low vision users. We examine the use of a novel white cane controller that enables navigation without vision of large virtual environments with complex architecture, such as winding paths and occluding walls and doors. The cane controller employs a lightweight three-axis brake mechanism to provide large-scale shape of virtual objects. The multiple degrees-of-freedom enables users to adapt the controller to their preferred techniques and grip. In addition, surface textures are rendered with a voice coil actuator based on contact vibrations; and spatialized audio is determined based on the progression of sound through the geometry around the user. We design a scavenger hunt game that demonstrates how our device enables blind users to navigate a complex virtual environment. Seven out of eight users were able to successfully navigate the virtual room (6x6m) to locate targets while avoiding collisions. We conclude with design consideration on creating immersive non-visual VR experiences based on user preferences for cane techniques, and cane material properties.",virtual reality; haptic feedback; mobility; visual impairments; auditory feedback; blindness; 3d audio; white cane,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,PoCoPo: Handheld Pin-based Shape Display for Haptic Rendering in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We introduce PoCoPo, the first handheld pin-based shape display that can render various 2.5D shapes in hand in realtime. We designed the display small enough for a user to hold it in hand and carry it around, thereby enhancing the haptic experiences in a virtual environment. PoCoPo has 18 motor-driven pins on both sides of a cuboid, providing the sensation of skin contact on the user's palm and fingers. We conducted two user studies to understand the capability of PoCoPo. The first study showed that the participants were generally successful in distinguishing the shapes rendered by PoCoPo with an average success rate of 88.5%. In the second study, we investigated the acceptable visual size of a virtual object when PoCoPo rendered a physical object of a certain size. The result led to a better understanding of the acceptable differences between the perceptions of visual size and haptic size.",virtual reality; handheld device; haptic device; shape display,Keywords_Title,
ACM DL,conferencePaper,2020,"Embedding a VR Game Studio in a Sedentary Workplace: Use, Experience and Exercise Benefits",CHI - Human Factors in Computing Systems,A*,"Many people, especially those in sedentary occupations, fail to achieve the recommended levels of physical activity. Virtual reality (VR) games have the potential to overcome this because they are fun and also can be physically demanding. This paper explores whether a VR game studio can help workers in sedentary jobs to get valuable levels of exercise. We studied how 11 participants used our VR game studio in a sedentary workplace over 8-weeks and their perceptions of the experience. We analysed the physical exertion in the VR game studio, comparing this to their step counts from a smartwatch. All participants achieved valuable levels of physical activity and mood benefits. Importantly, for 6 participants, only with the VR game studio did they meet recommended activity levels. Our key contributions are insights about the use of a workplace VR game studio and its health benefits.",head-mounted display; exercise; sedentary workplace; virtual reality game,Abstract_Keywords,
ACM DL,conferencePaper,2020,Pac-Euglena: A Living Cellular Pac-Man Meets Virtual Ghosts,CHI - Human Factors in Computing Systems,A*,"The advancement of biotechnology enabled the development of ""biotic video games"", where human players manipulate real biological samples for fun and educational human-biology interactions. However, new design principles are needed to both leverage and mitigate biological properties (e.g., variability and stochasticity), and create unique play experiences that transcend traditional video games. This paper describes the implementation of Pac-Euglena, a biotic Pac-Man analog, where players guide live microscopic Euglena cells with light stimuli through a physical microfluidic maze. Through use of multi-modal stimuli, a mixed biology-digital-human reality is achieved, enabling cell interactions with virtual ghosts and collectibles. Through an iterative design process, we illustrate challenges and strategies for designing games with living organisms. A user study (n=18, conducted at a university event) showed that Pac-Euglena was fun, stimulated curiosity, and taught users about Euglena. We conclude with five general guidelines for the design and development of biotic games and HBI interfaces.",augmented reality; mixed reality; biological user interfaces; biotic games; euglena gracilis; human-biology interaction (hbi),Keywords,
ACM DL,conferencePaper,2020,Supporting Self-Injury Recovery: The Potential for Virtual Reality Intervention,CHI - Human Factors in Computing Systems,A*,"In this paper, we explore the use of virtual reality (VR) in assisting individuals who self-injure. Past work on self-injury in HCI has focused almost exclusively on mobile applications and message boards. As VR systems become more common, it is worth exploring what unique affordances of the technology can be leveraged to support self-injury reduction and cessation. Research on VR intervention and self-injury treatment informed the design of three novel virtual reality experiences. Nineteen interviews were conducted with individuals with current, or a past history of, self-injury with the goals of uncovering overall impressions of the perceived efficacy of VR with this population, as well as better understanding key mechanisms which impact their experience. Our analysis reveals four key elements common across all experiences: transportation, embodiment, immersion/distraction, and sense of control, and additional themes within each unique experience. We discuss the implications of these findings for future intervention design.",intervention; virtual reality; self-injury,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Utilizing VR and Gaze Tracking to Develop AR Solutions for Industrial Maintenance,CHI - Human Factors in Computing Systems,A*,"Augmented reality (AR) presents a variety of possibilities for industrial maintenance. However, the development of real-world AR solutions has been limited due to the technological capabilities and uncertainty with respect to safety at deployment. We introduce the approach of using AR simulation in virtual reality (VR) coupled with gaze tracking to enable resource-efficient AR development. We tested in-field AR guidance and safety awareness features in an iterative development-evaluation process with experts from the elevator maintenance industry. We further conducted a survey, utilizing actual gaze data from the evaluation to elicit comments from industry experts on the usefulness of AR simulation and gaze tracking. Our results show the potential of AR within VR approach combined with gaze tracking. With this framework, AR solutions can be iteratively and safely tested without actual implementation, while gaze data provide advanced objective means to evaluate the designed AR content, documentation usage, and safety awareness.",augmented reality; virtual reality; industrial maintenance; gaze tracking; safety; virtual prototyping,Abstract_Keywords,
ACM DL,conferencePaper,2020,ElastOscillation: 3D Multilevel Force Feedback for Damped Oscillation on VR Controllers,CHI - Human Factors in Computing Systems,A*,"Force feedback from damped oscillation is a common effect in our daily lives, especially when shaking an elastic object, an object hanging or containing other stuff, or a container with liquid, e.g., casting with a fishing pole or wine-swirling. Such a force, affected by complex physical variations and collisions, is difficult to properly simulate using current force feedback methods. Therefore, we propose ElastOscillation on a virtual reality (VR) controller to provide 3D multilevel force feedback for damped oscillation to enhance VR experiences. ElastOscillation consists of a proxy, six elastic bands and DC motors. It leverages the motors to control the bands' elasticity to restrain the movement of the proxy, which is connected with the bands. Therefore, when users shake the ElastOscillation device, the proxy shakes or moves in corresponding ranges of movement. The users then perceive the force from oscillation at different levels. In addition, elastic force from the bands further reinforces the oscillation force feedback. We conducted a force perception study to understand users' distinguishability for perceiving oscillation forces in 1D and 2D movement, respectively. Based on the results, we performed a VR experience study to show that the force feedback provided by ElastOscillation enhances VR realism.",virtual reality; haptic feedback; force feedback; elastic force; oscillation,Abstract_Keywords,
ACM DL,conferencePaper,2020,Levitation Simulator: Prototyping Ultrasonic Levitation Interfaces in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We present the Levitation Simulator, a system that enables researchers and designers to iteratively develop and prototype levitation interface ideas in Virtual Reality. This includes user tests and formal experiments. We derive a model of the movement of a levitating particle in such an interface. Based on this, we develop an interactive simulation of the levitation interface in VR, which exhibits the dynamical properties of the real interface. The results of a Fitts' Law pointing study show that the Levitation Simulator enables performance, comparable to the real prototype. We developed the first two interactive games, dedicated for levitation interfaces: LeviShooter and BeadBounce, in the Levitation Simulator, and then implemented them on the real interface. Our results indicate that participants experienced similar levels of user engagement when playing the games, in the two environments. We share our Levitation Simulator as Open Source, thereby democratizing levitation research, without the need for a levitation apparatus.",simulation; modeling; vr; virtual prototyping; ultrasonic levitation,Abstract_Title,
ACM DL,conferencePaper,2020,Inducing and Mitigating Stereotype Threat Through Gendered Virtual Body-Swap Illusions,CHI - Human Factors in Computing Systems,A*,"A psychological phenomenon termed ""stereotype threat"" has been shown to contribute to women's underperformance and underrepresentation in math and science fields. Within the virtual reality literature, a recent study utilized gendered body-swap illusions (i.e., women in male virtual bodies) to mitigate the effects of stereotype threat among a sample of female participants. The present research provides a much needed replication of this intervention, as well as a critical extension of virtual reality research on the Proteus Effect to test whether stereotype threat can be induced among male participants immersed in a female virtual body. Results supported both the replication and extension hypotheses; female participants embodied in male avatars were buffered from stereotype threat whereas male participants embodied in female avatars suffered from stereotype threat. Avatar gender also influenced participants' math confidence and awareness of the negative societal stereotype regarding women's math ability.",virtual reality; embodiment; gender; gender identity; body-swap illusions; proteus effect; self-avatars; stereotype threat,Abstract_Keywords,
ACM DL,conferencePaper,2020,Soundr: Head Position and Orientation Prediction Using a Microphone Array,CHI - Human Factors in Computing Systems,A*,"Although state-of-the-art smart speakers can hear a user's speech, unlike a human assistant these devices cannot figure out users' verbal references based on their head location and orientation. Soundr presents a novel interaction technique that leverages the built-in microphone array found in most smart speakers to infer the user's spatial location and head orientation using only their voice. With that extra information, Soundr can figure out users references to objects, people, and locations based on the speakers' gaze, and also provide relative directions. To provide training data for our neural network, we collected 751 minutes of data (50x that of the best prior work) from human speakers leveraging a virtual reality headset to accurately provide head tracking ground truth. Our results achieve an average positional error of 0.31m and an orientation angle accuracy of 34.3° for each voice command. A user study to evaluate user preferences for controlling IoT appliances by talking at them found this new approach to be fast and easy to use.",machine learning; smart speakers; internet of things; acoustic source localization,Abstract,
ACM DL,conferencePaper,2020,Augmenting Static Visualizations with PapARVis Designer,CHI - Human Factors in Computing Systems,A*,"This paper presents an authoring environment for augmenting static visualizations with virtual content in augmented reality.Augmenting static visualizations can leverage the best of both physical and digital worlds, but its creation currently involves different tools and devices, without any means to explicitly design and debug both static and virtual content simultaneously. To address these issues, we design an environment that seamlessly integrates all steps of a design and deployment workflow through its main features: i) an extension to Vega, ii) a preview, and iii) debug hints that facilitate valid combinations of static and augmented content. We inform our design through a design space with four ways to augment static visualizations. We demonstrate the expressiveness of our tool through examples, including books, posters, projections, wall-sized visualizations. A user study shows high user satisfaction of our environment and confirms that participants can create augmented visualizations in an average of 4.63 minutes.",augmented static visualization; data visualization authoring; visualization in augmented reality,Abstract_Keywords,
ACM DL,conferencePaper,2020,Outline Pursuits: Gaze-assisted Selection of Occluded Objects in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"In 3D environments, objects can be difficult to select when they overlap, as this affects available target area and increases selection ambiguity. We introduce Outline Pursuits which extends a primary pointing modality for gaze-assisted selection of occluded objects. Candidate targets within a pointing cone are presented with an outline that is traversed by a moving stimulus. This affords completion of the selection by gaze attention to the intended target's outline motion, detected by matching the user's smooth pursuit eye movement. We demonstrate two techniques implemented based on the concept, one with a controller as the primary pointer, and one in which Outline Pursuits are combined with head pointing for hands-free selection. Compared with conventional raycasting, the techniques require less movement for selection as users do not need to reposition themselves for a better line of sight, and selection time and accuracy are less affected when targets become highly occluded.",virtual reality; eye tracking; occlusion; smooth pursuits,Keywords_Title,
ACM DL,conferencePaper,2020,C-Space: An Interactive Prototyping Platform for Collaborative Spatial Design Exploration,CHI - Human Factors in Computing Systems,A*,"C-Space is an interactive prototyping platform for collaborative spatial design exploration. Spatial design projects often begin with conceptualization that includes abstract diagramming, zoning, and massing to provide a foundation for making design decisions. Specifically, abstract diagrams guide designers to explore alternative designs without thinking prematurely about the details. However, complications arise when communicating ambiguous and incomplete designs to collaborators. To overcome this drawback, designers devote considerable amounts of time and resources into searching for design references and creating rough prototypes to explicate their design concepts better. Therefore, this study proposes C-Space, a novel design support system that integrates the abstract diagram with design reference retrieval and prototyping through a tangible user interface and augmented reality. Through a user study with 12 spatial designers, we verify that C-Space promotes rapid and robust spatial design exploration, inducing collaborative discussions and motivating users to interact with designs.",augmented reality; human-computer interaction; prototyping; tangible user interface; design support system; spatial design; design collaboration,Abstract_Keywords,
ACM DL,conferencePaper,2020,Wireality: Enabling Complex Tangible Geometries in Virtual Reality with Worn Multi-String Haptics,CHI - Human Factors in Computing Systems,A*,"Today's virtual reality (VR) systems allow users to explore immersive new worlds and experiences through sight. Unfortunately, most VR systems lack haptic feedback, and even high-end consumer systems use only basic vibration motors. This clearly precludes realistic physical interactions with virtual objects. Larger obstacles, such as walls, railings, and furniture are not simulated at all. In response, we developed Wireality, a self-contained worn system that allows for individual joints on the hands to be accurately arrested in 3D space through the use of retractable wires that can be programmatically locked. This allows for convincing tangible interactions with complex geometries, such as wrapping fingers around a railing. Our approach is lightweight, low-cost, and low-power, criteria important for future, worn consumer uses. In our studies, we further show that our system is fast-acting, spatially-accurate, high-strength, comfortable, and immersive.",virtual reality; haptics; touch; force feedback; grasp; string-driven,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Towards Inclusive External Communication of Autonomous Vehicles for Pedestrians with Vision Impairments,CHI - Human Factors in Computing Systems,A*,"People with vision impairments (VIP) are among the most vulnerable road users in traffic. Autonomous vehicles are believed to reduce accidents but still demand some form of external communication signaling relevant information to pedestrians. Recent research on the design of vehicle-pedestrian communication (VPC) focuses strongly on concepts for a non-disabled population. Our work presents an inclusive user-centered design for VPC, beneficial for both vision impaired and seeing pedestrians. We conducted a workshop with VIP (N=6), discussing current issues in road traffic and comparing communication concepts proposed by literature. A thematic analysis unveiled two important themes: number of communicating vehicles and content (affecting duration). Subsequently, we investigated these in a second user study in virtual reality (N=33, 8 VIP) comparing the VPC between groups of abilities. We found that trust and understanding is enhanced and cognitive load reduced when all relevant vehicles communicate; high content messages also reduce cognitive load.",accessibility; autonomous vehicles; vulnerable road users; external communication; inclusive design research,Abstract,
ACM DL,conferencePaper,2020,Haptic-go-round: A Surrounding Platform for Encounter-type Haptics in Virtual Reality Experiences,CHI - Human Factors in Computing Systems,A*,"We present Haptic-go-round, a surrounding platform that allows deploying props and devices to provide haptic feedbacks in any direction in virtual reality experiences. The key component of Haptic-go-round is a motorized turntable that rotates the correct haptic device to the right direction at the right time to match what users are about to touch. We implemented a working platform including plug-and-play prop cartridges and a software interface that allow experience designers to agilely add their haptic components and use the platform for their applications. We conducted technical experiments and two user studies on Haptic-go-round to evaluate its performance. We report the results and discuss our insights and limitations.",virtual reality; encounter-type haptic feedback; props,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,FaceHaptics: Robot Arm based Versatile Facial Haptics for Immersive Environments,CHI - Human Factors in Computing Systems,A*,"This paper introduces FaceHaptics, a novel haptic display based on a robot arm attached to a head-mounted virtual reality display. It provides localized, multi-directional and movable haptic cues in the form of wind, warmth, moving and single-point touch events and water spray to dedicated parts of the face not covered by the head-mounted display.The easily extensible system, however, can principally mount any type of compact haptic actuator or object. User study 1 showed that users appreciate the directional resolution of cues, and can judge wind direction well, especially when they move their head and wind direction is adjusted dynamically to compensate for head rotations. Study 2 showed that adding FaceHaptics cues to a VR walkthrough can significantly improve user experience, presence, and emotional responses.",emotion; virtual reality; perception; haptics; user study; presence; immersive environments; robot arm,Abstract_Keywords,
ACM DL,conferencePaper,2020,Head-Coupled Kinematic Template Matching: A Prediction Model for Ray Pointing in VR,CHI - Human Factors in Computing Systems,A*,"This paper presents a new technique to predict the ray pointer landing position for selection movements in virtual reality (VR) environments. The technique adapts and extends a prior 2D kinematic template matching method to VR environments where ray pointers are used for selection. It builds on the insight that the kinematics of a controller and Head-Mounted Display (HMD) can be used to predict the ray's final landing position and angle. An initial study provides evidence that the motion of the head is a key input channel for improving prediction models. A second study validates this technique across a continuous range of distances, angles, and target sizes. On average, the technique's predictions were within 7.3° of the true landing position when 50% of the way through the movement and within 3.4° when 90%. Furthermore, compared to a direct extension of Kinematic Template Matching, which only uses controller movement, this head-coupled approach increases prediction accuracy by a factor of 1.8x when 40% of the way through the movement.",virtual reality; target prediction; vr; endpoint prediction; kinematics; ray pointing; template matching,Abstract_Keywords,
ACM DL,conferencePaper,2020,The Effectiveness of Visual and Audio Wayfinding Guidance on Smartglasses for People with Low Vision,CHI - Human Factors in Computing Systems,A*,"Wayfinding is a critical but challenging task for people who have low vision, a visual impairment that falls short of blindness. Prior wayfinding systems for people with visual impairments focused on blind people, providing only audio and tactile feedback. Since people with low vision use their remaining vision, we sought to determine how audio feedback compares to visual feedback in a wayfinding task. We developed visual and audio wayfinding guidance on smartglasses based on de facto standard approaches for blind and sighted people and conducted a study with 16 low vision participants. We found that participants made fewer mistakes and experienced lower cognitive load with visual feedback. Moreover, participants with a full field of view completed the wayfinding tasks faster when using visual feedback. However, many participants preferred audio feedback because of its shorter learning curve. We propose design guidelines for wayfinding systems for low vision.",augmented reality; accessibility; low vision; wayfinding; visual feedback; audio feedback,Keywords,
ACM DL,conferencePaper,2020,RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots,CHI - Human Factors in Computing Systems,A*,"RoomShift is a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.",virtual reality; haptic interfaces; room-scale haptics; swarm robots,Abstract_Keywords,
ACM DL,conferencePaper,2020,Manufacturing Change: The Impact of Virtual Environments on Real Organizations,CHI - Human Factors in Computing Systems,A*,"Manufacturing workplaces are becoming sites of intense change as technologies like IoT and AR/VR are beginning to make deep inroads into how complex products are engi-neered and assembled. These categories of technologies are becoming prominent in manufacturing because they offer potential solutions to the problems of unskilled labor and workforce shortages. Technology has the potential to shift manufacturing in both large and small ways, to better un-derstand how a manufacturing organization might appropri-ate VR, we ran a study with a global aviation manufacturer headquartered the United States. To document the changing nature of work via this class of technologies we conducted a VR study which facilitated access to participant observation and interviews (n=21). Our findings provide initial insights into the organizational impact of VR on human perfor-mance augmentation and skill acquisition revealing the larger infrastructural challenges facing the adoption of con-sumer grade smart technologies in industrial workplace settings.",virtual reality; qualitative methods; field studies,Keywords,
ACM DL,conferencePaper,2020,Augmented Reality to Enable Users in Learning Case Grammar from Their Real-World Interactions,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) provides a unique opportunity to situate learning content in one's environment. In this work, we investigated how AR could be developed to provide an interactive context-based language learning experience. Specifically, we developed a novel handheld-AR app for learning case grammar by dynamically creating quizzes, based on real-life objects in the learner's surroundings. We compared this to the experience of learning with a non-contextual app that presented the same quizzes with static photographic images. Participants found AR suitable for use in their everyday lives and enjoyed the interactive experience of exploring grammatical relationships in their surroundings. Nonetheless, Bayesian tests provide substantial evidence that the interactive and context-embedded AR app did not improve case grammar skills, vocabulary retention, and usability over the experience with equivalent static images. Based on this, we propose how language learning apps could be designed to combine the benefits of contextual AR and traditional approaches.",augmented reality; language learning; self-directed learning; grammar; contextual learning,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,CollabAR  Investigating the Mediating Role of Mobile AR Interfaces on Co-Located Group Collaboration,CHI - Human Factors in Computing Systems,A*,"Mobile Augmented Reality (AR) technology is enabling new applications for different domains including architecture, education or medical work. As AR interfaces project digital data, information and models into the real world, it allows for new forms of collaborative work. However, despite the wide availability of AR applications, very little is known about how AR interfaces mediate and shape collaborative practices. This paper presents a study which examines how a mobile AR (M-AR) interface for inspecting and discovering AR models of varying complexity impacts co-located group practices. We contribute new insights into how current mobile AR interfaces impact co-located collaboration. Our results show that M-AR interfaces induce high mental load and frustration, cause a high number of context switches between devices and group discussion, and overall leads to a reduction in group interaction. We present design recommendations for future work focusing on collaborative AR interfaces.",mobile augmented reality; co-located collaboration,Abstract_Keywords,
ACM DL,conferencePaper,2020,A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing,CHI - Human Factors in Computing Systems,A*,"Supporting natural communication cues is critical for people to work together remotely and face-to-face. In this paper we present a Mixed Reality (MR) remote collaboration system that enables a local worker to share a live 3D panorama of his/her surroundings with a remote expert. The remote expert can also share task instructions back to the local worker using visual cues in addition to verbal communication. We conducted a user study to investigate how sharing augmented gaze and gesture cues from the remote expert to the local worker could affect the overall collaboration performance and user experience. We found that by combing gaze and gesture cues, our remote collaboration system could provide a significantly stronger sense of co-presence for both the local and remote users than using the gaze cue alone. The combined cues were also rated significantly higher than the gaze in terms of ease of conveying spatial actions.",augmented reality; virtual reality; mixed reality; remote collaboration; eye gaze; 3d panorama; hand gesture; scene reconstruction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Too Hot to Handle: An Evaluation of the Effect of Thermal Visual Representation on User Grasping Interaction in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Influence of interaction fidelity and rendering quality on perceived user experience have been largely explored in Virtual Reality (VR). However, differences in interaction choices triggered by these rendering cues have not yet been explored. We present a study analysing the effect of thermal visual cues and contextual information on 50 participants' approach to grasp and move a virtual mug. This study comprises 3 different temperature cues (baseline empty, hot and cold) and 4 contextual representations; all embedded in a VR scenario. We evaluate 2 different hand representations (abstract and human) to assess grasp metrics. Results show temperature cues influenced grasp location, with the mug handle being predominantly grasped with a smaller grasp aperture for the hot condition, while the body and top were preferred for baseline and cold conditions.",virtual reality; grasping metrics; hand interaction; hand tracking,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Evaluation of a Financial Portfolio Visualization using Computer Displays and Mixed Reality Devices with Domain Experts,CHI - Human Factors in Computing Systems,A*,"With the advent of mixed reality devices such as the Microsoft HoloLens, developers have been faced with the challenge to utilize the third dimension in information visualization effectively. Research on stereoscopic devices has shown that three-dimensional representation can improve accuracy in specific tasks (e.g., network visualization). Yet, so far the field has remained mute on the underlying mechanism. Our study systematically investigates the differences in user perception between a regular monitor and a mixed reality device. In a real-life within-subject experiment in the field with twenty-eight investment bankers, we assessed subjective and objective task performance with two- and three-dimensional systems, respectively. We tested accuracy with regard to position, size, and color using single and combined tasks. Our results do not show a significant difference in accuracy between mixed-reality and standard 2D monitor visualizations.",hololens; information visualization; user study; mixed reality displays; UX study,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Virtually-Extended Proprioception: Providing Spatial Reference in VR through an Appended Virtual Limb,CHI - Human Factors in Computing Systems,A*,"Selecting targets directly in the virtual world is difficult due to the lack of haptic feedback and inaccurate estimation of egocentric distances. Proprioception, the sense of self-movement and body position, can be utilized to improve virtual target selection by placing targets on or around one's body. However, its effective scope is limited closely around one's body. We explore the concept of virtually-extended proprioception by appending virtual body parts mimicking real body parts to users' avatars, to provide spatial reference to virtual targets. Our studies suggest that our approach facilitates more efficient target selection in VR as compared to no reference or using an everyday object as reference. Besides, by cultivating users' sense of ownership on the appended virtual body part, we can further enhance target selection performance. The effects of transparency and granularity of the virtual body part on target selection performance are also discussed.",virtual reality; target selection; appended limb; proprioception; spatial reference,Keywords,
ACM DL,conferencePaper,2020,Augmented Reality for Older Adults: Exploring Acceptability of Virtual Coaches for Home-based Balance Training in an Aging Population,CHI - Human Factors in Computing Systems,A*,"Balance training has been shown to be effective in reducing risks of falling, which is a major concern for older adults. Usually, exercise programs are individually prescribed and monitored by physiotherapeutic or medical experts. Unfortunately, supervision and motivation of older adults during home-based exercises cannot be provided on a large scale, in particular, considering an ageing population. Augmented reality (AR) in combination with virtual coaches could provide a reasonable solution to this challenge.We present a first investigation of the acceptance of an AR coaching system for balance training, which can be performed at home. In a human-centered design approach we developed several mock-ups and prototypes, and evaluated them with 76 older adults. The results suggest that older adults find the system encouraging and stimulating. The virtual coach is perceived as an alive, calm, intelligent, and friendly human. However, usability of the entire AR system showed a significant negative correlation with participants' age.",augmented reality; older adults; balance training; health and well-being,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Walking by Cycling: A Novel In-Place Locomotion User Interface for Seated Virtual Reality Experiences,CHI - Human Factors in Computing Systems,A*,"We introduce VR Strider, a novel locomotion user interface (LUI) for seated virtual reality (VR) experiences, which maps cycling biomechanics of the user's legs to virtual walking movements. The core idea is to translate the motion of pedaling on a mini exercise bike to a corresponding walking animation of a virtual avatar while providing audio-based tactile feedback on virtual ground contacts. We conducted an experiment to evaluate the LUI and our novel anchor-turning rotation control method regarding task performance, spatial cognition, VR sickness, sense of presence, usability and comfort in a path-integration task. The results show that VR Strider has a significant positive effect on the participants' angular and distance estimation, sense of presence and feeling of comfort compared to other established locomotion techniques, such as teleportation and joystick-based navigation. A confirmatory study further indicates the necessity of synchronized avatar animations for virtual vehicles that rely on pedalling.",virtual reality; embodied interaction; locomotion; input techniques,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Reflexive VR Storytelling Design Beyond Immersion: Facilitating Self-Reflection on Death and Loneliness,CHI - Human Factors in Computing Systems,A*,"This research examines the reflexive dimensions of cinematic virtual reality (CVR) storytelling. We created Anonymous, an interactive CVR piece that employs a reflexive storytelling method. This method is based on distancing effects and is used to elicit audience awareness and self-reflection about loneliness and death. To understand the audience's experiences, we conducted in-depth interviews to study which design factors and elements prompted reflexive thoughts and feelings. Our findings highlight how the audience experience was impacted by four reflexive dimensions: abstract and minimal aesthetics, everyday materials and textures, the restriction of control, and multiple, disembodied points of view. We use our findings to discuss how these dimensions can inform the design of VR storytelling experiences that provoke self and social reflection.",virtual reality; reflexivity; alienation; cinematic vr; distancing effect; estrangement; immersive storytelling,Abstract_Keywords,
ACM DL,conferencePaper,2020,Understanding Viewport- and World-based Pointing with Everyday Smart Devices in Immersive Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Personal smart devices have demonstrated a variety of efficient techniques for pointing and selecting on physical displays. However, when migrating these input techniques to augmented reality, it is both unclear what the relative performance of different techniques will be given the immersive nature of the environment, and it is unclear how viewport-based versus world-based pointing methods will impact performance. To better understand the impact of device and viewing perspectives on pointing in augmented reality, we present the results of two controlled experiments comparing pointing conditions that leverage various smartphone- and smartwatch-based external display pointing techniques and examine viewport-based versus world-based target acquisition paradigms. Our results demonstrate that viewport-based techniques offer faster selection and that both smartwatch- and smartphone-based pointing techniques represent high-performance options for performing distant target acquisition tasks in augmented reality.",augmented reality; mobile devices; virtual reality; input devices; 3D pointing,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Neo-Noumena: Augmenting Emotion Communication,CHI - Human Factors in Computing Systems,A*,"The subjective experience of emotion is notoriously difficult to interpersonally communicate. We believe that technology can challenge this notion through the design of neuroresponsive systems for interpersonal communication. We explore this through ""Neo-Noumena"", a communicative neuroresponsive system that uses brain-computer interfacing and artificial intelligence to read one's emotional states and dynamically represent them to others in mixed reality through two head-mounted displays. In our study five participant pairs were given Neo-Noumena for three days, using the system freely. Measures of emotional competence demonstrated a statistically significant increase in participants' ability to interpersonally regulate emotions. Furthermore, participant interviews revealed themes regarding Spatiotemporal Actualization, Objective Representation, and Preternatural Transmission. We also suggest design strategies for future augmented emotion communication systems. We intend that work gives guidance towards a future in which our ability to interpersonally communicate emotion is augmented beyond traditional experience.",machine learning; mixed reality; emotion recognition; emotion communication; brain-computer interfacing; eeg,Abstract_Keywords,
ACM DL,conferencePaper,2020,Closer Object Looks Smaller: Investigating the Duality of Size Perception in a Spherical Fish Tank VR Display,CHI - Human Factors in Computing Systems,A*,"Fish Tank Virtual Reality (FTVR) displays provide compelling 3D experiences by rendering view-dependent imagery on a 2D screen. While users perceive a 3D object in space, they are actually looking at pixels on a 2D screen, thus, a perceptual duality exists between the object's pixels and the 3D percept potentially interfering with the experience. To investigate, we conducted an experiment to see whether the on-screen size of the 2D imagery affects the perceived object size in 3D space with different viewing conditions, including stereopsis. We found that the size of on-screen imagery significantly influenced object size perception, causing 83.3% under/overestimation of perceived size when viewing without stereopsis and reducing to 64.7% with stereopsis. Contrary to reality, objects look smaller when the viewer gets closer. Understanding the perceptual duality helps us to provide accurate perception of real-world objects depicted in the virtual environment and pave the way for 3D applications.",3d perception; fish tank virtual reality; spherical display,Abstract_Keywords,
ACM DL,conferencePaper,2020,Who Put That There? Temporal Navigation of Spatial Recordings by Direct Manipulation,CHI - Human Factors in Computing Systems,A*,"Spatial recordings allow viewers to move within them and freely choose their viewpoint. However, such recordings make it easy to miss events and difficult to follow moving objects when skipping through the recording. To alleviate these problems we present the Who Put That There system that allows users to navigate through time by directly manipulating objects in the scene. By selecting an object, the user can navigate to moments where the object changed. Users can also view trajectories of objects that changed location and directly manipulate them to navigate. We evaluated the system with a set of sensemaking questions in a think-aloud study. Participants understood the system and found it useful for finding events of interest, while being present and engaged in the recording.",virtual reality; navigation techniques; spatial recordings; temporal navigation,Keywords,
ACM DL,conferencePaper,2020,"""How do I make this thing smile?"": An Inventory of Expressive Nonverbal Communication in Commercial Social Virtual Reality Platforms",CHI - Human Factors in Computing Systems,A*,"Despite the proliferation of platforms for social Virtual Reality (VR) communicating emotional expression via an avatar remains a significant design challenge. In order to better understand the design space for expressive Nonverbal Communication (NVC) in social VR we undertook an inventory of the ten most prominent social VR platforms. Our inventory identifies the dominant design strategies for movement, facial control, and gesture in commercial VR applications, and identifies opportunities and challenges for future design and research into social expression in VR. Specifically, we highlight the paucity of interaction paradigms for facial expression and the near nonexistence of meaningful control over ambient aspects of nonverbal communication such as posture, pose, and social status.",virtual reality; nonverbal communication; social interactions,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,"Embodied Axes: Tangible, Actuated Interaction for 3D Augmented Reality Data Spaces",CHI - Human Factors in Computing Systems,A*,"We present Embodied Axes, a controller which supports selection operations for 3D imagery and data visualisations in Augmented Reality. The device is an embodied representation of a 3D data space – each of its three orthogonal arms corresponds to a data axis or domain specific frame of reference. Each axis is composed of a pair of tangible, actuated range sliders for precise data selection, and rotary encoding knobs for additional parameter tuning or menu navigation. The motor actuated sliders support alignment to positions of significant values within the data, or coordination with other input: e.g., mid-air gestures in the data space, touch gestures on the surface below the data, or another Embodied Axes device supporting multi-user scenarios. We conducted expert enquiries in medical imaging which provided formative feedback on domain tasks and refinements to the design. Additionally, a controlled user study was performed and found that the Embodied Axes was overall more accurate than conventional tracked controllers for selection tasks.",augmented reality; tangible interaction; actuation; 3d visualisation; device,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,ARchitect: Building Interactive Virtual Experiences from Physical Affordances by Bringing Human-in-the-Loop,CHI - Human Factors in Computing Systems,A*,"Automatic generation of Virtual Reality (VR) worlds which adapt to physical environments have been proposed to enable safe walking in VR. However, such techniques mainly focus on the avoidance of physical objects as obstacles and overlook their interaction affordances as passive haptics. Current VR experiences involving interaction with physical objects in surroundings still require verbal instruction from an assisting partner. We present ARchitect, a proof-of-concept prototype that allows flexible customization of a VR experience with human-in-the-loop. ARchitect brings in an assistant to map physical objects to virtual proxies of matching affordances using Augmented Reality (AR). In a within-subjects study (9 user pairs) comparing ARchitect to a baseline condition, assistants and players experienced decreased workload and players showed increased VR presence and trust in the assistant. Finally, we defined design guidelines of ARchitect for future designers and implemented three demonstrative experiences.",virtual reality; passive haptics; affordance; architect; asymmetric,Abstract_Keywords,
ACM DL,conferencePaper,2020,Body-Penetrating Tactile Phantom Sensations,CHI - Human Factors in Computing Systems,A*,"In tactile interaction, a phantom sensation refers to an illusion felt on the skin between two distant points at which vibrations are applied. It can improve the perceptual spatial resolution of tactile stimulation with a few tactors. All phantom sensations reported in the literature act on the skin or out of the body, but no such reports exist for those eliciting sensations penetrating the body. This paper addresses tactile phantom sensations in which two vibration actuators on the dorsal and palmar sides of the hand present an illusion of vibration passing through the hand. We also demonstrate similar tactile illusions for the torso. For optimal design, we conducted user studies while varying vibration frequency, envelope function, stimulus duration, and penetrating direction. Based on the results, we present design guidelines on penetrating phantom sensations for its use in immersive virtual reality applications.",vibrotactile feedback; phantom sensation; penetrating tactile sensation; tactile illusion,Abstract,
ACM DL,conferencePaper,2020,Podoportation: Foot-Based Locomotion in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) allows for infinitely large environments. However, the physical traversable space is always limited by real-world boundaries. This discrepancy between physical and virtual dimensions renders traditional locomotion methods used in real world unfeasible. To alleviate these limitations, research proposed various artificial locomotion concepts such as teleportation, treadmills, and redirected walking. However, these concepts occupy the user's hands, require complex hardware or large physical spaces. In this paper, we contribute nine VR locomotion concepts for foot-based locomotion, relying on the 3D position of the user's feet and the pressure applied to the sole as input modalities. We evaluate our concepts and compare them to state-of-the-art point &amp; teleport technique in a controlled experiment with 20 participants. The results confirm the viability of our approaches for foot-based and engaging locomotion. Further, based on the findings, we contribute a wireless hardware prototype implementation.",virtual reality; locomotion; foot-based input,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,VRSketchIn: Exploring the Design Space of Pen and Tablet Interaction for 3D Sketching in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Sketching in virtual reality (VR) enhances perception and understanding of 3D volumes, but is currently a challenging task, as spatial input devices (e.g., tracked controllers) do not provide any scaffolding or constraints for mid-air interaction. We present VRSketchIn, a VR sketching application using a 6DoF-tracked pen and a 6DoF-tracked tablet as input devices, combining unconstrained 3D mid-air with constrained 2D surface-based sketching. To explore what possibilities arise from this combination of 2D (pen on tablet) and 3D input (6DoF pen), we present a set of design dimensions and define the design space for 2D and 3D sketching interaction metaphors in VR. We categorize prior art inside our design space and implemented a subset of metaphors for pen and tablet sketching in our prototype. To gain a deeper understanding which specific sketching operations users perform with 2D and which with 3D metaphors, we present findings of usability walkthroughs with six participants.",virtual reality; design space; sketching; interaction metaphors; mid-air painting; pen and tablet,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Performance and Experience of Throwing in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Throwing is a fundamental movement in many sports and games. Given this, accurate throwing in VR applications today is surprisingly difficult. In this paper we explore the nature of the difficulties of throwing in VR in more detail. We present the results of a user study comparing throwing in VR and in the physical world. In a short pre-study with 3 participants we determine an optimal number of throwing repetitions for the main study by exploring the learning curve and subjective fatigue of throwing in VR. In the main study, with 12 participants, we find that throwing precision and accuracy in VR are lower particularly in the distance and height dimensions. It also requires more effort and exhibits different kinematic patterns.",virtual reality; user study; throwing,Keywords_Title,
ACM DL,conferencePaper,2020,"Again, Together: Socially Reliving Virtual Reality Experiences When Separated",CHI - Human Factors in Computing Systems,A*,"To share a virtual reality (VR) experience remotely together, users usually record videos from an individual's point of view and then co-watch these videos. However, co-watching recorded videos limits users to reliving their memories from the perspective from which the video was captured. In this paper, we describe ReliveInVR, a new time-machine-like VR experience sharing method. ReliveInVR allows multiple users to immerse themselves in the relived experience together and independently view the experience from any perspective. We conducted a 1x3 within-subject study with 26 dyads to compare ReliveInVR with (1) co-watching 360-degree videos on desktop, and (2) co-watching 360-degree videos in VR. Our results suggest that participants reported higher levels of immersion and social presence in ReliveInVR. Participants in ReliveInVR also understood the shared experience better, discovered unnoticed things together and found the sharing experience more fulfilling. We discuss the design implications for sharing VR experiences over time and space.",virtual reality; immersion; presence; social; replay; shared experience,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Isness: Using Multi-Person VR to Design Peak Mystical Type Experiences Comparable to Psychedelics,CHI - Human Factors in Computing Systems,A*,"Studies combining psychotherapy with psychedelic drugs (Ds) have demonstrated positive outcomes that are often associated with 'Ds' ability to induce 'mystical-type' experiences (MTEs) i.e., subjective experiences whose characteristics include a sense of connectedness, transcendence, and ineffability. We suggest that both PsiDs and virtual reality can be situated on a broader spectrum of psychedelic technologies. To test this hypothesis, we used concepts, methods, and analysis strategies from D research to design and evaluate 'Isness', a multi-person VR journey where participants experience the collective emergence, fluctuation, and dissipation of their bodies as energetic essences. A study (N=57) analyzing participant responses to a commonly used D experience questionnaire (MEQ30) indicates that Isness participants reported MTEs comparable to those reported in double-blind clinical studies after high doses of psilocybin and LSD. Within a supportive setting and conceptual framework, VR phenomenology can create the conditions for MTEs from which participants derive insight and meaning.",user experience; virtual reality; altered states; meaning in HCI; mystical-type experiences; psychedelic drugs,Abstract_Keywords,
ACM DL,conferencePaper,2020,"""In VR, everything is possible!"": Sketching and Simulating Spatially-Aware Interactive Spaces in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"We propose using virtual reality (VR) as a design tool for sketching and simulating spatially-aware interactive spaces. Using VR, designers can quickly experience their envisioned spaces and interactions by simulating technologies such as motion tracking, multiple networked devices, or unusual form factors such as spherical touchscreens or bezel-less display tiles. Design ideas can be rapidly iterated without restrictions by the number, size, or shape and availability of devices or sensors in the lab. To understand the potentials and challenges of designing in VR, we conducted a user study with 12 interaction designers. As their tool, they used a custom-built virtual design environment with finger tracking and physics simulations for natural interactions with virtual devices and objects. Our study identified the designers' experience of space in relation to their own bodies and playful design explorations as key opportunities. Key challenges were the complexities of building a usable yet versatile VR-based ""World Editor"".",interaction design; virtual reality; sketching; simulation; prototyping; spatial awareness; design tools; interactive spaces,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Towards an Understanding of Augmented Reality Extensions for Existing 3D Data Analysis Tools,CHI - Human Factors in Computing Systems,A*,"We present an observational study with domain experts to understand how augmented reality (AR) extensions to traditional PC-based data analysis tools can help particle physicists to explore and understand 3D data. Our goal is to allow researchers to integrate stereoscopic AR-based visual representations and interaction techniques into their tools, and thus ultimately to increase the adoption of modern immersive analytics techniques in existing data analysis workflows. We use Microsoft's HoloLens as a lightweight and easily maintainable AR headset and replicate existing visualization and interaction capabilities on both the PC and the AR view. We treat the AR headset as a second yet stereoscopic screen, allowing researchers to study their data in a connected multi-view manner. Our results indicate that our collaborating physicists appreciate a hybrid data exploration setup with an interactive AR extension to improve their understanding of particle collision events.",user interface; immersive analytics; 3D visualization; hybrid visualization system,Abstract_Title,
ACM DL,conferencePaper,2020,Me vs. Super(wo)man: Effects of Customization and Identification in a VR Exergame,CHI - Human Factors in Computing Systems,A*,"Customised avatars are a powerful tool to increase identification, engagement and intrinsic motivation in digital games. We investigated the effects of customisation in a self-competitive VR exergame by modelling players and their previous performance in the game with customised avatars. In a first study we found that, similar to non-exertion games, customisation significantly increased identification and intrinsic motivation, as well as physical performance in the exergame. In a second study we identified a more complex relationship with the customisation style: idealised avatars increased wishful identification but decreased exergame performance compared to realistic avatars. In a third study, we found that 'enhancing' realistic avatars with idealised characteristics increased wishful identification, but did not have any adverse effects. We discuss the findings based on feedforward and self-determination theory, proposing notions of intrinsic identification (fostering a sense of self) and extrinsic identification (drawing away from the self) to explain the results.",virtual reality; identification; avatar customisation; exergaming; feedforward,Keywords,
ACM DL,conferencePaper,2020,RestoreVR: Generating Embodied Knowledge and Situated Experience of Dunhuang Mural Conservation via Interactive Virtual Reality,CHI - Human Factors in Computing Systems,A*,"In Dunhuang Mogao Grottoes, unique Buddhist murals of ancient China are preserved. Unfortunately, the exquisite murals are suffering from degradation. Experts have been trying to enhance public's awareness of mural protection, but there's no efficacious means to attract interest and popularize knowledge yet. In this paper, we propose RestoreVR, an interactive virtual reality (VR) system engaging users to experience Dunhuang mural restoration in a digital tour in the cave. Based on an online survey with the public and in-depth interviews with five Dunhuang experts, we derive a set of design requirements for generating embodied knowledge and situated experience in VR to bridge the gap between highly specialized experts and general audiences. Accordingly, we design RestoreVR and conduct a between-subjects user study to compare our system with traditional methods. The results suggest that RestoreVR significantly improves user experience and awareness of CH protection over existing methods.",survey; cultural heritage; dunhuang; implementation; interactive virtual reality; mural conservation,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Assessing 2D and 3D Heatmaps for Comparative Analysis: An Empirical Study,CHI - Human Factors in Computing Systems,A*,"Heatmaps are a popular visualization technique that encode 2D density distributions using color or brightness. Experimental studies have shown though that both of these visual variables are inaccurate when reading and comparing numeric data values. A potential remedy might be to use 3D heatmaps by introducing height as a third dimension to encode the data. Encoding abstract data in 3D, however, poses many problems, too. To better understand this tradeoff, we conducted an empirical study (N=48) to evaluate the user performance of 2D and 3D heatmaps for comparative analysis tasks. We test our conditions on a conventional 2D screen, but also in a virtual reality environment to allow for real stereoscopic vision. Our main results show that 3D heatmaps are superior in terms of error rate when reading and comparing single data items. However, for overview tasks, the well-established 2D heatmap performs better.",virtual reality; visual analytics; heatmaps,Abstract_Keywords,
ACM DL,conferencePaper,2020,Improving Virtual Reality Ergonomics Through Reach-Bounded Non-Linear Input Amplification,CHI - Human Factors in Computing Systems,A*,"Input amplification enables easier movement in virtual reality (VR) for users with mobility issues or in confined spaces. However, current techniques either do not focus on maintaining feelings of body ownership, or are not applicable to general VR tasks. We investigate a general purpose non-linear transfer function that keeps the user's reach within reasonable bounds to maintain body ownership. The technique amplifies smaller movements from a user-definable neutral point into the expected larger movements using a configurable Hermite curve. Two experiments evaluate the approach. The first establishes that the technique has comparable performance to the state-of-the-art, increasing physical comfort while maintaining task performance and body ownership. The second explores the characteristics of the technique over a wide range of amplification levels. Using the combined results, design and implementation recommendations are provided with potential applications to related VR transfer functions.",ergonomics; interaction techniques; input re-mapping,Abstract_Title,
ACM DL,conferencePaper,2020,An Exploratory Study of Augmented Reality Presence for Tutoring Machine Tasks,CHI - Human Factors in Computing Systems,A*,"Machine tasks in workshops or factories are often a compound sequence of local, spatial, and body-coordinated human-machine interactions. Prior works have shown the merits of video-based and augmented reality (AR) tutoring systems for local tasks. However, due to the lack of a bodily representation of the tutor, they are not as effective for spatial and body-coordinated interactions. We propose avatars as an additional tutor representation to the existing AR instructions. In order to understand the design space of tutoring presence for machine tasks, we conduct a comparative study with 32 users. We aim to explore the strengths/limitations of the following four tutor options: video, non-avatar-AR, half-body+AR, and full-body+AR. The results show that users prefer the half-body+AR overall, especially for the spatial interactions. They have a preference for the full-body+AR for the body-coordinated interactions and the non-avatar-AR for the local interactions. We further discuss and summarize design recommendations and insights for future machine task tutoring systems.",augmented reality; avatar tutor; exploratory study; machine task; tutoring system design,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Get a Grip: Evaluating Grip Gestures for VR Input using a Lightweight Pen,CHI - Human Factors in Computing Systems,A*,"The use of Virtual Reality (VR) in applications such as data analysis, artistic creation, and clinical settings requires high precision input. However, the current design of handheld controllers, where wrist rotation is the primary input approach, does not exploit the human fingers' capability for dexterous movements for high precision pointing and selection. To address this issue, we investigated the characteristics and potential of using a pen as a VR input device. We conducted two studies. The first examined which pen grip allowed the largest range of motion—we found a tripod grip at the rear end of the shaft met this criterion. The second study investigated target selection via 'poking' and ray-casting, where we found the pen grip outperformed the traditional wrist-based input in both cases. Finally, we demonstrate potential applications enabled by VR pen input and grip postures.",virtual reality; pen input; finger and wrist dexterity; grip postures; handheld controller; spatial target selection,Abstract_Keywords,
ACM DL,conferencePaper,2020,A Skin-Stroke Display on the Eye-Ring Through Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"We present the Skin-Stroke Display, a system mounted on the lens inside the head-mounted display, which exerts subtle yet recognizable tactile feedback on the eye-ring using a motorized air jet. To inform our design of noticeable air-jet haptic feedback, we conducted a user study to identify absolute detection thresholds. Our results show that the tactile sensation had different sensitivity around the eyes, and we determined a standard intensity (8 mbar) to prevent turbulent airflow blowing into the eyes. In the second study, we asked participants to adjust the intensity around the eye for equal sensation based on standard intensity. Next, we investigated the recognition of point and stroke stimuli with or without inducing cognitive load on eight directions on the eye-ring. Our longStroke stimulus can achieve an accuracy of 82.6% without cognitive load and 80.6% with cognitive load simulated by the Stroop test. Finally, we demonstrate example applications using the skin-stroke display as the off-screen indicator, tactile I/O progress display, and tactile display.",virtual reality; haptics; head-mounted display; air jet; eye-ring; skin-stroke display,Keywords,
ACM DL,conferencePaper,2020,Acoustic Transparency and the Changing Soundscape of Auditory Mixed Reality,CHI - Human Factors in Computing Systems,A*,"Auditory headsets capable of actively or passively intermixing both real and virtual sounds are in-part acoustically transparent. This paper explores the consequences of acoustic transparency, both on the perception of virtual audio content, given the presence of a real-world auditory backdrop, and more broadly in facilitating a wearable, personal, private, always-available soundspace. We experimentally compare passive acoustically transparent, and active noise cancelling, orientation-tracked auditory headsets across a range of content types, both indoors and outdoors for validity. Our results show differences in terms of presence, realness and externalization for select content types. Via interviews and a survey, we discuss attitudes toward acoustic transparency (e.g. being perceived as safer), the potential shifts in audio usage that might be precipitated by adoption, and reflect on how such headsets and experiences fit within the area of Mixed Reality.",mixed reality; audio; acoustic transparency,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Evaluation of Machine Learning Techniques for Hand Pose Estimation on Handheld Device with Proximity Sensor,CHI - Human Factors in Computing Systems,A*,"Tracking finger movement for natural interaction using hand is commonly studied. For vision-based implementations of finger tracking in virtual reality (VR) application, finger movement is occluded by a handheld device which is necessary for auxiliary input, thus tracking finger movement using cameras is still challenging. Finger tracking controllers using capacitive proximity sensors on the surface are starting to appear. However, research on estimating articulated hand pose from curved capacitance sensing electrodes is still immature. Therefore, we built a prototype with 62 electrodes and recorded training datasets using an optical tracking system. We have introduced 2.5D representation to apply convolutional neural network methods on a capacitive image of the curved surface, and two types of network architectures based on recent achievements in the computer vision field were evaluated with our dataset. We also implemented real-time interactive applications using the prototype and demonstrated the possibility of intuitive interaction using fingers in VR applications.",virtual reality; hand pose estimation; capacitive image; finger tracking controller; human computer interactiton,Abstract_Keywords,
ACM DL,conferencePaper,2020,Touché: Data-Driven Interactive Sword Fighting in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"VR games offer new freedom for players to interact naturally using motion. This makes it harder to design games that react to player motions convincingly. We present a framework for VR sword fighting experiences against a virtual character that simplifies the necessary technical work to achieve a convincing simulation. The framework facilitates VR design by abstracting from difficult details on the lower ""physical"" level of interaction, using data-driven models to automate both the identification of user actions and the synthesis of character animations. Designers are able to specify the character's behaviour on a higher ""semantic"" level using parameterised building blocks, which allow for control over the experience while minimising manual development work. We conducted a technical evaluation, a questionnaire study and an interactive user study. Our results suggest that the framework produces more realistic and engaging interactions than simple hand-crafted interaction logic, while supporting a controllable and understandable behaviour design.",machine learning; virtual reality; gesture recognition; animation; sword fighting,Keywords_Title,
ACM DL,conferencePaper,2020,A Human Touch: Social Touch Increases the Perceived Human-likeness of Agents in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual Reality experiences and games present believable virtual environments based on graphical quality, spatial audio, and interactivity. The interaction with in-game characters, controlled by computers (agents) or humans (avatars), is an important part of VR experiences. Pre-captured motion sequences increase the visual humanoid resemblance. However, this still precludes realistic social interactions (eye contact, imitation of body language), particularly for agents. We aim to make social interaction more realistic via social touch. Social touch is non-verbal, conveys feelings and signals (coexistence, closure, intimacy). In our research, we created an artificial hand to apply social touch in a repeatable and controlled fashion to investigate its effect on the perceived human-likeness of avatars and agents. Our results show that social touch is effective to further blur the boundary between computer- and human-controlled virtual characters and contributes to experiences that closely resemble human-to-human interactions.",virtual reality; agency; human-likeness; social touch,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Augmented Reality Training for Industrial Assembly Work - Are Projection-based AR Assistive Systems an Appropriate Tool for Assembly Training?,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) systems are on their way to industrial application, e.g. projection-based AR is used to enhance assembly work. Previous studies showed advantages of the systems in permanent-use scenarios, such as faster assembly times. In this paper, we investigate whether such systems are suitable for training purposes. Within an experiment, we observed the training with a projection-based AR system over multiple sessions and compared it with a personal training and a paper manual training. Our study shows that projection-based AR systems offer only small benefits in the training scenario. While a systematic mislearning of content is prevented through immediate feedback, our results show that the AR training does not reach the personal training in terms of speed and recall precision after 24 hours. Furthermore, we show that once an assembly task is properly trained, there are no differences in the long-term recall precision, regardless of the training method.",empirical study; training; experiment; assistive system; assembly; industrial augmented reality; projection-based augmented reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,"Creating Augmented and Virtual Reality Applications: Current Practices, Challenges, and Opportunities",CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) and Virtual Reality (VR) devices are becoming easier to access and use, but the barrier to entry for creating AR/VR applications remains high. Although the recent spike in HCI research on novel AR/VR tools is promising, we lack insights into how AR/VR creators use today's state-of-the-art authoring tools as well as the types of challenges that they face. We interviewed 21 AR/VR creators, which we grouped into hobbyists, domain experts, and professional designers. Despite having a variety of motivations and skillsets, they described similar challenges in designing and building AR/VR applications. We synthesize 8 key barriers that AR/VR creators face nowadays, starting from prototyping the initial experiences to dealing with ""the many unknowns"" during implementation, to facing difficulties in testing applications. Based on our analysis, we discuss the importance of considering end-user developers as a growing population of AR/VR creators, how we can build learning opportunities into AR/VR tools, and the need for building AR/VR toolchains that integrate debugging and testing.",augmented reality; virtual reality; AR/VR authoring; AR/VR design; AR/VR development; end-user development,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,"""Transport Me Away"": Fostering Flow in Open Offices through Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Open offices are cost-effective and continue to be popular. However, research shows that these environments, brimming with distractions and sensory overload, frequently hamper productivity. Our research investigates the use of virtual reality (VR) to mitigate distractions in an open office setting and improve one's ability to be in flow. In a lab study, 35 participants performed visual programming tasks in four combinations of physical (open or closed office) and virtual environments (beach or virtual office). While participants both preferred and were in flow more in a closed office without VR, in an open office, the VR environments outperformed the no VR condition in all measures of flow, performance, and preference. Especially considering the recent rapid advancements in VR, our findings illustrate the potential VR has to improve flow and satisfaction in open offices.",work; virtual reality; flow; open offices,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Getting out of Out of Sight: Evaluation of AR Mechanisms for Awareness and Orientation Support in Occluded Multi-Room Settings,CHI - Human Factors in Computing Systems,A*,"Augmented Reality can provide orientation and awareness in situations in which objects or people are occluded by physical structures. This is relevant for many situations in the workplace, where objects are scattered across rooms and people are out of sight. While several AR mechanisms have been proposed to provide awareness and orientation in these situations, little is known about their effect on people's performance when searching objects and coordinating with each other. In this paper, we compare three AR based mechanisms (map, x-ray, compass) according to their utility, usability, social presence, task load and users' preferences. 48 participants had to work together in groups of four to find people and objects located around different rooms. Results show that map and x-ray performed best but provided least social presence among participants. We discuss these and other observations as well as potential impacts on designing AR awareness and orientation support.",cooperation; awareness; coordination; social presence; occlusion; ar; orientation support,Abstract,
ACM DL,conferencePaper,2020,"The Curious Case of the Transdiegetic Cow, or a Mission to Foster Other-Oriented Empathy Through Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Socially aware persuasive games that use immersive technologies often appeal to empathy, prompting users to feel and understand the struggles of another. However, the often sought-after standing in another's shoes' experience, in which users virtually inhabit another in distress, may complicate other-oriented empathy. Following a Research through Design approach, we designed for other-oriented empathy - focusing on a partaker-perspective and diegetic reflection - which resulted in Permanent; a virtual reality game designed to foster empathy towards evacuees from the 2011 Fukushima Daiichi nuclear disaster. We deployed Permanent 'in the wild' and carried out a qualitative study with 78 participants in the Netherlands and Japan to capture user experiences. Content Analysis of the data showed a predominance of other-oriented empathy across countries, and in our Thematic Analysis, we identified the themes of 'Spatial, Other, and Self -Awareness', 'Personal Accounts', 'Ambivalence', and 'Transdiegetic Items', resulting in design insights for fostering other-oriented empathy through virtual reality.",empathy; virtual reality; game; interactive narrative,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Investigating Roleplaying and Identity Transformation in a Virtual Reality Narrative Experience,CHI - Human Factors in Computing Systems,A*,"In this paper we describe the design and evaluation of The Next Fairy Tale (TNFT) VR, a theatrical interactive storytelling system created in virtual reality and informed by performing arts theories. TNFT was designed to produce opportunities for interactors to experience role-taking and character identification using design principles drawn from actor training and theatrical performance. We report the results of a pilot qualitative study of interactors using TNFT to explore the elements of the design that supported or hindered roleplaying behavior. We identify four design patterns that supported roleplaying in the system: (1) using explicit roles to set player expectations, (2) embracing the ""mask and the mirror"" effect, (3) attending to visual and interactional details, and (4) easing the player gently into the roleplaying experience. These patterns speak to a broader need to support roleplay through explicit scaffolding of desired player behaviors in digital narrative experiences.",virtual reality; narrative; interactive performance; drama; interactive digital storytelling; roleplaying,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Toward Immersive Self-Driving Simulations: Reports from a User Study across Six Platforms,CHI - Human Factors in Computing Systems,A*,"As self-driving car technology matures, autonomous vehicle research is moving toward building more human-centric interfaces and accountable experiences. Driving simulators avoid many ethical and regulatory concerns about self-driving cars and play a key role in testing new interfaces or autonomous driving scenarios. However, apart from validity studies for manual driving simulation, the capabilities of driving simulators in replicating the experience of self-driving cars have not been widely investigated. In this paper, we build six self-driving simulation platforms with varying levels of visual and motion fidelities ranging from a screen-based in-lab simulator to the mixed-reality on-road simulator we propose. We compare the sense of presence and simulator sickness for each simulator composition, as well as its visual and motion fidelities with a user study. Our novel mixed-reality automotive driving simulator, named MAXIM, showed highest fidelity and presence. Our findings suggest how visual and motion configurations affect experience in autonomous driving simulators.",mixed reality; user studies; autonomous driving; immersive technology; driving simulator; on-road simulation,Keywords,
ACM DL,conferencePaper,2020,Would you do it?: Enacting Moral Dilemmas in Virtual Reality for Understanding Ethical Decision-Making,CHI - Human Factors in Computing Systems,A*,"A moral dilemma is a decision-making paradox without unambiguously acceptable or preferable options. This paper investigates if and how the virtual enactment of two renowned moral dilemmas—the Trolley and the Mad Bomber—influence decision-making when compared with mentally visualizing such situations. We conducted two user studies with two gender-balanced samples of 60 participants in total that compared between paper-based and virtual-reality (VR) conditions, while simulating 5 distinct scenarios for the Trolley dilemma, and 4 storyline scenarios for the Mad Bomber's dilemma. Our findings suggest that the VR enactment of moral dilemmas further fosters utilitarian decision-making, while it amplifies biases such as sparing juveniles and seeking retribution. Ultimately, we theorize that the VR enactment of renowned moral dilemmas can yield ecologically-valid data for training future Artificial Intelligence (AI) systems on ethical decision-making, and we elicit early design principles for the training of such systems.",VR; decision-making; ethics; ethical AI; moral dilemmas,Title,
ACM DL,conferencePaper,2020,StoryMakAR: Bringing Stories to Life With An Augmented Reality &amp; Physical Prototyping Toolkit for Youth,CHI - Human Factors in Computing Systems,A*,"Makerspaces can support educational experiences in prototyping for children. Storytelling platforms enable high levels of creativity and expression, but have high barriers of entry. We introduce StoryMakAR, which combines making and storytelling. StoryMakAR is a new AR-IoT system for children that uses block programming, physical prototyping, and event-based storytelling to bring stories to life. We reduce the barriers to entry for youth (Age=14-18) by designing an accessible, plug-and-play system through merging both electro-mechanical devices and virtual characters to create stories. We describe our initial design process, the evolution and workflow of StoryMakAR, and results from multiple single-session workshops with 33 high school students. Our preliminary studies led us to understand what students want to make. We provide evidence of how students both engage and have difficulties with maker-based storytelling. We also discuss the potential for StoryMakAR to be used as a learning environment for classrooms and younger students.",augmented reality; children; storytelling; maker culture,Keywords_Title,
ACM DL,conferencePaper,2020,Body Follows Eye: Unobtrusive Posture Manipulation Through a Dynamic Content Position in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"While virtual objects are likely to be a part of future interfaces, we lack knowledge of how the dynamic position of virtual objects influences users' posture. In this study, we investigated users' posture change following the unobtrusive and swift motions of a content window in virtual reality (VR). In two perception studies, we estimated the perception threshold on undetectable slow motions and displacement during an eye blink. In a formative study, we compared users' performance, posture change as well as subjective responses on unobtrusive, swift, and no motions. Based on the result, we designed concept applications and explored potential design space of moving virtual content for unobtrusive posture change. With our study, we discuss the interfaces that control users and the initial design guidelines of unobtrusive posture manipulation.",virtual reality; posture change; unobtrusive interaction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Virtual Field Studies: Conducting Studies on Public Displays in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Field studies on public displays can be difficult, expensive, and time-consuming. We investigate the feasibility of using virtual reality (VR) as a test-bed to evaluate deployments of public displays. Specifically, we investigate whether results from virtual field studies, conducted in a virtual public space, would match the results from a corresponding real-world setting. We report on two empirical user studies where we compared audience behavior around a virtual public display in the virtual world to audience behavior around a real public display. We found that virtual field studies can be a powerful research tool, as in both studies we observed largely similar behavior between the settings. We discuss the opportunities, challenges, and limitations of using virtual reality to conduct field studies, and provide lessons learned from our work that can help researchers decide whether to employ VR in their research and what factors to account for if doing so.",virtual reality; research methods; public displays; field studies,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,HiveFive: Immersion Preserving Attention Guidance in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Recent advances in Virtual Reality (VR) technology, such as larger fields of view, have made VR increasingly immersive. However, a larger field of view often results in a user focusing on certain directions and missing relevant content presented elsewhere on the screen. With HiveFive, we propose a technique that uses swarm motion to guide user attention in VR. The goal is to seamlessly integrate directional cues into the scene without losing immersiveness. We evaluate HiveFive in two studies. First, we compare biological motion (from a prerecorded swarm) with non-biological motion (from an algorithm), finding further evidence that humans can distinguish between these motion types and that, contrary to our hypothesis, non-biological swarm motion results in significantly faster response times. Second, we compare HiveFive to four other techniques and show that it not only results in fast response times but also has the smallest negative effect on immersion.",virtual reality; immersion; user studies; eye-tracking; attention guidance; particle swarms,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Soma Design and Sensory Misalignment,CHI - Human Factors in Computing Systems,A*,"We report on a workshop bringing together researchers working in soma design and sensory misalignment. Creating experiences that make use of sensory misalignment has become increasingly common, often associated with virtual reality research. However, little attention has been paid to how to design such experiences. We argue that the practice of soma design is a relevant candidate method for designing misalignment experiences, since soma design brings with it concepts such as estrangement and disrupting the habitual as a path to design. We further argue that sensory misalignment may in turn extend soma design methods, adding methods for explicitly disrupting sensory perception using technology interventions. Finally, we draw on the findings of that workshop to discuss the ideas of: pluralism in experience; orchestration of overall experience; as well as the broader intersection of soma design and sensory misalignment approaches.",soma design; bodily sensation; sensory misalignment,Abstract,
ACM DL,conferencePaper,2020,Improving Reliability of Virtual Collision Responses: A Cue Integration Technique,CHI - Human Factors in Computing Systems,A*,"In virtual reality (VR), a user's virtual avatar can interact with a virtual object by colliding with it. If collision responses do not occur in the direction that the user expects, the user experiences degradation of accuracy and precision in applications such as VR sports games. In determining the response of a virtual collision, existing physics engines have not considered the direction in which the user perceived and estimated the collision. Based on the cue integration theory, this study presents a statistical model explaining how users estimate the direction of a virtual collision from their body's orientation and velocity vectors. The accuracy and precision of virtual collisions can be improved by 8.77% and 30.29%, respectively, by setting the virtual collision response in the direction that users perceive.",virtual reality; collision response; cue integration theory,Abstract_Keywords,
ACM DL,conferencePaper,2020,Telewalk: Towards Free and Endless Walking in Room-Scale Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Natural navigation in VR is challenging due to spatial limitations. While Teleportation enables navigation within very small physical spaces and without causing motion sickness symptoms, it may reduce the feeling of presence and spacial awareness. Redirected walking (RDW), in contrast, allows users to naturally walk while staying inside a finite, but still very large, physical space. We present Telewalk, a novel locomotion approach that combines curvature and translation gains known from RDW research in a perceivable way. This combination enables Telewalk to be applied even within a physical space of 3m x 3m. Utilizing the head rotation as input device enables directional changes without any physical turns to keep the user always on an optimal circular path inside the real world while freely walking inside the virtual one. In a user study we found that even though motion sickness susceptible participants reported respective symptoms, Telewalk did result in stronger feelings of presence and immersion and was seen as more natural then Teleportation.",virtual reality; redirected walking; Telewalk,Keywords_Title,
ACM DL,conferencePaper,2020,"""It Should Be a Game for Fun, Not Exercise"": Tensions in Designing Health-Related Features for Pokémon GO",CHI - Human Factors in Computing Systems,A*,"Leveraging existing popular games such as Pokémon GO to promote health can engage people in healthy activities without sacrificing gaming appeal. However, little is known about what potential tensions arise from incorporating new health-related features to already existing and popular games and how to resolve those tensions from players' perspectives. In this paper, we identify design tensions surrounding the appeals of Pokémon GO, perspectives on different health needs, and mobile health technologies. By conducting surveys and design workshops with 20 avid Pokémon GO players, we demonstrate four design tensions: (1) diverse goals and rewards vs. data accuracy, (2) strong bonds between players and characters vs. gaming obsession, (3) collaborative play vs. social anxiety, and (4) connection of in-real-life experiences with the game vs. different individual contexts. We provide design implications to resolve these tensions in Pokémon GO and discuss how to extend our findings to the broader context of health promotion in location-based games.",game design; health promotion; augmented reality game; design tension; health-related game feature; location-based game; pokemon go,Keywords,
ACM DL,conferencePaper,2020,Mix&amp;Match: Towards Omitting Modelling Through In-situ Remixing of Model Repository Artifacts in Mixed Reality,CHI - Human Factors in Computing Systems,A*,"The accessibility of tools to model artifacts is one of the core driving factors for the adoption of Personal Fabrication. Subsequently, model repositories like Thingiverse became important tools in (novice) makers' processes. They allow them to shorten or even omit the design process, offloading a majority of the effort to other parties. However, steps like measurement of surrounding constraints (e.g., clearance) which exist only inside the users' environment, can not be similarly outsourced. We propose Mix&amp;Match a mixed-reality-based system which allows users to browse model repositories, preview the models in-situ, and adapt them to their environment in a simple and immediate fashion. Mix&amp;Match aims to provide users with CSG operations which can be based on both virtual and real geometry. We present interaction patterns and scenarios for Mix&amp;Match, arguing for the combination of mixed reality and model repositories. This enables almost modelling-free personal fabrication for both novices and expert makers.",mixed reality; 3D printing; personal fabrication; in-situ modelling; in-situ previews; model repositories,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,WalkingVibe: Reducing Virtual Reality Sickness and Improving Realism while Walking in VR using Unobtrusive Head-mounted Vibrotactile Feedback,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) sickness is common with symptoms such as headaches, nausea, and disorientation, and is a major barrier to using VR. We propose WalkingVibe, which applies unobtrusive vibrotactile feedback for VR walking experiences, and also reduces VR sickness and discomfort while improving realism. Feedback is delivered through two small vibration motors behind the ears at a frequency that strikes a balance in inducing vestibular response while minimizing annoyance. We conducted a 240-person study to explore how visual, audio, and various tactile feedback designs affect the locomotion experience of users walking passively in VR while seated statically in reality. Results showed timing and location for tactile feedback have significant effects on VR sickness and realism. With WalkingVibe, 2-sided step-synchronized design significantly reduces VR sickness and discomfort while significantly improving realism. Furthermore, its unobtrusiveness and ease of integration make WalkingVibe a practical approach for improving VR experiences with new and existing VR headsets.",discomfort; realism; vestibular system; vibrotactile feedback; virtual reality sickness,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,"Heatmaps, Shadows, Bubbles, Rays: Comparing Mid-Air Pen Position Visualizations in Handheld AR",CHI - Human Factors in Computing Systems,A*,"In Handheld Augmented Reality, users look at AR scenes through the smartphone held in their hand. In this setting, having a mid-air pointing device like a pen in the other hand greatly expands the interaction possibilities. For example, it lets users create 3D sketches and models while on the go. However, perceptual issues in Handheld AR make it difficult to judge the distance of a virtual object, making it hard to align a pen to it. To address this, we designed and compared different visualizations of the pen's position in its virtual environment, measuring pointing precision, task time, activation patterns, and subjective ratings of helpfulness, confidence, and comprehensibility of each visualization. While all visualizations resulted in only minor differences in precision and task time, subjective ratings of perceived helpfulness and confidence favor a 'heatmap' technique that colors the objects in the scene based on their distance to the pen.",augmented reality; smartphone; modeling; interaction; depth perception; 3D pen; depth cues; mid-air,Abstract_Keywords,
ACM DL,conferencePaper,2020,Walk The Line: Leveraging Lateral Shifts of the Walking Path as an Input Modality for Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"Recent technological advances have made head-mounted displays (HMDs) smaller and untethered, fostering the vision of ubiquitous interaction in a digitally augmented physical world. Consequently, a major part of the interaction with such devices will happen on the go, calling for interaction techniques that allow users to interact while walking. In this paper, we explore lateral shifts of the walking path as a hands-free input modality. The available input options are visualized as lanes on the ground parallel to the user's walking path. Users can select options by shifting the walking path sideways to the respective lane. We contribute the results of a controlled experiment with 18 participants, confirming the viability of our approach for fast, accurate, and joyful interactions. Further, based on the findings of the controlled experiment, we present three example applications.",augmented reality; head-mounted display; walking; input,Keywords,
ACM DL,conferencePaper,2020,Gaiters: Exploring Skin Stretch Feedback on Legs for Enhancing Virtual Reality Experiences,CHI - Human Factors in Computing Systems,A*,"We propose generating two-dimensional skin stretch feedback on the user's legs. Skin stretch is useful cutaneous feedback to induce the perception of virtual textures and illusory forces and to deliver directional cues. This feedback has been applied to the head, body, and upper limbs to simulate rich physical properties in virtual reality (VR). However, how to expand the benefit of skin stretch feedback and apply it to the lower limbs, remains to be explored. Our first two psychophysical studies examined the minimum changes in skin stretch distance and stretch angle that are perceivable by participants. We then designed and implemented Gaiters, a pair of ungrounded, leg-worn devices, each of which is able to generate multiple two-dimensional skin stretches on the skin of the user's leg. With Gaiters, we conducted an exploratory study to understand participants' experiences when coupling skin stretch patterns with various lower limb actions. The results indicate that rich haptic experiences can be created by our prototype. Finally, a user evaluation indicates that participants enjoyed the experiences when using Gaiters and considered skin stretch as compelling haptic feedback on the legs.",virtual reality; haptics; leg-worn device; sheartactors; skin stretch feedback,Abstract_Keywords_Title,
ACM DL,conferencePaper,2020,Investigating Representation of Text and Audio in Educational VR using Learning Outcomes and EEG,CHI - Human Factors in Computing Systems,A*,"This paper reports findings from a between-subjects experiment that investigates how different learning content representations in virtual environments (VE) affect the process and outcomes of learning. Seventy-eight participants were subjected to an immersive virtual reality (VR) application, where they received identical instructional information, rendered in three different formats: as text in an overlay interface, as text embedded semantically in a virtual book, or as audio. Learning outcome measures, self-reports, and an electroencephalogram (EEG) were used to compare conditions. Results show that reading was superior to listening for the learning outcomes of retention, self-efficacy, and extraneous attention. Reading text from a virtual book was reported to be less cognitively demanding, compared to reading from an overlay interface. EEG analyses show significantly lower theta and higher alpha activation in the audio condition. The findings provide important considerations for the design of educational VR environments.",virtual reality; learning; educational technology; cognitive load; eeg,Abstract_Keywords,
ACM DL,conferencePaper,2020,Understanding the Heisenberg Effect of Spatial Interaction: A Selection Induced Error for Spatially Tracked Input Devices,CHI - Human Factors in Computing Systems,A*,"Virtual and augmented reality head-mounted displays (HMDs) are currently heavily relying on spatially tracked input devices (STID) for interaction. These STIDs are all prone to the phenomenon that a discrete input (e.g. button press) will disturb the position of the tracker, resulting in a different selection point during ray-cast interaction (Heisenberg Effect of Spatial Interaction). Besides the knowledge of its existence, there is currently a lack of a deeper understanding of its severity, structure and impact on throughput and angular error during a selection task. In this work, we present a formal evaluation of the Heisenberg effect and the impact of body posture, arm position and STID degrees of freedom on its severity. In a Fitt's Law inspired user study (N=16), we found that the Heisenberg effect is responsible for 30.45% of the overall errors occurring during a pointing task, but can be reduced by 25.4% using a correction function.",selection; pointing; correction; vr; heisenberg effect; offset; stid,Abstract,
ACM DL,conferencePaper,2019,Transcalibur: A Weight Shifting Virtual Reality Controller for 2D Shape Rendering based on Computational Perception Model,CHI - Human Factors in Computing Systems,A*,"Humans can estimate the shape of a wielded object through the illusory feeling of the mass properties of the object obtained using their hands. Even though the shape of hand-held objects influences immersion and realism in virtual reality (VR), it is difficult to design VR controllers for rendering desired shapes according to the perceptions derived from the illusory effects of mass properties and shape perception. We propose Transcalibur, which is a hand-held VR controller that can render a 2D shape by changing its mass properties on a 2D planar area. We built a computational perception model using a data-driven approach from the collected data pairs of mass properties and perceived shapes. This enables Transcalibur to easily and effectively provide convincing shape perception based on complex illusory effects. Our user study showed that the system succeeded in providing the perception of various desired shapes in a virtual environment.",computational interaction; haptic display; shape perception; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,TabletInVR: Exploring the Design Space for Using a Multi-Touch Tablet in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Complex virtual reality (VR) tasks, like 3D solid modelling, are challenging with standard input controllers. We propose exploiting the affordances and input capabilities when using a 3D-tracked multi-touch tablet in an immersive VR environment. Observations gained during semi-structured interviews with general users, and those experienced with 3D software, are used to define a set of design dimensions and guidelines. These are used to develop a vocabulary of interaction techniques to demonstrate how a tablet's precise touch input capability, physical shape, metaphorical associations, and natural compatibility with barehand mid-air input can be used in VR. For example, transforming objects with touch input, ""cutting"" objects by using the tablet as a physical ""knife"", navigating in 3D by using the tablet as a viewport, and triggering commands by interleaving bare-hand input around the tablet. Key aspects of the vocabulary are evaluated with users, with results validating the approach.",interaction techniques; touch interaction; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,RotoSwype: Word-Gesture Typing using a Ring,CHI - Human Factors in Computing Systems,A*,"We propose RotoSwype, a technique for word-gesture typing using the orientation of a ring worn on the index finger. RotoSwype enables one-handed text-input without encumbering the hand with a device, a desirable quality in many scenarios, including virtual or augmented reality. The method is evaluated using two arm positions: with the hand raised up with the palm parallel to the ground; and with the hand resting at the side with the palm facing the body. A five-day study finds both hand positions achieved speeds of at least 14 words-per-minute (WPM) with uncorrected error rates near 1%, outperforming previous comparable techniques.",controlled experiments; interaction techniques,Abstract,
ACM DL,conferencePaper,2019,MilliSonic: Pushing the Limits of Acoustic Motion Tracking,CHI - Human Factors in Computing Systems,A*,"Recent years have seen interest in device tracking and localization using acoustic signals. State-of-the-art acoustic motion tracking systems however do not achieve millimeter accuracy and require large separation between microphones and speakers, and as a result, do not meet the requirements for many VR/AR applications. Further, tracking multiple concurrent acoustic transmissions from VR devices today requires sacrificing accuracy or frame rate. We present MilliSonic, a novel system that pushes the limits of acoustic based motion tracking. Our core contribution is a novel localization algorithm that can provably achieve sub-millimeter 1D tracking accuracy in the presence of multipath, while using only a single beacon with a small 4-microphone array.Further, MilliSonic enables concurrent tracking of up to four smartphones without reducing frame rate or accuracy. Our evaluation shows that MilliSonic achieves 0.7mm median 1D accuracy and a 2.6mm median 3D accuracy for smartphones, which is 5x more accurate than state-of-the-art systems. MilliSonic enables two previously infeasible interaction applications: a) 3D tracking of VR headsets using the smartphone as a beacon and b) fine-grained 3D tracking for the Google Cardboard VR system using a small microphone array.",acoustic; localization; mobile system; motion tracking; virtual reality,Keywords,
ACM DL,conferencePaper,2019,"VirtualBricks: Exploring a Scalable, Modular Toolkit for Enabling Physical Manipulation in VR",CHI - Human Factors in Computing Systems,A*,"Often Virtual Reality (VR) experiences are limited by the design of standard controllers. This work aims to liberate a VR developer from these limitations in the physical realm to provide an expressive match to the limitless possibilities in the virtual realm. VirtualBricks is a LEGO based toolkit that enables construction of a variety of physical-manipulation enabled controllers for VR, by offering a set of feature bricks that emulate as well as extend the capabilities of default controllers. Based on the LEGO platform, the toolkit provides a modular, scalable solution for enabling passive haptics in VR. We demonstrate the versatility of our designs through a rich set of applications including re-implementations of artifacts from recent research. We share a VR Integration package for integration with Unity VR IDE, the CAD models for the feature bricks, for easy deployment of VirtualBricks within the community.",construction toolkit; passive haptics; physical manipulation,Abstract,
ACM DL,conferencePaper,2019,An Evaluation of Radar Metaphors for Providing Directional Stimuli Using Non-Verbal Sound,CHI - Human Factors in Computing Systems,A*,"We compared four audio-based radar metaphors for providing directional stimuli to users of AR headsets. The metaphors are clock face, compass, white noise, and scale. Each metaphor, or method, signals the movement of a virtual arm in a radar sweep. In a user study, statistically significant differences were observed for accuracy and response time. Beat-based methods (clock face, compass) elicited responses biased to the left of the stimulus location, and non-beat-based methods (white noise, scale) produced responses biased to the right of the stimulus location. The beat methods were more accurate than the non-beat methods. However, the non-beat methods elicited quicker responses. We also discuss how response accuracy varies along the radar sweep between methods. These observations contribute design insights for non-verbal, non-visual directional prompting.",accessibility; augmented reality; directional prompting; headset; radar; spatial audio; visual impairment,Keywords,
ACM DL,conferencePaper,2019,TORC: A Virtual Reality Controller for In-Hand High-Dexterity Finger Interaction,CHI - Human Factors in Computing Systems,A*,"Recent hand-held controllers have explored a variety of haptic feedback sensations for users in virtual reality by producing both kinesthetic and cutaneous feedback from virtual objects. These controllers are grounded to the user's hand and can only manipulate objects through arm and wrist motions, not using the dexterity of their fingers as they would in real life. In this paper, we present TORC, a rigid haptic controller that renders virtual object characteristics and behaviors such as texture and compliance. Users hold and squeeze TORC using their thumb and two fingers and interact with virtual objects by sliding their thumb on TORC's trackpad. During the interaction, vibrotactile motors produce sensations to each finger that represent the haptic feel of squeezing, shearing or turning an object. Our evaluation showed that using TORC, participants could manipulate virtual objects more precisely (e.g., position and rotate objects in 3D) than when using a conventional VR controller.",haptic compliance; haptic texture; haptics; vr object manipulation,Abstract_Title,
ACM DL,conferencePaper,2019,PlaneVR: Social Acceptability of Virtual Reality for Aeroplane Passengers,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) headsets allow wearers to escape their physical surroundings, immersing themselves in a virtual world. Although escape may not be realistic or acceptable in many everyday situations, air travel is one context where early adoption of VR could be very attractive. While travelling, passengers are seated in restricted spaces for long durations, reliant on limited seat-back displays or mobile devices. This paper explores the social acceptability and usability of VR for in-flight entertainment. In an initial survey, we captured respondents' attitudes towards the social acceptability of VR headsets during air travel. Based on the survey results, we developed a VR in-flight entertainment prototype and evaluated this in a focus group study. Our results discuss methods for improving the acceptability of VR in-flight, including using mixed reality to help users transition between virtual and physical environments and supporting interruption from other co-located people.",in-flight entertainment; social acceptability; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Direct Finger Manipulation of 3D Object Image with Ultrasound Haptic Feedback,CHI - Human Factors in Computing Systems,A*,"In this study, we prototype and examine a system that allows a user to manipulate a 3D virtual object with multiple fingers without wearing any device. An autostereoscopic display produces a 3D image and a depth sensor measures the movement of the fingers. When a user touches a virtual object, haptic feedback is provided by ultrasound phased arrays. By estimating the cross section of the finger in contact with the virtual object and by creating a force pattern around it, it is possible for the user to recognize the position of the surface relative to the finger. To evaluate our system, we conducted two experiments to show that the proposed feedback method is effective in recognizing the object surface and thereby enables the user to grasp the object quickly without seeing it.",touch/haptic/pointing/gesture; virtual/augmented reality,Keywords,
ACM DL,conferencePaper,2019,RayCursor: A 3D Pointing Facilitation Technique based on Raycasting,CHI - Human Factors in Computing Systems,A*,"Raycasting is the most common target pointing technique in virtual reality environments. However, performance on small and distant targets is impacted by the accuracy of the pointing device and the user's motor skills. Current pointing facilitation techniques are currently only applied in the context of the virtual hand, i.e. for targets within reach. We propose enhancements to Raycasting: filtering the ray, and adding a controllable cursor on the ray to select the nearest target. We describe a series of studies for the design of the visual feedforward, filtering technique, as well as a comparative study between different 3D pointing techniques. Our results show that highlighting the nearest target is one of the most efficient visual feedforward technique. We also show that filtering the ray reduces error rate in a drastic way. Finally we show the benefits of RayCursor compared to Raycasting and another technique from the literature.",virtual reality; pointing technique; visual feedforward,Abstract_Keywords,
ACM DL,conferencePaper,2019,Augmentation not Duplication: Considerations for the Design of Digitally-Augmented Comic Books,CHI - Human Factors in Computing Systems,A*,"Digital-augmentation of print-media can provide contextually relevant audio, visual, or haptic content to supplement the static text and images. The design of such augmentation–its medium, quantity, frequency, content, and access technique–can have a significant impact on the reading experience. In the worst case, such as where children are learning to read, the print medium can become a proxy for accessing digital content only, and the textual content is avoided. In this work, we examine how augmented content can change the reader's behaviour with a comic book. We first report on the usage of a commercially available augmented comic for children, providing evidence that a third of all readers converted to simply viewing the digital media when printed content is duplicated. Second, we explore the design space for digital content augmentation in print media. Third, we report a user study with 136 children that examined the impact of both content length and presentation in a digitally-augmented comic book. From this, we report a series of design guidelines to assist designers and editors in the development of digitally-augmented print media.",augmented reality; comic books; digital augmentation,Keywords,
ACM DL,conferencePaper,2019,Behavioural Biometrics in VR: Identifying People from Body Motion and Relations in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Every person is unique, with individual behavioural characteristics: how one moves, coordinates, and uses their body. In this paper we investigate body motion as behavioural biometrics for virtual reality. In particular, we look into which behaviour is suitable to identify a user. This is valuable in situations where multiple people use a virtual reality environment in parallel, for example in the context of authentication or to adapt the VR environment to users' preferences. We present a user study (N=22) where people perform controlled VR tasks (pointing, grabbing, walking, typing), monitoring their head, hand, and eye motion data over two sessions. These body segments can be arbitrarily combined into body relations, and we found that these movements and their combination lead to characteristic behavioural patterns. We present an extensive analysis of which motion/relation is useful to identify users in which tasks using classification methods. Our findings are beneficial for researchers and practitioners alike who aim to build novel adaptive and secure user interfaces in virtual reality.",virtual reality; motion; proprioception; adaptive uis; behavioural biometrics; relation,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,SeeingVR: A Set of Tools to Make Virtual Reality More Accessible to People with Low Vision,CHI - Human Factors in Computing Systems,A*,"Current virtual reality applications do not support people who have low vision, i.e., vision loss that falls short of complete blindness but is not correctable by glasses. We present SeeingVR, a set of 14 tools that enhance a VR application for people with low vision by providing visual and audio augmentations. A user can select, adjust, and combine different tools based on their preferences. Nine of our tools modify an existing VR application post hoc via a plugin without developer effort. The rest require simple inputs from developers using a Unity toolkit we created that allows integrating all 14 of our low vision support tools during development. Our evaluation with 11 participants with low vision showed that SeeingVR enabled users to better enjoy VR and complete tasks more quickly and accurately. Developers also found our Unity toolkit easy and convenient to use.",accessibility; virtual reality; low vision; unity,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,I'm Sensing in the Rain: Spatial Incongruity in Visual-Tactile Mid-Air Stimulation Can Elicit Ownership in VR Users,CHI - Human Factors in Computing Systems,A*,"Major virtual reality (VR) companies are trying to enhance the sense of immersion in virtual environments by implementing haptic feedback in their systems (e.g., Oculus Touch). It is known that tactile stimulation adds realism to a virtual environment. In addition, when users are not limited by wearing any attachments (e.g., gloves), it is even possible to create more immersive experiences. Mid-air haptic technology provides contactless haptic feedback and offers the potential for creating such immersive VR experiences. However, one of the limitations of mid-air haptics resides in the need for freehand tracking systems (e.g., Leap Motion) to deliver tactile feedback to the user's hand. These tracking systems are not accurate, limiting designers capability of delivering spatially precise tactile stimulation. Here, we investigated an alternative way to convey incongruent visual-tactile stimulation that can be used to create the illusion of a congruent visual-tactile experience, while participants experience the phenomenon of the rubber hand illusion in VR.",virtual reality; mid-air haptics; touch; vr; illusions; rubber hand illusion; tracking systems; virtual hand illusion,Abstract_Keywords,
ACM DL,conferencePaper,2019,Putting the Value in VR: How to Systematically and Iteratively Develop a Value-Based VR Application with a Complex Target Group,CHI - Human Factors in Computing Systems,A*,"In development, implementation and evaluation of eHealth it is essential to account for stakeholders' perspectives, opinions and values, which are statements that specify what stakeholders want to achieve or improve via a technology. The use of values enables developers to systematically include stakeholders' perspectives and the context of use in an eHealth development process. However, there are relatively few papers that explain how to use values in technology development. Consequently, in this paper we show how we formulated values during the multi-method, interdisciplinary and iterative development process of a VR application for a complex setting: forensic mental healthcare. We report the main foundations for these values: the outcomes of an online questionnaire with patients, therapists and other stakeholders (n=146) and interviews with patients and therapists (n=18). We show how a multidisciplinary project team used these qualitative results to formulate and adapt values and create lo-fi prototypes of a VR application. We discuss the importance of a systematic development process with multiple formative evaluations for eHealth and reflect on the role of values within this.",virtual reality; ehealth; forensic mental healthcare; formative evaluation; iterative development; value-based technology,Keywords,
ACM DL,conferencePaper,2019,Virtual Showdown: An Accessible Virtual Reality Game with Scaffolds for Youth with Visual Impairments,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) is a growing source of entertainment, but people who are visually impaired have not been effectively included. Audio cues are motivated as a complement to visuals, making experiences more immersive, but are not a primary cue. To address this, we implemented a VR game called Virtual Showdown. We based Virtual Showdown on an accessible real-world game called Showdown, where people use their hearing to locate and hit a ball against an opponent. Further, we developed Verbal and Verbal/Vibration Scaffolds to teach people how to play Virtual Showdown. We assessed the acceptability of Virtual Showdown and compared our scaffolds in an empirical study with 34 youth who are visually impaired. Thirty-three participants wanted to play Virtual Showdown again, and we learned that participants scored higher with the Verbal Scaffold or if they had prior Showdown experience. Our empirical findings inform the design of future accessible VR experiences.",virtual reality; blind; low vision; spatial audio; haptics; youth,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Assessing the Accuracy of Point &amp; Teleport Locomotion with Orientation Indication for Virtual Reality using Curved Trajectories,CHI - Human Factors in Computing Systems,A*,"Room-scale Virtual Reality (VR) systems have arrived in users' homes where tracked environments are set up in limited physical spaces. As most Virtual Environments (VEs) are larger than the tracked physical space, locomotion techniques are used to navigate in VEs. Currently, in recent VR games, point &amp; teleport is the most popular locomotion technique. However, it only allows users to select the position of the teleportation and not the orientation that the user is facing after the teleport. This results in users having to manually correct their orientation after teleporting and possibly getting entangled by the cable of the headset. In this paper, we introduce and evaluate three different point &amp; teleport techniques that enable users to specify the target orientation while teleporting. The results show that, although the three teleportation techniques with orientation indication increase the average teleportation time, they lead to a decreased need for correcting the orientation after teleportation.",virtual reality; locomotion; teleportation; virtual environments; orientation indication; point &amp; teleport,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Virtual Performance Augmentation in an Immersive Jump &amp; Run Exergame,CHI - Human Factors in Computing Systems,A*,"Human performance augmentation through technology has been a recurring theme in science and culture, aiming to increase human capabilities and accessibility. We investigate a related concept: virtual performance augmentation (VPA), using VR to give users the illusion of greater capabilities than they actually have. We propose a method for VPA of running and jumping, based on in place movements, and studied its effects in a VR exergame. We found that in place running and jumping in VR can be used to create a somewhat natural experience and can elicit medium to high physical exertion in an immersive and intrinsically motivating manner. We also found that virtually augmenting running and jumping can increase intrinsic motivation, perceived competence and flow, and may also increase motivation for physical activity in general. We discuss implications of VPA for safety and accessibility, with initial evidence suggesting that VPA may help users with physical impairments enjoy the benefits of exergaming.",virtual reality; performance; jumping; exergame; running,Keywords,
ACM DL,conferencePaper,2019,HaptiVec: Presenting Haptic Feedback Vectors in Handheld Controllers using Embedded Tactile Pin Arrays,CHI - Human Factors in Computing Systems,A*,"HaptiVec is a new haptic feedback paradigm for handheld controllers which allows users to feel directional haptic pressure vectors on their fingers and hands while interacting with virtual environments. We embed a 3 by 5 tactile pin array (with an average pin spacing of 25 mm) into the handles of two custom VR type controllers. By presenting directional pressure vectors in eight cardinal directions (N, NE, E, SE, S, SW, W, NW) to users without prior training, they were able to distinguish the correct direction with an accuracy of at least 79%. We illustrate two applications where our device enhances virtual experiences over traditional vibrotactile feedback. In the first application, through the classic first-person shooter Doom, we demonstrate that users can receive directional pressure feedback corresponding to the direction of incident enemy projectiles. In the second application, we demonstrate how our controller can create a more immersive experience by allowing the user to feel their virtual climate by randomizing the directional vectors and presenting the user with ""haptic rain"" which adapts with the intensity of the rainfall.",virtual reality; haptics; controller; tactile display,Keywords,
ACM DL,conferencePaper,2019,Evaluating the Combination of Visual Communication Cues for HMD-based Mixed Reality Remote Collaboration,CHI - Human Factors in Computing Systems,A*,"Many researchers have studied various visual communication cues (e.g. pointer, sketching, and hand gesture) in Mixed Reality remote collaboration systems for real-world tasks. However, the effect of combining them has not been so well explored. We studied the effect of these cues in four combinations: hand only, hand + pointer, hand + sketch, and hand + pointer + sketch, with three problem tasks: Lego, Tangram, and Origami. The study results showed that the participants completed the task significantly faster and felt a significantly higher level of usability when the sketch cue is added to the hand gesture cue, but not with adding the pointer cue. Participants also preferred the combinations including hand and sketch cues over the other combinations. However, using additional cues (pointer or sketch) increased the perceived mental effort and did not improve the feeling of co-presence. We discuss the implications of these results and future research directions.",usability; mixed reality; remote collaboration; co-presence; visual communication cue,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Usability of Gamified Knowledge Learning in VR and Desktop-3D,CHI - Human Factors in Computing Systems,A*,"Affine Transformations (ATs) often escape an intuitive approach due to their high complexity. Therefore, we developed GEtiT that directly encodes ATs in its game mechanics and scales the knowledge's level of abstraction. This results in an intuitive application as well as audiovisual presentation of ATs and hence in a knowledge learning. We also developed a specific Virtual Reality (VR) version to explore the effects of immersive VR on the learning outcomes. This paper presents our approach of directly encoding abstract knowledge in game mechanics, the conceptual design of GEtiT and its technical implementation. Both versions are compared in regard to their usability in a user study. The results show that both GEtiT versions induce a high degree of flow and elicit a good intuitive use. They validate the effectiveness of the design and the resulting knowledge application requirements. Participants favored GEtiT VR thus showing a potentially higher learning quality when using VR.",gamification; knowledge learning; serious games design,Abstract,
ACM DL,conferencePaper,2019,Mobi3DSketch: 3D Sketching in Mobile AR,CHI - Human Factors in Computing Systems,A*,"Mid-air 3D sketching has been mainly explored in Virtual Reality (VR) and typically requires special hardware for motion capture and immersive, stereoscopic displays. The recently developed motion tracking algorithms allow real-time tracking of mobile devices, and have enabled a few mobile applications for 3D sketching in Augmented Reality (AR). However, they are more suitable for making simple drawings only, since they do not consider special challenges with mobile AR 3D sketching, including the lack of stereo display, narrow field of view, and the coupling of 2D input, 3D input and display. To address these issues, we present Mobi3DSketch, which integrates multiple sources of inputs with tools, mainly different versions of 3D snapping and planar/curves surface proxies. Our multimodal interface supports both absolute and relative drawing, allowing easy creation of 3D concept designs in situ. The effectiveness and expressiveness of Mobi3DSketch are demonstrated via a pilot study.",augmented reality; mobile interaction; 3d concept design; 3d sketching; mid-air drawing; relative drawing,Abstract_Keywords,
ACM DL,conferencePaper,2019,The Role of Physical Props in VR Climbing Environments,CHI - Human Factors in Computing Systems,A*,"Dealing with fear of falling is a challenge in sport climbing. Virtual reality (VR) research suggests that using physical and reality-based interaction increases the presence in VR. In this paper, we present a study that investigates the influence of physical props on presence, stress and anxiety in a VR climbing environment involving whole body movement. To help climbers overcoming fear of falling, we compared three different conditions: Climbing in reality at 10 m height, physical climbing in VR (with props attached to the climbing wall) and virtual climbing in VR using game controllers. From subjective reports and biosignals, our results show that climbing with props in VR increases the anxiety and sense of realism in VR for sport climbing. This suggests that VR in combination with physical props are an effective simulation setup to induce the sense of height.",virtual reality; presence; biosensors; climbing; fear of falling; physical props,Abstract_Keywords,
ACM DL,conferencePaper,2019,Experimental Analysis of Barehand Mid-air Mode-Switching Techniques in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We present an empirical comparison of eleven bare hand, mid-air mode-switching techniques suitable for virtual reality in two experiments. The first evaluates seven techniques spanning dominant and non-dominant hand actions. Techniques represent common classes of actions selected by a methodical examination of 56 examples of prior art. The standard ""subtraction method"" protocol is adapted for 3D interfaces, with two baseline selection methods, bare hand pinch and device controller button. A second experiment with four techniques explores more subtle dominant-hand techniques and the effect of using a dominant hand device for selection. Results provide guidance to practitioners when choosing bare hand, mid-air mode-switching techniques, and for researchers when designing new mode-switching methods in VR.",interaction techniques; controlled experiment,Abstract_Title,
ACM DL,conferencePaper,2019,Mixed Reality Remote Collaboration Combining 360 Video and 3D Reconstruction,CHI - Human Factors in Computing Systems,A*,"Remote Collaboration using Virtual Reality (VR) and Augmented Reality (AR) has recently become a popular way for people from different places to work together. Local workers can collaborate with remote helpers by sharing 360-degree live video or 3D virtual reconstruction of their surroundings. However, each of these techniques has benefits and drawbacks. In this paper we explore mixing 360 video and 3D reconstruction together for remote collaboration, by preserving benefits of both systems while reducing drawbacks of each. We developed a hybrid prototype and conducted user study to compare benefits and problems of using 360 or 3D alone to clarify the needs for mixing the two, and also to evaluate the prototype system. We found participants performed significantly better on collaborative search tasks in 360 and felt higher social presence, yet 3D also showed potential to complement. Participant feedback collected after trying our hybrid system provided directions for improvement.",virtual reality; mixed reality; remote collaboration; 360 panorama; 3d scene reconstruction; interaction methods,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Drag:on: A Virtual Reality Controller Providing Haptic Feedback Based on Drag and Weight Shift,CHI - Human Factors in Computing Systems,A*,"Standard controllers for virtual reality (VR) lack sophisticated means to convey a realistic, kinesthetic impression of size, resistance or inertia. We present the concept and implementation of Drag:on, an ungrounded shape-changing VR controller that provides dynamic passive haptic feedback based on drag, i.e. air resistance, and weight shift. Drag:on leverages the airflow occurring at the controller during interaction. By dynamically adjusting its surface area, the controller changes the drag and rotational inertia felt by the user. In a user study, we found that Drag:on can provide distinguishable levels of haptic feedback. Our prototype increases the haptic realism in VR compared to standard controllers and when rotated or swung improves the perception of virtual resistance. By this, Drag:on provides haptic feedback suitable for rendering different virtual mechanical resistances, virtual gas streams, and virtual objects differing in scale, material and fill state.",air resistance feedback; dynamic passive haptic feedback; haptic virtual reality controller; shape-changing devices,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,ElasticVR: Providing Multilevel Continuously-Changing Resistive Force and Instant Impact Using Elasticity for VR,CHI - Human Factors in Computing Systems,A*,"Resistive force (e.g., due to object elasticity) and impact (e.g., due to recoil) are common effects in our daily life. However, resistive force continuously changes due to users' movements while impact instantly occurs when an event triggers it. These feedback are still not realistically provided by current VR haptic methods. In this paper, a wearable device, ElasticVR, which consists of an elastic band, servo motors and mechanical brakes, is proposed to provide the continuously-changing resistive force and instantly-occurring impact upon the user's hand to enhance VR realism. By changing two physical properties, length and extension distance, of the elastic band, ElasticVR provides multilevel resistive force with no delay and impact with little delay, respectively, for realistic and versatile VR applications. A force perception study was performed to observe users' force distinguishability of the resistive force and impact, and the prototype was built based on its results. A VR experience study further proves that the resistive force and impact from ElasticVR both outperform those from current approaches in realism. Applications using ElasticVR are also demonstrated.",virtual reality; haptic feedback; impact; force feedback; wearable device; elastic force; resistive force,Keywords,
ACM DL,conferencePaper,2019,On the Shoulder of the Giant: A Multi-Scale Mixed Reality Collaboration with 360 Video Sharing and Tangible Interaction,CHI - Human Factors in Computing Systems,A*,"We propose a multi-scale Mixed Reality (MR) collaboration between the Giant, a local Augmented Reality user, and the Miniature, a remote Virtual Reality user, in Giant-Miniature Collaboration (GMC). The Miniature is immersed in a 360-video shared by the Giant who can physically manipulate the Miniature through a tangible interface, a combined 360-camera with a 6 DOF tracker. We implemented a prototype system as a proof of concept and conducted a user study (n=24) comprising of four parts comparing: A) two types of virtual representations, B) three levels of Miniature control, C) three levels of 360-video view dependencies, and D) four 360-camera placement positions on the Giant. The results show users prefer a shoulder mounted camera view, while a view frustum with a complimentary avatar is a good visualization for the Miniature virtual representation. From the results, we give design recommendations and demonstrate an example Giant-Miniature Interaction.",mixed reality; tangible user interface; remote collaboration; live panorama sharing; multi-scale; wearable interface,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,"""What's Happening at that Hip?"": Evaluating an On-body Projection based Augmented Reality System for Physiotherapy Classroom",CHI - Human Factors in Computing Systems,A*,"We present two studies to discuss the design, usability analysis, and educational outcome resulting from our system Augmented Body in physiotherapy classroom. We build on prior user-centric design work that investigates existing teaching methods and discuss opportunities for intervention. We present the design and implementation of a hybrid system for physiotherapy education combining an on-body projection based virtual anatomy supplemented by pen-based tablets to create real-time annotations. We conducted a usability evaluation of this system, comparing with projection only and traditional teaching conditions. Finally, we focus on a comparative study to evaluate learning outcome among students in actual classroom settings. Our studies showed increased usage of visual representation techniques in students'11 note taking behavior and statistically significant improvement in some learning aspects. We discuss challenges for designing augmented reality systems for education, including minimizing attention split, addressing text-entry issues, and digital annotations on a moving physical body.",augmented reality; projection mapping; annotation; educational system; pen-based interactions,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Bring the Outside In: Providing Accessible Experiences Through VR for People with Dementia in Locked Psychiatric Hospitals,CHI - Human Factors in Computing Systems,A*,"Many people with dementia (PWD) residing in long-term care may face barriers in accessing experiences beyond their physical premises; this may be due to location, mobility constraints, legal mental health act restrictions, or offence-related restrictions. In recent years, there have been research interests towards designing non-pharmacological interventions aiming to improve the Quality of Life (QoL) for PWD within long-term care. We explored the use of Virtual Reality (VR) as a tool to provide 360°-video based experiences for individuals with moderate to severe dementia residing in a locked psychiatric hospital. We discuss at depth the appeal of using VR for PWD, and the observed impact of such interaction. We also present the design opportunities, pitfalls, and recommendations for future deployment in healthcare services. This paper demonstrates the potential of VR as a virtual alternative to experiences that may be difficult to reach for PWD residing within locked setting.",virtual reality; dementia; locked psychiatric hospital; long-term care; patient-centred design; person-centred care,Abstract_Keywords,
ACM DL,conferencePaper,2019,Examining and Enhancing the Illusory Touch Perception in Virtual Reality Using Non-Invasive Brain Stimulation,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) can be immersive to such a degree that users sometimes report feeling tactile sensations based on visualization of the touch, without any actual physical contact. This effect is not only interesting for studies of human perception, but can also be leveraged to improve the quality of VR by evoking tactile sensations without usage of specialized equipment. The aim of this paper is to study brain processing of the illusory touch and its enhancement for purposes of exploitation in VR scene design. To amplify the illusory touch, transcranial direct current stimulation (tDCS) was used. Participants attended two sessions with blinded stimulation and interacted with a virtual ball using tracked hands in VR. The effects were studied using electroencephalography (EEG), that allowed us to examine stimulation-induced changes in processing of the illusory touch in the brain, as well as to identify its neural correlates. Results confirm enhanced processing of the illusory touch after the stimulation, and some of these changes were correlated to subjective rating of its magnitude.",virtual reality; embodiment; electroencephalography; illusory touch; transcranial direct current stimulation,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Enhancing Texture Perception in Virtual Reality Using 3D-Printed Hair Structures,CHI - Human Factors in Computing Systems,A*,"Experiencing materials in virtual reality (VR) is enhanced by combining visual and haptic feedback. While VR easily allows changes to visual appearances, modifying haptic impressions remains challenging. Existing passive haptic techniques require access to a large set of tangible proxies. To reduce the number of physical representations, we look towards fabrication to create more versatile counterparts. In a user study, 3D-printed hairs with length varying in steps of 2.5 mm were used to influence the feeling of roughness and hardness. By overlaying fabricated hair with visual textures, the resolution of the user's haptic perception increased. As changing haptic sensations are able to elicit perceptual switches, our approach can extend a limited set of textures to a much broader set of material impressions. Our results give insights into the effectiveness of 3D-printed hair for enhancing texture perception in VR.",3d printing; passive haptic feedback; texture perception,Abstract_Title,
ACM DL,conferencePaper,2019,Crowdsourcing Interface Feature Design with Bayesian Optimization,CHI - Human Factors in Computing Systems,A*,"Designing novel interfaces is challenging. Designers typically rely on experience or subjective judgment in the absence of analytical or objective means for selecting interface parameters. We demonstrate Bayesian optimization as an efficient tool for objective interface feature refinement. Specifically, we show that crowdsourcing paired with Bayesian optimization can rapidly and effectively assist interface design across diverse deployment environments. Experiment 1 evaluates the approach on a familiar 2D interface design problem: a map search and review use case. Adding a degree of complexity, Experiment 2 extends Experiment 1 by switching the deployment environment to mobile-based virtual reality. The approach is then demonstrated as a case study for a fundamentally new and unfamiliar interaction design problem: web-based augmented reality. Finally, we show how the model generated as an outcome of the refinement process can be used for user simulation and queried to deliver various design insights.",optimization; crowdsourcing; interface design,Abstract,
ACM DL,conferencePaper,2019,Clench Interface: Novel Biting Input Techniques,CHI - Human Factors in Computing Systems,A*,"People eat every day and biting is one of the most fundamental and natural actions that they perform on a daily basis. Existing work has explored tooth click location and jaw movement as input techniques, however clenching has the potential to add control to this input channel. We propose clench interaction that leverages clenching as an actively controlled physiological signal that can facilitate interactions. We conducted a user study to investigate users' ability to control their clench force. We found that users can easily discriminate three force levels, and that they can quickly confirm actions by unclenching (quick release). We developed a design space for clench interaction based on the results and investigated the usability of the clench interface. Participants preferred the clench over baselines and indicated a willingness to use clench-based interactions. This novel technique can provide an additional input method in cases where users' eyes or hands are busy, augment immersive experiences such as virtual/augmented reality, and assist individuals with disabilities.",design space; clench force control; clench interface,Abstract,
ACM DL,conferencePaper,2019,Exploring Virtual Agents for Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Prior work has shown that embodiment can benefit virtual agents, such as increasing rapport and conveying non-verbal information. However, it is unclear if users prefer an embodied to a speech-only agent for augmented reality (AR) headsets that are designed to assist users in completing real-world tasks. We conducted a study to examine users' perceptions and behaviors when interacting with virtual agents in AR. We asked 24 adults to wear the Microsoft HoloLens and find objects in a hidden object game while interacting with an agent that would offer assistance. We presented participants with four different agents: voice-only, non-human, full-size embodied, and a miniature embodied agent. Overall, users preferred the miniature embodied agent due to the novelty of his size and reduced uncanniness as opposed to the larger agent. From our results, we draw conclusions about how agent representation matters and derive guidelines on designing agents for AR headsets.",augmented reality; embodied conversational agents,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,TutoriVR: A Video-Based Tutorial System for Design Applications in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual Reality painting is a form of 3D-painting done in a Virtual Reality (VR) space. Being a relatively new kind of art form, there is a growing interest within the creative practices community to learn it. Currently, most users learn using community posted 2D-videos on the internet, which are a screencast recording of the painting process by an instructor. While such an approach may suffice for teaching 2D-software tools, these videos by themselves fail in delivering crucial details that required by the user to understand actions in a VR space. We conduct a formative study to identify challenges faced by users in learning to VR-paint using such video-based tutorials. Informed by results of this study, we develop a VR-embedded tutorial system that supplements video tutorials with 3D and contextual aids directly in the user's VR environment. An exploratory evaluation showed users were positive about the system and were able to use the proposed system to recreate painting tasks in VR.",virtual reality; software tutorials; design applications,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Can Mobile Augmented Reality Stimulate a Honeypot Effect? Observations from Santa's Lil Helper,CHI - Human Factors in Computing Systems,A*,"In HCI, the honeypot effect describes a form of audience engagement in which a person's interaction with a technology stimulates passers-by to observe, approach and engage in an interaction themselves. In this paper we explore the potential for honeypot effects to arise in the use of mobile augmented reality (AR) applications in urban spaces. We present an observational study of Santa's Lil Helper, a mobile AR game that created a Christmas-themed treasure hunt in a metropolitan area. Our study supports a consideration of three factors that may impede the honeypot effect: the presence of people in relation to the game and its interactive components; the visibility of gameplay in urban space; and the extent to which the game permits a shared experience. We consider how these factors can inform the design of future AR experiences that are capable of stimulating honeypot effects in public space.",augmented reality; audience; public space; honeypot effect,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Pseudo-Haptic Weight: Changing the Perceived Weight of Virtual Objects By Manipulating Control-Display Ratio,CHI - Human Factors in Computing Systems,A*,"In virtual reality, the lack of kinesthetic feedback often prevents users from experiencing the weight of virtual objects. Control-to-display (C/D) ratio manipulation has been proposed as a method to induce weight perception without kinesthetic feedback. Based on the fact that lighter (heavier) objects are easier (harder) to move, this method induces an illusory perception of weight by manipulating the rendered position of users' hands—increasing or decreasing their displayed movements. In a series of experiments we demonstrate that C/D-ratio induces a genuine perception of weight, while preserving ownership over the virtual hand. This means that such a manipulation can be easily introduced in current VR experiences without disrupting the sense of presence. We discuss these findings in terms of estimation of physical work needed to lift an object. Our findings provide the first quantification of the range of C/D-ratio that can be used to simulate weight in virtual reality.",pseudo-haptics; multisensory integration; virtual weight,Abstract,
ACM DL,conferencePaper,2019,Brick: Toward A Model for Designing Synchronous Colocated Augmented Reality Games,CHI - Human Factors in Computing Systems,A*,"Augmented reality (AR) games have been growing in popularity in recent years. However, current AR games offer limited opportunities for a synchronous multiplayer experience. This paper introduces a model for designing AR experiences in which players inhabit a shared, real-time augmented environment and can engage in synchronous and collaborative interactions with other players. We explored the development of this model through the creation of Brick, a two-player mobile AR game at the room scale. We refined Brick over multiple rounds of iteration, and we used our playtests to investigate a range of issues involved in designing shared-world AR games. Our findings suggest that there are five major categories of interactions in a shared-world AR system: single-player, intrapersonal, multiplayer, interpersonal, and environmental. We believe that this model can support the development of collaborative AR games and new forms of social gameplay.",augmented reality; collaborative; colocated; synchronous,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Scaptics and Highlight-Planes: Immersive Interaction Techniques for Finding Occluded Features in 3D Scatterplots,CHI - Human Factors in Computing Systems,A*,"Three-dimensional scatterplots suffer from well-known perception and usability problems. In particular, overplotting and occlusion, mainly due to density and noise, prevent users from properly perceiving the data. Thanks to accurate head and hand tracking, immersive Virtual Reality (VR) setups provide new ways to interact and navigate with 3D scatterplots. VR also supports additional sensory modalities such as haptic feedback. Inspired by methods commonly used in Scientific Visualisation to visually explore volumes, we propose two techniques that leverage the immersive aspects of VR: first, a density-based haptic vibration technique (Scaptics) which provides feedback through the controller; and second, an adaptation of a cutting plane for 3D scatterplots (Highlight-Plane). We evaluated both techniques in a controlled study with two tasks involving density (finding high- and low-density areas). Overall, Scaptics was the most time-efficient and accurate technique, however, in some conditions, it was outperformed by Highlight-Plane.",virtual reality; vibrotactile feedback; haptic; 3d scatterplot,Abstract_Keywords,
ACM DL,conferencePaper,2019,RealityCheck: Blending Virtual Environments with Situated Physical Reality,CHI - Human Factors in Computing Systems,A*,"Today's virtual reality (VR) systems offer chaperone rendering techniques that prevent the user from colliding with physical objects. Without a detailed geometric model of the physical world, these techniques offer limited possibility for more advanced compositing between the real world and the virtual. We explore this using a realtime 3D reconstruction of the real world that can be combined with a virtual environment. RealityCheck allows users to freely move, manipulate, observe, and communicate with people and objects situated in their physical space without losing the sense of immersion or presence inside their virtual world. We demonstrate RealityCheck with seven existing VR titles, and describe compositing approaches that address the potential conflicts when rendering the real world and a virtual environment together. A study with frequent VR users demonstrate the affordances provided by our system and how it can be used to enhance current VR experiences.",virtual reality; 3d compositing; depth cameras,Abstract_Keywords,
ACM DL,conferencePaper,2019,Lost in Style: Gaze-driven Adaptive Aid for VR Navigation,CHI - Human Factors in Computing Systems,A*,"A key challenge for virtual reality level designers is striking a balance between maintaining the immersiveness of VR and providing users with on-screen aids after designing a virtual experience. These aids are often necessary for wayfinding in virtual environments with complex paths. We introduce a novel adaptive aid that maintains the effectiveness of traditional aids, while equipping designers and users with the controls of how often help is displayed. Our adaptive aid uses gaze patterns in predicting user's need for navigation aid in VR and displays mini-maps or arrows accordingly. Using a dataset of gaze angle sequences of users navigating a VR environment and markers of when users requested aid, we trained an LSTM to classify user's gaze sequences as needing navigation help and display an aid. We validated the efficacy of the adaptive aid for wayfinding compared to other commonly-used wayfinding aids.",wayfinding; games; vr; eyetracking,Abstract,
ACM DL,conferencePaper,2019,The Effect of Field-of-View Restriction on Sex Bias in VR Sickness and Spatial Navigation Performance,CHI - Human Factors in Computing Systems,A*,"Recent studies show that women are more susceptible to visually-induced VR sickness, which might explain the low adoption rate of VR technology among women. Reducing field-of-view (FOV) during locomotion is already a widely used strategy to reduce VR sickness as it blocks peripheral optical flow perception and mitigates visual/vestibular conflict. Prior studies show that men are more adept at 3D spatial navigation than women, though this sex bias can be minimized by providing women with a larger FOV. Our study provides insight into the relationship between sex and FOV restriction with respect to VR sickness and spatial navigation performance which seem to conflict. We find the use of an FOV restrictor to be effective in mitigating VR sickness in both sexes while we did not find a negative effect of FOV restriction on spatial navigation performance.",virtual reality; field-of-view manipulation; sex differences; spatial navigation performance; virtual locomotion; vr sickness,Keywords,
ACM DL,conferencePaper,2019,Beyond The Force: Using Quadcopters to Appropriate Objects and the Environment for Haptics in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Quadcopters have been used as hovering encountered-type haptic devices in virtual reality. We suggest that quadcopters can facilitate rich haptic interactions beyond force feedback by appropriating physical objects and the environment. We present HoverHaptics, an autonomous safe-to-touch quadcopter and its integration with a virtual shopping experience. HoverHaptics highlights three affordances of quadcopters that enable these rich haptic interactions: (1) dynamic positioning of passive haptics, (2) texture mapping, and (3) animating passive props. We identify inherent challenges of hovering encountered-type haptic devices, such as their limited speed, inadequate control accuracy, and safety concerns. We then detail our approach for tackling these challenges, including the use of display techniques, visuo-haptic illusions, and collision avoidance. We conclude by describing a preliminary study (n = 9) to better understand the subjective user experience when interacting with a quadcopter in virtual reality using these techniques.",virtual reality; haptics; drone; human-drone interaction; quadcopter; encountered-type; robotic graphics; uav,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Using Presence Questionnaires in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Virtual Reality (VR) is gaining increasing importance in science, education, and entertainment. A fundamental characteristic of VR is creating presence, the experience of 'being' or 'acting', when physically situated in another place. Measuring presence is vital for VR research and development. It is typically repeatedly assessed through questionnaires completed after leaving a VR scene. Requiring participants to leave and re-enter the VR costs time and can cause disorientation. In this paper, we investigate the effect of completing presence questionnaires directly in VR. Thirty-six participants experienced two immersion levels and filled three standardized presence questionnaires in the real world or VR. We found no effect on the questionnaires' mean scores; however, we found that the variance of those measures significantly depends on the realism of the virtual scene and if the subjects had left the VR. The results indicate that, besides reducing a study's duration and reducing disorientation, completing questionnaires in VR does not change the measured presence but can increase the consistency of the variance.",evaluation; virtual reality; questionnaire; presence,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Examining Augmented Virtuality Impairment Simulation for Mobile App Accessibility Design,CHI - Human Factors in Computing Systems,A*,"With mobile apps rapidly permeating all aspects of daily living with use by all segments of the population, it is crucial to support the evaluation of app usability for specific impaired users to improve app accessibility. In this work, we examine the effects of using our augmented virtuality impairment simulation system–Empath-D–to support experienced designer-developers to redesign a mockup of commonly used mobile application for cataract-impaired users, comparing this with existing tools that aid designing for accessibility. We show that the use of augmented virtuality for assessing usability supports enhanced usability challenge identification, finding more defects and doing so more accurately than with existing methods. Through our user interviews, we also show that augmented virtuality impairment simulation supports realistic interaction and evaluation to provide a concrete understanding over the usability challenges that impaired users face, and complements the existing guidelines-based approaches meant for general accessibility.",accessibility; virtual reality; augmented virtuality; empathetic design; mobile app design,Keywords,
ACM DL,conferencePaper,2019,Affinity Lens: Data-Assisted Affinity Diagramming with Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Despite the availability of software to support Affinity Diagramming (AD), practitioners still largely favor physical sticky-notes. Physical notes are easy to set-up, can be moved around in space and offer flexibility when clustering un-structured data. However, when working with mixed data sources such as surveys, designers often trade off the physicality of notes for analytical power. We propose AffinityLens, a mobile-based augmented reality (AR) application for Data-Assisted Affinity Diagramming (DAAD). Our application provides just-in-time quantitative insights overlaid on physical notes. Affinity Lens uses several different types of AR overlays (called lenses) to help users find specific notes, cluster information, and summarize insights from clusters. Through a formative study of AD users, we developed design principles for data-assisted AD and an initial collection of lenses. Based on our prototype, we find that Affinity Lens supports easy switching between qualitative and quantitative 'views' of data, without surrendering the lightweight benefits of existing AD practice.",augmented reality; visual analytics; affinity diagramming,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Exploring Interaction Fidelity in Virtual Reality: Object Manipulation and Whole-Body Movements,CHI - Human Factors in Computing Systems,A*,"High degrees of interaction fidelity (IF) in virtual reality (VR) are said to improve user experience and immersion, but there is also evidence of low IF providing comparable experiences. VR games are now increasingly prevalent, yet we still do not fully understand the trade-off between realism and abstraction in this context. We conducted a lab study comparing high and low IF for object manipulation tasks in a VR game. In a second study, we investigated players' experiences of IF for whole-body movements in a VR game that allowed players to crawl underneath virtual boulders and ""dangle” along monkey bars. Our findings show that high IF is preferred for object manipulation, but for whole-body movements, moderate IF can suffice, as there is a trade-off with usability and social factors. We provide guidelines for the development of VR games based on our results.",virtual reality; player experience; games; interaction fidelity; virtual objects; whole body interaction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Exploring Media Capture of Meaningful Experiences to Support Families Living with Dementia,CHI - Human Factors in Computing Systems,A*,"Although designing interactive media experiences for people with dementia has become a growing interest in HCI, a strong focus on family members has rarely been recognised as worthy of design intervention. This paper presents a research through design (RTD) approach working closely with families living with dementia in order to create personalised media experiences. Three families took part in day trips, which they co-planned, with data collection during these days providing insights into their shared social experiences. Workshops were also held in order to personalise the experience of the media created during these days out. Our qualitative analysis outlines themes focusing on individuality, relationships, and accepting changed realities. Furthermore, we outline directions for future research focusing on designing for contested realities, the personhood of carers, and the ageing body and immersion.",virtual reality; media; personalisation; dementia; care; family; research through design; experience-centered design,Keywords,
ACM DL,conferencePaper,2019,Detecting Visuo-Haptic Mismatches in Virtual Reality using the Prediction Error Negativity of Event-Related Brain Potentials,CHI - Human Factors in Computing Systems,A*,"Designing immersion is the key challenge in virtual reality; this challenge has driven advancements in displays, rendering and recently, haptics. To increase our sense of physical immersion, for instance, vibrotactile gloves render the sense of touching, while electrical muscle stimulation (EMS) renders forces. Unfortunately, the established metric to assess the effectiveness of haptic devices relies on the user's subjective interpretation of unspecific, yet standardized, questions.Here, we explore a new approach to detect a conflict in visuo-haptic integration (e.g., inadequate haptic feedback based on poorly configured collision detection) using electroencephalography (EEG). We propose analyzing event-related potentials (ERPs) during interaction with virtual objects. In our study, participants touched virtual objects in three conditions and received either no haptic feedback, vibration, or vibration and EMS feedback. To provoke a brain response in unrealistic VR interaction, we also presented the feedback prematurely in 25% of the trials.We found that the early negativity component of the ERP (so called prediction error) was more pronounced in the mismatch trials, indicating we successfully detected haptic conflicts using our technique. Our results are a first step towards using ERPs to automatically detect visuo-haptic mismatches in VR, such as those that can cause a loss of the user's immersion.",virtual reality; force feedback; eeg; elecrical muscle stimulation; erp; prediction error,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Around the (Virtual) World: Infinite Walking in Virtual Reality Using Electrical Muscle Stimulation,CHI - Human Factors in Computing Systems,A*,"Virtual worlds are infinite environments in which the user can move around freely. When shifting from controller-based movement to regular walking as an input, the limitation of the real world also limits the virtual world. Tackling this challenge, we propose the use of electrical muscle stimulation to limit the necessary real-world space to create an unlimited walking experience. We thereby actuate the users` legs in a way that they deviate from their straight route and thus, walk in circles in the real world while still walking straight in the virtual world. We report on a study comparing this approach to vision shift - the state of the art approach - as well as combining both approaches. The results show that particularly combining both approaches yield high potential to create an infinite walking experience.",virtual reality; locomotion; walking; electrical muscle stimulation,Keywords_Title,
ACM DL,conferencePaper,2019,Quantitative Measurement of Tool Embodiment for Virtual Reality Input Alternatives,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) strives to replicate the sensation of the physical environment by mimicking people's perceptions and experience of being elsewhere. These experiences are of-ten mediated by the objects and tools we interact with in the virtual world (e.g., a controller). Evidence from psychology posits that when using the tool proficiently, it becomes em-bodied (i.e., an extension of one's body). There is little work,however, on how to measure this phenomenon in VR, andon how different types of tools and controllers can affect the experience of interaction. In this work, we leverage cognitive psychology and philosophy literature to construct the Locus-of-Attention Index (LAI), a measure of tool embodiment. We designed and conducted a study that measures readiness-to-hand and unreadiness-to-hand for three VR interaction techniques: hands, a physical tool, and a VR controller. The study shows that LAI can measure differences in embodiment with working and broken tools and that using the hand directly results in more embodiment than using controllers.",virtual reality; embodied interaction; tools; ready-to-hand; unready-to-hand,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,DMove: Directional Motion-based Interaction for Augmented Reality Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"We present DMove, directional motion-based interaction for Augmented Reality (AR) Head-Mounted Displays (HMDs) that is both hands- and device-free. It uses directional walk-ing as a way to interact with virtual objects. To use DMove, a user needs to perform directional motions such as mov-ing one foot forward or backward. In this research, we first investigate the recognition accuracy of the motion direc-tions of our method and the social acceptance of this type of interactions together with users' comfort rating for each direction. We then optimize its design and conduct a sec-ond study to compare DMove in task performance and user preferences (workload, motion sickness, user experience), with two approaches-Hand interaction (Meta 2-like) and Head+Hand interaction (HoloLens-like) for menu selection tasks. Based on the results of these two studies, we provide a set of guidelines for DMove and further demonstrate two applications that utilize directional motions.",augmented reality; head-mounted display; menu selection; motion direction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Augmented Reality Views for Occluded Interaction,CHI - Human Factors in Computing Systems,A*,"We rely on our sight when manipulating objects. When objects are occluded, manipulation becomes difficult. Such occluded objects can be shown via augmented reality to re-enable visual guidance. However, it is unclear how to do so to best support object manipulation. We compare four views of occluded objects and their effect on performance and satisfaction across a set of everyday manipulation tasks of varying complexity. The best performing views were a see-through view and a displaced 3D view. The former enabled participants to observe the manipulated object through the occluder, while the latter showed the 3D view of the manipulated object offset from the object's real location. The worst performing view showed remote imagery from a simulated hand-mounted camera. Our results suggest that alignment of virtual objects with their real-world location is less important than an appropriate point-of-view and view stability.",augmented reality; finger-camera; manipulation task,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Face and Ecological Validity in Simulations: Lessons from Search-and-Rescue HRI,CHI - Human Factors in Computing Systems,A*,"In fields where in situ performance cannot be measured, ecological validity is difficult to estimate. Drawing on theory from social psychology and virtual reality, we argue that face validity can be a useful proxy for ecological validity. We provide illustrative examples of this relationship from work in search-and-rescue HRI, and conclude with some practical guidelines for the construction of immersive simulations in general.",virtual reality; immersion; human-robot interaction; presence; ecological validity; search and rescue; dependents; emergence; experimental realism; face validity; mundane realism; transposition,Abstract_Keywords,
ACM DL,conferencePaper,2019,"PaCaPa: A Handheld VR Device for Rendering Size, Shape, and Stiffness of Virtual Objects in Tool-based Interactions",CHI - Human Factors in Computing Systems,A*,"We present PaCaPa, a handheld device that renders haptics on a user's palm when the user interacts with virtual objects using virtual tools such as a stick. PaCaPa is a cuboid device with two wings that open and close. As the user's stick makes contact with a virtual object, the wings open by a specific degree to dynamically change the pressure on the palm and fingers. The open angle of the wings is calculated from the angle between the virtual stick and hand direction. As the stick bites into the target object, a large force is generated. Our device enables three kinds of renderings: size, shape, and stiffness. We conducted user studies to evaluate the performance of our device. We also evaluated our device in two application scenarios. User feedback and qualitative ratings indicated that our device can make indirect interaction with handheld tools more realistic.",virtual reality; haptics; tool-based interaction,Keywords,
ACM DL,conferencePaper,2019,Leveraging Distal Vibrotactile Feedback for Target Acquisition,CHI - Human Factors in Computing Systems,A*,"Many touch based interactions provide limited opportunities for direct tactile feedback; examples include multi-user touch displays, augmented reality based projections on passive surfaces, and mid-air input. In this paper, we consider distal feedback, through vibrotactile stimulation on a smart-watch placed on the user's non-dominant wrist, as an alternative feedback mechanism to interaction location vibrotactile feedback, under the user's finger. We compare the effectiveness of interaction location feedback vs. distal feedback through a Fitts's Law task completed on a smartphone. Results show that distal and interaction location feedback both reduce errors in target acquisition and exhibit statistically comparable performance, suggesting that distal vibrotactile feedback is a suitable alternative when interaction location feedback is not readily available.",vibrotactile feedback; smartwatch; touchscreen; targeting,Abstract,
ACM DL,conferencePaper,2019,"Springlets: Expressive, Flexible and Silent On-Skin Tactile Interfaces",CHI - Human Factors in Computing Systems,A*,"We introduce Springlets, expressive, non-vibrating mechanotactile interfaces on the skin. Embedded with shape memory alloy springs, we implement Springlets as thin and flexible stickers to be worn on various body locations, thanks to their silent operation even on the neck and head. We present a technically simple and rapid technique for fabricating a wide range of Springlet interfaces and computer-generated tactile patterns. We developed Springlets for six tactile primitives: pinching, directional stretching, pressing, pulling, dragging, and expanding. A study placing Springlets on the arm and near the head demonstrates Springlets' effectiveness and wearability in both stationary and mobile situations. We explore new interactive experiences in tactile social communication, physical guidance, health interfaces, navigation, and virtual reality gaming, enabled by Springlets' unique and scalable form factor.",wearable computing; fabrication; shape-changing; shape memory alloys; tactile display; on-body interaction,Abstract,
ACM DL,conferencePaper,2019,"Behind the Curtain of the ""Ultimate Empathy Machine"": On the Composition of Virtual Reality Nonfiction Experiences",CHI - Human Factors in Computing Systems,A*,"Virtual Reality nonfiction (VRNF) is an emerging form of immersive media experience created for consumption using panoramic ""Virtual Reality"" headsets. VRNF promises nonfiction content producers the potential to create new ways for audiences to experience ""the real""; allowing viewers to transition from passive spectators to active participants. Our current project is exploring VRNF through a series of ethnographic and experimental studies. In order to document the content available, we embarked on an analysis of VR documentaries produced to date. In this paper, we present an analysis of a representative sample of 150 VRNF titles released between 2012-2018. We identify and quantify 64 characteristics of the medium over this period, discuss how producers are exploiting the affordances of VR, and shed light on new audience roles. Our findings provide insight into the current state of the art in VRNF and provide a digital resource for other researchers in this area.",virtual reality; interaction; immersive media; nonfiction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,"Online, VR, AR, Lab, and In-Situ: Comparison of Research Methods to Evaluate Smart Artifacts",CHI - Human Factors in Computing Systems,A*,"Empirical studies are a cornerstone of HCI research. Technical progress constantly enables new study methods. Online surveys, for example, make it possible to collect feedback from remote users. Progress in augmented and virtual reality enables to collect feedback with early designs. In-situ studies enable researchers to gather feedback in natural environments. While these methods have unique advantages and disadvantages, it is unclear if and how using a specific method affects the results. Therefore, we conducted a study with 60 participants comparing five different methods (online, virtual reality, augmented reality, lab setup, and in-situ) to evaluate early prototypes of smart artifacts. We asked participants to assess four different smart artifacts using standardized questionnaires. We show that the method significantly affects the study result and discuss implications for HCI research. Finally, we highlight further directions to overcome the effect of the used methods.",surveys; user studies; empirical methods; prototype evaluation; smart artifacts,Abstract,
ACM DL,conferencePaper,2019,I'm a Giant: Walking in Large Virtual Environments at High Speed Gains,CHI - Human Factors in Computing Systems,A*,"Advances in tracking technology and wireless headsets enable walking as a means of locomotion in Virtual Reality. When exploring virtual environments larger than room-scale, it is often desirable to increase users' perceived walking speed, for which we investigate three methods. (1) Ground-Level Scaling increases users' avatar size, allowing them to walk farther. (2) Eye-Level Scaling enables users to walk through a World in Miniature, while maintaining a street-level view. (3) Seven-League Boots amplifies users' movements along their walking path. We conduct a study comparing these methods and find that users feel most embodied using Ground-Level Scaling and consequently increase their stride length. Using Seven-League Boots, unlike the other two methods, diminishes positional accuracy at high gains, and users modify their walking behavior to compensate for the lack of control. We conclude with a discussion on each technique's strength and weaknesses and the types of situation they might be appropriate for.",virtual reality; locomotion; body scale; seven-league boots; translationalgain; walking speed; world in miniuature,Abstract_Keywords,
ACM DL,conferencePaper,2019,Personalising the TV Experience using Augmented Reality: An Exploratory Study on Delivering Synchronised Sign Language Interpretation,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) technology has the potential to extend the screen area beyond the rigid frames of televisions. The additional display area can be used to augment televisions (TVs) with extra information tailored to individuals, for instance, the provision of access services like sign language interpretations. We invited 23 (11 in the UK, 12 in Germany) users of signed content to evaluate three methods of watching a sign language interpreted programme - one traditional in-vision method with signed programme content on TV and two AR-enabled methods in which an AR sign language interpreter (a 'half-body' version and a 'full-body' version) is projected just outside the frame of the TV presenting the programme. In the UK, participants were split 3-ways in their preferences while in Germany, half the participants preferred the traditional method followed closely by the 'half-body' version. We discuss our participants reasoning behind their preferences and implications for future research.",augmented reality; accessibility; hololens; sign language; personalisation; interaction techniques; television; bsl; companion screen; connected experiences; dgs; hbbtv 2.0; second screen; sse; synchronisation,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,FTVR in VR: Evaluation of 3D Perception With a Simulated Volumetric Fish-Tank Virtual Reality Display,CHI - Human Factors in Computing Systems,A*,"Spherical fish tank virtual reality (FTVR) displays attempt to create a virtual ""crystal ball"" experience using head-tracked rendering. Almost all of these systems have omitted stereo cues, making them easy to build, but it is not clear how much this omission degrades the 3D experience. In this study, we evaluate performance and subjective effects of stereo on 3D perception and interaction tasks with a spherical FTVR display. To control for calibration error and tracking latency, we perform the evaluation on a simulated spherical display in VR. The results of our study provide a clear recommendation for the design and use of spherical FTVR displays: while omitting stereo may not be readily apparent for users, their performance will be significantly degraded (20% - 91% increase in median task time). Therefore, including stereo viewing in spherical displays is critical for use in FTVR.",3d perception; fish tank virtual reality; spherical display,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,What is Mixed Reality?,CHI - Human Factors in Computing Systems,A*,"What is Mixed Reality (MR)? To revisit this question given the many recent developments, we conducted interviews with ten AR/VR experts from academia and industry, as well as a literature survey of 68 papers. We find that, while there are prominent examples, there is no universally agreed on, one-size-fits-all definition of MR. Rather, we identified six partially competing notions from the literature and experts' responses. We then started to isolate the different aspects of reality relevant for MR experiences, going beyond the primarily visual notions and extending to audio, motion, haptics, taste, and smell. We distill our findings into a conceptual framework with seven dimensions to characterize MR applications in terms of the number of environments, number of users, level of immersion, level of virtuality, degree of interaction, input, and output. Our goal with this paper is to support classification and discussion of MR applications' design and provide a better means to researchers to contextualize their work within the increasingly fragmented MR landscape.",augmented reality; virtual reality; mixed reality; literature review; expert interviews; conceptual framework; taxonomy,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,What Can We Learn from Augmented Reality (AR)? Benefits and Drawbacks of AR for Inquiry-based Learning of Physics,CHI - Human Factors in Computing Systems,A*,"Emerging technologies such as Augmented Reality (AR), have the potential to radically transform education by making challenging concepts visible and accessible to novices. In this project, we have designed a Hololens-based system in which collaborators are exposed to an unstructured learning activity in which they learned about the invisible physics involved in audio speakers. They learned topics ranging from spatial knowledge, such as shape of magnetic fields, to abstract conceptual knowledge, such as relationships between electricity and magnetism. We compared participants' learning, attitudes and collaboration with a tangible interface through multiple experimental conditions containing varying layers of AR information. We found that educational AR representations were beneficial for learning specific knowledge and increasing participants' self-efficacy (i.e., their ability to learn concepts in physics). However, we also found that participants in conditions that did not contain AR educational content, learned some concepts better than other groups and became more curious about physics. We discuss learning and collaboration differences, as well as benefits and detriments of implementing augmented reality for unstructured learning activities.",augmented reality; collaborative learning; physics education,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,VRsneaky: Increasing Presence in VR Through Gait-Aware Auditory Feedback,CHI - Human Factors in Computing Systems,A*,"While Virtual Reality continues to increase in fidelity, it remains an open question how to effectively reflect the user's movements and provide congruent feedback in virtual environments. We present VRsneaky, a system for producing auditory movement feedback, which helps participants orient themselves in a virtual environment by providing footstep sounds. The system reacts to the user's specific gait features and adjusts the audio accordingly. In a user study with 28 participants, we found that VRsneaky increases users' sense of presence as well as awareness of their own posture and gait. Additionally, we find that increasing auditory realism significantly influences certain characteristics of participants' gait. Our work shows that gait-aware audio feedback is a means to increase presence in virtual environments. We discuss opportunities and design requirements for future scenarios where users walk through immersive virtual worlds.",virtual reality; presence; auditory feedback; gait awareness,Abstract_Keywords,
ACM DL,conferencePaper,2019,"NVGaze: An Anatomically-Informed Dataset for Low-Latency, Near-Eye Gaze Estimation",CHI - Human Factors in Computing Systems,A*,"Quality, diversity, and size of training data are critical factors for learning-based gaze estimators. We create two datasets satisfying these criteria for near-eye gaze estimation under infrared illumination: a synthetic dataset using anatomically-informed eye and face models with variations in face shape, gaze direction, pupil and iris, skin tone, and external conditions (2M images at 1280x960), and a real-world dataset collected with 35 subjects (2.5M images at 640x480). Using these datasets we train neural networks performing with sub-millisecond latency. Our gaze estimation network achieves 2.06(±0.44)° of accuracy across a wide 30°×40° field of view on real subjects excluded from training and 0.5° best-case accuracy (across the same FOV) when explicitly trained for one real subject. We also train a pupil localization network which achieves higher robustness than previous methods.",machine learning; virtual reality; eye tracking; dataset,Keywords,
ACM DL,conferencePaper,2019,Pose-Guided Level Design,CHI - Human Factors in Computing Systems,A*,"Player's physical experience is a critical factor to consider in designing motion-based games that are played through motion sensor gaming consoles or virtual reality devices. However, adjusting the physical challenge involved in a motion-based game is difficult and tedious, as it is typically done manually by level designers on a trial-and-error basis. In this paper, we propose a novel approach for automatically synthesizing levels for motion-based games that can achieve desired physical movement goals. By formulating the level design problem as a trans-dimensional optimization problem which is solved by a reversible-jump Markov chain Monte Carlo technique, we show that our approach can automatically synthesize a variety of game levels, each carrying the desired physical movement properties. To demonstrate the generality of our approach, we synthesize game levels for two different types of motion-based games and conduct a user study to validate the effectiveness of our approach.",optimization; level design; generative design; exergaming,Abstract,
ACM DL,conferencePaper,2019,Investigating Implicit Gender Bias and Embodiment of White Males in Virtual Reality with Full Body Visuomotor Synchrony,CHI - Human Factors in Computing Systems,A*,"Previous research has shown that when White people embody a black avatar in virtual reality (VR) with full body visuomotor synchrony, this can reduce their implicit racial bias. In this paper, we put men in female and male avatars in VR with full visuomotor synchrony using wearable trackers and investigated implicit gender bias and embodiment. We found that participants embodied in female avatars displayed significantly higher levels of implicit gender bias than those embodied in male avatars. The implicit gender bias actually increased after exposure to female embodiment in contrast to male embodiment. Results also showed that participants felt embodied in their avatars regardless of gender matching, demonstrating that wearable trackers can be used for a realistic sense of avatar embodiment in VR. We discuss the future implications of these findings for both VR scenarios and embodiment technologies.",virtual reality; embodied avatars; implicit association test; implicit gender bias,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Shaping Pro-Social Interaction in VR: An Emerging Design Framework,CHI - Human Factors in Computing Systems,A*,"Commercial social VR applications represent a diverse and evolving ecology with competing models of what it means to be social in VR. Drawing from expert interviews, this paper examines how the creators of different social VR applications think about how their platforms frame, support, shape, or constrain social interaction. The study covers a range of applications including: Rec Room, High Fidelity, VRChat, Mozilla Hubs, Altspace VR, AnyLand, and Facebook Spaces. We contextualize design choices underlying these applications, with particular attention paid to the ways that industry experts perceive, and seek to shape, the relationship between user experiences and design choices. We underscore considerations related to: (1) aesthetics of place (2) embodied affordances, (3) social mechanics, (4) and tactics for shaping social norms and mitigating harassment. Drawing on this analysis, we discuss the stakes of these choices, suggest future research directions, and propose an emerging design framework for shaping pro-social behavior in VR.",virtual reality; social vr; pro-social interaction,Keywords,
ACM DL,conferencePaper,2019,Tool Extension in Human-Computer Interaction,CHI - Human Factors in Computing Systems,A*,"Tool use extends people's representations of the immediately actionable space around them. Physical tools thereby become integrated in people's body schemas. We introduce a measure for tool extension in HCI by using a visual-tactile interference paradigm. In this paradigm, an index of tool extension is given by response time differences between crossmodally congruent and incongruent stimuli; tactile on the hand and visual on the tool. We use this measure to examine if and how findings on tool extension apply to interaction with computer-based tools. Our first experiment shows that touchpad and mouse both provide tool extension over a baseline condition without a tool. A second experiment shows a higher degree of tool extension for a realistic avatar hand compared to an abstract pointer for interaction in virtual reality. In sum, our measure can detect tool extension with computer-based tools and differentiate interfaces by their degree of extension.",input; body schema; peripersonal space; tool extension,Abstract,
ACM DL,conferencePaper,2019,Participatory Design of VR Scenarios for Exposure Therapy,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) applications for exposure therapy predominantly use computer-generated imagery to create controlled environments in which users can be exposed to their fears. Creating 3D animations, however, is demanding and time-consuming. This paper presents a participatory approach for prototyping VR scenarios that are enabled by 360° video and grounded in lived experiences. We organized a participatory workshop with adolescents to prototype such scenarios, consisting of iterative phases of ideation, storyboarding, live-action plays recorded by a 360° camera, and group evaluation. Through an analysis of the participants' interactions, we outline how they worked to design prototypes that depict situations relevant to those with a fear of public speaking. Our analysis also explores how participants used their experiences and reflections as resources for design. Six clinical psychologists evaluated the prototypes from the workshop and concluded they were viable therapeutic tools, emphasizing the immersive, realistic experience they presented. We argue that our approach makes the design of VR scenarios more accessible.",virtual reality; workshop; participatory design; exposure therapy,Abstract_Keywords,
ACM DL,conferencePaper,2019,Abstract Machines: Overlaying Virtual Worlds on Physical Rides,CHI - Human Factors in Computing Systems,A*,"Overlaying virtual worlds onto existing physical rides and altering the sensations of motion can deliver new experiences of thrill, but designing how motion is mapped between physical ride and virtual world is challenging. In this paper, we present the notion of an abstract machine, a new form of intermediate design knowledge that communicates motion mappings at the level of metaphor, mechanism and implementation. Following a performance-led, in-the-wild approach we report lessons from creating and touring VR Playground, a ride that overlays four distinct abstract machines and virtual worlds on a playground swing. We compare the artist's rationale with riders' reported experiences and analysis of their physical behaviours to reveal the distinct thrills of each abstract machine. Finally, we discuss how to make and use abstract machines in terms of heuristics for designing motion mappings, principles for virtual world design and communicating experiences to riders.",virtual reality; design knowledge; abstract machines; rides; thrill; visual kinaesthetic experiences,Keywords,
ACM DL,conferencePaper,2019,TrackCap: Enabling Smartphones for 3D Interaction on Mobile Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"The latest generation of consumer market Head-mounted displays (HMD) now include self-contained inside-out tracking of head motions, which makes them suitable for mobile applications. However, 3D tracking of input devices is either not included at all or requires to keep the device in sight, so that it can be observed from a sensor mounted on the HMD. Both approaches make natural interactions cumbersome in mobile applications. TrackCap, a novel approach for 3D tracking of input devices, turns a conventional smartphone into a precise 6DOF input device for an HMD user. The device can be conveniently operated both inside and outside the HMD's field of view, while it provides additional 2D input and output capabilities.",augmented reality; mobile devices; mixed reality; wearable computing; input devices; 3d pointing; hmd,Keywords,
ACM DL,conferencePaper,2019,Group Interactions in Location-Based Gaming: A Case Study of Raiding in Pokémon GO,CHI - Human Factors in Computing Systems,A*,"Raiding is a format in digital gaming that requires groups of people to collaborate and/or compete for a common goal. In 2017, the raiding format was introduced in the location-based mobile game Pokémon GO, which offers a mixed reality experience to friends and strangers coordinating for in-person raids. To understand this technology-mediated social phenomenon, we conducted over a year of participant observations, surveys with 510 players, and interviews with 25 players who raid in Pokémon GO. Using the analytical lens of Arrow, McGrath, and Berdahl's theory of small groups as complex systems, we identify global, local, and contextual dynamics in location-based raiding that support and challenge ad-hoc group formation in real life. Based on this empirical and theoretical understanding, we discuss implications to design for transparency, social affordances, and bridging gaps between global and contextual dynamics for increased positive and inclusive community interactions.",groups; location based game; mixed reality game; raiding,Abstract_Keywords,
ACM DL,conferencePaper,2019,360proto: Making Interactive Virtual Reality &amp; Augmented Reality Prototypes from Paper,CHI - Human Factors in Computing Systems,A*,"We explore 360 paper prototyping to rapidly create AR/VR prototypes from paper and bring them to life on AR/VR devices. Our approach is based on a set of emerging paper prototyping templates specifically for AR/VR. These templates resemble the key components of many AR/VR interfaces, including 2D representations of immersive environments, AR marker overlays and face masks, VR controller models and menus, and 2D screens and HUDs. To make prototyping with these templates effective, we developed 360proto, a suite of three novel physical–digital prototyping tools: (1) the 360proto Camera for capturing paper mockups of all components simply by taking a photo with a smartphone and seeing 360-degree panoramic previews on the phone or stereoscopic previews in Google Cardboard; (2) the 360proto Studio for organizing and editing captures, for composing AR/VR interfaces by layering the captures, and for making them interactive with Wizard of Oz via live video streaming; (3) the 360proto App for running and testing the interactive prototypes on AR/VR capable mobile devices and headsets. Through five student design jams with a total of 86 participants and our own design space explorations, we demonstrate that our approach with 360proto is useful to create relatively complex AR/VR applications.",wizard of oz; ar/vr; physical-digital prototyping,Title,
ACM DL,conferencePaper,2019,Warping Deixis: Distorting Gestures to Enhance Collaboration,CHI - Human Factors in Computing Systems,A*,"When engaged in communication, people often rely on pointing gestures to refer to out-of-reach content. However, observers frequently misinterpret the target of a pointing gesture. Previous research suggests that to perform a pointing gesture, people place the index finger on or close to a line connecting the eye to the referent, while observers interpret pointing gestures by extrapolating the referent using a vector defined by the arm and index finger. In this paper we present Warping Deixis, a novel approach to improving the perception of pointing gestures and facilitate communication in collaborative Extended Reality environments. By warping the virtual representation of the pointing individual, we are able to match the pointing expression to the observer's perception. We evaluated our approach in a co-located side by side virtual reality scenario. Results suggest that our approach is effective in improving the interpretation of pointing gestures in shared virtual environments.",collaboration; body warping; deixis; pointing gestures,Abstract,
ACM DL,conferencePaper,2019,"Understanding Digitally-Mediated Empathy: An Exploration of Visual, Narrative, and Biosensory Informational Cues",CHI - Human Factors in Computing Systems,A*,"Digitally sharing our experiences engages a process of empathy shaped by available informational cues. Biosensory data is one informative cue, but the relationship to empathy is underexplored. In this study, we investigate this process by showing a video of a ""target” person's visual perspective watching a virtual reality film to sixty ""observers”. We vary information available to observers via three experimental conditions: a baseline unmodified video, video with narrative text, or with a graph of electrodermal activity (EDA) of the target. Compared to baseline, narrative text increased empathic accuracy (EA) while EDA had an opposite, negative effect. Qualitatively, observers describe their empathic processes as using their own feelings supplemented with the information presented depending on the interpretability of that information. Both narration and EDA prompted observers to reconsider assumptions about another's experience. Our findings lead to a discussion of digitally-mediated empathy with implications for associated research and product development.",empathy; social computing; computer-mediated communication; biosensing,Abstract,
ACM DL,conferencePaper,2019,Crossing-Based Selection with Virtual Reality Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"This paper presents the first investigation into using the goal-crossing paradigm for object selection with virtual reality (VR) head-mounted displays. Two experiments were carried out to evaluate ray-casting crossing tasks with target discs in 3D space and goal lines on 2D plane respectively in comparison to ray-casting pointing tasks. Five factors, i.e. task difficulty, the direction of movement constraint (collinear vs. orthogonal), the nature of the task (discrete vs. continuous), field of view of VR devices and target depth, were considered in both experiments. Our findings are: (1) crossing generally had shorter or no longer time, and higher or similar accuracy than pointing, indicating crossing can complement or substitute pointing; (2) crossing tasks can be well modelled with Fitts' Law; (3) crossing performance depended on target depth; (4) crossing target discs in 3D space differed from crossing goal lines on 2D plane in many aspects such as time and error performance, the effects of target depth and the parameters of Fitts' models. Based on these findings, we formulate a number of design recommendations for crossing-based interaction in VR.",pointing; fitts' law; crossing; ray-casting selection; virtual reality head-mounted displays,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,ARPen: Mid-Air Object Manipulation Techniques for a Bimanual AR System with Pen &amp; Smartphone,CHI - Human Factors in Computing Systems,A*,"Modeling in Augmented Reality (AR) lets users create and manipulate virtual objects in mid-air that are aligned to their real environment. We present ARPen, a bimanual input technique for AR modeling that combines a standard smartphone with a 3D-printed pen. Users sketch with the pen in mid-air, while holding their smartphone in the other hand to see the virtual pen traces in the live camera image. ARPen combines the pen's higher 3D input precision with the rich interactive capabilities of the smartphone touchscreen. We studied subjective preferences for this bimanual input technique, such as how people hold the smartphone while drawing, and analyzed the performance of different bimanual techniques for selecting and moving virtual objects. Users preferred a bimanual technique casting a ray through the pen tip for both selection and translation. We provide initial design guidelines for this new class of bimanual AR modeling systems.",augmented reality; translation; selection; bimanual interaction; pen; immersive modeling; mid-air interaction; smartphone ar,Abstract_Keywords,
ACM DL,conferencePaper,2019,Audible Panorama: Automatic Spatial Audio Generation for Panorama Imagery,CHI - Human Factors in Computing Systems,A*,"As 360 deg cameras and virtual reality headsets become more popular, panorama images have become increasingly ubiquitous. While sounds are essential in delivering immersive and interactive user experiences, most panorama images, however, do not come with native audio. In this paper, we propose an automatic algorithm to augment static panorama images through realistic audio assignment. We accomplish this goal through object detection, scene classification, object depth estimation, and audio source placement. We built an audio file database composed of over 500 audio files to facilitate this process. We designed and conducted a user study to verify the efficacy of various components in our pipeline. We run our method on a large variety of panorama images of indoor and outdoor scenes. By analyzing the statistics, we learned the relative importance of these components, which can be used in prioritizing for power-sensitive time-critical tasks like mobile augmented reality (AR) applications.",augmented reality; spatial audio; immersive media; panorama images; virtualreality,Abstract_Keywords,
ACM DL,conferencePaper,2019,Interactive Body-Driven Graphics for Augmented Video Performance,CHI - Human Factors in Computing Systems,A*,"We present a system that augments live presentation videos with interactive graphics to create a powerful and expressive storytelling environment. Using our system, the presenter interacts with the graphical elements in real-time with gestures and postures, thus leveraging our innate, everyday skills to enhance our communication capabilities with the audience. However, crafting such an interactive and expressive performance typically requires programming, or highly-specialized tools tailored for experts. Our core contribution is a flexible, direct manipulation UI which enables amateurs and experts to craft such presentations beforehand by mapping a variety of body movements to a wide range of graphical manipulations. By simplifying the mapping between gestures, postures, and their corresponding output effects, our UI enables users to craft customized, rich interactions with the graphical elements. Our user study demonstrates the potential usage and unique affordance of this mixed-reality medium for storytelling and presentation across a range of application domains.",user interface design; mixed/augmented reality; gestural input,Keywords,
ACM DL,conferencePaper,2019,A Design Space for Gaze Interaction on Head-mounted Displays,CHI - Human Factors in Computing Systems,A*,"Augmented and virtual reality (AR/VR) has entered the mass market and, with it, will soon eye tracking as a core technology for next generation head-mounted displays (HMDs). In contrast to existing gaze interfaces, the 3D nature of AR and VR requires estimating a user's gaze in 3D. While first applications, such as foveated rendering, hint at the compelling potential of combining HMDs and gaze, a systematic analysis is missing. To fill this gap, we present the first design space for gaze interaction on HMDs. Our design space covers human depth perception and technical requirements in two dimensions aiming to identify challenges and opportunities for interaction design. As such, our design space provides a comprehensive overview and serves as an important guideline for researchers and practitioners working on gaze interaction on HMDs. We further demonstrate how our design space is used in practice by presenting two interactive applications: EyeHealth and XRay-Vision.",augmented reality; interaction design; virtual reality; design space; head-mounted displays; gaze interaction; 3d gaze,Abstract_Keywords,
ACM DL,conferencePaper,2019,Measuring and Understanding Photo Sharing Experiences in Social Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Millions of photos are shared online daily, but the richness of interaction compared with face-to-face (F2F) sharing is still missing. While this may change with social Virtual Reality (socialVR), we still lack tools to measure such immersive and interactive experiences. In this paper, we investigate photo sharing experiences in immersive environments, focusing on socialVR. Running context mapping (N=10), an expert creative session (N=6), and an online experience clustering questionnaire (N=20), we develop and statistically evaluate a questionnaire to measure photo sharing experiences. We then ran a controlled, within-subject study (N=26 pairs) to compare photo sharing under F2F, Skype, and Facebook Spaces. Using interviews, audio analysis, and our questionnaire, we found that socialVR can closely approximate F2F sharing. We contribute empirical findings on the immersiveness differences between digital communication media, and propose a socialVR questionnaire that can in the future generalize beyond photo sharing.",virtual reality; immersion; questionnaire; presence; social; photo sharing,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Does It Feel Real? Using Tangibles with Different Fidelities to Build and Explore Scenes in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Professionals in domains like film, theater, or architecture often rely on physical models to visualize spaces. With virtual reality (VR) new tools are available providing immersive experiences with correct perceptions of depth and scale. However, these lack the tangibility of physical models. Using tangible objects in VR can close this gap but creates the challenges of producing suitable objects and interacting with them with only the virtual objects visible. This work addresses these challenges by evaluating tangibles with three haptic fidelities: equal disc-shaped tangibles for all virtual objects, Lego-built tangibles, and 3D-printed tangibles resembling the virtual shapes. We present results from a comparative study on immersion, performance, and intuitive interaction and interviews with domain experts. The results show that 3D-printed objects perform best, but Lego offers a good trade-off between fast creation of tangibles and sufficient fidelity. The experts rate our approach as useful and would use all three versions.",virtual reality; user study; previsualization; animation; tangibles; expert interview; visual tracking,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Adding Proprioceptive Feedback to Virtual Reality Experiences Using Galvanic Vestibular Stimulation,CHI - Human Factors in Computing Systems,A*,"We present a small and lightweight wearable device that enhances virtual reality experiences and reduces cybersickness by means of galvanic vestibular stimulation (GVS). GVS is a specific way to elicit vestibular reflexes that has been used for over a century to study the function of the vestibular system. In addition to GVS, we support physiological sensing by connecting heart rate, electrodermal activity and other sensors to our wearable device using a plug and play mechanism. An accompanying Android app communicates with the device over Bluetooth (BLE) for transmitting the GVS stimulus to the user through electrodes attached behind the ears. Our system supports multiple categories of virtual reality applications with different types of virtual motion such as driving, navigating by flying, teleporting, or riding. We present a user study in which participants (N = 20) experienced significantly lower cybersickness when using our device and rated experiences with GVS-induced haptic feedback as significantly more immersive than a no-GVS baseline.",interaction design; virtual reality; cybersickness; haptic feedback; wearables; galvanic vestibular stimulation,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,VibEye: Vibration-Mediated Object Recognition for Tangible Interactive Applications,CHI - Human Factors in Computing Systems,A*,"We present VibEye: a vibration-mediated recognition system of objects for tangible interaction. A user holds an object between two fingers wearing VibEye. VibEye triggers a vibration from one finger, and the vibration that has propagated through the object is sensed at the other finger. This vibration includes information about the object's identity, and we represent it using a spectrogram. Collecting the spectrograms of many objects, we formulate the object recognition problem to a classical classification problem among the images. This simple method, when tested with 20 users, shows 92.5% accuracy for 16 objects of the same shape with various materials. This material-based classifier is also extended to the recognition of everyday objects. Lastly, we demonstrate several tangible applications where VibEye provides the needed functionality while enhancing user experiences. VibEye is particularly effective for recognizing objects made of different materials, which is difficult to distinguish by other means such as light and sound.",augmented reality; virtual reality; tangible interaction; passive haptics; object recognition; vibration-based sensing,Keywords,
ACM DL,conferencePaper,2019,Geollery: A Mixed Reality Social Media Platform,CHI - Human Factors in Computing Systems,A*,"We present Geollery, an interactive mixed reality social media platform for creating, sharing, and exploring geotagged information. Geollery introduces a real-time pipeline to progressively render an interactive mirrored world with three-dimensional (3D) buildings, internal user-generated content, and external geotagged social media. This mirrored world allows users to see, chat, and collaborate with remote participants with the same spatial context in an immersive virtual environment. We describe the system architecture of Geollery, its key interactive capabilities, and our design decisions. Finally, we conduct a user study with 20 participants to qualitatively compare Geollery with another social media system, Social Street View. Based on the participants' responses, we discuss the benefits and drawbacks of each system and derive key insights for designing an interactive mirrored world with geotagged social media. User feedback from our study reveals several use cases for Geollery including travel planning, virtual meetings, and family gathering.",augmented reality; social media; virtual reality; visualization; mixed reality; 3d reconstruction; gis; 3d user interface; geographic information system; street view,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,HoloDoc: Enabling Mixed Reality Workspaces that Harness Physical and Digital Content,CHI - Human Factors in Computing Systems,A*,"Prior research identified that physical paper documents have many positive attributes, for example natural tangibility and inherent physical flexibility. When documents are presented on digital devices, however, they can provide unique functionality to users, such as the ability to search, view dynamic multimedia content, and make use of indexing. This work explores the fusion of physical and digital paper documents. It first presents the results of a study that probed how users perform document-intensive analytical tasks when both physical and digital versions of documents were available. The study findings then informed the design of HoloDoc, a mixed reality system that augments physical artifacts with rich interaction and dynamic virtual content. Finally, we present the interaction techniques that HoloDoc affords, and the results of a second study that assessed HoloDoc's utility when working with digital and physical copies of academic articles.",augmented reality; mixed reality; digital pen input; reading behavior,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,Virtual Objects in the Physical World: Relatedness and Psychological Ownership in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"As technology advances, people increasingly interact with virtual objects in settings such as augmented reality (AR) where the virtual layer is superimposed on top of the physical world. Similarly to interactions with physical objects, users may assign virtual objects with value, experience a sense of relatedness, and develop psychological ownership over these objects. The objective of this study is to understand how AR's unique characteristics influences the emergence of meaning and ownership perceptions amongst users. We conducted a study of users' interactions with a virtual dog over a three-week period, comparing AR and fully virtual settings. Our findings show that engagement with the application is a key determinant of the relation users develop with virtual objects. However, the effect of the background layer-whether physical or virtual-dominates the development of relatedness and ownership feelings, highlighting the importance of the ""real"" physical layer in shaping users' perceptions.",augmented reality; qualitative analysis; ownership; material culture; relatedness; virtual possessions,Abstract_Keywords_Title,
ACM DL,conferencePaper,2019,HapTwist: Creating Interactive Haptic Proxies in Virtual Reality Using Low-cost Twistable Artefacts,CHI - Human Factors in Computing Systems,A*,"In this paper, we present a series of studies on using Rubik's Twist, a type of low-cost twistable artefact, to create haptic proxies for various hand-graspable VR objects. Our pilot studies validated the feasibility and effectiveness of Rubik's-Twist-based haptic proxies. The pilot results also revealed user challenges in the physical shape creation, motivating the development of the HapTwist toolkit. The toolkit consists of the shape-generation algorithm, the software interface for shape-construction guidance and interaction authoring, and the hardware modules for constructing interactive haptic proxies. The user studies showed that HapTwist was easy to learn and use, and it significantly improved user performance in creating interactive haptic proxies with Rubik's Twist. Furthermore, HapTwist-generated haptic proxies achieved similar VR experience as the real objects.",virtual reality; haptics; toolkit; hand grasp; rubik's twist,Keywords_Title,
ACM DL,conferencePaper,2019,"""When the Elephant Trumps"": A Comparative Study on Spatial Audio for Orientation in 360º Videos",CHI - Human Factors in Computing Systems,A*,"Orientation is an emerging issue in cinematic Virtual Reality (VR), as viewers may fail in locating points of interest. Recent strategies to tackle this research problem have investigated the role of cues, specifically diegetic sound effects. In this paper, we examine the use of sound spatialization for orientation purposes, namely by studying different spatialization conditions (""none"", ""partial"", and ""full"" spatial manipulation) of multitrack soundtracks. We performed a between-subject mixed-methods study with 36 participants, aided by Cue Control, a tool we developed for dynamic spatial sound editing and data collection/analysis. Based on existing literature on orientation cues in 360º and theories on human listening, we discuss situations in which the spatialization was more effective (namely, ""full"" spatial manipulation both when using only music and when combining music and diegetic effects), and how this can be used by creators of 360º videos.",spatial audio; orientation; 360º video; cinematic virtual reality; virtual audio spaces,Abstract_Keywords,
ACM DL,conferencePaper,2019,Egocentric Smaller-person Experience through a Change in Visual Perspective,CHI - Human Factors in Computing Systems,A*,"This paper explores how human perceptions, actions, and interactions can be changed through an embodied and active experience of being a smaller person in a real-world environment, which we call an egocentric smaller person experience. We developed a wearable visual translator that provides the perspective of a smaller person by shifting the wearer's eyesight level down to their waist using a head-mounted display and a stereo camera module, while allowing for field of view control through head movements. In this study, we investigated how the developed device can modify the wearer's body representation and experiences based on a field study conducted at a nursing school and museums, and through lab studies. It was observed that the participants changed their perceptions, actions, and interactions because they are considered to have perceived themselves as being smaller. Using this device, designers and teachers can understand the perspectives of other people in an existing environment.",virtual reality; embodied interaction; wearable device; body representation; egocentric experience,Keywords,
ACM DL,conferencePaper,2018,MABLE: Mediating Young Children's Smart Media Usage with Augmented Reality,CHI - Human Factors in Computing Systems,A*,"There has been a growing concern over the huge increase in use of smart media by young children. This study explores the possibility of using augmented-reality(AR) for regulat-ing preschoolers' media usage behavior. With MABLE (mobile application for behavioral learning and education), parents can provide AR-assisted feedback by changing facial expressions and sound effects. When overlaying a smart media, which has MABLE running, in front of a QR marker on a puppet, a facial expression is displayed on top of the puppet's face. A two-week long experiment with 36 parent-child pairs showed that compared to using just the puppet, using MABLE showed higher amount of engage-ment among preschoolers. For the effectiveness of parental mediation in terms of self-control, our data showed mixed results. MABLE had positive effects in that the amount of rule-compliance increased and problematic behaviors de-creased, whereas the level of behavioral dependency on smart media was not influenced.",augmented reality (ar); engagement; parental medi-ation; preschoolers; rule compliance; smart media usage,Keywords_Title,
ACM DL,conferencePaper,2018,Customizing Hybrid Products,CHI - Human Factors in Computing Systems,A*,"We explore how the convergence of the digital and physical into hybrid products leads to new possibilities for customization. We report on a technology probe, a hybrid advent calendar with both paper form and digital layers of content, both of which were designed to be customizable. We reveal how over two hundred active users adapted its physical and digital aspects in various ways, some anticipated and familiar, but others surprising. This leads us to contribute concepts to help understand and design for hybrid customization – the idea of broad customization spanning physical and digital; end-to-end customization by different stakeholders along the value chain for a product; and the combination of these into customization maps.",augmented reality; customisation; gifting; hybrid products; internet of things; personalisation,Keywords,
ACM DL,conferencePaper,2018,Eyes-Free Target Acquisition in Interaction Space around the Body for Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Eyes-free target acquisition is a basic and important human ability to interact with the surrounding physical world, relying on the sense of space and proprioception. In this research, we leverage this ability to improve interaction in virtual reality (VR), by allowing users to acquire a virtual object without looking at it. We expect this eyes-free approach can effectively reduce head movements and focus changes, so as to speed up the interaction and alleviate fatigue and VR sickness. We conduct three lab studies to progressively investigate the feasibility and usability of eyes-free target acquisition in VR. Results show that, compared with the eyes-engaged manner, the eyes-free approach is significantly faster, provides satisfying accuracy, and introduces less fatigue and sickness; Most participants (13/16) prefer this approach. We also measure the accuracy of motion control and evaluate subjective experience of users when acquiring targets at different locations around the body. Based on the results, we make suggestions on designing appropriate target layout and discuss several design issues for eyes-free target acquisition in VR.",eyes-free; proprioception; target acquisition; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Mini-Me: An Adaptive Avatar for Mixed Reality Remote Collaboration,CHI - Human Factors in Computing Systems,A*,"We present Mini-Me, an adaptive avatar for enhancing Mixed Reality (MR) remote collaboration between a local Augmented Reality (AR) user and a remote Virtual Reality (VR) user. The Mini-Me avatar represents the VR user's gaze direction and body gestures while it transforms in size and orientation to stay within the AR user's field of view. A user study was conducted to evaluate Mini-Me in two collaborative scenarios: an asymmetric remote expert in VR assisting a local worker in AR, and a symmetric collaboration in urban planning. We found that the presence of the Mini-Me significantly improved Social Presence and the overall experience of MR collaboration.",augmented reality; avatar; awareness; gaze; gesture; mixed reality; redirected; remote collaboration; remote embodiment; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,FaceDisplay: Towards Asymmetric Multi-User Interaction for Nomadic Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Mobile VR HMDs enable scenarios where they are being used in public, excluding all the people in the surrounding (Non-HMD Users) and reducing them to be sole bystanders. We present FaceDisplay, a modified VR HMD consisting of three touch sensitive displays and a depth camera attached to its back. People in the surrounding can perceive the virtual world through the displays and interact with the HMD user via touch or gestures. To further explore the design space of FaceDisplay, we implemented three applications (FruitSlicer, SpaceFace and Conductor) each presenting different sets of aspects of the asymmetric co-located interaction (e.g. gestures vs touch). We conducted an exploratory user study (n=16), observing pairs of people experiencing two of the applications and showing a high level of enjoyment and social interaction with and without an HMD. Based on the findings we derive design considerations for asymmetric co-located VR applications and argue that VR HMDs are currently designed having only the HMD user in mind but should also include Non-HMD Users.",asymmetric virtual reality; co-located multiplayer; multi-user virtual reality; nomadic virtual reality,Keywords_Title,
ACM DL,conferencePaper,2018,Depth Conflict Reduction for Stereo VR Video Interfaces,CHI - Human Factors in Computing Systems,A*,"Applications for viewing and editing 360° video often render user interface (UI) elements on top of the video. For stereoscopic video, in which the perceived depth varies over the image, the perceived depth of the video can conflict with that of the UI elements, creating discomfort and making it hard to shift focus. To address this problem, we explore two new techniques that adjust the UI rendering based on the video content. The first technique dynamically adjusts the perceived depth of the UI to avoid depth conflict, and the second blurs the video in a halo around the UI. We conduct a user study to assess the effectiveness of these techniques in two stereoscopic VR video tasks: video watching with subtitles, and video search.",360; stereoscopic; subtitles; video interface; virtual reality,Keywords,
ACM DL,conferencePaper,2018,VirtualGrasp: Leveraging Experience of Interacting with Physical Objects to Facilitate Digital Object Retrieval,CHI - Human Factors in Computing Systems,A*,"We propose VirtualGrasp, a novel gestural approach to retrieve virtual objects in virtual reality. Using VirtualGrasp, a user retrieves an object by performing a barehanded gesture as if grasping its physical counterpart. The object-gesture mapping under this metaphor is of high intuitiveness, which enables users to easily discover, remember the gestures to retrieve the objects. We conducted three user studies to demonstrate the feasibility and effectiveness of the approach. Progressively, we investigated the consensus of the object-gesture mapping across users, the expressivity of grasping gestures, and the learnability and performance of the approach. Results showed that users achieved high agreement on the mapping, with an average agreement score [35] of 0.68 (SD=0.27). Without exposure to the gestures, users successfully retrieved 76% objects with VirtualGrasp. A week after learning the mapping, they could recall the gestures for 93% objects.",gesture; mapping; object selection,Abstract,
ACM DL,conferencePaper,2018,Pinpointing: Precise Head- and Eye-Based Target Selection for Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Head and eye movement can be leveraged to improve the user's interaction repertoire for wearable displays. Head movements are deliberate and accurate, and provide the current state-of-the-art pointing technique. Eye gaze can potentially be faster and more ergonomic, but suffers from low accuracy due to calibration errors and drift of wearable eye-tracking sensors. This work investigates precise, multimodal selection techniques using head motion and eye gaze. A comparison of speed and pointing accuracy reveals the relative merits of each method, including the achievable target size for robust selection. We demonstrate and discuss example applications for augmented reality, including compact menus with deep structure, and a proof-of-concept method for on-line correction of calibration drift.",augmented reality; eye tracking; gaze interaction; head-worn display; refinement techniques; target selection,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,"Haptic Revolver: Touch, Shear, Texture, and Shape Rendering on a Reconfigurable Virtual Reality Controller",CHI - Human Factors in Computing Systems,A*,"We present Haptic Revolver, a handheld virtual reality controller that renders fingertip haptics when interacting with virtual surfaces. Haptic Revolver's core haptic element is an actuated wheel that raises and lowers underneath the finger to render contact with a virtual surface. As the user's finger moves along the surface of an object, the controller spins the wheel to render shear forces and motion under the fingertip. The wheel is interchangeable and can contain physical textures, shapes, edges, or active elements to provide different sensations to the user. Because the controller is spatially tracked, these physical features can be spatially registered with the geometry of the virtual environment and rendered on-demand. We evaluated Haptic Revolver in two studies to understand how wheel speed and direction impact perceived realism. We also report qualitative feedback from users who explored three application scenarios with our controller.",haptics; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,iTurk: Turning Passive Haptics into Active Haptics by Making Users Reconfigure Props in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We present a system that complements virtual reality experiences with passive props, yet still allows modifying the virtual world at runtime. The main contribution of our system is that it does not require any actuators; instead, our system employs the user to reconfigure and actuate otherwise passive props. We demonstrate a foldable prop that users reconfigure to represent a suitcase, fuse cabinet, railing, and a seat. A second prop, suspended from a long pendulum, not only stands in for inanimate objects, but also for objects that move and demonstrate proactive behavior, such as a group of flying droids that physically attack the user. Our approach conveys a sense of a living, animate world, when in reality the user is the only animate entity present in the system, complemented with only one or two physical props. In our study, participants rated their experience as more enjoyable and realistic than a corresponding no-haptics condition.",haptics; human actuation; immersion; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,"Clusters, Trends, and Outliers: How Immersive Technologies Can Facilitate the Collaborative Analysis of Multidimensional Data",CHI - Human Factors in Computing Systems,A*,"Immersive technologies such as augmented reality devices are opening up a new design space for the visual analysis of data. This paper studies the potential of an augmented reality environment for the purpose of collaborative analysis of multidimensional, abstract data. We present ART, a collaborative analysis tool to visualize multidimensional data in augmented reality using an interactive, 3D parallel coordinates visualization. The visualization is anchored to a touch-sensitive tabletop, benefiting from well-established interaction techniques. The results of group-based, expert walkthroughs show that ART can facilitate immersion in the data, a fluid analysis process, and collaboration. Based on the results, we provide a set of guidelines and discuss future research areas to foster the development of immersive technologies as tools for the collaborative analysis of multidimensional data.",3d parallel coordinates; augmented reality; collaboration; immersive analytics; multi-touch table,Abstract_Keywords,
ACM DL,conferencePaper,2018,Object Manipulation in Virtual Reality Under Increasing Levels of Translational Gain,CHI - Human Factors in Computing Systems,A*,"Room-scale Virtual Reality (VR) has become an affordable consumer reality, with applications ranging from entertainment to productivity. However, the limited physical space available for room-scale VR in the typical home or office environment poses a significant problem. To solve this, physical spaces can be extended by amplifying the mapping of physical to virtual movement (translational gain). Although amplified movement has been used since the earliest days of VR, little is known about how it influences reach-based interactions with virtual objects, now a standard feature of consumer VR. Consequently, this paper explores the picking and placing of virtual objects in VR for the first time, with translational gains of between 1x (a one-to-one mapping of a 3.5m*3.5m virtual space to the same sized physical space) and 3x (10.5m*10.5m virtual mapped to 3.5m*3.5m physical). Results show that reaching accuracy is maintained for up to 2x gain, however going beyond this diminishes accuracy and increases simulator sickness and perceived workload. We suggest gain levels of 1.5x to 1.75x can be utilized without compromising the usability of a VR task, significantly expanding the bounds of interactive room-scale VR.",amplified movement; object manipulation; redirected walking; translational gain; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Enabling People with Visual Impairments to Navigate Virtual Reality with a Haptic and Auditory Cane Simulation,CHI - Human Factors in Computing Systems,A*,"Traditional virtual reality (VR) mainly focuses on visual feedback, which is not accessible for people with visual impairments. We created Canetroller, a haptic cane controller that simulates white cane interactions, enabling people with visual impairments to navigate a virtual environment by transferring their cane skills into the virtual world. Canetroller provides three types of feedback: (1) physical resistance generated by a wearable programmable brake mechanism that physically impedes the controller when the virtual cane comes in contact with a virtual object; (2) vibrotactile feedback that simulates the vibrations when a cane hits an object or touches and drags across various surfaces; and (3) spatial 3D auditory feedback simulating the sound of real-world cane interactions. We designed indoor and outdoor VR scenes to evaluate the effectiveness of our controller. Our study showed that Canetroller was a promising tool that enabled visually impaired participants to navigate different virtual spaces. We discuss potential applications supported by Canetroller ranging from entertainment to mobility training.",auditory feedback; blindness; haptic feedback; mobility; virtual reality; visual impairments; white cane,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Breaking the Tracking: Enabling Weight Perception using Perceivable Tracking Offsets,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) technology strives to enable a highly immersive experience for the user by including a wide variety of modalities (e.g. visuals, haptics). Current VR hardware however lacks a sufficient way of communicating the perception of weight of an object, resulting in scenarios where users can not distinguish between lifting a bowling ball or a feather. We propose a solely software based approach of simulating weight in VR by deliberately using perceivable tracking offsets. These tracking offsets nudge users to lift their arm higher and result in a visual and haptic perception of weight. We conducted two user studies showing that participants intuitively associated them with the sensation of weight and accept them as part of the virtual world. We further show that compared to no weight simulation, our approach led to significantly higher levels of presence, immersion and enjoyment. Finally, we report perceptional thresholds and offset boundaries as design guidelines for practitioners.",pseudo haptics; virtual reality; weight perception,Abstract_Keywords,
ACM DL,conferencePaper,2018,Remixed Reality: Manipulating Space and Time in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"We present Remixed Reality, a novel form of mixed reality. In contrast to classical mixed reality approaches where users see a direct view or video feed of their environment, with Remixed Reality they see a live 3D reconstruction, gathered from multiple external depth cameras. This approach enables changing the environment as easily as geometry can be changed in virtual reality, while allowing users to view and interact with the actual physical world as they would in augmented reality. We characterize a taxonomy of manipulations that are possible with Remixed Reality: spatial changes such as erasing objects; appearance changes such as changing textures; temporal changes such as pausing time; and viewpoint changes that allow users to see the world from different points without changing their physical location. We contribute a method that uses an underlying voxel grid holding information like visibility and transformations, which is applied to live geometry in real time.",augmented reality; remixed reality; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,A Visual Interaction Cue Framework from Video Game Environments for Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Based on an analysis of 49 popular contemporary video games, we develop a descriptive framework of visual interaction cues in video games. These cues are used to inform players what can be interacted with, where to look, and where to go within the game world. These cues vary along three dimensions: the purpose of the cue, the visual design of the cue, and the circumstances under which the cue is shown. We demonstrate that this framework can also be used to describe interaction cues for augmented reality applications. Beyond this, we show how the framework can be used to generatively derive new design ideas for visual interaction cues in augmented reality experiences.",augmented reality; game design; guidance; interaction cues,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Visuo-Haptic Illusions for Improving the Perceived Performance of Shape Displays,CHI - Human Factors in Computing Systems,A*,"In this work, we utilize visuo-haptic illusions to improve the perceived performance of encountered-type haptic devices, specifically shape displays, in virtual reality. Shape displays are matrices of actuated pins that travel vertically to render physical shapes; however, they have limitations such as low resolution, small display size, and low pin speed. To address these limitations, we employ illusions such as redirection, scaling, and retargeting that take advantage of the visual dominance effect, the idea that vision often dominates when senses conflict. Our evaluation of these techniques suggests that redirecting sloped lines with angles less than 40 degrees onto a horizontal line is an effective technique for increasing the perceived resolution of the display. Scaling up the virtual object onto the shape display by a factor less than 1.8x can also increase the perceived resolution. Finally, using vertical redirection a perceived 3x speed increase can be achieved.",haptics; illusion; perception; shape displays; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2018,VR-OOM: Virtual Reality On-rOad driving siMulation,CHI - Human Factors in Computing Systems,A*,"Researchers and designers of in-vehicle interactions and interfaces currently have to choose between performing evaluation and human factors experiments in laboratory driving simulators or on-road experiments. To enjoy the benefit of customizable course design in controlled experiments with the immediacy and rich sensations of on-road driving, we have developed a new method and tools to enable VR driving simulation in a vehicle as it travels on a road. In this paper, we describe how the cost-effective and flexible implementation of this platform allows for rapid prototyping. A preliminary pilot test (N = 6), centered on an autonomous driving scenario, yields promising results, illustrating proof of concept and indicating that a basic implementation of the system can invoke genuine responses from test participants.",autonomous vehicles; design evaluation; prototyping; virtual reality,Keywords_Title,
ACM DL,conferencePaper,2018,KickAR: Exploring Game Balancing Through Boosts and Handicaps in Augmented Reality Table Football,CHI - Human Factors in Computing Systems,A*,"When player skill levels are not matched, games provide an unsatisfying player experience. Player balancing is used across many digital game genres to address this, but has not been studied for co-located augmented reality (AR) tabletop games, where using boosts and handicaps can adjust for different player skill levels. In the setting of an AR table football game, we studied the importance of game balancing being triggered by the game system or the players, and whether player skill should be required to trigger game balancing. We implemented projected icons to prominently display game balancing mechanics in the AR table football game. In a within-subjects study (N=24), we found players prefer skill-based control over game balancing and that different triggers are perceived as having different fairness. Further, the study showed that even game balancing that is perceived as unfair can provide enjoyable game experiences. Based on our findings, we provide suggestions for player balancing in AR tabletop games.",augmented reality; foosball; game balancing; handicap; mario kart effect; player experience,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,SymbiosisSketch: Combining 2D &amp; 3D Sketching for Designing Detailed 3D Objects in Situ,CHI - Human Factors in Computing Systems,A*,"We present SymbiosisSketch, a hybrid sketching system that combines drawing in air (3D) and on a drawing surface (2D) to create detailed 3D designs of arbitrary scale in an augmented reality (AR) setting. SymbiosisSketch leverages the complementary affordances of 3D (immersive, unconstrained, life-sized) and 2D (precise, constrained, ergonomic) interactions for in situ 3D conceptual design. A defining aspect of our system is the ongoing creation of surfaces from unorganized collections of 3D curves. These surfaces serve a dual purpose: as 3D canvases to map strokes drawn on a 2D tablet, and as shape proxies to occlude the physical environment and hidden curves in a 3D sketch. SymbiosisSketch users draw interchangeably on a 2D tablet or in 3D within an ergonomically comfortable canonical volume, mapped to arbitrary scale in AR. Our evaluation study shows this hybrid technique to be easy to use in situ and effective in transcending the creative potential of either traditional sketching or drawing in air.",3d drawing; augmented reality; design sketching,Abstract_Keywords,
ACM DL,conferencePaper,2018,vrSocial: Toward Immersive Therapeutic VR Systems for Children with Autism,CHI - Human Factors in Computing Systems,A*,"Social communication frequently includes nuanced nonverbal communication cues, including eye contact, gestures, facial expressions, body language, and tone of voice. This type of communication is central to face-to-face interaction, but can be challenging for children and adults with autism. Innovative technologies can provide support by augmenting human-delivered cuing and automated prompting. Specifically, immersive virtual reality (VR) offers an option to generalize social skill interventions by concretizing nonverbal information in real-time social interactions. In this work, we explore the design and evaluation of three nonverbal communication applications in immersive VR. The results of this work indicate that delivering real-time visualizations of proximity, speaker volume, and duration of one's speech is feasible in immersive VR and effective for real-time support for proximity regulation for children with autism. We conclude with design considerations for therapeutic VR systems.",accessibility; assistive technology; autism; immersive vr; nonverbal communication; prosody; proximity; visualization,Abstract,
ACM DL,conferencePaper,2018,Simulator Sickness in Augmented Reality Training Using the Microsoft HoloLens,CHI - Human Factors in Computing Systems,A*,"Augmented Reality is on the rise with consumer-grade smart glasses becoming available in recent years. Those interested in deploying these head-mounted displays need to understand better the effect technology has on the end user. One key aspect potentially hindering the use is motion sickness, a known problem inherited from virtual reality, which so far remains under-explored. In this paper we address this problem by conducting an experiment with 142 subjects in three different industries: aviation, medical, and space. We evaluate whether the Microsoft HoloLens, an augmented reality head-mounted display, causes simulator sickness and how different symptom groups contribute to it (nausea, oculomotor and disorientation). Our findings suggest that the Microsoft HoloLens causes across all participants only negligible symptoms of simulator sickness. Most consumers who use it will face no symptoms while only few experience minimal discomfort in the training environments we tested it in.",augmented reality; microsoft hololens; motion sickness; simulator sickness,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Projective Windows: Bringing Windows in Space to the Fingertip,CHI - Human Factors in Computing Systems,A*,"In augmented and virtual reality (AR and VR), there may be many 3D planar windows with 2D texts, images, and videos on them. However, managing the position, orientation, and scale of such a window in an immersive 3D workspace can be difficult. Projective Windows strategically uses the absolute and apparent sizes of the window at various stages of the interaction to enable the grabbing, moving, scaling, and releasing of the window in one continuous hand gesture. With it, the user can quickly and intuitively manage and interact with windows in space without any controller hardware or dedicated widget. Through an evaluation, we demonstrate that our technique is performant and preferable, and that projective geometry plays an important role in the design of spatial user interfaces.",3d window management; augmented reality; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2018,Scenariot: Spatially Mapping Smart Things Within Augmented Reality Scenes,CHI - Human Factors in Computing Systems,A*,"The emerging simultaneous localizing and mapping (SLAM) based tracking technique allows the mobile AR device spatial awareness of the physical world. Still, smart things are not fully supported with the spatial awareness in AR. Therefore, we present Scenariot, a method that enables instant discovery and localization of the surrounding smart things while also spatially registering them with a SLAM based mobile AR system. By exploiting the spatial relationships between mobile AR systems and smart things, Scenariot fosters in-situ interactions with connected devices. We embed Ultra-Wide Band (UWB) RF units into the AR device and the controllers of the smart things, which allows for measuring the distances between them. With a one-time initial calibration, users localize multiple IoT devices and map them within the AR scenes. Through a series of experiments and evaluations, we validate the localization accuracy as well as the performance of the enabled spatial aware interactions. Further, we demonstrate various use cases through Scenariot.",augmented reality; context awareness; iot; localization; slam; smart environment; spatial interactions; uwb,Keywords_Title,
ACM DL,conferencePaper,2018,VirtualSpace - Overloading Physical Space with Multiple Virtual Reality Users,CHI - Human Factors in Computing Systems,A*,"Although virtual reality hardware is now widely available, the uptake of real walking is hindered by the fact that it requires often impractically large amounts of physical space. To address this, we present VirtualSpace, a novel system that allows overloading multiple users immersed in different VR experiences into the same physical space. VirtualSpace accomplishes this by containing each user in a subset of the physical space at all times, which we call tiles; app-invoked maneuvers then shuffle tiles and users across the entire physical space. This allows apps to move their users to where their narrative requires them to be while hiding from users that they are confined to a tile. We show how this enables VirtualSpace to pack four users into 16m2. In our study we found that VirtualSpace allowed participants to use more space and to feel less confined than in a control condition with static, pre-allocated space.",locomotion; real walking; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Communication Behavior in Embodied Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Embodied virtual reality faithfully renders users' movements onto an avatar in a virtual 3D environment, supporting nuanced nonverbal behavior alongside verbal communication. To investigate communication behavior within this medium, we had 30 dyads complete two tasks using a shared visual workspace: negotiating an apartment layout and placing model furniture on an apartment floor plan. Dyads completed both tasks under three different conditions: face-to-face, embodied VR with visible full-body avatars, and no embodiment VR, where the participants shared a virtual space, but had no visible avatars. Both subjective measures of users' experiences and detailed annotations of verbal and nonverbal behavior are used to understand how the media impact communication behavior. Embodied VR provides a high level of social presence with conversation patterns that are very similar to face-to-face interaction. In contrast, providing only the shared environment was generally found to be lonely and appears to lead to degraded communication.",computer-mediated communication; embodiment; social presence; virtual reality; vr,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,SpeechBubbles: Enhancing Captioning Experiences for Deaf and Hard-of-Hearing People in Group Conversations,CHI - Human Factors in Computing Systems,A*,"Deaf and hard-of-hearing (DHH) individuals encounter difficulties when engaged in group conversations with hearing individuals, due to factors such as simultaneous utterances from multiple speakers and speakers whom may be potentially out of view. We interviewed and co-designed with eight DHH participants to address the following challenges: 1) associating utterances with speakers, 2) ordering utterances from different speakers, 3) displaying optimal content length, and 4) visualizing utterances from out-of-view speakers. We evaluated multiple designs for each of the four challenges through a user study with twelve DHH participants. Our study results showed that participants significantly preferred speechbubble visualizations over traditional captions. These design preferences guided our development of SpeechBubbles, a real-time speech recognition interface prototype on an augmented reality head-mounted display. From our evaluations, we further demonstrated that DHH participants preferred our prototype over traditional captions for group conversations.",accessibility; augmented reality; closed captions; deaf and hard of hearing; hololens; text bubbles; word balloons,Abstract_Keywords,
ACM DL,conferencePaper,2018,Force Jacket: Pneumatically-Actuated Jacket for Embodied Haptic Experiences,CHI - Human Factors in Computing Systems,A*,"Immersive experiences seek to engage the full sensory system in ways that words, pictures, or touch alone cannot. With respect to the haptic system, however, physical feedback has been provided primarily with handheld tactile experiences or vibration-based designs, largely ignoring both pressure receptors and the full upper-body area as conduits for expressing meaning that is consistent with sight and sound. We extend the potential for immersion along these dimensions with the Force Jacket, a novel array of pneumatically-actuated airbags and force sensors that provide precisely directed force and high frequency vibrations to the upper body. We describe the pneumatic hardware and force control algorithms, user studies to verify perception of airbag location and pressure magnitude, and subsequent studies to define full-torso, pressure and vibration-based feel effects such as punch, hug, and snake moving across the body. We also discuss the use of those effects in prototype virtual reality applications.",force feedback; haptics; pneumatic actuation; vibrotactile; virtual reality; wearable,Abstract_Keywords,
ACM DL,conferencePaper,2018,Vanishing Importance: Studying Immersive Effects of Game Audio Perception on Player Experiences in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"Sound and virtual reality (VR) are two important output modalities for creating an immersive player experience (PX). While prior research suggests that sounds might contribute to a more immersive experience in games played on screens and mobile displays, there is not yet evidence of these effects of sound on PX in VR. To address this, we conducted a within-subjects experiment using a commercial horror-adventure game to study the effects of a VR and monitor-display version of the same game on PX. Subsequently, we explored, in a between-subjects study, the effects of audio dimensionality on PX in VR. Results indicate that audio has a more implicit influence on PX in VR because of the impact of the overall sensory experience and that audio dimensionality in VR may not be a significant factor contributing to PX. Based on our findings and observations, we provide five design guidelines for VR games.",ambient noises; audio; background music; games; player experience; sound effects; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Towards a Multisensory Augmented Reality Map for Blind and Low Vision People: a Participatory Design Approach,CHI - Human Factors in Computing Systems,A*,"Current low-tech Orientation &amp; Mobility (O&amp;M) tools for visually impaired people, e.g. tactile maps, possess limitations. Interactive accessible maps have been developed to overcome these. However, most of them are limited to exploration of existing maps, and have remained in laboratories. Using a participatory design approach, we have worked closely with 15 visually impaired students and 3 O&amp;M instructors over 6 months. We iteratively designed and developed an augmented reality map destined at use in O&amp;M classes in special education centers. This prototype combines projection, audio output and use of tactile tokens, and thus allows both map exploration and construction by low vision and blind people. Our user study demonstrated that all students were able to successfully use the prototype, and showed a high user satisfaction. A second phase with 22 international special education teachers allowed us to gain more qualitative insights. This work shows that augmented reality has potential for improving the access to education for visually impaired people.",accessibility; augmented reality; geographic maps; participatory design; visual impairment,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Flotation Simulation in a Cable-driven Virtual Environment – A Study with Parasailing,CHI - Human Factors in Computing Systems,A*,"This paper presents flotation simulation in a cable-driven virtual environment. For this, a virtual parasailing system was developed, where the visual stimulus was provided through a VR headset and the physical stimulus was given by wires. In order to prevent the user from moving out of the limited workspace of the cable-driven system, the visual acceleration was washout-filtered to produce the physical acceleration. In the parasailing trajectory, we focused on the stages of vertical acceleration/deceleration and conducted an experiment to identify how much gain can be applied to the visual acceleration, which makes the user feel the natural self-motion when integrated with physical stimulus. Then, the results were tested using several types of full-course virtual parasailing. The results showed that fairly large differences between visual and physical stimuli would be accepted and different gains could be assigned depending on the user's altitudes.",flotation simulation; flying sports; parasailing; virtual reality; visual gain,Keywords,
ACM DL,conferencePaper,2018,NavigaTone: Seamlessly Embedding Navigation Cues in Mobile Music Listening,CHI - Human Factors in Computing Systems,A*,"As humans, we have the natural capability of localizing the origin of sounds. Spatial audio rendering leverages this skill by applying special filters to recorded audio to create the impression that a sound emanates from a certain position in the physical space. A main application for spatial audio on mobile devices is to provide non-visual navigation cues. Current systems require users to either listen to artificial beacon sounds, or the entire audio source (e.g., a song) is repositioned in space, which impacts the listening experience. We present NavigaTone, a system that takes advantage of multi-track recordings and provides directional cues by moving a single track in the auditory space. While minimizing the impact of the navigation component on the listening experience, a user study showed that participants could localize sources as good as with stereo panning while the listening experience was rated to be closer to common music listening.",audio augmented reality; mobile devices; navigation; spatial audio; virtual audio spaces,Keywords,
ACM DL,conferencePaper,2018,Haptic Links: Bimanual Haptics for Virtual Reality Using Variable Stiffness Actuation,CHI - Human Factors in Computing Systems,A*,"We present Haptic Links, electro-mechanically actuated physical connections capable of rendering variable stiffness between two commodity handheld virtual reality (VR) controllers. When attached, Haptic Links can dynamically alter the forces perceived between the user's hands to support the haptic rendering of a variety of two-handed objects and interactions. They can rigidly lock controllers in an arbitrary configuration, constrain specific degrees of freedom or directions of motion, and dynamically set stiffness along a continuous range. We demonstrate and compare three prototype Haptic Links: Chain, Layer-Hinge, and Ratchet-Hinge. We then describe interaction techniques and scenarios leveraging the capabilities of each. Our user evaluation results confirm that users can perceive many two-handed objects or interactions as more realistic with Haptic Links than with typical unlinked VR controllers.",controller; haptic links; haptics; two-handed; variable stiffness; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,Selection-based Text Entry in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR.",mid-air; pointing; task performance; text entry; user experience; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2018,The Effect of Offset Correction and Cursor on Mid-Air Pointing in Real and Virtual Environments,CHI - Human Factors in Computing Systems,A*,"Pointing at remote objects to direct others' attention is a fundamental human ability. Previous work explored methods for remote pointing to select targets. Absolute pointing techniques that cast a ray from the user to a target are affected by humans' limited pointing accuracy. Recent work suggests that accuracy can be improved by compensating systematic offsets between targets a user aims at and rays cast from the user to the target. In this paper, we investigate mid-air pointing in the real world and virtual reality. Through a pointing study, we model the offsets to improve pointing accuracy and show that being in a virtual environment affects how users point at targets. In the second study, we validate the developed model and analyze the effect of compensating systematic offsets. We show that the provided model can significantly improve pointing accuracy when no cursor is provided. We further show that a cursor improves pointing accuracy but also increases the selection time.",cursor; mid-air pointing; modeling; offset correction; ray casting; virtual environment,Abstract,
ACM DL,conferencePaper,2018,"CLAW: A Multifunctional Handheld Haptic Controller for Grasping, Touching, and Triggering in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"CLAW is a handheld virtual reality controller that augments the typical controller functionality with force feedback and actuated movement to the index finger. Our controller enables three distinct interactions (grasping virtual object, touching virtual surfaces, and triggering) and changes its corresponding haptic rendering by sensing the differences in the user's grasp. A servo motor coupled with a force sensor renders controllable forces to the index finger during grasping and touching. Using position tracking, a voice coil actuator at the index fingertip generates vibrations for various textures synchronized with finger movement. CLAW also supports a haptic force feedback in the trigger mode when the user holds a gun. We describe the design considerations for CLAW and evaluate its performance through two user studies. The first study obtained qualitative user feedback on the naturalness, effectiveness, and comfort when using the device. The second study investigated the ease of the transition between grasping and touching when using our device.",controller design; force feedback; grasping; haptics; texture; touching; trigger; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,All about Acceptability? Identifying Factors for the Adoption of Data Glasses,CHI - Human Factors in Computing Systems,A*,"Innovations often trigger objections before becoming widely accepted. This paper assesses whether a familiarisation over time can be expected for data glasses, too. While user attitudes towards those devices have been reported to be prevalently negative [14], it is still unclear, to what extent this initial, negative user attitude might impede adoption. However, indepth understanding is crucial for reducing barriers early in order to gain access to potential benefits from the technology. With this paper we contribute to a better understanding of factors affecting data glasses adoption, as well as current trends and opinions. Our multiple-year case study (N=118) shows, against expectations, no significant change towards a more positive attitude between 2014 and 2016. We complement these findings with an expert survey (N=51) investigating prognoses, challenges and discussing the relevance of social acceptability. We elicit and contrast a controversial spectrum of expert opinions, and assess whether initial objections can be overwritten. Our analysis shows that while social acceptability is considered relevant for the time being, utility and usability are more valued for long-term adoption.",Augmented Reality; Data Glasses; Public Experiences; Social Acceptability; Technology Adoption; User Acceptance,Keywords,
ACM DL,conferencePaper,2017,Teaching Language and Culture with a Virtual Reality Game,CHI - Human Factors in Computing Systems,A*,"Many people want to learn a language but find it difficult to stay engaged. Ideally, we would have language learning tools that can make language learning more enjoyable by simulating immersion in a foreign language environment. Therefore, we adapted Crystallize, a 3D video game for learning Japanese, so that it can be played in virtual reality with the Oculus Rift. Specifically, we explored whether we could leverage virtual reality technology to teach embodied cultural interaction, such as bowing in Japanese greetings. To evaluate the impact of our virtual reality game designs, we conducted a formative user study with 68 participants. We present results showing that the virtual reality design trained players how and when to bow, and that it increased participants' sense of involvement in Japanese culture. Our results suggest that virtual reality technology provides an opportunity to leverage culturally-relevant physical interaction, which can enhance the design of language learning technology and virtual reality games.",language learning; video games; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,WatchThru: Expanding Smartwatch Displays with Mid-air Visuals and Wrist-worn Augmented Reality,CHI - Human Factors in Computing Systems,A*,"We introduce WatchThru, an interactive method for extended wrist-worn display on commercially-available smartwatches. To address the limited visual and interaction space, WatchThru expands the device into 3D through a transparent display. This enables novel interactions that leverage and extend smartwatch glanceability. We describe three novel interaction techniques, Pop-up Visuals, Second Perspective and Peek-through, and discuss how they can complement interaction on current devices. We also describe two types of prototypes that helped us to explore standalone interactions, as well as, proof-of-concept AR interfaces using our platform.",micro-interaction; smartwatches; wearable devices,Title,
ACM DL,conferencePaper,2017,HOBIT: Hybrid Optical Bench for Innovative Teaching,CHI - Human Factors in Computing Systems,A*,"Practical work in optics allows supporting the construction of knowledge, in particular when the concept to be learned remains diffuse. To overcome the limitations of the current experimental setups, we have designed a hybrid system that combines physical interaction and numerical simulation. This system relies on 3D-printed replicas of optical elements, which are augmented with pedagogical information. In this paper, we focus on the well-known Michelson interferometer experiment, widely studied in undergraduate programs of Science. A 3-months user study with 101 students and 6 teachers showed that, beyond the practical aspects offered by this system, such an approach enhances the technical and scientific learning compared to a standard Michelson interferometer experiment.",augmented reality; education and training; michelson interferometer; optics; simulation,Keywords,
ACM DL,conferencePaper,2017,The Geography of Pokémon GO: Beneficial and Problematic Effects on Places and Movement,CHI - Human Factors in Computing Systems,A*,"The widespread popularity of Pokémon GO presents the first opportunity to observe the geographic effects of location-based gaming at scale. This paper reports the results of a mixed methods study of the geography of Pokémon GO that includes a five-country field survey of 375 Pokémon GO players and a large scale geostatistical analysis of game elements. Focusing on the key geographic themes of places and movement, we find that the design of Pokémon GO reinforces existing geographically-linked biases (e.g. the game advantages urban areas and neighborhoods with smaller minority populations), that Pokémon GO may have instigated a relatively rare large-scale shift in global human mobility patterns, and that Pokémon GO has geographically-linked safety risks, but not those typically emphasized by the media. Our results point to geographic design implications for future systems in this space such as a means through which the geographic biases present in Pokémon GO may be counteracted.",algorithmic bias; augmented reality; geography; geoHCI; location-based games; pok'mon GO,Keywords,
ACM DL,conferencePaper,2017,Augmented Studio: Projection Mapping on Moving Body for Physiotherapy Education,CHI - Human Factors in Computing Systems,A*,"Physiotherapy students often struggle to translate anatomical knowledge from textbooks into a dynamic understanding of the mechanics of body movements in real life patients. We present the Augmented Studio, an augmented reality system that uses body tracking to project anatomical structures and annotations over moving bodies for physiotherapy education. Through a user and learner centered design approach, we established an understanding that through augmentation and annotation, augmented reality technology can enhance physiotherapy education. Augmented Studio enables augmentation through projection mapping to display anatomical information such as muscles and skeleton in real time on the body as it moves. We created a technique for annotation to create projected hand-drawing on the moving body, to enable explicit communication of the teacher's clinical reasoning strategies to the students. Findings from our pilot usability study demonstrate a more engaging learning and teaching experience and increased communication between teacher and students when using Augmented Studio.",annotation; physiotherapy education; projection mapping; spatial augmented reality,Abstract_Keywords,
ACM DL,conferencePaper,2017,Inner Garden: Connecting Inner States to a Mixed Reality Sandbox for Mindfulness,CHI - Human Factors in Computing Systems,A*,"Digital technology has been completely integrated into our daily lives, yet the potential of technology to improve its users' life satisfaction is still largely untapped. Mindfulness, the act of paying a deliberate and non-judgmental attention to the present moment, has been shown to have a positive impact on a person's health and subjective well-being–commonly called ""happiness"". Based on an iterative process with meditation teachers and practitioners, we designed a new tool to support mindfulness practices. This tool takes the shape of an augmented sandbox, designed to inspire the user's self-motivation and curiosity. By shaping the sand, the user creates a living miniature world that is projected back onto the sand. The natural elements of the garden are connected to real-time physiological measurements, such as breathing, helping the user to stay focused on the body. Moreover, using a Virtual Reality headset, they can travel inside their garden for a dedicated meditation session. Preliminary results seem to indicate that the system is well suited for mindfulness and induces a calm and mindful state on the user. The meditation teachers envisioned the use of Inner Garden in their practice.",calm technologies; mindfulness; mixed reality; spatial augmented reality; tangible interaction; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Providing Haptics to Walls &amp; Heavy Objects in Virtual Reality by Means of Electrical Muscle Stimulation,CHI - Human Factors in Computing Systems,A*,"We explore how to add haptics to walls and other heavy objects in virtual reality. When a user tries to push such an object, our system actuates the user's shoulder, arm, and wrist muscles by means of electrical muscle stimulation, creating a counter force that pulls the user's arm backwards. Our device accomplishes this in a wearable form factor.In our first user study, participants wearing a head-mounted display interacted with objects provided with different types of EMS effects. The repulsion design (visualized as an electrical field) and the soft design (visualized as a magnetic field) received high scores on ""prevented me from passing through"" as well as ""realistic"".In a second study, we demonstrate the effectiveness of our approach by letting participants explore a virtual world in which all objects provide haptic EMS effects, including walls, gates, sliders, boxes, and projectiles.",ems; force feedback; haptics; muscle interfaces; proprioception; real-walking; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,It wasn't really about the Pokémon: Parents' Perspectives on a Location-Based Mobile Game,CHI - Human Factors in Computing Systems,A*,"Though prior work shows parents worry about screen media experiences displacing physical activity and time outdoors, this research does not account for location-based mobile games like Pokémon GO, which specifically facilitate outdoor activity. To fill this gap in the research, we surveyed and interviewed parents to understand (1) their values and perceptions of this type of gameplay and (2) how they co-play Pokémon GO with their children. Our findings provide empirical evidence that, in addition to appreciating the increased exercise and time outdoors, parents valued how play led to family bonding experiences. Furthermore, some traditional concerns about screen time persisted in this context, and new concerns about safety in real-world environments emerged. Parents mitigated these concerns with rules and gameplay choices, such as maintaining control of the mobile device, to ensure children were safe. This work contributes an empirical understanding of families as co-users of technology and offers a generative lens to study and design for joint media engagement among family members where gameplay differs from normative notions of screen time.",augmented reality games; children; families; joint media engagement; location-based mobile games; parental mediation; pokemon go,Keywords,
ACM DL,conferencePaper,2017,"""These are not my hands!"": Effect of Gender on the Perception of Avatar Hands in Virtual Reality",CHI - Human Factors in Computing Systems,A*,"Rendering the user's body in virtual reality increases immersion and presence the illusion of ""being there"". Recent technology enables determining the pose and position of the hands to render them accordingly while interacting within the virtual environment. Virtual reality applications often use realistic male or female hands, mimic robotic hands, or cartoon hands. However, it is unclear how users perceive different hand styles. We conducted a study with 14 male and 14 female participants in virtual reality to investigate the effect of gender on the perception of six different hands. Quantitative and qualitative results show that women perceive lower levels of presence while using male avatar hands and male perceive lower levels of presence using non-human avatar hands. While women dislike male hands, men accept and feel presence with avatar hands of both genders. Our results highlight the importance of considering the users' diversity when designing virtual reality experiences.",avatars; immersion; presence; uncanny valley; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Looking Inside the Wires: Understanding Museum Visitor Learning with an Augmented Circuit Exhibit,CHI - Human Factors in Computing Systems,A*,"Understanding electrical circuits can be difficult for novices of all ages. In this paper, we describe a science museum exhibit that enables visitors to make circuits on an interactive tabletop and observe a simulation of electrons flowing through the circuit. Our goal is to use multiple representations to help convey basic concepts of current and resistance. To study visitor interaction and learning, we tested the design at a popular science museum with 60 parent-child dyads in three conditions: a control condition with no electron simulation; a condition with the simulation displayed alongside the circuit on the same screen; and an augmented reality condition, with the simulation displayed on a tablet that acts as a lens to see into the circuit. Our findings show that children did significantly better on a post-test in both experimental conditions, with children performing best in the AR condition. However, analysis of session videos shows unexpected parent-child collaboration in the AR condition.",agent-based modeling; augmented reality; design; electrical circuits; interactive surfaces; multiple representations; museum learning.,Abstract_Keywords,
ACM DL,conferencePaper,2017,Ambiotherm: Enhancing Sense of Presence in Virtual Reality by Simulating Real-World Environmental Conditions,CHI - Human Factors in Computing Systems,A*,"In this paper, we present and evaluate Ambiotherm, a wearable accessory for Head Mounted Displays (HMD) that provides thermal and wind stimuli to simulate real-world environmental conditions, such as ambient temperatures and wind conditions, to enhance the sense of presence in Virtual Reality (VR). Ambiotherm consists of a Ambient Temperature Module that is attached to the user's neck, a Wind Simulation Module focused towards the user's face, and a Control Module utilizing Bluetooth communication. We demonstrate Ambiotherm with two VR environments, a hot desert, and a snowy mountain, to showcase the different types of simulated environmental conditions. We conduct several studies to 1) address design factors of the system and 2) evaluate Ambiotherm's effect on factors related to a user's sense of presence. Our findings show that the addition of wind and thermal stimuli significantly improves sensory and realism factors, contributing towards an enhanced sense of presence when compared to traditional VR experiences.",ambient temperature; multimodal interaction; presence; virtual reality; virtual wind,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Inferring Motion Direction using Commodity Wi-Fi for Interactive Exergames,CHI - Human Factors in Computing Systems,A*,"In-air interaction acts as a key enabler for ambient intelligence and augmented reality. As an increasing popular example, exergames, and the alike gesture recognition applications, have attracted extensive research in designing accurate, pervasive and low-cost user interfaces. Recent advances in wireless sensing show promise for a ubiquitous gesture-based interaction interface with Wi-Fi. In this work, we extract complete information of motion-induced Doppler shifts with only commodity Wi-Fi. The key insight is to harness antenna diversity to carefully eliminate random phase shifts while retaining relevant Doppler shifts. We further correlate Doppler shifts with motion directions, and propose a light-weight pipeline to detect, segment, and recognize motions without training. On this basis, we present WiDance, a Wi-Fi-based user interface, which we utilize to design and prototype a contactless dance-pad exergame. Experimental results in typical indoor environment demonstrate a superior performance with an accuracy of 92%, remarkably outperforming prior approaches.",exergame; motion direction recognition; off-the-shelf wi-fi; wireless sensing,Abstract,
ACM DL,conferencePaper,2017,A Thing of Beauty: Steering Behavior in an Interactive Playground,CHI - Human Factors in Computing Systems,A*,"Interactive playgrounds are spaces where players engage in collocated, playful activities, in which added digital technology can be designed to promote cognitive, social, and motor skills development. To promote such development, different strategies can be used to implement game mechanics that change player's in-game behavior. One of such strategies is enticing players to take action through incentives akin to game achievements. We explored if this strategy could be used to influence players' proxemic behavior in the Interactive Tag Playground, an installation that enhances the traditional game of tag. We placed the ITP in an art gallery, observed hundreds of play sessions, and refined the mechanics, which consisted in projecting collectible particles around the tagger that upon collection by runners resulted only in the embellishment of their circles. We implemented the refined mechanics in a study with 48 children. The playground automatically collected the players' positions, and analyses show that runners got closer to and moved more towards taggers when using our enticing strategy. This suggests an enticing strategy can be used to influence physical in-game behavior.",augmented reality; entice; interactive floor; interactive playgrounds; persuasion; play; proxemics; social; steering behavior,Keywords,
ACM DL,conferencePaper,2017,The Pokémon GO Experience: A Location-Based Augmented Reality Mobile Game Goes Mainstream,CHI - Human Factors in Computing Systems,A*,"Pokémon GO is a location-based augmented reality mobile game based on the Pokémon franchise. After the game was launched globally in July 2016, it quickly became the most successful mobile game in both popularity and revenue generation at the time, and the first location-based augmented reality game to reach a mainstream status. We explore the game experiences through a qualitative survey (n=1000) in Finland focusing on the positive and the negative aspects of Pokémon GO as told by the players. The positive experiences are related to movement, sociability, game mechanics, and brand while the negative experiences emerge from technical problems, unequal gaming opportunities, bad behavior of other players and non-players, and unpolished game design. Interestingly, the augmented reality features, safety issues or the free-to-play revenue model did not receive considerable feedback. The findings are useful for academics and industry practitioners for studying and designing location-based augmented reality game experiences.",augmented reality; game design; game experience; location-based; mobile game; pervasive game; Pokemon GO; qualitative study,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Fingertip Tactile Devices for Virtual Object Manipulation and Exploration,CHI - Human Factors in Computing Systems,A*,"One of the main barriers to immersivity during object manipulation in virtual reality is the lack of realistic haptic feedback. Our goal is to convey compelling interactions with virtual objects, such as grasping, squeezing, pressing, lifting, and stroking, without requiring a bulky, world-grounded kinesthetic feedback device (traditional haptics) or the use of predetermined passive objects (haptic retargeting). To achieve this, we use a pair of finger-mounted haptic feedback devices that deform the skin on the fingertips to convey cutaneous force information from object manipulation. We show that users can perceive differences in virtual object weight and that they apply increasing grasp forces when lifting virtual objects as rendered mass is increased. Moreover, we show how naive users perceive changes of a virtual object's physical properties when we use skin deformation to render objects with varying mass, friction, and stiffness. These studies demonstrate that fingertip skin deformation devices can provide a compelling haptic experience appropriate for virtual reality scenarios involving object manipulation.",haptics; mass perception; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2017,WireDraw: 3D Wire Sculpturing Guided with Mixed Reality,CHI - Human Factors in Computing Systems,A*,"The availability of commodity 3D extruder pen allows direct drawing of 3D wire sculptures for novice users, enabling many novel applications such as intuitive spatial intelligence development for school students. However, the lack of spatial and structural cues among individual pen strokes makes the 3D drawing process challenging, which often leads to highly distorted and even incomplete wire sculptures. We present a mixed reality system, called `WireDraw', to immersively guide the 3D drawing for easy wire sculpturing. The system design is based on novel 3D drawing principles and the subsequent optimization, making the stroke sequence of the wire model drawable and easy to draw. On-the-fly edits on unsatisfactory strokes are also allowed for creative design. We demonstrate the effectiveness of our system by testing on a variety of wire models and a user study. The results show that the visual guidance provided by our system is extremely helpful for drawing high-quality wire sculptures.",3D extruder pen; drawing optimization; mixed reality; stroke generation; wire sculpture,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Sparse Haptic Proxy: Touch Feedback in Virtual Environments Using a General Passive Prop,CHI - Human Factors in Computing Systems,A*,"We propose a class of passive haptics that we call Sparse Haptic Proxy: a set of geometric primitives that simulate touch feedback in elaborate virtual reality scenes. Unlike previous passive haptics that replicate the virtual environment in physical space, a Sparse Haptic Proxy simulates a scene's detailed geometry by redirecting the user's hand to a matching primitive of the proxy. To bridge the divergence of the scene from the proxy, we augment an existing Haptic Retargeting technique with an on-the-fly target remapping: We predict users' intentions during interaction in the virtual space by analyzing their gaze and hand motions, and consequently redirect their hand to a matching part of the proxy. We conducted three user studies on haptic retargeting technique and implemented a system from three main results: 1) The maximum angle participants found acceptable for retargeting their hand is 40°, with an average rating of 4.6 out of 5. 2) Tracking participants' eye gaze reliably predicts their touch intentions (97.5%), even while simultaneously manipulating the user's hand-eye coordination for retargeting. 3) Participants preferred minimized retargeting distances over better-matching surfaces of our Sparse Haptic Proxy when receiving haptic feedback for single-finger touch input. We demonstrate our system with two virtual scenes: a flight cockpit and a room quest game. While their scene geometries differ substantially, both use the same sparse haptic proxy to provide haptic feedback to the user during task completion.",passive haptics; perception; retargeting; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2017,HapticHead: A Spherical Vibrotactile Grid around the Head for 3D Guidance in Virtual and Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Current virtual and augmented reality head-mounted displays usually include no or only a single vibration motor for haptic feedback and do not use it for guidance. We present HapticHead, a system utilizing multiple vibrotactile actuators distributed in three concentric ellipses around the head for intuitive haptic guidance through moving tactile cues. We conducted three experiments, which indicate that HapticHead vibrotactile feedback is both faster (2.6 s vs. 6.9 s) and more precise (96.4% vs. 54.2% success rate) than spatial audio (generic head-related transfer function) for finding visible virtual objects in 3D space around the user. The baseline of visual feedback is as expected more precise (99.7% success rate) and faster (1.3 s) in comparison, but there are many applications in which visual feedback is not desirable or available due to lighting conditions, visual overload, or visual impairments. Mean final precision with HapticHead feedback on invisible targets is 2.3° compared to 0.8° with visual feedback. We successfully navigated blindfolded users to real household items at different heights using HapticHead vibrotactile feedback independently of a head-mounted display.",3d output; augmented reality; guidance; haptic feedback; navigation; spatial interaction; vibrotactile; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Interactive Visual Calibration of Volumetric Head-Tracked 3D Displays,CHI - Human Factors in Computing Systems,A*,"Head-tracked 3D displays can provide a compelling 3D effect, but even small inaccuracies in the calibration of the participant's viewpoint to the display can disrupt the 3D illusion. We propose a novel interactive procedure for a participant to easily and accurately calibrate a head-tracked display by visually aligning patterns across a multi-screen display. Head-tracker measurements are then calibrated to these known viewpoints. We conducted a user study to evaluate the effectiveness of different visual patterns and different display shapes. We found that the easiest to align shape was the spherical display and the best calibration pattern was the combination of circles and lines. We performed a quantitative camera-based calibration of a cubic display and found visual calibration outperformed manual tuning and generated viewpoint calibrations accurate to within a degree. Our work removes the usual, burdensome step of manual calibration when using head-tracked displays and paves the way for wider adoption of this inexpensive and effective 3D display technology.",calibration; fish tank virtual reality; head tracking; visual perception,Keywords,
ACM DL,conferencePaper,2017,Changing the Appearance of Real-World Objects By Modifying Their Surroundings,CHI - Human Factors in Computing Systems,A*,"We present an approach to alter the perceived appearance of physical objects by controlling their surrounding space. Many real-world objects cannot easily be equipped with displays or actuators in order to change their shape. While common approaches such as projection mapping enable changing the appearance of objects without modifying them, certain surface properties (e.g. highly reflective or transparent surfaces) can make employing these techniques difficult. In this work, we present a conceptual design exploration on how the appearance of an object can be changed by solely altering the space around it, rather than the object itself. In a proof-of-concept implementation, we place objects onto a tabletop display and track them together with users to display perspective-corrected 3D graphics for augmentation. This enables controlling properties such as the perceived size, color, or shape of objects. We characterize the design space of our approach and demonstrate potential applications. For example, we change the contour of a wallet to notify users when their bank account is debited. We envision our approach to gain in importance with increasing ubiquity of display surfaces.",augmented reality; dynamic appearance,Keywords,
ACM DL,conferencePaper,2017,ShareVR: Enabling Co-Located Experiences for Virtual Reality between HMD and Non-HMD Users,CHI - Human Factors in Computing Systems,A*,"Virtual reality (VR) head-mounted displays (HMD) allow for a highly immersive experience and are currently becoming part of the living room entertainment. Current VR systems focus mainly on increasing the immersion and enjoyment for the user wearing the HMD (HMD user), resulting in all the bystanders (Non-HMD users) being excluded from the experience. We propose ShareVR, a proof-of-concept prototype using floor projection and mobile displays in combination with positional tracking to visualize the virtual world for the Non-HMD user, enabling them to interact with the HMD user and become part of the VR experience. We designed and implemented ShareVR based on the insights of an initial online survey (n=48) with early adopters of VR HMDs. We ran a user study (n=16) comparing ShareVRto a baseline condition showing how the interaction using ShareVR led to an increase of enjoyment, presence and social interaction. In a last step we implemented several experiences for ShareVR, exploring its design space and giving insights for designers of co-located asymmetric VR experiences.",asymmetric virtual reality; co-located virtual reality; consumer virtual reality; multi-user virtual reality; sharevr,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,CarVR: Enabling In-Car Virtual Reality Entertainment,CHI - Human Factors in Computing Systems,A*,"Mobile virtual reality (VR) head-mounted displays (HMDs) allow users to experience highly immersive entertainment whilst being in a mobile scenario. Long commute times make casual gaming in public transports and cars a common occupation. However, VR HMDs can currently not be used in moving vehicles since the car's rotation affects the HMD's sensors and simulator sickness occurs when the visual and vestibular system are stimulated with incongruent information. We present CarVR, a solution to enable VR in moving vehicles by subtracting the car's rotation and mapping vehicular movements with the visual information. This allows the user to actually feel correct kinesthetic forces during the VR experience. In a user study (n = 21), we compared CarVR inside a moving vehicle with the baseline of using VR without vehicle movements. We show that the perceived kinesthetic forces caused by CarVR increase enjoyment and immersion significantly while simulator sickness is reduced compared to a stationary VR experience. Finally, we explore the design space of in-car VR entertainment applications using real kinesthetic forces and derive design considerations for practitioners.",automotive; entertainment; force-feedback; gaming; immersion; motion platform; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Effects of Sharing Physiological States of Players in a Collaborative Virtual Reality Gameplay,CHI - Human Factors in Computing Systems,A*,"Interfaces for collaborative tasks, such as multiplayer games can enable more effective and enjoyable collaboration. However, in these systems, the emotional states of the users are often not communicated properly due to their remoteness from one another. In this paper, we investigate the effects of showing emotional states of one collaborator to the other during an immersive Virtual Reality (VR) gameplay experience. We created two collaborative immersive VR games that display the real-time heart-rate of one player to the other. The two different games elicited different emotions, one joyous and the other scary. We tested the effects of visualizing heart-rate feedback in comparison with conditions where such a feedback was absent. The games had significant main effects on the overall emotional experience.",collaborative gameplay; emotions; empathic computing; physiological sensors; user study.; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,VRRRRoom: Virtual Reality for Radiologists in the Reading Room,CHI - Human Factors in Computing Systems,A*,"Reading room conditions such as illumination, ambient light, human factors and display luminance, play an important role on how radiologists analyze and interpret images. Indeed, serious diagnostic errors can appear when observing images through everyday monitors. Typically, these occur whenever professionals are ill-positioned with respect to the display or visualize images under improper light and luminance conditions. In this work, we show that virtual reality can assist radiodiagnostics by considerably diminishing or cancel out the effects of unsuitable ambient conditions. Our approach combines immersive head-mounted displays with interactive surfaces to support professional radiologists in analyzing medical images and formulating diagnostics. We evaluated our prototype with two senior medical doctors and four seasoned radiology fellows. Results indicate that our approach constitutes a viable, flexible, portable and cost-efficient option to traditional radiology reading rooms.",interaction design; medical visualization; multitouch surfaces; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Handsfree Omnidirectional VR Navigation using Head Tilt,CHI - Human Factors in Computing Systems,A*,"Navigating mobile virtual reality (VR) is a challenge due to limited input options and/or a requirement for handsfree interaction. Walking-in-place (WIP) is considered to offer a higher presence than controller input but only allows unidirectional navigation in the direction of the user's gaze–which impedes navigation efficiency. Leaning input enables omnidirectional navigation but currently relies on bulky controllers, which aren't feasible in mobile VR contexts. This note evaluates the use of head-tilt - implemented using inertial sensing - to allow for handsfree omnidirectional VR navigation on mobile VR platforms. A user study with 24 subjects compared three input methods using an obstacle avoidance navigation task: (1) head-tilt alone (TILT) (2) a hybrid method (WIP-TILT) that uses head tilting for direction and WIP to control speed; and (3) traditional controller input. TILT was significantly faster than WIP-TILT and joystick input, while WIP-TILT and TILT offered the highest presence. There was no difference in cybersickness between input methods.",games; head-tilt; inertial sensing; locomotion; mobile vr; simulator-sickness; virtual reality; walking-in-place,Abstract_Keywords,
ACM DL,conferencePaper,2017,Understanding Low Vision People's Visual Perception on Commercial Augmented Reality Glasses,CHI - Human Factors in Computing Systems,A*,"People with low vision have a visual impairment that affects their ability to perform daily activities. Unlike blind people, low vision people have functional vision and can potentially benefit from smart glasses that provide dynamic, always-available visual information. We sought to determine what low vision people could see on mainstream commercial augmented reality (AR) glasses, despite their visual limitations and the device's constraints. We conducted a study with 20 low vision participants and 18 sighted controls, asking them to identify virtual shapes and text in different sizes, colors, and thicknesses. We also evaluated their ability to see the virtual elements while walking. We found that low vision participants were able to identify basic shapes and read short phrases on the glasses while sitting and walking. Identifying virtual elements had a similar effect on low vision and sighted people's walking speed, slowing it down slightly. Our study yielded preliminary evidence that mainstream AR glasses can be powerful accessibility tools. We derive guidelines for presenting visual output for low vision people and discuss opportunities for accessibility applications on this platform.",accessibility; augmented reality; low vision; user study,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,The Geometry of Storytelling: Theatrical Use of Space for 360-degree Videos and Virtual Reality,CHI - Human Factors in Computing Systems,A*,"360-degree filming and head-mounted displays (HMDs) give recorded media a new sense of space. Theatre practitioners' expertise in manipulating spatial interactions has much to contribute to immersive recorded content. Four theatre directors led teams of three actors to stage the same scene for both immersive theatre and for 360-degree filming. Each team was recorded performing the scene at least six times, three in each condition, to extract actors' coordinates. This study establishes how to quantify theatre practitioners' use of spatial interactions and examines the spatial adaptations made when transferring these relationships to 360-degree filming.Staging for a 360-degree camera compared to staging for an audience member had shorter distances from the camera and between performers, along with fewer instances of the camera being in the middle of the action. Across all groups, interpersonal distance between characters and between the audience/camera dropped at the end of the scene when the characters come together as a team, suggesting that elements of Proxemics may be applicable to narrative performance.",360-degree video; cinematic vr; head-mounted display; interpersonal space; narrative; performance; theatre; virtual reality; workflow,Keywords_Title,
ACM DL,conferencePaper,2017,MagicFace: Stepping into Character through an Augmented Reality Mirror,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) is coming of age and appearing in various smartphone apps. One emerging AR type uses the front-facing camera and overlays a user's face with digital features that transform the physical appearance, making the user look like someone else, such as a popstar or a historical character. However, little is known about how people react to such stepping into character and how convincing they perceive it to be. We developed an app with two Egyptian looks, MagicFace, which was situated both in an opera house and a museum. In the first setting, people were invited to use the app, while in the second setting they came across it on their own when visiting the exhibition. Our findings show marked differences in how people approach and experience the MagicFace in these different contexts. We discuss how realistic and compelling this kind of AR technology is, as well as its implications for educational and cultural settings.",augmented reality; in-the-wild study; interface design; opera characters,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Supporting Easy Physical-to-Virtual Creation of Mobile VR Maze Games: A New Genre,CHI - Human Factors in Computing Systems,A*,"With the fast development of virtual reality games, one of the key research questions is how players may express their creativity and participate in the process of game design. In this paper, we present a new game genre which combines user-controlled game design in physical space with game play in virtual space on a mobile device. The new system supports authoring by anyone, creating virtual reality games that can be easily modified or developed for physical space, and be used anywhere by novice end-users without any knowledge of tracking technology. We present the design and implementation of the system, as well as a user experiment. Findings illustrate that the proposed system promotes participation and provides a richer, more interactive and engaging experience.",head-mounted display; mobile 3d; natural interaction; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2017,"The World-as-Support: Embodied Exploration, Understanding and Meaning-Making of the Augmented World",CHI - Human Factors in Computing Systems,A*,"Current technical capabilities of mobile technologies are consolidating the interest in developing context-aware Augmented/Mixed Reality applications. Most of these applications are designed based on the Window-on-the-World (WoW) interaction paradigm. A significant decrease in cost of projection technology and advances in pico-sized projectors have spurred applications of Projective Augmented Reality. This research has focused mainly on technological development. However, there is still a need to fully understand its communicational and expressive potential. Hence, we define a conceptual paradigm that we call World-as-Support (WaS). We compare the WaS and WoW paradigms by contrasting their assumptions and cultural values, as well as through a study of an application aimed at supporting the collaborative improvisation of site-specific narratives by children. Our analysis of children's understanding of the physical and social environment and of their imaginative play allowed us to identify the affordances, strengths and weaknesses of these two paradigms.",augmented reality; embodied cognition; embodied interaction; meaning making; mixed reality; window-on-the-world; world-as-support.,Abstract_Keywords,
ACM DL,conferencePaper,2017,Extending the Body for Interaction with Reality,CHI - Human Factors in Computing Systems,A*,"In this paper, we explore how users can control remote devices with a virtual long arm, while preserving the perception that the artificial arm is actually part of their own body. Instead of using pointing, speech, or a remote control, the users' arm is extended in augmented reality, allowing access to devices that are out of reach. Thus, we allow users to directly manipulate real-world objects from a distance using their bare hands. A core difficulty we focus on is how to maintain ownership for the unnaturally long virtual arm, which is the strong feeling that one's limbs are actually part of the own body. Fortunately, what the human brain experiences as being part of the own body is very malleable and we find that during interaction the user's virtual arm can be stretched to more than twice its real length, without breaking the user's sense of ownership for the virtual limb.",augmented reality; ownership; ubiquitous computing; virtual hand illusion,Abstract_Keywords,
ACM DL,conferencePaper,2017,Vremiere: In-Headset Virtual Reality Video Editing,CHI - Human Factors in Computing Systems,A*,"Creative professionals are creating Virtual Reality (VR) experiences today by capturing spherical videos, but video editing is still done primarily in traditional 2D desktop GUI applications such as Premiere. These interfaces provide limited capabilities for previewing content in a VR headset or for directly manipulating the spherical video in an intuitive way. As a result, editors must alternate between editing on the desktop and previewing in the headset, which is tedious and interrupts the creative process. We demonstrate an application that enables a user to directly edit spherical video while fully immersed in a VR headset. We first interviewed professional VR filmmakers to understand current practice and derived a suitable workflow for in-headset VR video editing. We then developed a prototype system implementing this new workflow. Our system is built upon a familiar timeline design, but is enhanced with custom widgets to enable intuitive editing of spherical video inside the headset. We conducted an expert review study and found that with our prototype, experts were able to edit videos entirely within the headset. Experts also found our interface and widgets useful, providing intuitive controls for their editing needs.",video editing; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,"""They're Just Tixel Pits, Man"": Disputing the 'Reality' of Virtual Reality Pornography through the Story Completion Method",CHI - Human Factors in Computing Systems,A*,"Pornography is a substantial part of humans' everyday interaction with computers, yet to date the topic has been underconsidered by HCI. Here, we examine some of the common cultural ideals non-experts constructed of a ""new"" pornographic experience - Virtual Reality (VR) Porn - through use of the ""Story Completion Method"". Forty five participants completed a story stem about a male character who was about to have his ""very first virtual reality porn experience"". Through our analysis, we demonstrate a narrative of a ""perfect"", idealised sexual experience, as well as one which emphasised the imagined ""precarious"" and dangerous consequences around this technology use. We indicate how the stories reproduced ideals around heteronormativity and hegemonic masculinity, suggesting an agenda of ""Designing for Eroticism"" as a tactic which could avoid such problematic discourses. We also suggest the opportunities and challenges presented through use of the ""Story Completion Method"".",design fiction; porn; pornography; speculative design; thematic analysis; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Experimental Evaluation of Sketching on Surfaces in VR,CHI - Human Factors in Computing Systems,A*,"Sketching in immersive 3D virtual reality (VR) environments has great potential for a variety of interactive 3D design applications. Precisely sketching the intended strokes in mid-air, however, can be a challenge. In this paper, we present a set of controlled studies to analyze the factors affecting human ability to sketch freely in a 3D VR environment. In our first study, we directly compare traditional sketching on a physical surface to sketching in VR, with and without a physical surface to rest the stylus on. Our results indicate that the lack of a physical drawing surface is a major cause of inaccuracies in VR drawing, and that the effect is dependent on the orientation of the drawing surface. In a second experiment, we evaluate the extent to which visual guidance can compensate for the loss of sketching precision in VR. We found that while additional visual guidance improves positional accuracy, it can be detrimental to the aesthetic quality of strokes. We conclude by distilling our experimental findings into design guidelines for sketching tools in immersive 3D environments.",3d drawing; motor ability; virtual reality; visual factors,Abstract_Keywords,
ACM DL,conferencePaper,2017,I Am The Passenger: How Visual Motion Cues Can Influence Sickness For In-Car VR,CHI - Human Factors in Computing Systems,A*,"This paper explores the use of VR Head Mounted Displays (HMDs) in-car and in-motion for the first time. Immersive HMDs are becoming everyday consumer items and, as they offer new possibilities for entertainment and productivity, people will want to use them during travel in, for example, autonomous cars. However, their use is confounded by motion sickness caused in-part by the restricted visual perception of motion conflicting with physically perceived vehicle motion (accelerations/rotations detected by the vestibular system). Whilst VR HMDs restrict visual perception of motion, they could also render it virtually, potentially alleviating sensory conflict. To study this problem, we conducted the first on-road and in motion study to systematically investigate the effects of various visual presentations of the real-world motion of a car on the sickness and immersion of VR HMD wearing passengers. We established new baselines for VR in-car motion sickness, and found that there is no one best presentation with respect to balancing sickness and immersion. Instead, user preferences suggest different solutions are required for differently susceptible users to provide usable VR in-car. This work provides formative insights for VR designers and an entry point for further research into enabling use of VR HMDs, and the rich experiences they offer, when travelling.",automobile; autonomous car; hmd; in-car; in-motion; mixed reality; motion sickness; passenger; virtual reality,Keywords,
ACM DL,conferencePaper,2017,VaiR: Simulating 3D Airflows in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"The integration of multi-sensory stimuli, e.g. haptic airflow, in virtual reality (VR) has become an important topic of VR research and proved to enhance the feeling of presence. VaiR focuses on an accurate and realistic airflow simulation that goes far beyond wind. While previous works on the topic of airflow in VR are restricted to wind, while focusing on the feeling of presence, there is to the best of our knowledge no work considering the conceptual background or on the various application areas. Our pneumatic prototype emits short and long term flows with a minimum delay and is able to animate wind sources in 3D space around the user's head. To get insights on how airflow can be used in VR and how such a device should be designed, we arranged focus groups and discussed the topic. Based on the gathered knowledge, we developed a prototype which proved to increase presence, as well as enjoyment and realism, while not disturbing the VR experience.",airflow; evaluation; presence; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Exploring Seasonality in Mobile Cultural Heritage,CHI - Human Factors in Computing Systems,A*,"We present results of an investigation into the role of seasonality in mobile cultural heritage applications. 45 participants in 26 groups used one of two applications when visiting the Finnish recreational island of Seurasaari. Each provided summer and winter content, but varied in how this was presented. We uncovered how users consider seasonality in content, seasonal preferences, as well as how different media becomes more or less interesting if shown in or out of season. We present design considerations for future researchers to consider seasonality in cultural heritage applications.",cultural heritage; location-based interaction; mixed reality; open-air museum; outdoor heritage site; seasonality,Keywords,
ACM DL,conferencePaper,2017,Malleable Embodiment: Changing Sense of Embodiment by Spatial-Temporal Deformation of Virtual Human Body,CHI - Human Factors in Computing Systems,A*,"We hypothesize that replacing the visual perception of one's body with a spatial-temporal deformed state would change sensations associated with the body. We developed a system that captures full-body movement and generates estimated past and future body movement by deformation. With a head mounted display, people could see their bodies as slightly deformed. We then investigated 1) how human movement is physically changed, and 2) how humans feel about the change in physical and emotional views of the body due to virtual body deformation. Our results show that spatial-temporal deformation of a virtual body actually changes the sense of body as well as physical movement. For instance, a body image generated at approximately 25-100 ms in the future induced a ""lighter weight"" sensation. On the basis of our findings, we discuss the design implication of computational control for the physical and emotional sense of body.","""body ownership""; ""embodiment""; ""motion""; ""perception""; ""virtual reality""",Keywords,
ACM DL,conferencePaper,2017,Remote Collaboration With Mixed Reality Displays: How Shared Virtual Landmarks Facilitate Spatial Referencing,CHI - Human Factors in Computing Systems,A*,"HCI research has demonstrated Mixed Reality (MR) as being beneficial for co-located collaborative work. For remote collaboration, however, the collaborators' visual contexts do not coincide due to their individual physical environments. The problem becomes apparent when collaborators refer to physical landmarks in their individual environments to guide each other's attention. In an experimental study with 16 dyads, we investigated how the provisioning of shared virtual landmarks (SVLs) influences communication behavior and user experience. A quantitative analysis revealed that participants used significantly less ambiguous spatial expressions and reported an improved user experience when SVLs were provided. Based on these findings and a qualitative video analysis we provide implications for the design of MRs to facilitate remote collaboration.",mixed reality; remote collaboration; virtual landmarks,Abstract_Keywords_Title,
ACM DL,conferencePaper,2017,Retargeting Video Tutorials Showing Tools With Surface Contact to Augmented Reality,CHI - Human Factors in Computing Systems,A*,"A video tutorial effectively conveys complex motions, but may be hard to follow precisely because of its restriction to a predetermined viewpoint. Augmented reality (AR) tutorials have been demonstrated to be more effective. We bring the advantages of both together by interactively retargeting conventional, two-dimensional videos into three-dimensional AR tutorials. Unlike previous work, we do not simply overlay video, but synthesize 3D-registered motion from the video. Since the information in the resulting AR tutorial is registered to 3D objects, the user can freely change the viewpoint without degrading the experience. This approach applies to many styles of video tutorials. In this work, we concentrate on a class of tutorials which alter the surface of an object.",augmented reality; retargeting; video tutorial; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2016,Compensating for Distance Compression in Audiovisual Virtual Environments Using Incongruence,CHI - Human Factors in Computing Systems,A*,"A key requirement for a sense of presence in Virtual Environments (VEs) is for a user to perceive space as naturally as possible. One critical aspect is distance perception. When judging distances, compression is a phenomenon where humans tend to underestimate the distance between themselves and target objects (termed egocentric or absolute compression), and between other objects (exocentric or relative compression). Results of studies in virtual worlds rendered through head mounted displays are striking, demonstrating significant distance compression error. Distance compression is a multisensory phenomenon, where both audio and visual stimuli are often compressed with respect to their distances from the observer. In this paper, we propose and test a method for reducing crossmodal distance compression in VEs. We report an empirical evaluation of our method via a study of 3D spatial perception within a virtual reality (VR) head mounted display. Applying our method resulted in more accurate distance perception in a VE at longer range, and suggests a modification that could adaptively compensate for distance compression at both shorter and longer ranges. Our results have a significant and intriguing implication for designers of VEs: an incongruent audiovisual display, i.e. where the audio and visual information is intentionally misaligned, may lead to better spatial perception of a virtual scene.",binaural audio; distance perception; head mounted display; incongruent display; spatial audio; virtual environment,Abstract,
ACM DL,conferencePaper,2016,miniStudio: Designers' Tool for Prototyping Ubicomp Space with Interactive Miniature,CHI - Human Factors in Computing Systems,A*,"Recently, it has become common for designers to deal with complex and large-scale ubicomp or IoT spaces. Designers without technical implementation skills have difficulties in prototyping such spaces, especially in the early phases of design. We present miniStudio, a designers' tool for prototyping ubicomp space with proxemic interactions. It is built on designers' existing software and modeling materials (Photoshop, Lego, and paper). Interactions can be defined in Photoshop based on five spatial relations: location, distance, motion, orientation, and custom. Projection-based augmented reality was applied to miniatures in order to enable tangible interactions and dynamic representations. Hidden marker stickers and a camera-projector system enable the unobtrusive integration of digital images on the physical miniature. Through the user study with 12 designers and researchers in the ubicomp field, we found that miniStudio supported rapid prototyping of large and complex ideas with multiple connected components. Based on the tool development and the study, we discuss the implications for prototyping ubicomp environments in the early phase of the design.",augmented reality; interactive miniature; photoshop; prototyping; proxemic interaction; ubiquitous computing,Abstract_Keywords,
ACM DL,conferencePaper,2016,The Augmented Climbing Wall: High-Exertion Proximity Interaction on a Wall-Sized Interactive Surface,CHI - Human Factors in Computing Systems,A*,"We present the design and evaluation of the Augmented Climbing Wall (ACW). The system combines computer vision and interactive projected graphics for motivating and instructing indoor wall climbing. We have installed the system in a commercial climbing center, where it has been successfully used by hundreds of climbers, including both children and adults. Our primary contribution is a novel movement-based game system that can inform the design of future games and augmented sports. We evaluate ACW based on three user studies (N=50, N=10, N=10) and further observations and interviews. We highlight three central themes of how digital augmentation can contribute to a sport: increasing diversity of movement and challenges, enabling user-created content in an otherwise risky environment, and enabling procedurally generated content. We further discuss how ACW represents an underexplored class of interactive systems, i.e., proximity interaction on wall-sized interactive surfaces, which presents novel human-computer interaction challenges.",augmented reality; climbing; exertion interfaces; human-computer interaction; movement-based games; sports,Keywords,
ACM DL,conferencePaper,2016,Pmomo: Projection Mapping on Movable 3D Object,CHI - Human Factors in Computing Systems,A*,"We introduce Pmomo (acronym of projection mapping on movable object), a dynamic projection mapping system that tracks the 6-DOF position of real-world object, and shades it with virtual 3D contents by projection. The system can precisely lock the projection on the moving object in real-time, even the one with complex geometry. Based on depth camera, we developed a novel and robust tracking method that samples the structure of the object into low-density point cloud, then performs an adaptive searching scheme for the registration procedure. As a fully interactive system, our method can handle both internal and external complex occlusions, and can quickly track back the object even when losing track. In order to further improve the realism of the projected virtual textures, our system innovatively culls occlusions away from projection, which is achieved by a facet-covering method. As a result, the Pmomo system enables the possibility of new interactive Augmented Reality applications that require high-quality dynamic projection effect.",augmented reality; dynamic projection mapping; real-time; tracking,Abstract_Keywords,
ACM DL,conferencePaper,2016,Combining Shape-Changing Interfaces and Spatial Augmented Reality Enables Extended Object Appearance,CHI - Human Factors in Computing Systems,A*,"We propose combining shape-changing interfaces and spatial augmented reality for extending the space of appearances and interactions of actuated interfaces. While shape-changing interfaces can dynamically alter the physical appearance of objects, the integration of spatial augmented reality additionally allows for dynamically changing objects' optical appearance with high detail. This way, devices can render currently challenging features such as high frequency texture or fast motion. We frame this combination in the context of computer graphics with analogies to established techniques for increasing the realism of 3D objects such as bump mapping. This extensible framework helps us identify challenges of the two techniques and benefits of their combination. We utilize our prototype shape-changing device enriched with spatial augmented reality through projection mapping to demonstrate the concept. We present a novel mechanical distance-fields algorithm for real-time fitting of mechanically constrained shape-changing devices to arbitrary 3D graphics. Furthermore, we present a technique for increasing effective screen real estate for spatial augmented reality through view-dependent shape change.",shape-changing interfaces; spatial augmented reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2016,Accountable Artefacts: The Case of the Carolan Guitar,CHI - Human Factors in Computing Systems,A*,"We explore how physical artefacts can be connected to digital records of where they have been, who they have encountered and what has happened to them, and how this can enhance their meaning and utility. We describe how a travelling technology probe in the form of an augmented acoustic guitar engaged users in a design conversation as it visited homes, studios, gigs, workshops and lessons, and how this revealed the diversity and utility of its digital record. We describe how this record was captured and flexibly mapped to the physical guitar and proxy artefacts. We contribute a conceptual framework for accountable artefacts that articulates how multiple and complex mappings between physical artefacts and their digital records may be created, appropriated, shared and interrogated to deliver accounts of provenance and use as well as methodological reflections on technology probes.",archiving; augmented reality; digital record; guitar; internet of things; music; physical artefact; provenance; tangible and embedded interaction; technology probe,Keywords,
ACM DL,conferencePaper,2016,Novel Optical Configurations for Virtual Reality: Evaluating User Preference and Performance with Focus-tunable and Monovision Near-eye Displays,CHI - Human Factors in Computing Systems,A*,"Emerging virtual reality (VR) displays must overcome the prevalent issue of visual discomfort to provide high-quality and immersive user experiences. In particular, the mismatch between vergence and accommodation cues inherent to most stereoscopic displays has been a long standing challenge. In this paper, we evaluate several adaptive display modes afforded by focus-tunable optics or actuated displays that have the promise to mitigate visual discomfort caused by the vergence-accommodation conflict, and improve performance in VR environments. We also explore monovision as an unconventional mode that allows each eye of an observer to accommodate to a different distance. While this technique is common practice in ophthalmology, we are the first to report its effectiveness for VR applications with a custom built set up. We demonstrate that monovision and other focus-tunable display modes can provide better user experiences and improve user performance in terms of reaction times and accuracy, particularly for nearby simulated distances in VR.",focus cues; user comfort; user performance; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2016,Augmenting the Field-of-View of Head-Mounted Displays with Sparse Peripheral Displays,CHI - Human Factors in Computing Systems,A*,"In this paper, we explore the concept of a sparse peripheral display, which augments the field-of-view of a head-mounted display with a lightweight, low-resolution, inexpensively produced array of LEDs surrounding the central high-resolution display. We show that sparse peripheral displays expand the available field-of-view up to 190º horizontal, nearly filling the human field-of-view. We prototyped two proof-of-concept implementations of sparse peripheral displays: a virtual reality headset, dubbed SparseLightVR, and an augmented reality headset, called SparseLightAR. Using SparseLightVR, we conducted a user study to evaluate the utility of our implementation, and a second user study to assess different visualization schemes in the periphery and their effect on simulator sickness. Our findings show that sparse peripheral displays are useful in conveying peripheral information and improving situational awareness, are generally preferred, and can help reduce motion sickness in nausea-susceptible people.",augmented reality; peripheral vision; sparse peripheral display; virtual reality; wide field-of-view,Abstract_Keywords,
ACM DL,conferencePaper,2016,SnapToReality: Aligning Augmented Reality to the Real World,CHI - Human Factors in Computing Systems,A*,"Augmented Reality (AR) applications may require the precise alignment of virtual objects to the real world. We propose automatic alignment of virtual objects to physical constraints calculated from the real world in real time (""snapping to reality""). We demonstrate SnapToReality alignment techniques that allow users to position, rotate, and scale virtual content to dynamic, real world scenes. Our proof-of-concept prototype extracts 3D edge and planar surface constraints. We furthermore discuss the unique design challenges of snapping in AR, including the user's limited field of view, noise in constraint extraction, issues with changing the view in AR, visualizing constraints, and more. We also report the results of a user study evaluating SnapToReality, confirming that aligning objects to the real world is significantly faster when assisted by snapping to dynamically extracted constraints. Perhaps more importantly, we also found that snapping in AR enables a fresh and expressive form of AR content creation.",3D user interaction; augmented reality; interaction techniques; snapping; user studies,Abstract_Keywords_Title,
ACM DL,conferencePaper,2016,Virtual Objects as Spatial Cues in Collaborative Mixed Reality Environments: How They Shape Communication Behavior and User Task Load,CHI - Human Factors in Computing Systems,A*,"In collaborative activities, collaborators can use physical objects in their shared environment as spatial cues to guide each other's attention. Collaborative mixed reality environments (MREs) include both, physical and digital objects. To study how virtual objects influence collaboration and whether they are used as spatial cues, we conducted a controlled lab experiment with 16 dyads. Results of our study show that collaborators favored the digital objects as spatial cues over the physical environment and the physical objects: Collaborators used significantly less deictic gestures in favor of more disambiguous verbal references and a decreased subjective workload when virtual objects were present. This suggests adding additional virtual objects as spatial cues to MREs to improve user experience during collaborative mixed reality tasks.",collaboration; mixed reality; virtual spatial cues,Abstract_Keywords_Title,
ACM DL,conferencePaper,2016,VR-STEP: Walking-in-Place using Inertial Sensing for Hands Free Navigation in Mobile VR Environments,CHI - Human Factors in Computing Systems,A*,"Low-cost smartphone adapters can bring virtual reality to the masses, but input is typically limited to using head tracking, which makes it difficult to perform complex tasks like navigation. Walking-in-place (WIP) offers a natural and immersive form of virtual locomotion that can reduce simulation sickness. WIP, however, is difficult to implement in mobile contexts as it typically relies on bulky controllers or an external camera. We present VR-STEP; a WIP implementation that uses real-time pedometry to implement virtual locomotion. VR-STEP requires no additional instrumentation outside of a smartphone's inertial sensors. A user study with 18 users compares VR-STEP with a commonly used auto-walk navigation method and finds no significant difference in performance or reliability, though VR-STEP was found to be more immersive and intuitive.",games; head-mounted display; motion-sickness; pedometry; virtual locomotion; virtual reality; walking-in-place,Abstract_Keywords,
ACM DL,conferencePaper,2016,Finexus: Tracking Precise Motions of Multiple Fingertips Using Magnetic Sensing,CHI - Human Factors in Computing Systems,A*,"With the resurgence of head-mounted displays for virtual reality, users need new input devices that can accurately track their hands and fingers in motion. We introduce Finexus, a multipoint tracking system using magnetic field sensing. By instrumenting the fingertips with electromagnets, the system can track fine fingertip movements in real time using only four magnetic sensors. To keep the system robust to noise, we operate each electromagnet at a different frequency and leverage bandpass filters to distinguish signals attributed to individual sensing points. We develop a novel algorithm to efficiently calculate the 3D positions of multiple electromagnets from corresponding field strengths. In our evaluation, we report an average accuracy of 1.33 mm, as compared to results from an optical tracker. Our real-time implementation shows Finexus is applicable to a wide variety of human input tasks, such as writing in the air.",3D space; electromagnet; fingertips; localization; magnetic field; real-time; tracking; wearable,Abstract,
ACM DL,conferencePaper,2016,Head Mounted Projection Display &amp; Visual Attention: Visual Attentional Processing of Head Referenced Static and Dynamic Displays while in Motion and Standing,CHI - Human Factors in Computing Systems,A*,"The Head Mounted Projection Display (HMPD) is a growing interest area in HCI. Although various aspects of HMPDs have been investigated, there is not enough information regarding the effect of HMPDs (i.e., head referenced static and dynamic displays while a user is in motion and standing) on visual attentional performance. For this purpose, we conducted a user study (N=18) with three experimental conditions (control, standing, walking) and two visual perceptual tasks (with dynamic and static displays). Significant differences between conditions were only found for the task with dynamic display; accuracy was lower in walking condition compared to the other two conditions. Our work contributes an empirical investigation of the effect of HMPDs on visual attentional performance by providing data-driven benchmarks for developing graphical user interface design guidelines for HMPDs.",body mounted projection display; ergonomics; graphical user interface; GUI; head mounted display; head mounted projection display; HMD; HMPD; human factors; mixed reality; mobile projector; pico-projector; visual attention; wearable,Keywords,
ACM DL,conferencePaper,2016,Stabilized Annotations for Mobile Remote Assistance,CHI - Human Factors in Computing Systems,A*,"Recent mobile technology has provided new opportunities for creating remote assistance systems. However, mobile support systems present a particular challenge: both the camera and display are held by the user, leading to shaky video. When pointing or drawing annotations, this means that the desired target often moves, causing the gesture to lose its intended meaning. To address this problem, we investigate annotation stabilization techniques, which allow annotations to stick to their intended location. We studied two annotation systems, using three different forms of annotations, with both tablets and head-mounted displays. Our analysis suggests that stabilized annotations and head-mounted displays are only beneficial in certain situations. However, the simplest approach of automatically freezing video while drawing annotations was surprisingly effective in facilitating the completion of remote assistance tasks.",annotation systems; augmented reality; head-mounted displays; mobile video conferencing; remote assistance,Keywords,
ACM DL,conferencePaper,2016,Annexing Reality: Enabling Opportunistic Use of Everyday Objects as Tangible Proxies in Augmented Reality,CHI - Human Factors in Computing Systems,A*,"Advances in display and tracking technologies hold the promise of increasingly immersive augmented-reality experiences. Unfortunately, the on-demand generation of haptic experiences is lagging behind these advances in other feedback channels. We present Annexing Reality; a system that opportunistically annexes physical objects from a user's current physical environment to provide the best-available haptic sensation for virtual objects. It allows content creators to a priori specify haptic experiences that adapt to the user's current setting. The system continuously scans user's surrounding, selects physical objects that are similar to given virtual objects, and overlays the virtual models on to selected physical ones reducing the visual-haptic mismatch. We describe the developer's experience with the Annexing Reality system and the techniques utilized in realizing it. We also present results of a developer study that validates the usability and utility of our method of defining haptic experiences.",augmented reality; augmented reality content authoring; opportunistic tangible interfaces,Keywords_Title,
ACM DL,conferencePaper,2016,Haptic Retargeting: Dynamic Repurposing of Passive Haptics for Enhanced Virtual Reality Experiences,CHI - Human Factors in Computing Systems,A*,"Manipulating a virtual object with appropriate passive haptic cues provides a satisfying sense of presence in virtual reality. However, scaling such experiences to support multiple virtual objects is a challenge as each one needs to be accompanied with a precisely-located haptic proxy object. We propose a solution that overcomes this limitation by hacking human perception. We have created a framework for repurposing passive haptics, called haptic retargeting, that leverages the dominance of vision when our senses conflict. With haptic retargeting, a single physical prop can provide passive haptics for multiple virtual objects. We introduce three approaches for dynamically aligning physical and virtual objects: world manipulation, body manipulation and a hybrid technique which combines both world and body manipulation. Our study results indicate that all our haptic retargeting techniques improve the sense of presence when compared to typical wand-based 3D control of virtual objects. Furthermore, our hybrid haptic retargeting achieved the highest satisfaction and presence scores while limiting the visible side-effects during interaction.",haptics; perception; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2016,Dexmo: An Inexpensive and Lightweight Mechanical Exoskeleton for Motion Capture and Force Feedback in VR,CHI - Human Factors in Computing Systems,A*,"We present Dexmo: an inexpensive and lightweight mechanical exoskeleton system for motion capturing and force feedback in virtual reality applications. Dexmo combines multiple types of sensors, actuation units and link rod structures to provide users with a pleasant virtual reality experience. The device tracks the user's motion and uniquely provides passive force feedback. In combination with a 3D graphics rendered environment, Dexmo provides the user with a realistic sensation of interaction when a user is for example grasping an object. An initial evaluation with 20 participants demonstrate that the device is working reliably and that the addition of force feedback resulted in a significant reduction in error rate. Informal comments by the participants were overwhelmingly positive.",exoskeleton; force feedback; motion capture; virtual reality,Abstract_Keywords,
ACM DL,conferencePaper,2016,SwiVRChair: A Motorized Swivel Chair to Nudge Users' Orientation for 360 Degree Storytelling in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We present SwiVRChair, a motorized swivel chair to nudge users' orientation in 360 degree storytelling scenarios. Since rotating a scene in virtual reality (VR) leads to simulator sickness, storytellers currently have no way of controlling users' attention. SwiVRChair allows creators of 360 degree VR movie content to be able to rotate or block users' movement to either show certain content or prevent users from seeing something. To enable this functionality, we modified a regular swivel chair using a 24V DC motor and an electromagnetic clutch. We developed two demo scenarios using both mechanisms (rotate and block) for the Samsung GearVR and conducted a user study (n=16) evaluating the presence, enjoyment and simulator sickness for participants using SwiVRChair compared to self control (Foot Control). Users rated the experience using SwiVRChair to be significantly more immersive and enjoyable whilst having a decrease in simulator sickness.",360 degree storytelling; 360 degree video; consumer virtual reality; swivrchair; virtual environments; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2016,Cross-Field Aerial Haptics: Rendering Haptic Feedback in Air with Light and Acoustic Fields,CHI - Human Factors in Computing Systems,A*,"We present a new method of rendering aerial haptic images that uses femtosecond-laser light fields and ultrasonic acoustic fields. In conventional research, a single physical quantity has been used to render aerial haptic images. In contrast, our method combines multiple fields (light and acoustic fields) at the same time. While these fields have no direct interference, combining them provides benefits such as multi-resolution haptic images and a synergistic effect on haptic perception. We conducted user studies with laser haptics and ultrasonic haptics separately and tested their superposition. The results showed that the acoustic field affects the tactile perception of the laser haptics. We explored augmented reality/virtual reality (AR/VR) applications such as providing haptic feedback of the combination of these two methods. We believe that the results of this study contribute to the exploration of laser haptic displays and expand the expression of aerial haptic displays based on other principles.",aerial interaction; femtosecond laser; focused ultrasound; haptic feedback; laser plasma,Abstract,
ACM DL,conferencePaper,2016,Mirror Mirror: An On-Body T-shirt Design System,CHI - Human Factors in Computing Systems,A*,"Virtual fitting rooms equipped with magic mirrors let people evaluate fashion items without actually putting them on. The mirrors superimpose virtual clothes on the user's reflection. We contribute the Mirror Mirror system, which not only supports mixing and matching of existing fashion items, but also lets users design new items in front of the mirror and export designs to fabric printers. While much of the related work deals with interactive cloth simulation on live user data, we focus on collaborative design activities and explore various ways of designing on the body with a mirror.",augmented reality; fashion; design interface; magic mirror,Keywords,
ACM DL,conferencePaper,2015,LApp: A Speech Loudness Application for People with Parkinson's on Google Glass,CHI - Human Factors in Computing Systems,A*,Reduced vocal volume in Parkinson's is extremely common and can have significant social and emotional impact. We describe the development and evaluation of LApp–an application for Google Glass to help people with Parkinson's (PwP) monitor their speech volume and cue themselves to speak louder when necessary. Our findings highlight enthusiasm for using the application both at home as a volume training tool and in public social settings as a situated cueing device. We contribute insights to the literature on how eyewear technologies can provide assistance to people with health conditions and offer insights for the design of future self-monitoring and management applications on Google Glass.,augmented reality; google glass; parkinson's disease; self- monitoring; wearable technology,Keywords,
ACM DL,conferencePaper,2015,The Virtual Meditative Walk: Virtual Reality Therapy for Chronic Pain Management,CHI - Human Factors in Computing Systems,A*,"Because the nature of chronic pain is complex, pharmacological analgesics are often not enough to achieve an ideal treatment plan. Virtual Reality (VR) technologies have emerged within medical research in recent years for treating acute pain, and proved to be an effective strategy based on pain distraction. This paper describes a VR system designed for chronic pain patients. The system incorporates biofeedback sensors, an immersive virtual environment, and stereoscopic sound titled the ""Virtual Meditative Walk"" (VMW). It was designed to enable chronic pain patients to learn Mindfulness-based stress reduction (MBSR), a form of meditation. By providing real-time visual and sonic feedback, VMW enables patients to learn how to manage their pain. A proof-of-concept user study was conducted to investigate the effectiveness of the VR system with chronic pain patients in clinical settings. Results show that the VMW was more effective in reducing perceived pain compared to the non-VR control condition.",biofeedback; chronic pain; mindfulness meditation.; virtual reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2015,Mapping out Work in a Mixed Reality Project Room,CHI - Human Factors in Computing Systems,A*,"We present results from a study examining how the physical layout of a project room and task affect the cognitive maps acquired of a connected virtual environment during mixed-presence collaboration. Results indicate that a combination of physical layout and task impacts cognitive maps of the virtual space. Participants did not form a strong model of how different physical work regions were situated relative to each other in the virtual world when the tasks performed in each region differed. Egocentric perspectives of multiple displays enforced by different furniture arrangements encouraged cognitive maps of the virtual world that reflected these perspectives, when the displays were used for the same task. These influences competed or coincided with document-based, audiovisual and interface cues, influencing collaboration. We consider the implications of our findings on WYSIWIS mappings between real and virtual for mixed-presence collaboration.",cognitive map; cross reality; display ecology; mixed presence; mixed reality; spatial cognition,Keywords_Title,
ACM DL,conferencePaper,2015,AudioScope: Smartphones as Directional Microphones in Mobile Audio Augmented Reality Systems,CHI - Human Factors in Computing Systems,A*,"Mobile audio augmented reality systems (MAARS) provide a new and engaging modality to present information or to create playful experiences. Using special filters, spatial audio rendering creates the impression that the sound of a virtual source emanates from a certain position in the physical space. So far, most of the implementations of such systems rely on head tracking to create a realistic effect, which requires additional hardware. Recent results indicate that the built-in sensors of a smartphone can be used as source for orientation measurement, reducing deployment to a simple app download. AudioScope presents an alternative interaction technique to create such an experience, using the metaphor of pointing a directional microphone at the environment. In an experiment with 20 users, we compared the time to locate a proximate audio source and the perceived presence in the virtual environment. Results show that there is no significant difference between head-orientation measurement and AudioScope regarding accuracy and perceived presence. This means that MAARS, such as audio guides for museums, do not require special hardware but can run on the visitor's smartphones with standard headphones.",audio augmented reality; mobile devices; navigation.; virtual audio spaces,Abstract_Keywords_Title,
ACM DL,conferencePaper,2015,Exploring Expressive Augmented Reality: The FingAR Puppet System for Social Pretend Play,CHI - Human Factors in Computing Systems,A*,"We present ""FingAR Puppet"", an Augmented Reality (AR) system enhancing social pretend play by young children. Unlike goal-oriented AR systems that augment reality with informative instructions, FingAR Puppet helps children associate expressive interpretations with immediate reality. Empirical results show that FingAR Puppet promotes reasoning about emotional states, communication and divergent thinking during social pretend play for children 4-6 years old. We suggest that this study opens an interesting space for future AR systems to support complex cognitive and social development in early childhood. We also identify broader implications from using theories of cognitive development to guide the design of tangible and augmented interactions.",children; augmented reality; tangible user interface; pretend play,Abstract_Keywords_Title,
ACM DL,conferencePaper,2015,Session details: HMDs in Augmented &amp; Virtual Reality,CHI - Human Factors in Computing Systems,A*,,,Title,
ACM DL,conferencePaper,2015,A Dose of Reality: Overcoming Usability Challenges in VR Head-Mounted Displays,CHI - Human Factors in Computing Systems,A*,"We identify usability challenges facing consumers adopting Virtual Reality (VR) head-mounted displays (HMDs) in a survey of 108 VR HMD users. Users reported significant issues in interacting with, and being aware of their real-world context when using a HMD. Building upon existing work on blending real and virtual environments, we performed three design studies to address these usability concerns. In a typing study, we show that augmenting VR with a view of reality significantly corrected the performance impairment of typing in VR. We then investigated how much reality should be incorporated and when, so as to preserve users' sense of presence in VR. For interaction with objects and peripherals, we found that selectively presenting reality as users engaged with it was optimal in terms of performance and users' sense of presence. Finally, we investigated how this selective, engagement-dependent approach could be applied in social environments, to support the user's awareness of the proximity and presence of others.",virtual reality; augmented virtuality; engagement,Abstract_Keywords,
ACM DL,conferencePaper,2015,Level-Ups: Motorized Stilts that Simulate Stair Steps in Virtual Reality,CHI - Human Factors in Computing Systems,A*,"We present ""Level-Ups"", computer-controlled stilts that allow virtual reality users to experience walking up and down steps. Each Level-Up unit is a self-contained device worn like a boot. Its main functional element is a vertical actuation mechanism mounted to the bottom of the boot that extends vertically. Unlike traditional solutions that are integrated with locomotion devices, Level-Ups allow users to walk around freely (""real-walking""). We present Level-Ups in a demo environment based on a head-mounted display, optical motion capture, and integrated with two different game engines. In a user study, participants rated the realism of stepping onto objects 6.0 out of 7.0 when wearing Level-Ups compared to 3.5 without.",virtual reality; head-mounted display; real-walking,Abstract_Keywords_Title,
ACM DL,conferencePaper,2015,The Smartphone Project: An Augmented Dance Performance,CHI - Human Factors in Computing Systems,A*,"The Smartphone Project (TSP) is an interactive dance-performance in a professional setting that exploits the communication channels provided by smartphone-apps as a new material in the dance-theatre domain. We present an account of the experience and its staging. Based on an initial study with 36 participants from the audience, we present results and discuss lessons learned from this project that might guide similar future work.",mixed reality; augmented reality; smartphone; audience interaction; theatre; art; performance; material; dance,Keywords,
ACM DL,conferencePaper,2015,I'd Hide You: Performing Live Broadcasting in Public,CHI - Human Factors in Computing Systems,A*,"We present a study of a mixed reality game called 'I'd Hide You' that involves live video streaming from the city streets. We chart the significant challenges facing performers on the streets who must simultaneously engage in the game, stream compelling video footage featuring themselves, and interact with a remote online audience. We reveal how these street performers manage four key tensions: between their body and camera; between the demands of online audiences and what takes place on-the-street; between what appears 'frontstage' on camera versus what happens 'backstage'; and balancing being a player of the game with being a performer. By reflecting on how they achieve this, we are able to draw out wider lessons for future interfaces aimed at supporting people broadcasting video of themselves to online audiences while engaged in games, sports and other demanding real-world activities.",video; public settings; camerawork; live broadcasting,Abstract,
ACM DL,conferencePaper,2015,Cyclops: Wearable and Single-Piece Full-Body Gesture Input Devices,CHI - Human Factors in Computing Systems,A*,"This paper presents Cyclops, a single-piece wearable device that sees its user's whole body postures through an ego-centric view of the user that is obtained through a fisheye lens at the center of the user's body, allowing it to see only the user's limbs and interpret body postures effectively. Unlike currently available body gesture input systems that depend on external cameras or distributed motion sensors across the user's body, Cyclops is a single-piece wearable device that is worn as a pendant or a badge. The main idea proposed in this paper is the observation of limbs from a central location of the body. Owing to the ego-centric view, Cyclops turns posture recognition into a highly controllable computer vision problem. This paper demonstrates a proof-of-concept device, and an algorithm for recognizing static and moving bodily gestures based on motion history images (MHI) and a random decision forest (RDF). Four example applications of interactive bodily workout, a mobile racing game that involves hands and feet, a full-body virtual reality system, and interaction with a tangible toy are presented. The experiment on the bodily workout demonstrates that, from a database of 20 body workout gestures that were collected from 20 participants, Cyclops achieved a recognition rate of 79% using MHI and simple template matching, which increased to 92% with the more advanced machine learning approach of RDF.",ego-centric view; full-body gesture input; posture recognition; single-point wearable devices,Abstract,
ACM DL,conferencePaper,2015,Session details: Augmented &amp; Virtual Reality in the Real World,CHI - Human Factors in Computing Systems,A*,,,Title,
ACM DL,conferencePaper,2015,Substitutional Reality: Using the Physical Environment to Design Virtual Reality Experiences,CHI - Human Factors in Computing Systems,A*,"Experiencing Virtual Reality in domestic and other uncontrolled settings is challenging due to the presence of physical objects and furniture that are not usually defined in the Virtual Environment. To address this challenge, we explore the concept of Substitutional Reality in the context of Virtual Reality: a class of Virtual Environments where every physical object surrounding a user is paired, with some degree of discrepancy, to a virtual counterpart. We present a model of potential substitutions and validate it in two user studies. In the first study we investigated factors that affect participants' suspension of disbelief and ease of use. We systematically altered the virtual representation of a physical object and recorded responses from 20 participants. The second study investigated users' levels of engagement as the physical proxy for a virtual object varied. From the results, we derive a set of guidelines for the design of future Substitutional Reality experiences.",virtual reality; passive haptics; substitutional reality,Abstract_Keywords_Title,
ACM DL,conferencePaper,2015,The Semantic Paintbrush: Interactive 3D Mapping and Recognition in Large Outdoor Spaces,CHI - Human Factors in Computing Systems,A*,"We present an augmented reality system for large scale 3D reconstruction and recognition in outdoor scenes. Unlike existing prior work, which tries to reconstruct scenes using active depth cameras, we use a purely passive stereo setup, allowing for outdoor use and extended sensing range. Our system not only produces a map of the 3D environment in real-time, it also allows the user to draw (or 'paint') with a laser pointer directly onto the reconstruction to segment the model into objects. Given these examples our system then learns to segment other parts of the 3D map during online acquisition. Unlike typical object recognition systems, ours therefore very much places the user 'in the loop' to segment particular objects of interest, rather than learning from predefined databases. The laser pointer additionally helps to 'clean up' the stereo reconstruction and final 3D map, interactively. Using our system, within minutes, a user can capture a full 3D map, segment it into objects of interest, and refine parts of the model during capture. We provide full technical details of our system to aid replication, as well as quantitative evaluation of system components. We demonstrate the possibility of using our system for helping the visually impaired navigate through spaces. Beyond this use, our system can be used for playing large-scale augmented reality games, shared online to augment streetview data, and used for more detailed car and person navigation.",augmented reality; visually impaired; 3d reconstruction; laser pointer interaction; semantic segmentation; stereo,Abstract_Keywords,
ACM DL,conferencePaper,2015,Retargeting Technical Documentation to Augmented Reality,CHI - Human Factors in Computing Systems,A*,"We present a system which automatically transfers printed technical documentation, such as handbooks, to three-dimensional Augmented Reality. Our system identifies the most frequent forms of instructions found in printed documentation, such as image sequences, explosion diagrams, textual annotations and arrows indicating motion. The analysis of the printed documentation works automatically, with minimal user input. The system only requires the documentation itself and a CAD model or 3D scan of the object described in the documentation. The output is a fully interactive Augmented Reality application, presenting the information from the printed documentation in 3D, registered to the real object.",virtual reality; augmented reality; retargeting,Abstract_Keywords_Title,
ACM DL,conferencePaper,2015,MultiFi: Multi Fidelity Interaction with Displays On and Around the Body,CHI - Human Factors in Computing Systems,A*,"Display devices on and around the body such as smartwatches, head-mounted displays or tablets enable users to interact on the go. However, diverging input and output fidelities of these devices can lead to interaction seams that can inhibit efficient mobile interaction, when users employ multiple devices at once. We present MultiFi, an interactive system that combines the strengths of multiple displays and overcomes the seams of mobile interaction with widgets distributed over multiple devices. A comparative user study indicates that combined head-mounted display and smartwatch interfaces can outperform interaction with single wearable devices.",augmented reality; wearables; mobile multi display environments,Keywords,
ACM DL,conferencePaper,2015,Physio@Home: Exploring Visual Guidance and Feedback Techniques for Physiotherapy Exercises,CHI - Human Factors in Computing Systems,A*,"Physiotherapy patients exercising at home alone are at risk of re-injury since they do not have corrective guidance from a therapist. To explore solutions to this problem, we designed Physio@Home, a prototype that guides people through prerecorded physiotherapy exercises using real-time visual guides and multi-camera views. Our design addresses several aspects of corrective guidance, including: plane and range of movement, joint positions and angles, and extent of movement. We evaluated our design, com-paring how closely people could follow exercise movements under various feedback conditions. Participants were most accurate when using our visual guide and multi-views. We provide suggestions for exercise guidance systems drawn from qualitative findings on visual feedback complexity.",visualization; augmented reality; physiotherapy; movement guidance,Keywords,
ACM DL,conferencePaper,2015,G-raff: An Elevating Tangible Block for Spatial Tabletop Interaction,CHI - Human Factors in Computing Systems,A*,"We present an elevating tangible block, G-raff that supports spatial interaction in a tabletop computing environment. The elevating head part of G-raff moves according to the given height and angle data. We adopted two rollable metal tape structures to create large movements with a small volume block. On the head part, a smartphone can be mounted either horizontally or vertically. G-raff becomes a device that can connect a mobile device with a tabletop computer, thus a new spatial representation and control interaction is made. This paper introduces the design details, key features and applications of G-raff. We report on the results of a preliminary user study and discuss future work to improve the elevating device. This work contributes to maximizing the availability of augmented reality space above the tabletop display.",spatial interaction; actuated display; interface devices; tangible interface,Abstract,
ACM DL,conferencePaper,2014,The use of surrounding visual context in handheld AR: device vs. user perspective rendering,CHI - Human Factors in Computing Systems,A*,"The magic lens paradigm, a commonly used descriptor for handheld Augmented Reality (AR), presents the user with dual views: the augmented view (magic lens) that appears on the device, and the real view of the surroundings (what the user can see around the perimeter of the device). The augmented view is typically implemented by rendering the video captured by the rear-facing camera directly onto the device's screen. This results in dual perspectives - the real world being captured from the device's perspective rather than the user's perspective (what an observer would see looking through a transparent glass pane). These differences manifest themselves in misaligned and/or incorrectly scaled transparency resulting in the dual-view problem.This paper presents two user studies comparing (a) device-perspective and (b) fixed Point-of-View (POV) user-perspective magic lenses to analyze the effect of the dual-view problem on the use of the surrounding visual context. The results confirm that the dual-view problem, a result of dual perspective, has a significant effect on the use of information from the surrounding visual context. The study also highlights that magnification and not the dual-view problem is the key factor explaining the correlation between magic lens size and the increased intensity of the magic lens type effect. From the results, we derive design guidelines for future magic lenses.",ar; dual views; dual-view; magic lens; user-perspective,Abstract,
ACM DL,conferencePaper,2014,Evaluation of hear-through sound localization,CHI - Human Factors in Computing Systems,A*,"Listening and interacting with audio commonly relies on using earphones which limit the ability of users to perceive their auditory environment. Earphone sets that integrate miniature microphones on their exterior can, however, be used to hear-through the auditory environment. We present an evaluation study in which sound localization when wearing such a hear-through system is compared to normal earphones, open headphones and unblocked ears. Although localization performance is improved compared to open headphones, we find that it is compromised in comparison to listening without earphones because confusions of sound direction increase and localization judgment distributions are more dispersed and show a weaker correlation to the test directions. The implications of the results to human computer interaction and possible improvements to hear-through system design are discussed.",auditory augmented reality; hear-through systems,Keywords,
ACM DL,conferencePaper,2014,Simplifying orientation measurement for mobile audio augmented reality applications,CHI - Human Factors in Computing Systems,A*,"Audio augmented reality systems overlay the physical world with a virtual audio space. Today's smartphones provide enough processing power to create the impression of virtual sound sources being located in the real world. To achieve this, information about the user's location and orientation is necessary which requires additional hardware. In a real-world installation, however, we observed that instead of turning their head to localize sounds, users tend to turn their entire body. Therefore, we suggest to simply measure orientation of the user's body - or even just the mobile device she is holding - to generate the spatial audio.To verify this approach, we present two studies: Our first study in examines the user's head, body, and mobile device orientation when moving through an audio augmented reality system in a lab setting. Our second study analyzes the user experience in a real-world installation when using head, body, or device orientation to control the audio spatialization. We found that when navigating close to sound sources head tracking is necessary, but that it can potentially be replaced by device tracking in larger or more explorative usage scenarios. These findings help reduce the technical complexity of mobile audio augmented reality systems (MAARS), and enable their wider dissemination as mobile software-only apps.",audio augmented reality; binaural rendering; mobile devices; orientation; presence; spatial audio,Abstract_Keywords_Title,
ACM DL,conferencePaper,2014,Comparing flat and spherical displays in a trust scenario in avatar-mediated interaction,CHI - Human Factors in Computing Systems,A*,"We report on two experiments that investigate the influence of display type and viewing angle on how people place their trust during avatar-mediated interaction. By monitoring advice seeking behavior, our first experiment demonstrates that if participants observe an avatar at an oblique viewing angle on a flat display, they are less able to discriminate between expert and non-expert advice than if they observe the avatar face-on. We then introduce a novel spherical display and a ray-traced rendering technique that can display an avatar that can be seen correctly from any viewing direction. We expect that a spherical display has advantages over a flat display because it better supports non-verbal cues, particularly gaze direction, since it presents a clear and undistorted viewing aspect at all angles. Our second experiment compares the spherical display to a flat display. Whilst participants can discriminate expert advice regardless of display, a negative bias towards the flat screen emerges at oblique viewing angles. This result emphasizes the ability of the spherical display to be viewed qualitatively similarly from all angles. Together the experiments demonstrate how trust can be altered depending on how one views the avatar.",avatars; mixed reality; spherical displays; telecommunication; trust,Keywords,
ACM DL,conferencePaper,2014,In situ with bystanders of augmented reality glasses: perspectives on recording and privacy-mediating technologies,CHI - Human Factors in Computing Systems,A*,"Augmented reality (AR) devices are poised to enter the market. It is unclear how the properties of these devices will affect individuals' privacy. In this study, we investigate the privacy perspectives of individuals when they are bystanders around AR devices. We conducted 12 field sessions in cafés and interviewed 31 bystanders regarding their reactions to a co-located AR device. Participants were predominantly split between having indifferent and negative reactions to the device. Participants who expressed that AR devices change the bystander experience attributed this difference to subtleness, ease of recording, and the technology's lack of prevalence. Additionally, participants surfaced a variety of factors that make recording more or less acceptable, including what they are doing when the recording is being taken. Participants expressed interest in being asked permission before being recorded and in recording-blocking devices. We use the interview results to guide an exploration of design directions for privacy-mediating technologies.",augmented reality; privacy; surveillance; wearable camera,Abstract_Keywords_Title,
ACM DL,conferencePaper,2014,Experimental evaluation of user interfaces for visual indoor navigation,CHI - Human Factors in Computing Systems,A*,"Mobile location recognition by capturing images of the environment (visual localization) is a promising technique for indoor navigation in arbitrary surroundings. However, it has barely been investigated so far how the user interface (UI) can cope with the challenges of the vision-based localization technique, such as varying quality of the query images. We implemented a novel UI for visual localization, consisting of Virtual Reality (VR) and Augmented Reality (AR) views that actively communicate and ensure localization accuracy. If necessary, the system encourages the user to point the smartphone at distinctive regions to improve localization quality. We evaluated the UI in a experimental navigation task with a prototype, informed by initial evaluation results using design mockups. We found that VR can contribute to efficient and effective indoor navigation even at unreliable location and orientation accuracy. We discuss identified challenges and share lessons learned as recommendations for future work.",augmented reality; indoor navigation; mobile interaction; virtual reality; visual localization,Abstract_Keywords,
ACM DL,conferencePaper,2014,MixFab: a mixed-reality environment for personal fabrication,CHI - Human Factors in Computing Systems,A*,"Personal fabrication machines, such as 3D printers and laser cutters, are becoming increasingly ubiquitous. However, designing objects for fabrication still requires 3D modeling skills, thereby rendering such technologies inaccessible to a wide user-group. In this paper, we introduce MixFab, a mixed-reality environment for personal fabrication that lowers the barrier for users to engage in personal fabrication. Users design objects in an immersive augmented reality environment, interact with virtual objects in a direct gestural manner and can introduce existing physical objects effortlessly into their designs. We describe the design and implementation of MixFab, a user-defined gesture study that informed this design, show artifacts designed with the system and describe a user study evaluating the system's prototype.",3d modeling; 3d printing; direct manipulation; mixed-reality; personal fabrication,Abstract,
ACM DL,conferencePaper,2014,Pervasive information through constant personal projection: the ambient mobile pervasive display (AMP-D),CHI - Human Factors in Computing Systems,A*,"The vision of pervasive ambient information displays which show relevant information has not yet come true. One of the main reasons is the limited number of available displays in the environment which is a fundamental requirement of the original vision. We introduce the concept of an Ambient Mobile Pervasive Display AMP-D which is a wearable projector system that constantly projects an ambient information display in front of the user. The floor display provides serendipitous access to public and personal information. The display is combined with a projected display on the user's hand, forming a continuous interaction space that is controlled by hand gestures. The paper introduces this novel device concept, discusses its interaction design, and explores its advantages through various implemented application examples. Furthermore, we present the AMP-D prototype which illustrates the involved challenges concerning hardware, sensing, and visualization.",ambient displays; augmented reality; personal projection; pervasive displays,Keywords,
ACM DL,conferencePaper,2013,Augmented endurance: controlling fatigue while handling objects by affecting weight perception using augmented reality,CHI - Human Factors in Computing Systems,A*,"The main contribution of this paper is to develop a method for alleviating fatigue during handling medium-weight objects and augmenting our endurance by affecting our weight perception with augmented reality technology. To assist people to lift medium-weight objects without a complex structure or various costs, we focus on the phenomenon that our weight perception during handling objects is affected by visual properties. Our hypothesis is that this illusionary effect in weight perception can be applied to reduce fatigue while handling medium-weight objects without mechatronics-based physical assistance. In this paper, we propose an augmented reality system that changes the brightness value of an object in order to reduce fatigue while handling the object. We conducted two fundamental experiments to investigate the effectiveness of the proposed system. Our results suggested that the system eliminates the need to use excess energy for handling objects and reduces fatigue during the handling task.",augmented reality; weight perception; assistance for physical work; cross-modal interaction,Abstract_Keywords_Title,
ACM DL,conferencePaper,2013,BeThere: 3D mobile collaboration with spatial input,CHI - Human Factors in Computing Systems,A*,"We present BeThere, a proof-of-concept system designed to explore 3D input for mobile collaborative interactions. With BeThere, we explore 3D gestures and spatial input which allow remote users to perform a variety of virtual interactions in a local user's physical environment. Our system is completely self-contained and uses depth sensors to track the location of a user's fingers as well as to capture the 3D shape of objects in front of the sensor. We illustrate the unique capabilities of our system through a series of interactions that allow users to control and manipulate 3D virtual content. We also provide qualitative feedback from a preliminary user study which confirmed that users can complete a shared collaborative task using our system.",augmented reality; collaboration; around device interaction; depth sensors,Keywords,
ACM DL,conferencePaper,2013,SpaceTop: integrating 2D and spatial 3D interactions in a see-through desktop environment,CHI - Human Factors in Computing Systems,A*,"SpaceTop is a concept that fuses spatial 2D and 3D interactions in a single workspace. It extends the traditional desktop interface with interaction technology and visualization techniques that enable seamless transitions between 2D and 3D manipulations. SpaceTop allows users to type, click, draw in 2D, and directly manipulate interface elements that float in the 3D space above the keyboard. It makes it possible to easily switch from one modality to another, or to simultaneously use two modalities with different hands. We introduce hardware and software configurations for co-locating these various interaction modalities in a unified workspace using depth cameras and a transparent display. We describe new interaction and visualization techniques that allow users to interact with 2D elements floating in 3D space. We present the results from a preliminary user study that indicates the benefit of such hybrid workspaces.",augmented reality; 3d ui; desktop management,Keywords,
ACM DL,conferencePaper,2013,Access lens: a gesture-based screen reader for real-world documents,CHI - Human Factors in Computing Systems,A*,"Gesture-based touch screen user interfaces, when designed to be accessible to blind users, can be an effective mode of interaction for those users. However, current accessible touch screen interaction techniques suffer from one serious limitation: they are only usable on devices that have been explicitly designed to support them. Access Lens is a new interaction method that uses computer vision-based gesture tracking to enable blind people to use accessible gestures on paper documents and other physical objects, such as product packages, device screens, and home appliances. This paper describes the development of Access Lens hardware and software, the iterative design of Access Lens in collaboration with blind computer users, and opportunities for future development.",augmented reality; accessibility; computer vision; gestures; blindness,Keywords,
ACM DL,conferencePaper,2013,IllumiRoom: peripheral projected illusions for interactive experiences,CHI - Human Factors in Computing Systems,A*,"IllumiRoom is a proof-of-concept system that augments the area surrounding a television with projected visualizations to enhance traditional gaming experiences. We investigate how projected visualizations in the periphery can negate, include, or augment the existing physical environment and complement the content displayed on the television screen. Peripheral projected illusions can change the appearance of the room, induce apparent motion, extend the field of view, and enable entirely new physical gaming experiences. Our system is entirely self-calibrating and is designed to work in any room. We present a detailed exploration of the design space of peripheral projected illusions and we demonstrate ways to trigger and drive such illusions from gaming content. We also contribute specific feedback from two groups of target users (10 gamers and 15 game designers); providing insights for enhancing game experiences through peripheral projected illusions.",immersion; augmented reality; gaming; spatial augmented reality; projection mapping; apparent motion,Keywords,
ACM DL,conferencePaper,2013,WorldKit: rapid and easy creation of ad-hoc interactive applications on everyday surfaces,CHI - Human Factors in Computing Systems,A*,"Instant access to computing, when and where we need it, has long been one of the aims of research areas such as ubiquitous computing. In this paper, we describe the WorldKit system, which makes use of a paired depth camera and projector to make ordinary surfaces instantly interactive. Using this system, touch-based interactivity can, without prior calibration, be placed on nearly any unmodified surface literally with a wave of the hand, as can other new forms of sensed interaction. From a user perspective, such interfaces are easy enough to instantiate that they could, if desired, be recreated or modified ""each time we sat down"" by ""painting"" them next to us. From the programmer's perspective, our system encapsulates these capabilities in a simple set of abstractions that make the creation of interfaces quick and easy. Further, it is extensible to new, custom interactors in a way that closely mimics conventional 2D graphical user interfaces, hiding much of the complexity of working in this new domain. We detail the hardware and software implementation of our system, and several example applications built using the library.",augmented reality; ubiquitous computing; sensors; interactive spaces; smart rooms; touch screens; surface computing; depth sensing cameras,Keywords,
ACM DL,conferencePaper,2013,Panoinserts: mobile spatial teleconferencing,CHI - Human Factors in Computing Systems,A*,"We present PanoInserts: a novel teleconferencing system that uses smartphone cameras to create a surround representation of meeting places. We take a static panoramic image of a location into which we insert live videos from smartphones. We use a combination of marker- and image-based tracking to position the video inserts within the panorama, and transmit this representation to a remote viewer. We conduct a user study comparing our system with fully-panoramic video and conventional webcam video conferencing for two spatial reasoning tasks. Results indicate that our system performs comparably with fully-panoramic video, and better than webcam video conferencing in tasks that require an accurate surrounding representation of the remote space. We discuss the representational properties and usability of varying video presentations, exploring how they are perceived and how they influence users when performing spatial reasoning tasks.",mixed reality; mobile phones; telepresence; remote collaboration; teleconferencing; camera tracking; panoramas,Keywords,
ACM DL,conferencePaper,2013,Sublimate: state-changing virtual and physical rendering to augment interaction with shape displays,CHI - Human Factors in Computing Systems,A*,"Recent research in 3D user interfaces pushes towards immersive graphics and actuated shape displays. Our work explores the hybrid of these directions, and we introduce sublimation and deposition, as metaphors for the transitions between physical and virtual states. We discuss how digital models, handles and controls can be interacted with as virtual 3D graphics or dynamic physical shapes, and how user interfaces can rapidly and fluidly switch between those representations. To explore this space, we developed two systems that integrate actuated shape displays and augmented reality (AR) for co-located physical shapes and 3D graphics. Our spatial optical see-through display provides a single user with head-tracked stereoscopic augmentation, whereas our handheld devices enable multi-user interaction through video seethrough AR. We describe interaction techniques and applications that explore 3D interaction for these new modalities. We conclude by discussing the results from a user study that show how freehand interaction with physical shape displays and co-located graphics can outperform wand-based interaction with virtual 3D graphics.",spatial augmented reality; shape display; actuated tangibles; 3d interaction,Abstract_Keywords,
ACM DL,conferencePaper,2013,SimMed: combining simulation and interactive tabletops for medical education,CHI - Human Factors in Computing Systems,A*,"A large body of work asserts that interactive tabletops are well suited for group work, and numerous studies have examined these devices in educational contexts. However, few of the described systems support simulations for collaborative learning, and none of them explicitly address immersion. We present SimMed, a system allowing medical students to collaboratively diagnose and treat a virtual patient using an interactive tabletop. The hybrid user interface combines elements of virtual reality with multitouch input. The paper delineates the development process of the system and rationale behind a range of interface design decisions. Thereby, the role of realism in gaining procedural knowledge is discussed - in particular, the interplay between realism, immersion and training goals. We implemented several medical test cases and evaluated our approach with a user study that suggests the great potential of the system. Results show a high level of immersion, cooperation and engagement by the students.",education; collaboration; learning; interactive surfaces; medicine; tabletop; multitouch; procedural knowledge,Abstract,
ACM DL,conferencePaper,2013,Reducing disruption from subtle information delivery during a conversation: mode and bandwidth investigation,CHI - Human Factors in Computing Systems,A*,"With proliferation of mobile devices that provide ubiquitous access to information, the question arises of how distracting processing information in social settings can be, especially during face-to-face conversations. However, relevant information presented at opportune moments may help enhance conversation quality. In this paper, we study how much information users can consume during a conversation and what information delivery mode, via audio or visual aids, helps them effectively conceal the fact that they are receiving information. We observe that users can internalize more information while still disguising this fact the best when information is delivered visually in batches (multiple pieces of information at a time) and perform better on both dimensions if information is delivered while they are not speaking. Interestingly, participants qualitatively did not prefer this mode as being the easiest to use, preferring modes that displayed one piece of information at a time.",design; augmented reality; attention; human factors,Keywords,
